,paper_id,summary,summary,summary,summary,precision,precision,precision,precision,precision,precision,recall,recall,recall,recall,recall,recall,fmeasure,fmeasure,fmeasure,fmeasure,fmeasure,fmeasure
,,0,1,2,3,0-1,0-2,0-3,1-2,1-3,2-3,0-1,0-2,0-3,1-2,1-3,2-3,0-1,0-2,0-3,1-2,1-3,2-3
0,SP:00130f3b3a6b3b71f9b487003a18b43517cacbbb,"This work proposes a new method for subgame solving in two-player zero-sum extensive-form games with imperfect information. Traditional subgame solvers rely on the so-called common-knowledge closure that guarantees the opponent can not exploit the strategy computed in the subgame in the whole game. The problem with the common-knowledge closure is that its size may be beyond the abilities of contemporary solvers in case the game does not reveal enough information observable by both players during the course of play. To enable solving even subgames with large common-knowledge closures of the information set in the root, the authors introduce order-k knowledge sets as sets of nodes in the game tree that are reachable from the base set in at most k-1hyperedges in the infoset hypergraph. The order-k knowledge sets reach the common-knowledge closure as k approaches infinity. The order-k knowledge sets may be much smaller than the common-knowledge sets, which results in more efficient subgame solving, but this comes at the expense of the exploitability of the computed strategy. As the authors show, the exploitability may increase by a factor linear in the size of the game. The authors hence propose two methods for mitigating it. The first method requires updating the blueprint strategy after every iteration of subgame solving, while the second method restricts the possible deviations from the blueprint strategy. Both methods guarantee that the exploitability of the new strategy will not exceed the exploitability of the blueprint. Using the order-1 knowledge-limited subgame solver as well as several other heuristics, the authors built an agent for playing a large extensive-form game called dark chess. In the experimental part of the manuscript, the authors first show that their method successfully decreases the exploitability of the blueprint strategy for seven medium-sized games. In the second part, they report results achieved with the agent playing dark chess. The agent can beat a baseline player and an amateur human player but loses against the current champion. ","This paper proposes a novel technique for search in imperfect information games. One problem with existing subgame solving techniques is that they construct a gadget game with a chance node that samples over all possible states in the subgame. This gadget game will be prohibitively large in large games. To overcome this problem, the authors create a smaller gadget game where only states corresponding to the k-th order common knowledge. While an unsafe method as-is, the authors introduce three fixes to make this technique safe. They show empirical results on a number of openspiel games and show that k-KLSS always improves exploitability. Their main result is impressive state-of-the-art performance on dark chess.   ","The authors develop a new approximation to for subgame solving that approximate common knowledge using lower-order knowledge. Using theoretical and empirical tests they show that these algorithms can reduce exploitability when the information sets are large. Finally, they use these ideas to develop a game playing agent for dark chess and test it against algorithms, a coauthor and the current top online player. ","This paper deals with two related problems. The more fundamental contribution is an analysis and partial algorithmic solution to the problem of subgame solving in a subclass of games with intractable large public states. The other is creation and evaluation of an AI for playing dark chess - a chess variant with imperfect information. The key solution to the first problem is to assume limited nesting of player’s beliefs about the opponent’s beliefs. The paper is quite thorough in exploring at least some theoretical guarantees of this approach, since finding counterexamples would not be hard. The empirical evaluation shows that the algorithm practically works much better than the theoretical guarantees suggest. For the second problem, the paper introduces an algorithm which shares the key idea with the solution to the first problem, but then adds many theoretically unsound domain-specific modifications. The paper shows that this algorithm performs well against other programs as well as moderately strong human players. ",0.11470588235294117,0.06764705882352941,0.10588235294117647,0.1271186440677966,0.15254237288135594,0.171875,0.3305084745762712,0.359375,0.225,0.234375,0.1125,0.06875,0.17030567685589518,0.11386138613861385,0.144,0.16483516483516483,0.12949640287769784,0.09821428571428573
1,SP:00215e91570b72ae8202535812037e710e766253,"The paper studies continual learning and that context the important and prominent problem of catastrophic forgetting. Similar to related work the paper achieves this through learning task-dependent masks on the neural network representation (effectively generating different sub-networks for different tasks).  The main contribution w.r.t. related work is that the masks are constrained to a sub-space which is spanned by a ""basis of masks"". The experiments show some reduction in the number of required parameters over a single baseline method.","The paper learns the binary basis mask for a few task sequence tasks, later linear combinations of the basis mask can be used as a mask for the new task. The parameter learned for combining the basis mask is significantly fewer than learning a novel mask for each new task sequence. The paper also proposes a homogeneous mask learning. For most of the datasets, we don't have the flexibility to learn an enormous basis mask and model performance highly depends on the number of learned basis tasks. The proposed model is evaluated over the MNIST, CIFAR100 and Split-ImageNet dataset over the smaller architecture. ",The paper describes an approach to continual learning by randomly initialized deep networks. The main idea is to extend the SupSup (Wortzman 2020) and apply linear combination of binary masks for efficiency. The paper also considers learning from multiple instances of the same task. The evaluation results indicate reasonable performance for the reduced parameters in common benchmarks.,"In the current paper, the authors propose a novel way to adapt an existing continual learning algorithm, leveraging principles from transfer learning.  More concretely, they learn an initial set of masks (or impressions) from a small number of basis tasks, and they use afterwards linear combinations of these masks in the learning process of new tasks. Therefore, their approach is able  to generalize to new tasks, allowing for scalable and parameter efficient continual learning (with much lower parameter overhead than existing methods). ",0.21428571428571427,0.17857142857142858,0.15476190476190477,0.14285714285714285,0.14285714285714285,0.22807017543859648,0.17142857142857143,0.2631578947368421,0.15853658536585366,0.2631578947368421,0.18292682926829268,0.15853658536585366,0.19047619047619047,0.2127659574468085,0.15662650602409636,0.18518518518518517,0.160427807486631,0.18705035971223025
2,SP:006a99a453b861691e5ea2c02012a2aef44d393e,"The paper considers the setting in which a single RL policy has to be learned for several tasks (or styles), to be selected at run-time after the agent has been trained on all the tasks. The core of the paper is the identification of a gap in the literature, where it has not been tried to train several critics and a single actor for multi-task RL, with each critic seeing only one task to be learned. The paper proposes to do just that, and shows promising experimental results in a variety of environments.","This paper proposes a single-actor, multi-critic approach to address the well-known problem of negative interference in multi-task RL (MTRL). The work focuses on a special case of MTRL described as “multi-style RL”, where the goal is to learn several distinct behaviours under the same environment dynamics. Experiments were conducted on a wide variety of environments: a path following domain, Pong, Sonic the Hedgehog, and a UFC fighting game. Results provide evidence that the proposed approach achieves a better final performance than a single-actor, single-critic MTRL baseline.","This paper proposes to extend the actor-critic frame to tackle multi-objective (multi-task) reinforcement learning. The key idea is to learn multiple critics that correspond to different reward functions. Then a single policy is optimized for a weighted combination of these critics. The paper applies the multi-objective RL to learning different styles of completing tasks, such as aggressive or defensive style in a boxing game. The paper shows that the proposed algorithm can beat several multi-task learning baselines.","The paper introduces a variant of actor-critic reinforcement learning algorithms where multiple critics are trained in a reduced version of multi-task training where a task allows multiple reward functions. Two variants are proposed for handling multiple reward functions: one where a backbone value function is trained with multiple heads and one where completely distinct value functions are trained. Evaluation considers three domains: tracing different shapes, Pong with aggressive-defensive styles, and Sonic the Hedgehog game levels (where levels are styles). Results are ambiguous as to the improvement of the architecture over classical methods. Some qualitative results are shown for a fighting game trained with the multi-critic approach.",0.18947368421052632,0.16842105263157894,0.15789473684210525,0.21505376344086022,0.21505376344086022,0.21951219512195122,0.1935483870967742,0.1951219512195122,0.13636363636363635,0.24390243902439024,0.18181818181818182,0.16363636363636364,0.1914893617021277,0.1807909604519774,0.14634146341463414,0.22857142857142856,0.19704433497536947,0.18749999999999997
3,SP:006e9fb3f4bd9fce1b751e6491f93ca9a918b1d0,"Summary. Prior works have used auxiliary tasks (such as pixel control, value prediction) to help with representation learning in RL. These auxiliary tasks can be represented in the form of a question network (or TD network) of a generalized value function (GVF). Rather than handcrafting these question networks (of generalized value function), the paper proposes to randomly generate these question networks and use them to help with representation learning in RL. It shows that these randomly generated question networks can lead to better learning than handcrafted auxiliary tasks. Furthermore, the paper shows that only using these randomly generated question networks still leads to a good representation. ",This work proposes random General Value Functions (rGVFs) as auxiliary tasks to extract useful representations for reinforcement learning. The target feature and the connections between the prediction nodes are randomized. The proposed method outperforms end-to-end A2C and other baselines with auxiliary training in 49 Atari games and 12 DeepMind Lab environments.,"Summary -------  Owing to the importance of state representation in RL, this paper provides an empirical investigation of random General Value Functions (GVFs) for shaping state representation in RL. The paper briefly outlines the random generation process for their GVFs before conducting extensive experiments on GridWorld, as well as ablation studies in Atari and DeepMind Lab environments. The authors find that sufficiently deep random GVFs are able to provide state representations for policy evaluation in a visual gridworld environment. Surprisingly, constructing the state representation as an auxiliary task alone is enough to outperform a baseline actor-critic on Atari and DeepMind lab.   ","This paper introduces a new auxiliary task for learning state representations. This is done by predicting general value functions (GVFs), which are predictions of random features of observations conditioned on sequences of actions. The GVFs of interest can be represented with graphs which they call ""question networks"". The edges in this graph define nodes predicting the value of other nodes in a previous time-step, and many existing prediction tasks in the literature such as reward prediction, multi-horizon value prediction, and termination prediction can be represented using a question network.  The key contribution here is that predicting the value of nodes in a randomly generated question can provide a strong learning signal for learning representations. Several experiments are provided that show that: (1) representations learned by predicting the values of large random question networks (rGVF) outperform baselines like pixel-control and MHVP; (2) rGVF was the only method that learned good representations even when using a stop-gradient to prevent the RL learning signal from shaping the representation; and (3) several ablation tests were done to show the importance of action conditioning and depth.",0.09433962264150944,0.1509433962264151,0.2358490566037736,0.3018867924528302,0.3018867924528302,0.19801980198019803,0.18867924528301888,0.15841584158415842,0.13513513513513514,0.15841584158415842,0.08648648648648649,0.10810810810810811,0.12578616352201255,0.15458937198067632,0.1718213058419244,0.2077922077922078,0.13445378151260504,0.13986013986013987
4,SP:008b937acb21afd5449982967b6daac37b4134ab,"This paper studies a relatively little-concerned problem, that is, the problem of class-prior estimation(CPE) in PU scenes. The authors concluded that the existing CPE methods are based on a critical assumption that the support of the positive data distribution cannot be contained in the support of the negative data distribution. However, in actual scenarios, this assumption may not be satisfied. It is also difficult to prove that a certain data set satisfies this assumption. Existing CPE methods will systematically overestimate the class prior when the data does not meet the critical assumption. To remove the assumption to make CPE always valid, a strategy called Regrouping CPE (ReCPE), which builds an auxiliary probability distribution, are proposed so that the support of the positive data distribution is never contained in the support of the negative data distribution. Theoretically, this method can give a more accurate estimate regardless of whether the dataset meets the assumption. The authors also proved the effectiveness of the method through a series of experiments.","This paper studies positive and unlabeled (PU) learning, which apparently is an important problem in machine learning. In many existing researches, the PU classifiers assumes that the irreducibility assumption holds on input data, which, though, may not always be true in many real-world applications. As a result, the learn classifier might not work well due to estimation bias on data prior. To address this problem, this paper proposes to construct a transformed the probability distributions of input data by a re-grouping operation, such as the positive prior cannot be a component (or called as a support) of the negative prior. Good experiment results have been obtained to support the proposed method.","This paper addresses the problem of class-prior estimation (CPE) in positive-unlabeled (PU) learning. The authors consider that the existing methods usually fail if the data distribution dissatisfies the irreducibility assumption. To address this problem, the authors introduce a method named Regroping CPE (ReCPE) which tries to transform the original data distribution to an auxiliary data distribution, such that the produced distribution always guaranteeing the irreducibility assumption.   A practice implementation is proposed by firstly training a binary classifier and then picking some samples whose outputs are mostly dissimilar to the negative examples as the pseudo positive samples. The proposed methods can be considered as a pre-processing method and used together with any CPE method. The experiments on synthetic data show the intuition of the proposed ReCPE and the results on real-world data show the effectiveness of the proposed method when it is combined with other CPE methods.","This paper studies the prior $\pi$ in PU learning. More specifically, it studies the estimation method of the prior $\pi$ in a more general case, i.e., without the irreducibility assumption. When unsatisfied, such an assumption will often lead to the overestimation of $\pi$. However, current class-prior estimation methods are usually based on the irreducibility assumption. The authors propose a new CPE problem based on a new auxiliary distribution that always satisfies the irreducibility requirement. The technique is called regrouping, which transforms the true distributions to  Both theoretical and experimental results are included in the paper, showing the validness of the proposed ReCPE method. ",0.1834319526627219,0.28402366863905326,0.20710059171597633,0.2831858407079646,0.20353982300884957,0.22666666666666666,0.2743362831858407,0.32,0.3333333333333333,0.21333333333333335,0.21904761904761905,0.3238095238095238,0.2198581560283688,0.3009404388714734,0.2554744525547445,0.24334600760456274,0.21100917431192662,0.26666666666666666
5,SP:012528acf4cee3e1b0a1a27a7fecb73bebb9e661,"This paper explores the possibility and practibility to build compact, accurate, and robust deep neural networks (CARDs). The authors first conduct extensive empirical studies to verify the existence of CARDs. Meanwhile, to better understand CARDs, authors further analyze the Fourier sensitivity in the spectral domain. Based on the empirical observations, a domain-adaptive ensembling approach (CARDs-Deck) is proposed as a practical algorithm to utilize the CARDs models, which is able to achieve new SOTA results for robustness and accuracy on CIFAR-10. The paper is quite informative and solid throughout the text, and the claims are well supported by empirical observations. There are also theoretical analysis that justifies CARDs in depth. ",The authors evaluate several pruning techniques and their impact on accuracy and robustness on CIFAR10. They perform a spectral analysis of Fourier domain errors and attempt to link those to OOD robustness. They propose an ensembling approach based on the spectral properties of unlabeled test images.,"This paper studies on whether pruning is able to improve the model robustness. This paper first show that with a proper pruning algorithm, the robustness can be achieved. Then a spectrum analysis is proposed to visualize what kind of heat map is suitable for robustness. Thirdly, a test-time augmentation approached is proposed to further improve the robustness.","This paper performs an empirical study on the OOD robustness of compact models and shows the existence of compact, accurate, and robust DNNs (CARDs). It gives a frequency-domain analysis with the Fourier sensitivity method to explain why different pruning methods lead to compact models with varying OOD robustness. Then, the paper proposes CARD-Deck, a domain-adaptive test-time ensembling approach, to dynamically choose an appropriate CARD for each test sample using a spectral-similarity metric and presents some experimental results with CARD-Decks. Finally, it provides some theoretical analysis to support the existence of CARD and the ability to find a CARD with CARD-Deck.",0.09821428571428571,0.13392857142857142,0.21428571428571427,0.17391304347826086,0.30434782608695654,0.25862068965517243,0.2391304347826087,0.25862068965517243,0.22429906542056074,0.13793103448275862,0.1308411214953271,0.14018691588785046,0.13924050632911392,0.1764705882352941,0.2191780821917808,0.15384615384615385,0.18300653594771238,0.18181818181818185
6,SP:0156823b32dd1145169aa325fbb74ea7eb97c030,"This paper proposes to extend the typical GP formulation as the limiting case of a NN with Gaussian weights when the number of hidden units tends to infinity. In particular, it considers a scale of mixture of Gaussians at the last layer for the prior. When this scale mixture of Gaussians is a Inv. Gamma distribution it is well known that the result is a Student-t process. The paper also gives some insights about the resulting process obtained by training the weights of the last layer and following a similar initialization. The resulting processes are heavy-tailed and more robust as shown in the experiments. ","Building on the recent results on the correspondence between infinite-width neural networks and Gaussian processes (NNGPs), this paper proposes and studies a simple extension, looking at a scale mixture of such processes. The key idea is to introduce a scale prior on the parameters of the last layer, allowing construction of a richer classes of stochastic processes, particularly those with heavy tails. Some convergence results for these general processes are obtained by applying the tensor program and the Master Theorem, under various settings. Empirical results are also provided, showing the promise of the approach and robustness to out-of-distribution data.","The paper extends upon the corpus of works that consider the infinite-width limit of a deep net. The foundation of the work is the Neural Network Gaussian Process (NNGP) model, which is taken as the infinite-width limit of a deep net under certain (mild) assumptions; that is the so-called Master Theorem.   The here proposed approach treats the variances, σ^2, of the penultimate layer weights as random variables. Then, it imposes an appropriate prior distribution on these and proceeds with a reiteration of the Master Theorem (and how this varies in this setting).   They show that, under this setting, the infinite-width limit of the network is a scale-mixture of NNGPs. In the special case of an imposed Gamma-prior, the so-obtained scale mixture reduces to a Student's-t process; this is a well-known result.  ","Although the connection between Bayesian neural networks and Gaussian processes has been a topic of interest for several years, the work on tensor programs facilitating the formulation of the Neural Network-Gaussian Process (NNGP) model has been influential in further emphasising the correspondence between the two classes of models. In this work, the authors propose an extension to the NNGP model that allows for the formulation of more general stochastic processes that may follow alternative distributions such as Student-$t$. This is achieved by way of introducing a prior on the scale of the parameters in the last layer of the neural network. This formulation allows for greater flexibility without requiring the more involved changes proposed in related works such as Favran et al. (2020) and Bracale et al. (2021). Following the analysis carried out in the initial formulation of NNGPs, the authors also investigate the correspondence between GPs and infinitely-wide BNNs configured with the proposed set-up and trained using gradient descent. The experimental evaluation features a mixture of regression and classification tasks, with a particular emphasis on how the use of models with heavier-tailed distributions can be better suited to datasets with challenging properties such as corrupted data and label imbalance.",0.20754716981132076,0.2358490566037736,0.24528301886792453,0.22549019607843138,0.30392156862745096,0.24113475177304963,0.21568627450980393,0.1773049645390071,0.12682926829268293,0.16312056737588654,0.15121951219512195,0.16585365853658537,0.21153846153846154,0.20242914979757085,0.1672025723472669,0.18930041152263377,0.20195439739413681,0.19653179190751446
7,SP:019f47787a1734e4215fc3c90019676885bf24bc,The paper proposed to learn a set of global and task-specific parameters to get task-specific feature maps for different tasks. And the proposed method leverages the Fisher information matrix to measure task similarities for parameter initialization for expansion-based continual learning. The proposed method is demonstrated on unconditional generation tasks and label-conditioned generation tasks.,"This paper deals with the training of GAN in a continual learning setting, i.e. different tasks are sequentially coming. The authors designed learnable task-specific adapters (transformations) for avoid the catastrophic forgetting problem. And task-similarity based initialization is adopted to improve the performance further. Experiment results show that the proposed method achieves better or comparable performance compared to previous state-of-the-arts, but with remarkably fewer parameters. ","The paper presents an approach for continual learning in GANs by learning new task-specific modules over a base network. This work explores the contributions of different continual learning approaches for generative models and proposes an expansion-based model which learns new task-specific modules for each new task. These modules augment the features of the base task layer. To prevent instability issues, the model also creates additional residual pathways. To choose the parameters for a new task, a task similarity-based initialization is proposed which initializes the new task network parameters with the learnt parameters of the previous task which is most similar to the current task. Experiments are performed on conditional and unconditional image tasks, and ablations are performed on the effectiveness of the proposed modules. ","This paper proposes a lightweight adapter architecture, CAM-GAN, for generative adversarial networks (GAN) and its training method. This adapter enables continual adaptation to new tasks through feature transformation. The proposed CAM-GAN shows the state-of-the-art performance in continual adaptation for various tasks while minimizing the increase in adapter parameters and computation.",0.19298245614035087,0.3333333333333333,0.19298245614035087,0.2714285714285714,0.17142857142857143,0.1015625,0.15714285714285714,0.1484375,0.2,0.1484375,0.21818181818181817,0.23636363636363636,0.1732283464566929,0.20540540540540542,0.19642857142857145,0.1919191919191919,0.192,0.14207650273224046
8,SP:01e42205f6557cb74e8353f107f246663b193a8c,"This paper proposes a self-supervised pre-training task: code deobfuscation (recovering erased identifier names). This pre-training task only requires unlabeled natural programs as training data. The authors claim that models pre-trained with code deobfuscation significantly outperform existing approaches on many downstream program understanding tasks, including clone detection, code summarization, NL code search, and unsupervised program translation.","This work follows up with earlier efforts to pre-train BERT-like architectures on code, like CuBERT and CodeBERT. Instead of using a language-agnostic masked language model pre-training objective, however, it uses a de-obfuscation objective: masking all occurrences of an identifier (variable, function, class), ask the model to predict it. This code-specific pre-training objective is combined with other recent options (e.g., a denoising autoencoder) as well as traditional mask-language modelling, to produce improved performance on a number of code tasks such as summarization, clone detection, and transpilation.","This paper produces a new pretraining objective for models that provide program embeddings via the use of a name deobfuscation objective. This objective has advantages over the standard MLM objective as it excludes obvious masking possibilities and also does not allow the model to copy the names of one token from part of the document to another part.  The authors construct this objective and demonstrate that their model can learn this task fairly effectively. Interestingly, initializing their objective with a model pretrained on MLM seems to aid the performance of their model. They also demonstrate improvement over some baselines on some standard program processing objectives, and improvement over all baselines using a combination of their objective and a separate program recovery objective. ","This paper proposes a pre-training method for programming languages. The main idea is pre-training a seq2seq model to convert obfuscated functions back to their original forms. The method demonstrates performance improvement on multiple downstream tasks, e.g., code translation and code search.",0.23728813559322035,0.23728813559322035,0.2542372881355932,0.20212765957446807,0.13829787234042554,0.12295081967213115,0.14893617021276595,0.11475409836065574,0.3409090909090909,0.1557377049180328,0.29545454545454547,0.3409090909090909,0.1830065359477124,0.15469613259668508,0.2912621359223301,0.17592592592592596,0.18840579710144928,0.18072289156626506
9,SP:0239965b691a4526c814b4543649f0831254ed88,"The paper proposes a simple method that propagates the uncertainty in the data through a neural network with ReLU non-linearity. The proposed method simply transforms the mean and covariance of the input Gaussian/Cauchy distribution through linear layers, and applies a local linear approximation for the ReLU activation. The proposed transformation is shown to be optimal under the total variation criterion. The paper applies the method to train neural networks with regression or classification targets, and show that the proposed method can well estimate the uncertainty of the network's output. Moreover, they also show some improvements in the robustness of the trained networks.","This paper studies the problem of propagating probability distributions through neural networks and applies the results to quantify prediction uncertainties. It proposes a local linearization method to approximate a distribution transformed by a ReLU network, as well as new loss function for learning with distribution-valued inputs. It also provides empirical results, showing that the method can quantify two kinds of uncertainty (aleatoric and epistemic) in classification and regression tasks, and training with the new loss function can improve robustness to random and adversarial perturbations.","This paper proposed an approach to propagate probability distribution through neural networks. In particular, the authors use local linearization to handle nonlinearity and show its optimality in terms of total variation for ReLU. Empirically they show that the proposed method can provide calibrated confidence intervals for regression problems and improve detection of OOD data in classification problems compared to some baselines. ","The paper proposes a method to propagate uncertainties through neural networks by performing a first-order approximation at nonlinear activation functions. By injecting uncertainty into the inputs, prediction uncertainties are obtained. These prediction uncertainties are exploited at training time by using new uncertainty-aware loss functions, and at test-time to obtained prediction uncertainties, out-of-distribution detection, and adversarial robustness.",0.18095238095238095,0.1523809523809524,0.13333333333333333,0.2235294117647059,0.16470588235294117,0.16393442622950818,0.2235294117647059,0.26229508196721313,0.22950819672131148,0.3114754098360656,0.22950819672131148,0.16393442622950818,0.2,0.1927710843373494,0.1686746987951807,0.2602739726027397,0.1917808219178082,0.16393442622950818
10,SP:023c978ab5d89b69f7efa9681a81f8ef415ced67,"High-resolution image classification is a hard problem for DNN due to resource constraints. The authors propose a multi-scale hard-attention architecture, TNet, for high-resolution information, by only visiting the most informative image regions, to achieve better accuracy vs complexity tradeoff. The architecture achieves a better tradeoff than other baselines on both ImageNet and fMoW datasets.","The paper reduces the computational cost of processing large images by leveraging a multi-scale hard attention mechanism. Starting from a low resolution version of an image, it progressively identifies and zooms in the most discriminative areas of the image. By ignoring most of the irrelevant content of the image, it processes images faster than strong traditional approaches, while achieving higher accuracy than these baselines thanks to a novel regularization mechanism.","This paper proposes TNet (Traversal Network), a hard attention architecture for classification of high-resolution images using a pyramid of different resolution versions of the input. The proposed method: - applies a ""Feature extraction module"" to a 224 x 224 image - applies a ""Location module"" to the spatial output of the previous module to select which sub-regions of the input should be further processed - recursively applies the same steps to each selected subregion, but at a higher-resolution  Each time the ""Feature extraction module"" is run, it generates a fixed length feature embedding. These embeddings are passed through a ""Positional encoding module"" and the resulting embeddings are averaged to create a final fixed length representation of the input. The fixed representation can finally be used in a logistic regression classifier.  As this model must dynamically choose which parts of the image are used in the next iteration, it is not end-to-end differentiable, and thus uses a REINFORCE based learning process with M sampled decisions per image to approximate the loss.  The authors test this method against baseline methods on ImageNet, fMoW, CUB-200-2011, and NAbirds. The baseline methods include Saccader, DRAM, BagNet, and EfficientNet. ","The authors present a hard attention mechanism, called  TNet,  for hi-res image classification.  The methods employ a resolution cascade, where a feature network attends to only a few number of locations of each resolution. The non-differentiable attention sampling is optimized with a Reinfore-like learning rule. All feature vectors are simply pooled for final classification, thus avoiding overhead from recurrent processing. The authors compare their algorithm with two other hard-attention methods, Saccader and DRAM, and one fully convolutional architecture, BagNet-77 on ILSVRC 2012 ImageNet. Results on the opening figure indicate  that TNet is superior to the other methods on the Pareto frontier of accuracy and GFLOPS per image.",0.22413793103448276,0.27586206896551724,0.27586206896551724,0.28169014084507044,0.19718309859154928,0.15228426395939088,0.18309859154929578,0.08121827411167512,0.14285714285714285,0.10152284263959391,0.125,0.26785714285714285,0.20155038759689925,0.12549019607843134,0.18823529411764706,0.14925373134328357,0.15300546448087432,0.1941747572815534
11,SP:02513468848dadcff46b177a6b8544e99095d418,"This paper considers the domain adaptation setting where we are given training examples from $k$ different distributions $D_1, \cdots, D_k$. We assume a hypothesis class $H_k$ the at is good for each domain (in the weak learning sense). At test time, the samples are drawn from a convex combination $D = \sum_i \lambda_i D_i$. The goal is to find one linear combination of  weak learners which will perform well for any convex combination. It turns out that this is not possible, but that if you allow the weights to depend on the conditional probability of coming from a particular domain $D(j|x)$, then a single learner is possible. This I believe is already proved in the paper of Mansour-Mohri-Rostemizadeh 2008, although the result there relies on a fixed-point theorem and perhaps does not give an efficient algorithm.      The main contribution of this work is a boosting style algorithm that achieves this  guarantee. The algorithm does simple co-ordinate wise gradient descent like in Adaboost. They show theoretical margin-based guarantees for their algorithm, and experimental results showing that it outperforms more naive boosting-based approaches. ","Utilizing recent formulations/results, the authors propose MutliBoost, a multi-source ensemble learning algorithm where: (1) it is assumed that any target domain is a mixture of the source domain distributions and (2) the learned function form is a convex combination of base functions weighted by a domain identification classifier, Q(k|x), and referred to as Q-ensembles. The domain identification classifier can be learned from unlabeled data (other than the domain label) and an important innovation as it leads to a more natural multi-source Boosting objective and generalizes many existing (and widely-used) multi-source Boosting works. This formulation aligns with recent results regarding agnostic loss [Mohri, Siven, and Suresh; Agnostic Federated Learning, ICML19] and is able to use related findings to prove favorable theoretical properties wrt previous work and leads to FedMultiBoost, a federated learning variant. Experiments are conducted on five binary datasets converted to multi-domain settings (sentiment analysis, hand-writing recognition, object recognition, and UCI adult tabular data) against natural single domain transfer and combined domain data baselines, showing statistically significant improvements. Several additional experiments are included in the Appendices to also show further results (e.g., domain-identification classifier, FedMultiBoost) that addresses multiple natural questions in the paper. ","The paper studies the problem of learning boosted classifiers with data from multiple source domains. The authors propose Q-ensembles to deal with it and seek a solution for any mixture of the source distribution. They propose the algorithm MultiBoost to solve it inspired by coordinate descent. They present theoretical analysis in terms of margin-based learning bounds. Moreover, they present an extension to the federated learning scenario. In experiments, they compare experimental results with AdaBoost. ","The paper addresses the question of learning an accurate classifier using ensemble methods and based on multiple sources. The purpose of the classifier is to achieve good prediction on a set of possible future domains, modelled as linear combinations of the source domains. This is done by minimizing an agnostic loss function over the weights of a linear combination of source predictors, such as traditionally done in ensemble methods. The paper proposes a main algorithm solving this problem, as well as a version for the federated setting. A theoretical analysis and experimental validation are also proposed. ",0.12886597938144329,0.0979381443298969,0.13402061855670103,0.08780487804878048,0.1024390243902439,0.2236842105263158,0.12195121951219512,0.25,0.2708333333333333,0.23684210526315788,0.21875,0.17708333333333334,0.12531328320802004,0.14074074074074075,0.17931034482758618,0.12811387900355872,0.13953488372093023,0.19767441860465115
12,SP:0260319f71562dd708e6055c37d4b6969f2a3ad0,"The paper derives bounds for the Bayesian simple regret of two GP optimization policies that are variants of the popular expected improvement and UCB, specifically under the assumption that the querying budget is very low compared to the size of the search space. The authors first do this in the discrete setting where there are a finite number of actions to be taken and then extend their results to the continuous setting. The bounds are derived using a procedure inspired by the adaptive submodularity framework. Results show that these new bounds are lower than those from a previous work.","This paper considers Gaussian process optimization and provides bounds for the (normalized) Bayesian simple regret of two policies. Each of these policies can be seen as a symmetrization of either the classical expected improvement (EI) or upper confidence bound (UCB) policies. The symmetrized version of EI, which is denoted by EI2,  picks the point expected to either increase the current observed maximum or decrease the current observed minimum the most. UCB2 is defined analogously with respect to UCB. The bounds obtained are shown to be tight and suggest that it is possible to find points with high (normalized) objective value even when the number of evaluations is small relative to the number of points in the domain, $N$,  (in the finite case, or relative to the smoothness of the prior covariance function in the continuous case), although it still grows polynomially with respect to $N$. Such guarantee contrasts with previously obtained results, which require the number of evaluations to be large with respect to the number of points in the domain.","This paper studies the problem of Gaussian process optimization when the number of arms (the domain) is larger than the horizon of the problem. In the finite arm case, it is assumed that the function is drawn from a zero-mean Gaussian prior and the learner is allowed to make T noiseless queries. Two algorithms based on expected improvement and UCB are proposed to solve this by achieving a simple regret that is some fraction of the optimal value. These are shown to be tight. It is shown that similar results can be extended to the continuous domain, but instead depending on a large Lipschitz constant.","The paper studies regret bounds for Bayesian Optimization (with UCB & EI as acquisition functions) in the setting where the function domain is large relative to the number of admissible function evaluations, and the function evaluation is noiseless. The paper proposes two modified version of EI and UCB for this purpose and characterize the Bayesian simple regret of these methods, for both discrete and continuous domains. The work shows that the methods can find nontrivial function values even when the number of evaluations is far too small to find the global optimum.",0.3333333333333333,0.2222222222222222,0.2222222222222222,0.17543859649122806,0.15789473684210525,0.24528301886792453,0.19298245614035087,0.20754716981132076,0.24175824175824176,0.2830188679245283,0.2967032967032967,0.2857142857142857,0.24444444444444444,0.2146341463414634,0.23157894736842102,0.21660649819494585,0.20610687022900764,0.2639593908629441
13,SP:0260593a94ae46e9825651c50d94f33bd37e6d53,"The paper studies the performance of dynamical systems learned from data with a focus on out of distribution (OOD) evaluations. Authors consider the question whether disentangling dynamical system parameters in the latent space can improve the generalization of the models, which is perceived as privileged information available from the reference (ground truth) simulations. Authors carry out experiments on several dynamical systems: pendulum, Lotka-Volterra system and three-body problem. Additionally an experiment on video prediction of a singing pendulum is performed. Authors found that additional disentanglement can improve generalization performance of the models and in video prediction setting leads to better long-term predictions based on structural and perceptual image metrics.",This paper introduces a supervised disentanglement method to learn dynamical systems. The method relies on the provision of privileged information (true parameters of a sequence) in order to disentangle them from observations. The method is evaluated on three toy datasets.,"In this paper, the authors propose a supervised approach to disentangle domain parameters from the dynamics in a deep latent variable model like VAE. Extending VAEs to dynamical systems is a relevant problem and has been a focus of interest in many recent works [1,2,3,4,5,6,7]. This paper identifies two issues for developing dynamical VAEs,   i) out of distribution generalisation  ii) long term trajectory prediction  The main contribution is to address the aforementioned issues using a supervised loss defined between latent variables and domain parameters. The authors present empirical experiments to support the idea.","This paper introduces a supervised loss to encourage dynamical systems predictors to retain systems' parameters (e.g. appearing in the underlying ODE) in their latent space. It presents multiple experiments to evaluate the advantages of this approach, including better long-term forecasting ability as well as improved prediction performance for out-of-distribution parameters, that had not been seen during training.",0.11711711711711711,0.14414414414414414,0.12612612612612611,0.3,0.3,0.1717171717171717,0.325,0.16161616161616163,0.22950819672131148,0.12121212121212122,0.19672131147540983,0.2786885245901639,0.17218543046357618,0.1523809523809524,0.16279069767441862,0.1726618705035971,0.2376237623762376,0.21249999999999997
14,SP:02706e1c3a8fcc422b3f3d2e46ca67a6ec3d018e,"The paper presents a method which uses Kernel mean embedding for mean-field based MARLwith  offline settings. Under a weak data coverage assumption, it shows a suboptimality bound in offline settings and extend the result to online settings. No experiments are included in this paper.","This paper studies the Mean-Field Multi-Agent RL problem in the offline regime where pre-collected data is available. Under the assumption that data provides “sufficient” information regarding the optimal policy, the authors propose SAFARI, a pessimistic algorithm, and prove sample complexity guarantees to achieve a certain sub-optimality gap. The obtained data-dependent guarantees do not depend on the (large) number of agents and scale linearly with the effective dimension of the function class (used to parametrize the value function). Moreover, the authors propose another algorithm OMPPO for the online setting and obtain similar sublinear (in $N$; for some kernels) regret guarantees. ",The paper (mainly) provides a pessimism based offline RL algorithm for MF-MARL. It extends existing literature by 1. Generalizing pessimism based offline RL algorithms from single agent to MF-MARL 2. Considering MF-MARL from an offline regime It also extends existing literature on online MF-MARL by converting the pessimism penalty to a optimistic bonus and showed the sample efficiency of the algorithm under this regime.,"The paper introduces an offline algorithm named SAFARI for solving Mean Field Games, using RKHS embedding of the infinite dimensional distribution of states. No experience are provided, but a bound on the efficiency of the method in terms of approximated value function is provided. This bound depends on the time horizon, the number of offline available trajectories as well as the effective dimension of the for parametrising the value function. It is valid whenever the set of available trajectories satisfies a natural weak coverage assumption. As a by-product, the authors also provide an online version of the algorithm.",0.26666666666666666,0.24444444444444444,0.28888888888888886,0.11538461538461539,0.20192307692307693,0.19117647058823528,0.11538461538461539,0.16176470588235295,0.13131313131313133,0.17647058823529413,0.21212121212121213,0.13131313131313133,0.16107382550335572,0.19469026548672566,0.18055555555555558,0.13953488372093026,0.20689655172413793,0.15568862275449102
15,SP:02832fd94d32158728931c320620c2684dc9a09d,"The paper proposes an approach to ensure a model trained on a set of source domains generalizes well to an unseen target domain based on a single unlabeled target sample. Unlike the standard domain generalization (DG) setting, where there is no scope for adaptation in the target domain, and domain adaptation (DA), where the model has access to source and unlabeled target data during training, the paper falls somewhere in between where no access to target data is assumed during training and a single unlabeled target sample is allowed for quick adaptation. In principle, the proposed setting is somewhat similar to source-free domain adaptation with the distinction that only one sample (and not the entire target dataset) and one quick adaptation pass on the data is allowed. The authors leverage a meta-learning paradigm to mimic domain shift by defining meta-source and meta-target domains within the source domain vocabulary (has also been considered in prior DG settings). The approach adopted by the authors relies on modeling a conditional distribution over the model parameters given a meta-target domain sample and source domain representative features. The authors set this up within a variational inference framework that allows them to explicitly parameterize this conditional distribution to infer model parameters in a target domain. The authors further explore the utility of different instantiations and alternative formulations of their objective and show that their proposed version has the best performance. When compared with prior DG and test-time adaptive methods, the proposed approach leads to competitive or improved performance. Further ablations demonstrate the utility of the proposed approach over other test-time adaptive approaches in terms of number of target samples available for unsupervised adaptation.","The paper deals with the problem of domain generalization, specifically performing inference on a single test example.  The key idea is to use meta-learning to train the model on the source data -- in the training stage, the source data is divided into meta-source and meta-target, and the model is trained to adapt to the meta-target domain.  This mimics domain shift within the source data.  The method is seamlessly transferred to the target domain, where inference is performed on a single test example, without finetuning model parameters or using additional networks. ","This paper studied the problem of single test sample generalization. Its goal is to adapt a pre-trained model to unseen target domains without extra fine-tuning. The paper formulated the single test sample generalization problem as a variational inference problem and proposed a meta-learning framework. To bypass extra fine-tuning, a single test sample was used as a conditional to generate model parameters. Experiments and ablation studies on common-used benchmarks demonstrate the effectiveness of the proposed method. ",- The paper describes a method for domain generalization that performs test-time adaptation using a single test example at a time (as opposed to a transductive setting used in other works where a whole batch of test examples are used). - The method is cast as a meta learning task. The training data is split into meta-training/test domains to mimic the adaptation process during training.,0.13380281690140844,0.10915492957746478,0.07394366197183098,0.20212765957446807,0.2127659574468085,0.175,0.40425531914893614,0.3875,0.3181818181818182,0.2375,0.30303030303030304,0.21212121212121213,0.20105820105820105,0.1703296703296703,0.11999999999999998,0.21839080459770116,0.25,0.1917808219178082
16,SP:02d9d7f58ec6076388353f4d6a60c7b8c8a79cb6,"This paper conducted a comprehensive study of high probability generalization bounds for minimax problems in terms of various forms of measuring generalization such as strong/weak PD generalization and primal generalization error.  Given uniform stability $\epsilon_{stab}$,  it proved sharper bounds of order $O(1/n + \epsilon_{stab})$  while sacrificing the empirical error (e.g.   extra $\eta F_S(A(S))$,  $\eta \bigtriangleup^s_S(A(s))$ on the bound). The results improves the previous excess generalization bound $O(1/\sqrt{n}+ \epsilon_{stab}))$ when  $F_S(A(S))$ or $\bigtriangleup^s_S(A(s))$ is very small.  The results extend the existing work by Lei et al. (2021) where most of the bounds were derived in expectation and high probability ones are given by f order $O(1/\sqrt{n} + \epsilon_{stab})$.  Then, specific bounds were given for GDA, PPM and SGDA for the strongly convex and strongly concave case as they are uniform stable.","As a theory work, this paper considers the generalization of the minimax learning problems which has a wide applications in machine learning. In comparison with the results of the previous work, which either provide expectation bounds or high probability generalization bounds of $O(1/\sqrt{n})$, this paper gives improved high generalization bounds of $O(1/n)$. Besides, these bounds are applied in many popular optimization algorithms, including ESP, GDA, SGDA and so on.","This paper consider a stochastic minimax problem of the form $\min_x \max_y F(x,y)$ where $F(x,y) = \mathbb{E}[f(x,y,z)]$. The goal is to obtain ``fast'' rates of $O(1/n)$ with a training set of $n$ i.i.d. $z_1,\dots,z_n$ in high probability. In this setting there are a variety of convergence measures one might consider, most notably perhaps the gap $\Delta(x,y) = \max_{\hat y} F(x,\hat y) - \min_{\hat x} F(\hat x, y)$. For these measures the fast rates are achieved for various specific problems such as strongly-convex-concave saddle-point detection. The main technique is to generalize classical results on algorithmic stability developed for stochastic minimization towards the stochastic minimax problem, and then to demonstrate that a variety of algorithms (mostly variants on gradient descent) are stable in particular settings. ","High-probability generalization bounds for minimax problems are proposed. The authors establish bounds for four quantities: plain generalization error, primal generalization error, strong primal-dual risk and strong primal-dual generalization error. The main contribution of the paper is in fast rates of the bounds compared to previous work for various applications to popular algorithms.",0.14743589743589744,0.16666666666666666,0.1282051282051282,0.21621621621621623,0.16216216216216217,0.087248322147651,0.3108108108108108,0.174496644295302,0.36363636363636365,0.10738255033557047,0.21818181818181817,0.23636363636363636,0.2,0.17049180327868851,0.1895734597156398,0.14349775784753363,0.186046511627907,0.12745098039215685
17,SP:0309aca15a888219390dcd26e0aa2413f8d42927,"This paper study possible adaptations of Neural Arithmetic Linear Units (Trask 2018) to better handle division. The authors first propose improvements for training Real Neural Power Unit (Heim 2020), a neural arithmetic unit which performs division as substraction in log-space, using cosines to handle signs, and a gating mechanism to select operands, while constraining all weights to remain close to 0, 1 or -1. They show that introducing L1 regularisation, gradient clipping, and tuning weight constraint and initialization greatly improve its accuracy when dividing numbers.  They introduce two new designs, the Neural Reciprocal Unit, which functions as an addition in log-space with weights constrained to -1, 0 and 1, and can therefore perform multiplication or division of numbers (and ""unselect"" them when weights are zero, which dispenses of the gating mechanism), and the Neural Reciprocal Modified Unit, where the reciprocals are provided as additional inputs, constraining weights to 0 and 1 (as a selection mechanism), and adds a cosine function to handle signs.   Testing over a variety of generated samples, they show that whereas the new designs improve on the original Real NPU, most of the still struggle on input of mixed signs, and have difficulty working with numbers close to zero, and learning both selection (choosing the operands) and division.","The paper is an addition to the line of study concerning neural arithmetic. Several new architectures are proposed to tackle the problem of learning the division operation. Out of the four arithmetic operations division is arguably the more challenging one and previous works have struggled with it.  Through several experimental results, the new architectures are shown the outperform previous designs, both in training accuracy and in other desirable properties","In this paper, the authors explore neural arithmetic architectures that could learn to perform division tasks, both in the case of input redundancy as well as with only 2 operators. Besides improving an existing architecture (NPU), other two related architectures are introduced (NRU and NMRU), both based on the idea that division can be framed as a multiplication of reciprocals. Extrapolation capabilities are probed by considering testing intervals that do not overlap with training intervals. The newly introduced architectures achieve better performance, though in the case of input redundancy all models are still error prone.","This manuscript proposed new types of neural arithmetic modules called Neural Reciprocal Units (NRU) and Neural Multiplicative Reciprocal Units (NMRU). NRU and NMRU extend Neural Multiplication Units by applying power terms and reciprocal inputs, respectively, and thus they enable divide operation. They also proposed improvement method of Neural Power Units (NPU).",0.07981220657276995,0.107981220657277,0.07511737089201878,0.21739130434782608,0.07246376811594203,0.09473684210526316,0.2463768115942029,0.24210526315789474,0.3137254901960784,0.15789473684210525,0.09803921568627451,0.17647058823529413,0.12056737588652482,0.14935064935064937,0.12121212121212122,0.18292682926829268,0.08333333333333333,0.12328767123287672
18,SP:03835515e384595fed6194e175bfe4c8a958d76d,"This paper considers the backdoor attack settings, where the adversary has full control over the training process and tries to encode a trigger function into the classifier such that the classifier fails at test time when the trigger function is activated. They propose a new algorithm that trains a parameterized trigger function at the same as the classifier in a min-max fashion. To ensure that the trigger function is not detected, the paper add a regularization term that ensures that the latent representation of the modified samples and the clean samples have the same distribution, they propose to use a sliced wasserstein distance to do this.","This paper proposes how to improve the stealthiness of backdoor attacks. To explain, the distribution of latent vectors has been an effective method to defend against backdoor attacks. However, this paper claims that an adversary can intentionally make backdoor attacks sneakier so that adding backdoor noise does not change the latent vector distribution too much. As the result, the authors propose a new attack method, Wasserstein Backdoor (WB), that has an imperceptibly small trigger and shows small differences in the latent vector distribution.","This paper proposes a novel backdoor attack that is stealthy in both the input and latent spaces. It argues that the previously proposed backdoor techniques leave tangible footprints in the latent space, thus being easily detected. The authors propose to remove that weakness by adding a regularization term that minimizes the Wasserstein distance between the clean and backdoor embeddings when joint training the trigger function and the classification network. The complex Wasserstein distance is approximated with the sliced-Wasserstein distance (SWD). The authors consider the latent representation at the penultimate layer, thus further simplify SWD computation. The proposed backdoor attack has a high clean and attack accuracy while being stealthy under both input/latent space inspection and backdoor defenses, verified on various benchmark datasets.","This paper proposes a backdoor attack approach for image classifiers. The generated backdoor examples are imperceptible in both input space and the latent space. To do this, the paper optimize the noise to matching the latent representations of the clean and manipulated inputs via a Wasserstein-based regularization of the corresponding empirical distributions. The effectiveness of the method is evaluated on MNIST, CIFAR10, GTSRB, and TinyImagenet, and bypassing spectral signature defense and model mitigation defense.",0.17757009345794392,0.2336448598130841,0.16822429906542055,0.24096385542168675,0.20481927710843373,0.20161290322580644,0.2289156626506024,0.20161290322580644,0.24,0.16129032258064516,0.22666666666666666,0.3333333333333333,0.19999999999999998,0.21645021645021645,0.1978021978021978,0.19323671497584544,0.21518987341772147,0.25125628140703515
19,SP:038a7b8f17841622fff5e7b9e5314c39c07530ba,"The paper does a theoretical analysis of various gradient approximations for the non differentiable sign function. They investigate 3 gradient estimators based on the STE assumption, namely (identity) STE, ReLU STE, and clipped STE (cReLU). They show theoretically that a gradient estimator which breaks the scale symmetry (clipped STE) in the network is more likely to achieve a local minimum than one that keeps the symmetry.","The paper considers a network with one hidden layer with binary valued neurons sharing the same weight vector but having non-overlapping receptive fields. The authors analyse the stationary points of a particular MSE loss when a straight through estimator (STE) is used for the loss gradient. They analyse three variants for the STE: (1) derivative of the identity function, (2) derivative of ReLU and (3) derivative of a piecewise linear squashing function. Assuming a simple independent distribution over the inputs and a particular loss, the authors show that the first two variants are biased and only the third STE variant can be unbiased. The authors conjecture that the reason for this is that the third STE variant breaks the scaling symmetry w.r.t. the weights of the first layer.","The paper investigates analytically the scale invariance properties of  different straight-through estimators used in training neural networks with binary activation and weights.   The main claim is that the cReLU STE, among the 3 considered, is the one that is more likely to give zero gradients in the minima of the population loss. This is (weakly) supported by theoretical analysis on a little variation of the shallow setting analyzed in Yin et al. (2019), where the teacher now contains ReLU activations.    ","Training quantized neural network by the biased Straight-Through estimator (STE) is a common practice in the NN quantization literature. However, due to the biased nature of STE, there is no principled guidance in making algorithmic choices for such training methods. This work focuses on the choice of surrogate differentiable functions in STE, and looked at three common choices: identity, ReLU, and clipped ReLU. Through analysis of the scale invariance of gradients in STE, the paper suggests clipped ReLU might pick up local minima degenerated in scales, due to its property of breaking scale symmetry among the three. The authors further analyzed the stationary points of a simple misspecified model with Gaussian inputs to confirm their observations.",0.3181818181818182,0.22727272727272727,0.25757575757575757,0.15267175572519084,0.16793893129770993,0.2222222222222222,0.16030534351145037,0.18518518518518517,0.1452991452991453,0.24691358024691357,0.18803418803418803,0.15384615384615385,0.21319796954314718,0.20408163265306123,0.18579234972677594,0.18867924528301885,0.17741935483870966,0.18181818181818185
20,SP:03ab8a4a0a94fc09228f65e36a5e2ec22fe04236,"The authors propose a new framework for self-supervised learning from natural images. The proposed objective is a linear combination of two previously proposed objectives, namely student-teacher self-distillation (as in DINO), and masked-language modeling as is commonly used in NLP. The masking strategy is guided by the attention masks learned by the transformer backbone. The authors evaluate their model on standard image-SSL benchmarks: ImageNet classification and COCO detection and segmentation.   ","This submission combine the task of mask language modeling (MLM) and a contrastive learning task for self-supervised visual representation learning. The authors claim that due to he preserve of spatial information of an image, the proposed approach is more friendly to the downstream dense prediction tasks. The effectiveness of the approach is verified on ImageNet-1K linear evaluation task, and downstream tasks of COCO object detection and ADE20K semantic segmentation.","The paper proposes to add an image reconstruction task to the existing instance discrimination task, as the pre-text tasks for self-supervised learning. Specifically, the self-attention map from the teacher model is used to guide the random mask process, that the attended foreground regions will not be masked. A decoder is used to restore the original image from the masked image.","This paper presents a new self-supervised learning method based on a combination of mean-teacher distillation (in this case DINO) and masked patch modelling (like masked language modelling in NLP). They use this to train transformer based vision models and show downstream results for ImageNet classification using linear probing and knn eval, MS COCO object detection and instance segmentation using Mask-RCNN framework and Cityscapes semantic segmentation using SERT framework.  Their key technical contributions are (a) the overall training framework combining DINO with masked patch modelling. The latter requires a CNN decoder and reconstructs the original global image and not just the masked patches and (b) a smart mask generation scheme based on the attention map of the CLS token which turns out to be crucial for good results. They use the attention maps corresponding to the CLS token to select patches that are not important. The idea is that if important regions of the image are masked out then the model will end up modelling only local statistics.  At a deeper level, their method is trying to retain spatial structure in the latent representation while capturing global image properties. If I understood correctly, the spatial structure comes from masked patch modelling and global image properties come from DINO.",0.2702702702702703,0.20270270270270271,0.33783783783783783,0.19718309859154928,0.23943661971830985,0.3492063492063492,0.28169014084507044,0.23809523809523808,0.11904761904761904,0.2222222222222222,0.08095238095238096,0.10476190476190476,0.27586206896551724,0.21897810218978103,0.17605633802816897,0.208955223880597,0.12099644128113878,0.16117216117216115
21,SP:03ad9079ac0ea868f961f995b0094270cdf2d162,"The paper investigates the results of Emergent Communication learning in different settings (number of agents, communication connectivity, task difficulty). The authors use a reference game variant where the communicating parties are limited in their perceptual abilities and hence need to communicate in order to solve the task (select a specific object among a number of alternatives). The authors obtain a number of interesting observations (for example, observing that larger groups of agents result in more diverse communication protocols, which (in certain settings) still allow to achieve high communication success rate).  ## Strengths  - The authors tackle a highly relevant problem - The experimental setup aims to investigate the impact of a number of factors that may indeed be crucial in determining the properties of emergent communication - The paper has a large number of illustrations and is generally well-written, which promotes understanding.  ## Weaknesses - The decision to use early stopping, while motivated by valid concerns, does not actually address these concerns. A universally controlled amount of training in terms of total number of interaction per agent would be much easier to interpret. Ideally, more than one schedule should be tested: 1-training until convergence, 2-training for a set number of interactions for any given agent. - Literature review overlooks a number of relevant works and thus inadvertently presents the experimental setup as more novel than it is. ","This paper proposes a number of experiments within a novel referential game where agents must learn to communicate among a population of agents. Authors design a novel referential game played in an artificial dataset of shapes and colors, where speaker and listener have access to a description of the true shape and color but are only able to either perceive shape or color. Thus, an agents' message must fill in the missing information of their corresponding listener, such that the listener can identify the correct target image.  With this setup, the authors design an experiment where there are a group of agents, and agents either are jointly trained with all other agents, or in more isolated connectivity settings (e.g. agents only speak to one or two other agents). Their aim is to see how the group size and connectivity of the group affects the learned communication protocol along a variety of axes:  - Q: how similar are the agents' utterances as the group size increases? (A: agents maintain communicative success, but messages become *more diverse* as the group size increases.) - Q: Are agents able to generalize to new partners within a population, and to unseen tasks? (A: As agents learn from larger numbers of agents, zero-shot generalization to other agents also increases; agents do show some generalizability to unseen tasks, though it seems to vary based on group size and the complexity of the task distribution.) - What is the optimal communicative graph which results in the least number of links between agents, but most homogenous communication?","This paper studies population size and connectivity graph effects in emergent communication in a population of agents. By limiting the communication partners in specific ways, i.e. deleting edges from the fully connected graph, the authors show that dialects can evolve: communication protocols that are specific to sub-groups even when those groups are not completely isolated. Furthermore, by experimenting with different connectivity patterns that share a constant overall graph sparsity, they find that a pattern with few highly connected nodes (hubs) leads to significantly higher communication success than random or uniform patterns. Such hubs are a common feature of complex networks found in many natural systems. In terms of communicative success, the authors find that more complex tasks are associated with better generalization to unseen data (a smaller drop in success relative to the training data)  The paper also introduces a variant of the widely used referential game where the speaker and listener observe non-overlapping aspects of the images they communicate about, and where agents act as both listener and speaker, depending on how they are sampled at the start of a round in the game. ","The paper studies the effect of population size and graph connectivity on the emergence of communication. The main questions studied in the paper were 1) how does the size of agents' population affect task success? and 2) when agents don't get to train directly, but through some other agents, how well they can communicate with each other when they put in contact? The game that was used in this paper to study the problem, was a referential game, and the task was discrimination task. The agents can take 2 roles of speaker and listener, such that speaker gets some observation and send a message to listener, and listener gets some other observation along with speaker's message, and it has to make a decision in a discriminative setup. ",0.17937219730941703,0.14798206278026907,0.13452914798206278,0.13229571984435798,0.11673151750972763,0.17553191489361702,0.1556420233463035,0.17553191489361702,0.23255813953488372,0.18085106382978725,0.23255813953488372,0.2558139534883721,0.16666666666666669,0.16058394160583941,0.17045454545454541,0.15280898876404495,0.15544041450777202,0.2082018927444795
22,SP:03b91432585ae8c66c1f494e9e0d73be7e159b6d,"The paper considers the following question: say you observe n data points, drawn iid from some distribution p, and you know that p belongs to some class of distributions P, then how large does n have to be such that you can use the data to generate a single new point that is close in total variation distance to the underlying distribution p, while being differentially private? In the paper, this question is referred to as the sample complexity of differentially private sampling.  The paper then investigates this question for two classes of distributions: any distribution supported on k points, and product distributions on {0,1}^d. It shows that for all the settings, the sample complexity of differentially private sampling is a lower bound for the sample complexity of differentially private learning, and that in some settings, the sample complexity for sampling is strictly smaller than that of learning. ","The paper initiates the study of the following sampling problem under privacy: we are given n samples from a (discrete) distribution P, and want to output a (random) value x such that the distribution of the value we output (over the randomness of the samples and the coins of our algorithm) has small total variation distance (TVD) with P. The problem is trivial normally, since the first sample we are given is distributed exactly according to P. When we are concerned with privacy of the samples, i.e. want to output a value x with approximate differential privacy (viewing the n samples as a database), outputting the first sample is non-private, and the problem is more interesting. The motivation for studying this task, is that it is a weaker version of the problem of learning the distribution P (if we learn an approximation of the distribution, we can clearly approximately sample from it).  The authors consider the problem in three settings: When the support of P is [k], when P is a product distribution on the d-dimensional hypercube (i.e. each bit is independent Bernoulli, not necessarily identically distributed), and the distribution over the hypercube but when each coordinate is a Bernoulli with mean in [1/3, 2/3]. They show lower and upper bounds for each problem when delta = 1/n^c for some constant c > 1. For distributions on [k], the authors show matching upper and lower bounds of needing k/alpha*eps to get a sampler within TVD alpha. For product distributions on hypercubes, they show d/alpha*eps samples suffice, but the lower bound is sqrt(d). The authors are able to match this bound, obtaining TVD alpha with sqrt(d)/epsilon + log (d/alpha) samples when the coordinates are Bernoulli with mean close to 1/2.  In addition to quantitative results, the sample complexities shed some light on the problem of learning P rather than outputting a sample from P. In particular, the best upper bounds for private learning match the sum of upper bounds for nonprivate learning and the upper bounds for private sampling given in this paper. The authors' results also show that e.g. in the Bernoulli/hypercube setting, the regime where sampling is easiest is the regime where learning is hardest and vice-versa.","This paper initiates an algorithmic study of how to make sampling from discrete distributions private. The formulation is as follows: Given $n$ independent samples from a discrete distribution $P$ over a universe $U$,  output one sample $y$ whose distribution is close to $P$ in total variation distance.  The efficiency goal is to make the number of samples $n$ as small as possible. This appears to be a trivial task - just output any one sample from the input. However, if we require the algorithm to be differentially private then the task is non-trivial and this is the concern of the paper. The paper first considers the problem of private sampling for general distributions over a domain of size $k$ and establishes matching upper and lower bounds on the sample complexity. Then it considers private sampling from product of Bernoulli distributions and establishes upper and lower bounds. In this case, the complexity is not yet completely clear as there is a gap in their bounds. ","The paper studies the problem of sampling from a distribution in a differentially private manner.  More formally, the learner is given i.i.d samples from some unknown distribution $P$ over a domain $\mathcal{U}$ and its goal is to output an element of $\mathcal{U}$ such that the distribution of the output is close to $P$ in TV distance.  The problem of sampling is easier than the problem of learning the actual distribution $P$ and the authors show that in some cases it can be done using significantly fewer samples.   The main results are that for arbitrary discrete distributions over $k$, the sample complexity of differentially private sampling is $\theta(k/(\alpha \epsilon))$ (compared to $k/\alpha^2$ for learning and $k/\alpha^2 + k/(\alpha \epsilon)$ for private learning where $\alpha$ is the desired accuracy and $\epsilon$ is the privacy parameter).  For product distributions on $\{ 0,1 \}^d$, the main result is an upper bound of $\widetilde{O}(d/(\alpha \epsilon))$ although this can be improved to $\sqrt{d}/\epsilon + \log (1/\alpha)$ if the coordinates have means bounded away from $0$ and $1$. ",0.34,0.26666666666666666,0.29333333333333333,0.15584415584415584,0.16623376623376623,0.27439024390243905,0.13246753246753246,0.24390243902439024,0.23655913978494625,0.36585365853658536,0.34408602150537637,0.24193548387096775,0.19065420560747662,0.25477707006369427,0.2619047619047619,0.21857923497267762,0.22416812609457096,0.2571428571428572
23,SP:040bd4d3243cd4957aee23cc70c1951c44c2b995,"In this paper, the authors propose a data compression method for memory-replay based continual learning algorithms. With the data compression method,  more old training samples can be stored in the memory to better capture old data distribution. However, there is a trade-off between the quality and quantity of compressed data, the authors propose to use determinantal point processes to determine the quality of the data compression.  Extensive experiments show that with the proposed method, a naive compression method can achieve the SOTA on several continual learning benchmarks. ","In this paper, the problem of catastrophic forgetting for continual learning is explored. The core idea is to benefit from data compression to reduce the space to store data samples and then replay the reconstructed data points that are built using the compressed versions. JPEC method has been explored empirically to determine an optimal compression rate through solving an optimization problem. Experiments on three benchmark datasets are performed to demonstrate that the method is effective.","In this work, the authors propose memory replay with data compression, which is both an important yet neglected baseline and a promising direction for continual learning. Using a naive technique of data compression with a properly selected quality, the proposed method can achieve the SOTA performance in a time-efficient and plug-and-play way. Since the compression quality is highly nontrivial for the efficacy of memory replay, the authors provide a novel method based on determinantal point processes (DPPs) to determine it efficiently, and validate their method in both class-incremental learning and semi-supervised continual learning of object detection.","This paper studies the problems of classification and object detection from natural image datasets in a continual learning setting, whereby the different classes/objects to be learned are not observed together but sequentially. It is assumed that a memory buffer of a certain size is available to store data in. The strategy used by the paper is to fill this memory buffer with compressed data samples (with JPEG) rather than the original images. Such compression introduces a quantity-quality trade-off, which this paper empirically analyses. Additionally, the paper proposes an automated way to select the amount of data compression based on determinantal point processes.",0.25842696629213485,0.3258426966292135,0.24719101123595505,0.21333333333333335,0.25333333333333335,0.2079207920792079,0.30666666666666664,0.2871287128712871,0.20952380952380953,0.15841584158415842,0.18095238095238095,0.2,0.2804878048780488,0.30526315789473685,0.22680412371134023,0.18181818181818182,0.2111111111111111,0.20388349514563106
24,SP:041bcac0ae2b896a04f6d6c828cbfbbb8a0164b8,"The paper studies how contrastive learning (CL) behaves in the unsupervised domain adaptation (UDA) setting. In particular, it finds that CL brings a different mechanism for UDA compared to traditional adversarial DA methods, where the features learned are far apart between the source and target domains. It further develops a conceptual model for explaining the success of contrastive learning under domain shifts, and empirically demonstrates through experiments on several benchmark datasets.","This paper proposes to study the contrastive pre-training on domain adaptation tasks. The key contribution is to design a connectivity model for better data selection and data augmentations of pre-training. To verify the benefit of contrastive pre-training to domain adaptation, experiments are performed on a variety of benchmark tasks including BREEDS, DomainNet and CIFAR-10.","The paper investigates why contrastive learning method benefits unsupervied domain adaptation.  In particular, the authors use SwAV, a recently proposed self-supervised learning method to pretrain a model on both targeted and source datasets. The authors found that by using such a model for UDA tasks surprisingly nice results are achieved. The authors analyze the reasons behind using a connectivity model.","The authors tackle the problem of unsupervised domain adaptation (UDA), where most state-of-the-art methods aim to learn a joint embedding space for samples from the (labeled) source domain and (unlabeled) samples from the target domain.   In contrast to this, the authors propose to use contrastive learning pre-training techniques to first learn an encoder using target/source/additional data: Intuitively, this objective pushes positive samples together, and further away from other samples. The notion of ""positive set"" is defined as all samples which are obtained from data augmentations of a same sample natural image. A classifier is then learned on top of these features to solve the classification task on the labeled source dataset.  Using standard contrastive learning techniques, the authors show that they can achieve similar accuracies as some SoTA baselines on three UDA datasets. The remaining of the paper analyses the features learned by this techniques, and in particular, how they do not learn to align the two domains as is the common intuition in UDA. This analysis relies on a ""connectivity graph"" defined for contrastive learning techniques, which, in the current setting, can be used to connect samples from the source and target domain, to determine how likely they are to be in the same positive set.",0.2112676056338028,0.22535211267605634,0.28169014084507044,0.20689655172413793,0.3275862068965517,0.32786885245901637,0.25862068965517243,0.26229508196721313,0.09389671361502347,0.19672131147540983,0.0892018779342723,0.09389671361502347,0.23255813953488372,0.24242424242424243,0.14084507042253522,0.20168067226890757,0.14022140221402213,0.145985401459854
25,SP:0458a032f69e3f5b6b7cb0e791fc50c04bb294c4,"This paper introduces dPads, an approach to learning interpretable programs to perform sequence classification tasks, as an alternative to using RNNs which are less interpretable and possibly less robust. The programs are generated using a context-free grammar (adapted from prior work) of differentiable operations applied to sequences. The ultimate objective is to find a program that obtains high classification accuracy on a dataset, while having low architectural cost. This is done by using a continuous relaxation of the discrete choices in the grammar, using a softmax over all possible grammar production rules instead of a categorical choice, with learnable weights. Further optimizations include “node sharing” (where two nodes in the program derivation graph are shared if they represent the same nonterminal and thus can be expanded the same way) and “iterative graph unfolding” which is somewhat like a beam search over the program derivation process, narrowing down the earlier choices using a guess for their quality, resulting in less combinatorial explosion (higher efficiency) at the cost of a fully complete search. As a final step, an A* search is used to choose a single complete program with high accuracy and low cost. Experiments on 4 sequence classification datasets show that dPads performs better than prior work NEAR, but worse than plain RNNs (which are not interpretable programs).","This paper considers the task of synthesizing differentiable programs for sequence classification tasks. It introduces a novel method, dPads, that first constructs a program derivation graph representing a subset of the highest probability possible architectures under a CFG, and subsequently uses an A*-like search to find a program in the graph that balances minimizing complexity with maximizing accuracy. Central to the derivation graph construction method are the ideas of (1) iterative training of architecture weights and neural approximations of subexpressions, (2) node sharing, and (3) iterative graph unfolding. The method is evaluated on four sequence classification tasks, where it is found to have better performance than the program synthesis baselines considered, and worse performance than an RNN baseline, with the key advantage over the RNN being the interpretability of the resultant programs from dPads.",This work proposes a method for differentiable synthesis of program architectures. A program derivation graph is encoded as a differentiable program whose output is a weighted output of all the programs in the graph. The method learns a probability distribution over all possible program derivations induced by a context-free grammar. The grammar rule selection weights and the program parameters are co-optimized iteratively with respect to a loss function over program outputs. The approach is evaluated on four sequence classification tasks. The discovered architectures have better F1 scores as compared to the state-of-the-art program synthesis methods (A*-NEAR and IDS-BB-NEAR). ,"A method for synthesising programs using gradient descent is presented, alternating between optimisation of operator parameters (constants) and of the program structure (nested expressions). The submission focuses on reducing the complexity of the search for correct structures, using the idea of sharing different nodes in the search tree (i.e., allow them to appear in different contexts) as well as initially approximating subexpressions by neural networks and only starting to learn to represent them by programs when the subtree appears to be a promising candidate. Experiments on four small datasets show that the method compares favourably to a baseline in synthesizing programs that explain observed behaviour.",0.1651376146788991,0.14220183486238533,0.11926605504587157,0.23703703703703705,0.13333333333333333,0.1792452830188679,0.26666666666666666,0.29245283018867924,0.24528301886792453,0.3018867924528302,0.16981132075471697,0.1792452830188679,0.2039660056657224,0.19135802469135804,0.16049382716049385,0.2655601659751037,0.14937759336099585,0.1792452830188679
26,SP:047c430d10180e9ce4cb5d8bb2c6f199a4a6f5ca,This paper proposes a new method for performing early exits on a pre-trained architecture. Authors use support set to calculate class prototypes at intermediate representations. Then at evaluation execution on the network is stopped whenever the representations are significantly closer to a class at any level. Results on small datasets (cifar-mnist) using very deep networks show that proposed method achieve better results compared to some of the previous work.,"The paper introduces a novel mechanism to add early exiting on existing networks without re-training the original network. The paper mentions the reason for this is that this can be done on lower-powered devices on-the-fly, where retraining is not wanted. The method does so by calculating output means, and comparing new examples with that, by running a distance function through a softmax, and early exiting if the prediction is above a certain threshold.","The paper proposes a method for early exit during inference of DNNs. The method does not require fine-tuning or gradient computation. Rather, it uses the learned embeddings from intermediate model layers to decide where to terminate the execution. Specifically, for a trained model, the method calculates the per-class means of intermediate activation. The distance between the activations of new samples is then computed with the per-class means and inference is exited once the distance becomes lower than a set threshold.","The paper proposes a method for early-exit-based conditional computation for reducing the inference time of a given neural network. The approach works as follows: E2CM saves the mean activation of each class after each layer and then, during inference, compares the activation of a given example to the recorded mean activations to get a prediction and its certainty. If the certainty is high enough, this prediction is returned, otherwise, the next layer is queried instead.  The authors show empirically that the method outperforms other approaches in a setting with a very limited training budget, and is sometimes able to improve the performance of other methods when applied in combination. Finally, the method is also presented in an unsupervised learning setting where it allows for significant computation savings.",0.14084507042253522,0.2112676056338028,0.2676056338028169,0.22077922077922077,0.24675324675324675,0.27710843373493976,0.12987012987012986,0.18072289156626506,0.14728682170542637,0.20481927710843373,0.14728682170542637,0.17829457364341086,0.13513513513513514,0.19480519480519481,0.18999999999999997,0.2125,0.18446601941747573,0.2169811320754717
27,SP:04ba539f55ee6f32b40e6800dcd79a49dc289e98,This paper proposes an object-level self-supervised pre-training approach which extends and tries to improve existing work on image-level self-supervised pre-training. The authors motivate the problem by mentioning that image-level self-supervised pre-training methods are infeasible for images with many objects due to the hard constraints imposed by such methods. The paper proposes to apply self-supervised pre-training to similar objects in and across images. The authors use pre-trained image-level model to select the best candidate object pairs across images and then pre-train self-supervised models using the selected object regions. The authors report some performance improvements on the COCO dataset. ,"This work proposes a technique to adapt self-supervised contrastive learning to scene images where multiple objects are present in each image. Specifically, the proposed technique has three stages: 1. image-level pre-training with standard contrastive learning; 2. Finding object pairs images (crops) using selective search proposals and pre-trained features from stage-1; and 3. Doing contrastive learning with object image pairs. Several experimental comparisons demonstrate consistent improvements w.r.t. baselines that does only image-level contrastive learning.","This paper presents a new self-supervised learning framework that leverages scene images to enhance object-level representation learning that only relies on object images. The basic idea is to apply unsupervised region proposal and k-NN based object similarity to diversify intra- and inter-image pairs with more variances to aid object-level representation learning. The improvements on various downstream tasks, the ablations and visualizations all demonstrate the advantages of the presented framework. The main contribution is to utilize scene images for contrastive self-supervised learning.","This paper aims to learn object-centric self-supervised representation from non-iconic scene images. In addition to the standard self-supervised objective (e.g., BYOL), the paper suggests attracting the object patch with the (i) nearby patch of the chosen object and (ii) similar objects in different images. For (ii), the paper uses the similarity score of the standard image-level self-supervised model. This additional training signal from similar object patches improves the transfer performance of the learned representation on classification and detection/segmentation tasks.",0.16964285714285715,0.17857142857142858,0.23214285714285715,0.2222222222222222,0.1728395061728395,0.20689655172413793,0.2345679012345679,0.22988505747126436,0.2988505747126437,0.20689655172413793,0.16091954022988506,0.20689655172413793,0.19689119170984457,0.20100502512562815,0.2613065326633166,0.2142857142857143,0.16666666666666666,0.20689655172413793
28,SP:04d65c18e7c54d8cb64f2dd181a3c7033e1e4197,"This paper presents a novel approach to address Graph Similarity Computation (GSC) both accurately and efficiently. From my point of view, the technique contributions of this paper can be summarized in two perspectives, i.e., feature fusion and knowledge distillation. The proposed slow learning employs multi-scale integration and attention-based feature fusion for cross-graph interaction, making it achieve cutting-edge accuracy on most datasets within a comparable computation cost. The fast inference module is the major contribution and the most attractive point of this paper. The authors apply the embedding decomposition to separate the individual features for knowledge distillation, which successfully improves efficiency by vast scale without much accuracy drop. Overall, this paper has made a solid contribution to this field with comprehensive experimental results to support it.",The paper studies efficient graph similarity computation and proposes two connected techniques: a complex teacher model slowly learning through the graph and a simpler student model jointly learned and proposed to be used for faster inference. The proposed technique uses well-established Fusion network structures. The joint learning through knowledge distillation enables the simpler student model to learn more accurate latent representations through the teacher network. Both accuracy and run-time results have been provided using benchmark data sets. ,The paper presents a new method for computing graph similarity efficiently and approximately using graph neural networks. A novel attention-based neural network is proposed for combining the embeddings for the two input graphs. A novel way to compute graph similarity faster is proposed based on embedding decomposition and applying knowledge distillation to obtain a fast model from a slow model. Experiments on four graph datasets demonstrate the effectiveness and efficiency of the proposed method.,"This paper tackles the problem of graph similarity computation (GSC). The key insight of this paper is that to better model the problem of GSC, early and extensive graph feature fusion is needed, which is harmful to model efficiency. To address this issue, the authors propose a knowledge distillation solution that drops all early fusions. This allows the regression models to take in only the embeddings, which can be pre-computed, and thus saves time. Experimental evaluations indicate that the proposed framework achieves good performance and is fast in inference. ",0.13076923076923078,0.13846153846153847,0.17692307692307693,0.20253164556962025,0.21518987341772153,0.2,0.21518987341772153,0.24,0.25555555555555554,0.21333333333333335,0.18888888888888888,0.16666666666666666,0.1626794258373206,0.17560975609756097,0.20909090909090908,0.2077922077922078,0.20118343195266272,0.1818181818181818
29,SP:053831a5cd063ed2709bc84c909e614bda71ad29,"This paper provides a large dataset of structured 2D sketches and proposes transformer-based models capable of both unconditional and conditional generations of such sketches. The authors present a novel representation for structured sketches based on Protocol Buffers, encoding both objects and the constraint between them, and further devise a custom tokenization scheme to adapt it for the training and inference using a transformer decoder architecture, showing advantages compared to a simple byte tokenization. Their model can successfully generate valid structured sketches from scratch, and moreover infer valid objects and constraints from an input image (as a condition).","This paper proposes a deep learning model of automatically generating computer-aided design (CAD) sketches, which are typically composed of entities (e.g. line and arc) and constraints (e.g. tangent and mirror). The authors devise a method to describe CAD sketches as serialized Protocol Buffers, and further represent them as sequences of triplet tokens. Then they combine the devised data representation with language modeling techniques (Transformer + PointerNet) to capture the data distribution in an auto-regressive manner. The proposed method is evaluated mainly on unconditional generation and various design choices are ablated. Additional generation results are shown conditioned on binary images.",This paper proposes a novel method to represent structured CAD sketch objects via Protocol Buffers and demonstrate its generalization ability across the heterogeneous nature of sketches. They also proposed a transformer-based generative model for generating the CAD sketches and evaluate its effectiveness on carefully collected and preprocessed data which consists of over 4.7M sketches. They show the capability of the generative model for both unconditional and conditional generation tasks.,"This paper presents a learning-based model to generate 2D sketches that are widely used in 3D CAD applications. It considers 2D sketches consisting of entities (lines, curves, etc.) and constraints (mirror, coincide, perpendicular, etc.), a representation that is commonly used in modern CAD software. By design, 2D sketches are structured objects. To handle these structured data, this paper presents a linear representation using Protocol Buffers, which flattens the structured data into a sequence of triplets. Through this representation, the paper establishes similarities between natural language modeling and 2D sketches and proposes to use Transformer to train a generative model that captures the distribution of sketches in a large dataset. The paper reports quantitative and qualitative performance of the generative model.",0.19387755102040816,0.16326530612244897,0.22448979591836735,0.18627450980392157,0.28431372549019607,0.30985915492957744,0.18627450980392157,0.22535211267605634,0.18181818181818182,0.2676056338028169,0.2396694214876033,0.18181818181818182,0.19,0.1893491124260355,0.20091324200913244,0.21965317919075145,0.26008968609865474,0.22916666666666666
30,SP:05655e56ebbbbf501d35d93fa16630daf6a308fd,"The paper studies catastrophic forgetting and continual learning from the loss landscape perspective. The primary motivation is that from the plasticity/stability view, stable networks tend to have flatter minima. While this has been observed before in [1], the paper studies the problem in more depth. Also, it proposes a solution to reduce the sharpness of minima for each task based on the gradient projection memory work.   Overall, I like the paper, and I think it is an influential continual learning paper. However, I think the paper can be even better if the authors can address a few shortcomings.","In this paper, the authors propose a method called Flattening Sharpness for Dynamic Gradient Projection Memory (FS-DGPM) for continual learning. The main idea is to investigate the effect of weight loss landscape on overcoming catastrophic forgetting. With the understanding of the weight loss landscape, the authors further propose a method based on the recently proposed Gradient Projection Memory (GPM) method to predict the importance of bases which span the subspace of old tasks. Less important bases can be dynamically released based on the proposed soft weight. Extensive experiments show that the proposed method can consistently outperform the state-of-the-art methods across a range of widely used benchmark datasets.","The article proposes a continual learning method that innovatively improves on an existing algorithm. Precisely, the current work takes GPM (Gradient Projection Memory) which they show is good at preserving performance on old tasks (good stability), but it's not that efficient at learning new ones (low sensitivity) and modifies it in two ways: by dynamically adjusting the importance of the various bases such that the less important ones can be dropped to increase sensitivity. In addition it improves the stability by employing a form of adversarial parameter perturbation to flatten the loss landscape. The contributions of the paper are: - an analysis that shows the loss landscape and its ""flattness"" / ""sharpness""; this analysis is used to support the idea that flatter minima tend to forget less in continual learning setups; - changing the GPM such that it has an increased sensitivity while also improving the stability by flattening the loss landscape around the current solution ","The paper proposes a new continual learning method that promotes the loss shapes to be flattened. The main idea is to optimize the worst case perturbed loss function at every gradient step, and the empirical investigation of the loss values around the learned parameter corroborates their assertion. Also, experimentally, they showed promising results compared to other recent baselines. ",0.23232323232323232,0.23232323232323232,0.1414141414141414,0.23423423423423423,0.15315315315315314,0.12337662337662338,0.2072072072072072,0.14935064935064934,0.2413793103448276,0.16883116883116883,0.29310344827586204,0.3275862068965517,0.21904761904761905,0.1818181818181818,0.17834394904458598,0.1962264150943396,0.2011834319526627,0.17924528301886794
31,SP:056e7e83a31fb1cb71fc53e1c6d7fb6ca203a87d,"This submission introduces a calibrated testing procedure for the independence between two variables under non-iid sampling. The sampling is assumed to follow a clustered structure, where samples within clusters are correlated but the clusters are independent from each others. The procedure relies on the Hilbert-Schmidt Independence Criterion (HSIC), and a first contribution is an asymptotic null distribution under the clustered sampling assumption. The test is also shown to be consistent. Experiments on simulated and real data show that the proposed test guarantees a control on the type I error, and has higher power than existing alternatives in a variety of scenarios. ","The paper derives an approximation of the asymptotic null distribution of the Hilbert-Schmidt Independence Criterion under a matched and complete clustering. The null distribution corresponds to independence between the variables.  The samples consist of equally sized clusters each with matched within cluster correlation. The distribution is described as a weighted sum of chi-squared variables, where the weights are approximated by the eigenvalues of a matrix composed from the eigenstructure of the marginal kernel matrices grouped by cluster. Simulations confirm the increased power of the method. Specifically, simulation shows the test is conservative in lower sample size, but increases in power versus other methods (concatenating the samples longitudinally).  Experimental results demonstrate the increased power is able to identify additional metabolite pathways are associated with the vaginal microbiomes composition. ",The authors propose a test of independence for cluster correlated data. The test statistic is still HSIC. However they derive a different asymptotic distribution under cluster correlation.,"The paper proposed a modified version of Hilbert-Schmidt Independence Criterion (HSIC) to test the dependence between two multivariate variables. The method is suitable for data that is not iid, i.e. correlation exists between different data points. The paper first demonstrate the asymptotic distributions of HSIC under the null hypothesis (independent) and the condition for the alternative in Theorem 3.2 and 3.3. The approximation of the asymptotic HSIC distribution is provided in proposition 3.4. In the simulation, the proposed method HSICcl shows a better performance than earlier versions of HSICorig both in terms of Type I error and empirical power. In real data, HSICcl identifies more associated pathways than previous versions.  ",0.20388349514563106,0.08737864077669903,0.23300970873786409,0.06201550387596899,0.20930232558139536,0.37037037037037035,0.16279069767441862,0.3333333333333333,0.20869565217391303,0.2962962962962963,0.23478260869565218,0.08695652173913043,0.18103448275862072,0.13846153846153847,0.2201834862385321,0.10256410256410256,0.22131147540983606,0.14084507042253522
32,SP:0595d86b01ad6fe82e6afa8987a061e17980fa41,This paper proposes to address candidate generations of users and items for recommender and advertising systems simultaneously by a single proposed model. The proposed model provides two ideas for existing two-tower recommendation models: it introduces a normalization of the score function and a bidirectional version of the InfoNCE loss. This paper evaluated the proposed method through several experiments. ,"This work proposes a bidirectional candidate generation framework for both recommender system (which provides items for a user) and advertising system (which provides users for an item). Specifically, the authors conduct a Bi-InfoNCE loss on the classical two-tower architecture to jointly learn both user and item representations. The Bi-InfoNCE loss considers each (u, i) pair as a positive instance, and selects both users and items as negative instances in InfoNCE. In experiments, the authors find that the proposed TR model outperforms MF in both recommender system and advertising system datasets.","This paper proposes a method for both user2item and item2user candidates generation in recommender systems. The main idea is to employ infoNCE loss for optimizing the two tower model. To achieve bidirectional candidates generation, the authors propose to do negative sampling in batch from both user side and item side, and use the Bi-InfoNCE loss for optimization.","This paper jointly considers the problems of recommender systems (RS) and advertising systems (AS) as it is a coin’s two sides. The key is to model the joint probability of (user, item) instead of conditional probabilities (user | item) and (item | user) independently. The proposed model adapts an existing large-vocabulary loss function to the optimization objective. Experiments on real-world datasets are conducted to compare with both Matrix Factorization and Sequential Modeling methods.",0.288135593220339,0.2542372881355932,0.2711864406779661,0.22580645161290322,0.16129032258064516,0.25862068965517243,0.1827956989247312,0.25862068965517243,0.21621621621621623,0.3620689655172414,0.20270270270270271,0.20270270270270271,0.2236842105263158,0.25641025641025644,0.24060150375939848,0.27814569536423844,0.17964071856287425,0.2272727272727273
33,SP:06309da5b76c411a1c746affb2357894dafa9b59,"This paper propose one-step look ahead DRO for multitask learning considering the worst task’s performance as the objective. Due to the task’s interaction with each other, directly using gradient-based approach is suboptimal. To overcome, look-ahead DRO estimates the task interaction using Taylor expansion and returns the locally optimal weighting by solving a linear min-max subproblem. Several tricks for mini-batch and online updating version of DRO are proposed.","This paper studies balancing average case and worst case performance in a multi-task setting. The authors propose a new algorithm that extends the group DRO algorithm by 1. ) taking task interactions into account 2.) enforcing the maximum entropy principle when breaking ties. The interactions between tasks are modeled by a first order approximation to change in training loss after a single gradient update. Empirically, the authors propose methods to stabilize their estimation of the ""task interaction"" estimation by a online weight update procedure and also introduce an additional technique that re-normalize gradients to unit norm before calculating the ""task interaction"" estimation. In addition, the authors observed that the proposed algorithm only needs to update the task interaction estimation periodically during training, which reduces the computational overhead.   The submission empirically test the proposed algorithm on CIFAR100 and Multilingual Language Modeling. On CIFAR100, the proposed method achieves improved performance for smaller-scale neural networks but does not achieve significant improvement on larger neural networks. On Multilingual Language Modeling, the proposed method achieves best average performance and second best worst case performance (trailing a computationally expensive baseline).","In this work, the authors study the problem of multi-task learning in the presence of heterogenous tasks when there is an imbalance in the distribution of tasks. In such settings, we can learn models for multi-task learning using two approaches -- a) learn to minimize average loss across tasks, b) learn to minimize the worst case loss. Turns out both the approaches are not ideal. Minimizing average loss can compromise one task a lot and minimizing worst case loss can focus only on one very difficult task at the expense of improvements possible in other tasks. The authors propose a look ahead distributionally optimization based approach to balance the two -- worst case performance and the average performance. In the proposed approach, instead of solving a standard min-max optimization, the authors propose to constrain the min learner to find the optimal weight combination for weighted risk minimization, which leads to the minimum worst case risk. The authors carry out synthetic and real datasets to demonstrate the efficacy of the approach.","This submission proposes Lookahead-DRO, of which the core idea is to do DRO on the updated weights, i.e., to choose a weight that minimizes the DRO loss in the next step. The computation is not tractable so the authors propose to use first order Taylor expansion approximation. Experiments on synthetic datasets and two real-world datasets demonstrate its effectiveness.",0.24324324324324326,0.28378378378378377,0.14864864864864866,0.17204301075268819,0.08602150537634409,0.11695906432748537,0.0967741935483871,0.12280701754385964,0.18032786885245902,0.1871345029239766,0.26229508196721313,0.32786885245901637,0.13846153846153847,0.17142857142857143,0.16296296296296295,0.17927170868347342,0.12955465587044537,0.17241379310344826
34,SP:067094eb895cfb0284a0caf20936d7e14d67a83f,"The paper considers the problem where jobs of unknown weights and durations drawn from some distribution are available and need to be scheduled so as to minimize the sum of their weighted completion time (the paper calls this holding time). It is well known that when weights and durations are known then the so-called Smith's rule is optimal and the same is also the case when the durations used in Smiths rule are the ""mean values"" of the corresponding distribution(s).  Consider that the jobs can be preempted, i.e., the algorithm can choose at each time slot the job which Smiths rule would pick by using the aforementioned ""mean"" duration and the so-far learned information on the weight. Then in the worst case, all jobs would have weights that are close to each other, which would result in a -- more or less -- round robin schedule that completes all jobs close to the end of the schedule resulting in unnecessary delays, and in turn also regret.   On the other hand, an algorithm that non-preemptively picks a job and sticks to it, may select a job with an arbitrary weight first resulting in arbitrarily high regret.   The authors propose an algorithm that runs the preemptive schedule for some time-period before then switching to the non-preemptive one. The idea is that although, and as just described, at the extreme cases when the preemptive period has length either 0 or infinity the regret increases substantially, one can carefully choose a length so as to have learned enough information about the job weights by the end of that preemptive phase (intuitively, if the non-preemptive phase still processes two jobs in the wrong order then they have similar weights so it is not too bad), while at the same time not running the preemptive phase for too long. This results in an algorithm with sublinear regret.","Proposed work is considering a stochastic scheduling problem where we are given a single server and a set of jobs with unknown cost profile. It is well known that when the cost profile is known Smith's rule is optimal. Using this as a baseline, the authors define a natural regret measure. The central result is algorithm with sublinear regret and a roughly matching upper bound. ","This paper studies a scheduling problem in which there are multiple jobs available in the system each with a different holding cost and service time. This scheduling problem is studied previously for the case that holding cost is deterministic and known and the service time is stochastic but unknown. This paper tackles the case in which holding cost is unknown and service time deterministic/stochastic and known. Then based on some interesting motivating examples, an algorithm is proposed that starts with preemptive scheduling and after a while, it switches to a non-preemptive mode. The regret of the proposed algorithm for both deterministic and stochastic service time is obtained and by characterizing the lower bound, the authors show their policy is near-optimal. Last, there is one additional instance-dependent regret analysis that is useful when some additional information is known about jobs. Numerical experiments clearly demonstrate the improved performance of the proposed algorithms is compared to full preemptive of full non-preemptive alternatives. ","The paper presents algorithms for the problem of ordering jobs on a machine so as to minimize the cumulative holding cost. The assumption is that the holding cost in each timestep follows a sub-Gaussian distribution (i.i.d. across timesteps), but the the mean values of these distributions for individuals jobs are unknown to the algorithm. The lengths of the jobs are known upfront, or in an extension considered later in the paper, are generated from geometric distributions whose mean values are known upfront to the algorithm. If holding costs and job lengths were both known, then the optimal strategy is to process jobs in decreasing order of the ratio of the holding cost and job length. One natural strategy would be to implement this same rule, but with estimated mean holding costs based on previous iterations. This has the advantage of not committing to any job upfront, and therefore, being able to adjust to estimation errors of the holding costs. But, this strategy has the downside that if the holding costs are similar, then it can process the jobs round robin thereby keeping all jobs in the queue longer and incurring a large cost. The alternative is to commit to a job that has the largest value of this (estimated) ratio, and this strategy works well when the ratios are close to each other but not when they are well-separated, since estimation errors can be costly in the latter case. The algorithm proposed in the paper is natural combination of these ideas: start with a learning/exploration phase where you use the former strategy thereby not committing to any job but learning their holding costs, and then in the exploitation phase, commit to the job with the largest (estimated) ratio and run it to completion. In addition to obtaining regret bounds for this algorithm, the paper also gives lower bounds on regret that can obtained by any algorithm, and does numerical experiments to evaluate the performance in practice. ",0.0880503144654088,0.13836477987421383,0.20125786163522014,0.3181818181818182,0.2878787878787879,0.23780487804878048,0.42424242424242425,0.2682926829268293,0.1933534743202417,0.12804878048780488,0.05740181268882175,0.11782477341389729,0.14583333333333331,0.18257261410788383,0.19722650231124808,0.1826086956521739,0.09571788413098237,0.15757575757575756
35,SP:0749dc4e8d674165f59674dfb27595e14a8579bf,"The paper proposes another explanation for why SimSiam can avoid collapse without negative samples. Specifically, the paper decomposes the gradient of learned representation as center vector and residual vector and finds that the center vector gradient has the de-centering effect and the residual gradient vector has the de-correlation effect. Such an explanation can also be applied to Info-NCE, which unifies the theory of self-supervised learning with and without negative samples.","As the title suggests, the paper does a detailed investigation of how SimSiam avoids collapse without negative training examples. The key idea is to decompose the original vector into a center vector component and a residual vector component. The center vector cannot be too large (otherwise it is indicating collapse). The high-level idea is that the designs in SimSiam (and contrastive frameworks that have InfoNCE loss)  are mainly to prevent the center vector from getting too large. There are conjectures (verified empirically) about the relationship between the gradient w.r.t. the center vector ands the gradient w.r.t. the residual vector, and with these the paper finds that for SimSiam, the predictor is important for preventing collapse, particularly by doing de-correlation among features.",This paper proposes a framework to understand why SimSiam avoid collapse without negative samples? It provides a hidden flaw of the Alternating Optimization for explain why SimSiam works. And the authors claim that  the center vector gradient has the de-centering effect and the residual gradient vector has the de-correlation effect.,"The paper analyzes how the self-supervised learning (SSL) approach SimSiam avoids collapsed representations without explicit formulation of repulsive sample relations. To this end, first flaws in the original reasoning of the SimSiam paper are revealed. Next, based on center-residual vector decomposition, the role of the prediction head for preventing representation collapse in SimSiam is analyzed. Results indicate the importance of de-centralization and de-correlation as driving concepts for stable SSL.",0.35135135135135137,0.4189189189189189,0.20270270270270271,0.1732283464566929,0.1732283464566929,0.25,0.2047244094488189,0.5961538461538461,0.2054794520547945,0.4230769230769231,0.3013698630136986,0.1780821917808219,0.25870646766169153,0.49206349206349204,0.20408163265306123,0.2458100558659218,0.22,0.208
36,SP:075a0f5059375273cf164aa933c57ea6fb458b51,"The authors build on a striking empirical observation of Nakkiran & Bansal—namely, that if two neural networks with the same architecture are trained on independently drawn training sets, achieving generalization error $\epsilon$, then their rate of disagreement on a test set is typically nearly equal to the test error of the two networks. In this paper, it is shown that this observation also holds even if the two networks are trained on the same data, but with different random seeds (data ordering and/or initialization).  The authors prove that test error will equal disagreement rate _in expectation_ for any stochastic learning algorithm whenever the algorithm's predictions have a certain calibration property.","This paper builds upon and extends from Nakkiran/Bansal (2020). First, it shows empirically that models learned from two independent runs of SGD on the same training set have their prediction disagreement highly correlated (and nearly equal) to the test error of the models.  It then attempts a theoretical explanation for this phenomenon. Under two notions of ""calibration"", the paper proves that when the learning algorithm is well-calibrated, a ""generalization disagreement equality"" holds (i.e., disagreement in prediction is equal to the test error in expectation).  The paper performs further experiments and provides empirical evidence suggesting that SGD is well calibrated.  The paper also proves that the gap between the test error and the prediction disagreement is upper bounded by a calibration error.","This paper shows empirically that the test error of a network is approximately equal to the disagreement rate between two separate training runs of the network, measured on unlabeled test data. This is true even when the two training runs are on the same training set, and with varying sources of stochasticity (e.g. random seed, ordering of training data). The authors show theoretically that this phenomenon is due to ensembles trained with SGD being well-calibrated. ","This paper identifies and studies a new phenomena in the generalization of deep learning. First, it presents the observation of Generalization-Disagreement Equality (GDE) between same models trained with different stochasticity, strengthening the similar observation of Nakkiran and Bansal (2020). Second, it shows theoretically that calibration of ensembles (in both the per-class sense and a suitable aggregated sense) implies GDE, with accompanying experiments on their relationship.",0.2767857142857143,0.23214285714285715,0.125,0.18548387096774194,0.12903225806451613,0.19480519480519481,0.25,0.33766233766233766,0.208955223880597,0.2987012987012987,0.23880597014925373,0.22388059701492538,0.2627118644067797,0.2751322751322752,0.1564245810055866,0.2288557213930348,0.1675392670157068,0.20833333333333334
37,SP:07608ef77f2d3d1e6ec32e9756ec5f0be8684d18,"- **Motivation**. The paper argues that the original cross-entropy loss and focal loss are primarily used in the classification task.  A general loss function should get rid of constraints of learning tasks and datasets.    - **Method**. Motivated by this insight, the paper expresses the loss function as a linear combination of polynomial functions and shows that cross-entropy loss and focal loss are special cases.  Then inspired by Taylor expansion, the paper proposes a polynomial loss function, called PolyLoss. The final version $\text{Poly}-1$ is with first-order.    - **Experiments**. The paper verifies the proposed loss on ImageNet for classification, MS-COCO for detection and instance segmentation, and Waymo Open dataset for 3D detection.","This paper analyzes the cross-entropy loss and focal loss through the polynomial expansion perspective. Further, this paper proposes a family of loss functions called PloyLoss. Three instances of PloyLoss are analyzed and obtain the final version as ploy-1. Detailed and extensive experiments are provided."," In this paper, the authors introduce a new loss function for classification problems. To be specific, the authors introduce Taylor expansion of cross-entropy loss + focal loss and show that various subsets of this expansion can improve the models on image classification, 2D object detection and 3D object detection problems.  I've reviewed a previous version of this paper at NeurIPS2021. Compared to the previous version, I see that the authors improved the paper significantly, mainly in terms of structure and presentation.","The paper proposes a framework for creating loss functions based on the polynomial expansion of known loss functions. The authors show that, by fine-tuning the polynomial coefficients of those expansions can bring improvements in multiple computer vision tasks. The authors experiment with image classification (ImageNet-21k), instance segmentation and object detection (COCO), 3D object detection (Waymo), showing small improvements over the chosen baselines.   ",0.18584070796460178,0.17699115044247787,0.168141592920354,0.30434782608695654,0.2391304347826087,0.24390243902439024,0.45652173913043476,0.24390243902439024,0.296875,0.17073170731707318,0.171875,0.3125,0.26415094339622647,0.20512820512820512,0.21468926553672316,0.21875,0.2,0.273972602739726
38,SP:07773157ab0268944c905da340164535baa7df96,"Methods for embedding hypergraph-based knowledge bases, where different edge types encode different relation types, have a long history in machine learning. Alternately, there has been a more recent interest in geometric embedding approaches for knowledge bases, which associate more complex geometric objects, such as hyperbolic cones, with knowledge base entities, due to the favorable geometric properties of hyperbolic space for modeling hierarchies. However, the latter approaches have usually been limited to representing only single relation types, due to representing the relation of interest with some geometric property of the embedding space such as enclosure between entity cones. This work attempts to bridge the gap between the two approaches by modeling relations as transformations between hyperbolic cones, specifically containment in different subspaces. ","This paper proposes a knowledge graph (KG) embedding method that can simultaneously model hierarchical (e.g., is-a) and non-hierarchical (e.g., friend-of) relations. The authors propose to embed entities into hyperbolic space. Inspired by Hyperbolic Entailment Cones (Ganea et al., 2018), they apply the cone containment constraint to relation-specific subspaces to deal with hierarchical relations. Inspired by RotatE (Sun et al., 2019) and RotH (Chami et al., 2020), they use hyperbolic cone rotations from the head entity to the tail entity to model non-hierarchical relations.  Experiments are conducted on two tasks: KG completion and hierarchical reasoning. Results show that the proposed method can outperform both Euclidean and hyperbolic KG embedding baselines. Results in low dimensions, ablation studies, and results on a biological dataset are further shown in the supplementary material.",The authors propose ConE (Cone Embedding) to simultaneously model multiple hierarchical and non-hierarchical relations in a knowledge graph. ConE represents entities as hyperbolic cones and models relations as transformations between the cones. Experiments demonstrate that ConE outperforms many baselines on standard knowledge graph benchmarks. ,"This work proposes ConE, a knowledge graph embedding model which embeds entities into hyperbolic cones and represents relations as transformations between the cones. Non-hierarchical relations are modelled using rotations and hierarchical relations using restricted rotations, which impose the cone containment constraint. The proposed method is evaluated on the KG completion and ancestor-descendant prediction tasks.",0.13934426229508196,0.09836065573770492,0.09836065573770492,0.1259259259259259,0.17777777777777778,0.35555555555555557,0.1259259259259259,0.26666666666666666,0.21428571428571427,0.37777777777777777,0.42857142857142855,0.2857142857142857,0.13229571984435795,0.1437125748502994,0.1348314606741573,0.18888888888888888,0.2513089005235602,0.31683168316831684
39,SP:07a8240bbbb9c1de11e1ba295a033f43cf2fe93f,"This paper presents a comprehensive analysis of spectral clustering based approaches for node classification in graph-based data. The paper thoroughly analyzes design decisions of using spectral clustering in this supervised setting. The empirical results indicate that even compared to GCN / deeper approaches, the less parametric spectral clustering-based approach can be competitive or better at classification tasks. ","The paper argues for refocusing the efforts of the graph neural network community onto previous work on spectral clustering. The authors propose the addition of simple, but structured, classification methods to an existing unsupervised spectral clustering algorithm. A battery of empirical results on standard graph benchmarks demonstrate the effectiveness of such simple models and compare favourably to existing, more complex models.","The paper proposes a study about spectral clustering techniques. Given the standard spectral clustering approach, some variations are investigated, also using recent advancements in the field. The experiments show that such a simple approach can produce state-of-the-art results on social graphs.","This paper reviewed the application of spectral clustering embedding on node-level classification problems. It first revisited the theoretical foundation of spectral clustering, related to the graph NCut problems. Then it introduced neural network-based classifiers which utilize the embedding for classification tasks. Their benchmark results validated that the neural network framework showed certain advantages in accuracy. And the ""correction & smooth"" introduced by Huang et al (ICLR 2021) would also improve the performance in a relative big margin.",0.1896551724137931,0.20689655172413793,0.27586206896551724,0.18032786885245902,0.19672131147540983,0.22727272727272727,0.18032786885245902,0.2727272727272727,0.20512820512820512,0.25,0.15384615384615385,0.1282051282051282,0.18487394957983191,0.23529411764705882,0.23529411764705882,0.20952380952380953,0.17266187050359713,0.16393442622950816
40,SP:07bf013733ebff776af0148f30b28cc6c77a7b33,"Topic model evaluation in terms of the quality of learned topics is an important problem in topic modelling, the validation of which has a profound impact on the model development and its application in cross disciplines. This paper reassesses the automatic topic model evaluation metrics, e.g., topic coherence scores like NPMI, that are commonly adopted in the topic modelling literature. It points out the potential flaws of applying those measures to neural topic models developed recently, via conducting a set of human evaluations. The paper identifies two gaps existing in the current evaluation paradigm used in almost all NMTs, the validation gap and the standardisation gap through analysing a large number of NTMs. Its findings could have wide implications in the practice.","This paper investigates automated evaluation metrics of topic models in terms of coherence such as NPMI. NPMI is widely used to evaluate the performance of topic models, but the authors tackle that it is less related to human evaluation metrics such as rating and intrusion. For experiments, the authors train three topic models, LDA, D-VAE, and ETM with two different corpora, Wikipedia and new york times. To evaluate the topics by humans, the authors ask at least fifteen crowdworkers per topic to avoid insufficient power of human evaluation problem. Experiments show that D-VAE outperforms other models in terms of NPMI, but there are no significant differences among models by human evaluations.   The main strength of this paper is claiming the usefulness of automated evaluations of topic models with deep considerations that follows recent concerns of NLP research evaluations. I really enjoy reading this paper since it can guide the way to compare the two or more different evaluation metrics in various domains including topic modeling. ","This paper questions the use of automatic metric, such as the ones based on NPMI, for evaluating the quality of topic models. It sets up a precise, reproducible framework (with code) that compare classic approaches (LDA) and recent ones based on neural networks (D-VAE). From my point of view, it is more a positioning paper that ca n help the community to change its habits than a novel solution to a given problem. However I think we need this type of papers which can lead to designing better models.","The paper addresses standardization gap in automatic metrics used for evaluation of topic modeling as well as a validation gap for the coherence of those automatic metrics with the human judgement, both revisiting the models for which such validation had been done (LDA) as well as the recent Neural Top. Models which have been purely evaluated using the automatic metrics.  The authors provide a deep investigation on the automatic metrics currently in widespread use within Topic Modeling as well as a historic overview on the last studies of the coherence of the automatic metrics (NPMI) and human judgements.  One of the main contributions of the paper is the rigorous study and comparison of the human judgements and the automatic metrics NPMI and C_v ensuring the statistical power of the test (involving large-scale evaluation by crowdsourced judges) making this work a steady reference point for future works.",0.21138211382113822,0.12195121951219512,0.21138211382113822,0.1317365269461078,0.18562874251497005,0.17777777777777778,0.15568862275449102,0.16666666666666666,0.17567567567567569,0.24444444444444444,0.20945945945945946,0.10810810810810811,0.1793103448275862,0.14084507042253522,0.1918819188191882,0.17120622568093388,0.19682539682539682,0.13445378151260504
41,SP:08945ecebc73e7b3bd9dd2ae47c0d94311fb69b9,"This work proposes SPANDROP, a simple variant of dropout, working on the spans of long sequences. SPANDROP randomly ablates parts of a sequence at a time and asks the model to perform the same task to emulate counterfactual learning and achieve input attribution. The method is tested on both toy tasks and four NLP tasks.","The paper focuses on distilling supervision signal from long sequences. They focus on cases where the input is a long sequence of length n, but the target prediction is determined by a small subset of size m of sequence fragments, where m << n. The authors propose augmenting data by randomly dropping spans from the input sequence. They first propose SpanDrop which removes each span with a probability p. This process might result in shorter sequences, resulting in shift of training data distributions. As a fix, they also propose Beta-SpanDrop where the spans are dropped using probabilities sampled from beta bernoulli distribution. Beta SpanDrop preserves the length of the original utterance with higher probability, while generating similar variety of augmented utterances.   ","In this paper, we instead investigate learning problems for long sequences where not all input elements contribute equally to the desired output. SCANDROP is a simple algorithm to randomly drop segments in a sequence. The authors first establish that when the number of contributing segments is sparse, the algorithm will preserve them with a relatively large probability. Then, Beta-SCANDROP is proposed to preserve the original sequence length with higher probability. In experiments, consistent improvement is shown. ","This paper proposes a data augmentation method, SpanDrop (and its variant Beta-SpanDrop), for long sequence data, especially where supporting facts take small portions. They provide a theoretical background on their method and evaluate the method on a synthetic task (FindCats) and four real natural language processing tasks requiring reasoning over long texts. SpanDrop is effectively improves the accuracy in both low-resource and abundant-resource settings. ",0.18181818181818182,0.2,0.2545454545454545,0.17355371900826447,0.08264462809917356,0.14285714285714285,0.08264462809917356,0.14285714285714285,0.208955223880597,0.2727272727272727,0.14925373134328357,0.16417910447761194,0.11363636363636365,0.16666666666666666,0.22950819672131145,0.21212121212121213,0.10638297872340426,0.1527777777777778
42,SP:08a291fdb9d394c11dbe0ed0a454be951740491f,This paper proposes a novel linear complexity self-attention module that (1)	first replaces the softmax attention with the Gaussian kernel (2)	then apply Nystrom approximation to calculate the gram matrix with O(n) complexity. (3)	whose bottleneck samples are calculated using average pooling or convolutional layer.  The proposed method demonstrates good top-1 accuracy on ImageNet classification with low space and time computational complexity.,"The paper substitutes softmax self-attention with a Gaussian kernel function. This allows to approximate the self-attention matrix with a low-rank approximation. The robustness of the approximation is supported by theoretical guarantees, while previous works that don't provide these guarantees. The proposed method is evaluated on ImageNet, arguably the most popular benchmark for evaluating computer vision models, and shows impressive results when compared against other Linear Transformer alternatives and even when compared against the standard (full) self-attention approach.","This paper proposes a novel method named SOFT to calculate the token similarity for self-attention without softmax operation. The authors state that the computational complexity could be reduced to O(n) with SOFT. A family of backbones are designed and evaluated on ImageNet, which achieve SOTA performance. ","The paper proposes a softmax-free self-attention that replaces the previous dot-product between the query matrix and the key matrix with a Gaussian kernel. To facilitate the proposed softmax-free self-attention, this paper further proposes to decompose the self-attention matrix with Singular Value Decomposition. The robustness of the proposed low-rank decomposition can be theoretically proved by computing the Moore-Penrose inverse with the Newton-Raphson method.",0.24615384615384617,0.23076923076923078,0.27692307692307694,0.14634146341463414,0.2804878048780488,0.22916666666666666,0.1951219512195122,0.3125,0.2535211267605634,0.25,0.323943661971831,0.15492957746478872,0.217687074829932,0.2654867256637168,0.2647058823529412,0.1846153846153846,0.30065359477124187,0.18487394957983194
43,SP:08f4cb10c1ccf3885e7cc5ee10a24615ac74bc69,"This paper proposes to jointly optimize imputation and regression to learn from data with missing values, backed by theoretical analysis of impute-then-regress procedures. In particular, the authors show that the impute-then-regress procedure is asymptotically Bayes optimal for almost all (smooth) imputation functions, regardless of the missing data mechanism (including MNAR). However, apart from restrictive special cases, learning the regression function for conditional imputation to obtain a Bayes optimal function is hard due to discontinuities; similar with correcting the imputation for a given oracle function. The authors thus propose to jointly optimize impute-then-regress procedures, in particular chaining a NeuMiss architecture with an MLP. The proposed approach is empirically evaluated on synthetic data with various combinations of imputation and regression methods.","In handling missing data, a standard approach is to simply impute and then run a prediction model that requires fully-revealed data. However, this standard ""impute-then-regress"" approach is largely a hack without theoretical grounding. This paper provides theory to justify this standard approach, with a surprising finding that it turns out that for almost all imputation functions, impute-then-regress with a sufficiently powerful learner is Bayes optimal. Moreover, the paper theoretically justifies why imputation and regression should be done jointly rather than separately as doing so separately can require learning discontinuous functions that are harder to learn. Numerical experiments corroborate the theoretical findings.","The paper first seeks to study the theoretical properties of impute-then-regress procedures -- which are widely used. They show that most any imputations admit Bayes consistent estimators; however, that estimation may be difficult in certain settings. They show that conditional imputation is not necessarily an optimal strategy, supply bounds on the associated risk, and discuss when it can be expected to perform reasonably well. In particular, conditional imputation under some conditions requires learning a discontinuous regression function for optimal performance. Given all this, they examine the conditions under which imputations exist such that impute-then-regress procedures are optimal. Lastly, they propose a procedure for joint optimization of an imputation and a regression scheme that is empirically shown to 1. outperform common impute-then-regression procedures as well as gradient boosting which naturally can accommodate missingness and 2. perform similarly to oracle estimators in certain settings. ","The authors look at the impute and regress framework and provide arguments as to Bayes optimality of the process for missing data mechanisms and imputation functions. It gives a general theoretical framework for the impute and regress procedures. Unlike the missing at random setups, they look at more general missing value mechanisms. They also argue that the imputation and regression processes should be properly aligned with each other and also provide a method for optimizing the two processes jointly.",0.168,0.24,0.168,0.18867924528301888,0.1509433962264151,0.1292517006802721,0.19811320754716982,0.20408163265306123,0.26582278481012656,0.1360544217687075,0.20253164556962025,0.24050632911392406,0.18181818181818185,0.22058823529411764,0.20588235294117646,0.158102766798419,0.17297297297297295,0.168141592920354
44,SP:092f3b4cf74f66907abe82f4d528d0af5d19fd74,"The goal of the paper is to show that learning a representation to ground language can improve communication in multi-agent settings.  Their method: each agent encodes an image and the other agents messages (through a small conn-net and mlp, respectively). A GRU-based policy outputs an action. A communication autoencoder predicts a message from the image feature and is trained with an auxiliary construction objective (i.e., to reconstruct the image).  They evaluate their model on a referential game with CIFAR images and two grid environments: one where the agents must reach a goal collaboratively and another where one agent has to open a door for another to pass.  They show that their method with an auto encoder strongly outperforms the non-autoencoder variants on the CIFAR task and door task.","The paper proposes to learn the communication protocol in an emergent communication environment via an autoencoder; i.e. they train an autoencoder to learn a discrete representation of the image part of the observation, and use that as the message that gets broadcasted to all other agents at each timestep. The paper shows that this achieves significant gains over learning the communication protocol from scratch in a decentralized way.","This paper proposes an approach towards encouraging the development of communication protocols in multi-agent reinforcement learning systems. The authors rightly observe that naive training of a communication policy according to task reward often fails, due to high variability in the initial messages, the difficulty of connecting this communication to reward, and lack of common grounding among the agents.  The authors' idea is to imbue agents with a message-generating mechanism cast as an autoencoder, where the agents aim to generate discrete messages that can reconstruct the observations of the agents. Thus, the message generated for a given timestep is essentially a discretized summary of an agents' observation at that time. This encourages the messages to be maximally informative about an agents' current state—something that is shown by qualitatively.  The authors evaluate this communication idea on a variety of environments, from a reference game variant to a few multi-agent minigrid environments, and show that their autoencoded communication outperforms (1) non-communicative baselines and (2) communication policies trained with standard RL objectives. They then have a few analyses that show that the agents are making use of the generated messages, in that (1) messages are informative about agents' current states (by design) and (2) messages affect other agents' entropy over decisions once they indicate a change in state.","In this paper, the authors present a novel method for learning decentralized communication policies in fully cooperative multi-agent RL. The method is based on autoencoding agent observations and using these as messages, thus sidestepping the usual ""chicken and egg"" problem for simultaneously learning speaker and listener policies, which is a hard joint-exploration problem. The authors present results in which this method outperforms both non-communication and RIAL baselines, and include some analyses and ablations that shed light on how and why this method is successful.",0.13533834586466165,0.24060150375939848,0.15037593984962405,0.34782608695652173,0.15942028985507245,0.1187214611872146,0.2608695652173913,0.1461187214611872,0.22988505747126436,0.1095890410958904,0.12643678160919541,0.2988505747126437,0.1782178217821782,0.18181818181818182,0.18181818181818182,0.16666666666666666,0.14102564102564102,0.16993464052287582
45,SP:095d27873ce7f48cbaba835589a8202b75104924,"This paper proposes to model a time series directly with a neural controlled stochastic differential equation with multiple control agents. It introduces a concept of temporal privacy, which defines the attention of each control agent to be a certain time interval. Then the authors introduce Markov dynamic programming to efficiently minimize a loss function defined in terms of trajectory of the stochastic differential equations. In order to make the proposed temporal privacy work, the authors proposed two loss function: 1) MFcond minimizes the incoherence between the future estimates starting at any intermediate time stamp, 2) MBcond is to ensure the theoretical optimality of the control agents. ","This paper presents a new approach to train stochastic dynamical systems using a set of tools from stochastic optimal control theory. The methodology is built upon _controlled stochastic differential equations_ with multiple control agents that modulate the dynamical evolution. Each agent is deliberately chosen to be ""active"" in a certain time interval, which leads to so-called ""temporally private"" dynamics that allow for dynamic programming based optimization. Since the temporal segmentation of optimization objective requires arbitrary intermediate state $X_s$ with $0<s<T$ as initial values, the Markov forward condition (MFcond), which computes some sort of a mean value for future states, is proposed. To further stabilize the training and compensate for the methodology's lacking in theoretical optimality guarantees, the authors augment the original loss with the Markov backward conditional (MBcond) loss. The method is shown to outperform competing continuous-time methods on standard benchmarks. ","This paper presents a framework for learning a stochastic dynamics from observed trajectories. As opposed to conventional strategies based on recurrent neural networks, the procedure learns a model of the stochastic dynamics in real space. Secondly, in contrast to recent approaches based on neural ODEs / SDEs, the training is temporally localized through windowing functions that the authors refer to as temporal privacy. The main technical innovations involve the development of new loss functions that make learning with temporal privacy tractable. ","The authors propose a novel method to model stochastic dynamic of complex time series. They build connections between neural SDE and stochastic optimal control theory. By using the proposed MFcond and MBcond losses, they can train control agents to learn dynamics of time series more accurately than existing methods. ",0.3113207547169811,0.1792452830188679,0.12264150943396226,0.1292517006802721,0.10884353741496598,0.1,0.22448979591836735,0.2375,0.2653061224489796,0.2375,0.32653061224489793,0.16326530612244897,0.2608695652173913,0.2043010752688172,0.16774193548387098,0.16740088105726872,0.163265306122449,0.12403100775193797
46,SP:096121f11ca65612b8204b952097699aab1cab01,"This work focuses on improving Transformer inference efficiency by redesigning the aforementioned architecture such that the matrix multiplications can be sparse at inference time. This sparsity is introduced in the dense layers that compute the query, key, and value, and in the feed-forward layer as well. This results in a much faster decoding speed when compared to dense transformers.","The paper presents an approach to training large-scale sparse transformers by sparsifying the activations of linear layers in the network (feedforward and Q, K, V projections). The approach is particularly appealing because it doesn't appear to require any hardware-specific or low-level optimizations to be efficient since it sparsifies activations. A controller network produces a block-wise one-hot mask on the activations and weights for the subsequent layer are selected on the fly based on this mask. Gradients are propagated through this hard-selection operation via the straight-through Gumbel softmax estimator.  The authors are able to achieve fairly substantial inference time speedups via sparsification. They also apply the same approaches to sparsifying models that are tailored towards handling long sequences such as the Reformer with LSH attention.","Update: I've moved my score from 7 to 8.  The paper first introduces a sparse feedforward layer, which uses the input at every timestep to decide how to zero out the hidden vector (the one right in the middle) of the MLP feedforward layer. It uses the gumbel-softmax trick and zeros out (N-1) out of every N activations. This zeroing out of each hidden vector allows us to not use some of the rows & column of the W_1 and W_2 matrices of the feedforward layer.   The next method is the sparse QKV layer, where each input is compressed into a few smaller vectors that each do attention independently. The authors show that a convolutional layer can make this faster, without loss of accuracy, and also toss away the final output projection from their self-attention module.  The authors then apply a few off-the-shelf tricks to their transformer, such as reversible layers and replacing the enc/dec attention by attending to the concatenation of the enc/dec words.  The empirical results are very strong and show good performance on challenging tasks such as language modeling, summarization, and GLUE, while massively improving inference speed (training speed is not improved).   ","The paper proposes a variant of the Transformer layer that includes sparsified dense and attention layers. For FFN layers, the author introduces a controller layer that selects only a subset of hidden dimensions to be used in the computation. For the attention layer, The author proposes a multiplicative transformation to simulate the permutation of the input hidden states and applies a convolution layer on top of it. The author shows no accuracy drop with the proposed method and speedup in the decoding phase.  ",0.18333333333333332,0.21666666666666667,0.2,0.1893939393939394,0.13636363636363635,0.11274509803921569,0.08333333333333333,0.06372549019607843,0.14457831325301204,0.12254901960784313,0.21686746987951808,0.27710843373493976,0.11458333333333331,0.09848484848484848,0.16783216783216784,0.1488095238095238,0.16744186046511625,0.16027874564459932
47,SP:0965e7ac3664634b5cfdc7acd2acd6057b0d0927,"The paper addresses the task of detecting anomalies in multiple time-series, for the setting where multiple instances of a fixed time-window are available for learning. The authors propose a method that involves using a directed acyclic graph (DAG) to model the (contemporaneous) dependencies between the variables in the time-series. An RNN is applied to form a representative state for each time-series at each point in time. A mapping is applied using the DAG to process the states from the previous time-step and the ancestor states from the current time-step to construct a “dependency representation” vector for each time series. A normalizing flow is then applied to map to a base distribution so that the log-density of the instance (the observed multiple time-series) can be evaluated. The conditional densities for each time series can also be evaluated. The paper includes an evaluation of the performance of the approach on a proprietary dataset, with a comparison to state-of-the-art baselines. Some qualitative results are also included for the task of density estimation for a public traffic dataset. ","This paper focuses on unsupervised anomaly detection in multivariate time series. This is a important problem because anomalies are rare and labeling anomalies for supervised learning is labor intensive. To make progress in this challenging domain, they  use learned graph structure and normalizing flows, which seem well suited for the task. They evaluate their methods on multiple datasets, and compare against multiple modern deep networks. ","The authors propose a model for anomaly detection on multivariate, high-dimensional time series data, where there are statistical dependencies across the different time series (referred to as ""constituents"" in the paper). The authors model statistical dependence using a Bayesian network and temporal dependence using an RNN. One key feature of the proposed model is that it simultaneously learns the structure of the Bayesian network AND a representation of the temporal state. The authors combine these two representations, which then gets used as input to a normalized flow technique, which makes computation of the likelihood of the data mathematically and computationally feasible.  The authors show that the proposed model has improved performance for the anomaly detection task compared to recently-proposed neural-network anomaly detection methods.","This paper proposes to combine learning of the DAG structure between variables and learning of distribution using normalizing flows for unsupervised anomaly detection from multivariate time series. Once the distribution is learned, one can detect anomalies as low density regions. The effectiveness of the proposed method is empirically evaluated on real-world datasets.",0.08648648648648649,0.16756756756756758,0.08648648648648649,0.23076923076923078,0.2,0.1111111111111111,0.24615384615384617,0.24603174603174602,0.3018867924528302,0.11904761904761904,0.24528301886792453,0.2641509433962264,0.128,0.19935691318327975,0.13445378151260504,0.15706806282722513,0.22033898305084745,0.1564245810055866
48,SP:097596ab2b4f87852126f755cf70ec66e5c6a9f4,"The paper discusses the k-median algorithm under the online, no-substitution setting. The paper considers the random order arrival of the points and obtains a constant factor approximation without requiring any structural assumptions. They provide an algorithm that gives a constant factor approximation with a $k\log^2k$ centers assuming the $n$ is known (in the discussion they state about the use of doubling to get similar results with $\log n$ overhead).","The paper studies $k$-median clustering in the random-order streaming model. In this particular setting, the input is randomly permuted and fed to the algorithm element by element. The input itself is a unknown finite set of points that together with a distance function forms a discrete metric space. The goal of the algorithm is to construct a clustering, by selecting a only small number of the input points as cluster centers, such that the cost of the clustering approximates the cost of an optimal $k$-median clustering of the whole space. Imposed restrictions are that the algorithm can only pick an input element as cluster center before it sees the next element and centers that have been chosen can not be substituted.  The paper contributes an algorithm that returns with high probability of success a clustering with $O(k \log^2(k/\delta))$ centers that approximates the cost of an optimal $k$-median clustering by a constant factor.",The authors design a sequential random-order no-substitution algorithm for k-median clustering. Their algorithm outputs O(k\log^2 k) centers and gives constant factor approximation with high probability. The algorithm does not depend on any assumption on the dataset and gives substantially better guarantees over earlier works.,"This is a theoretical paper, studying sequentially assigning points as k-median cluster centers.   It assumes a random arrival order, but has no other structural assumptions.  It offers a constant factor approximation using O(k log^2 k) centers.  This improves on a recent line of work in this model removing structural assumptions and the approximation factor.    The main idea is using geometrically increasing prefixes of the stream to estimate the k-median solution and risk, and for each using this estimate to choose centers.  A key insight is that by increasing the size of centers maintained a bit, the risk can be approximated after excluding outliers -- which turns out to be the key step in the analysis.   ",0.3013698630136986,0.1643835616438356,0.273972602739726,0.1,0.15,0.3,0.1375,0.24,0.1694915254237288,0.32,0.2033898305084746,0.1271186440677966,0.1888412017167382,0.1951219512195122,0.2094240837696335,0.15238095238095237,0.1726618705035971,0.17857142857142855
49,SP:097676b70983ff006342997928f26a7ab4eb36f7,This study presents a label encoding method (binary encoded labels) to transform a regression problem into multiple binary classifiation problems. The authors designed encoding and decoding functions to encode a real-value into multiple classes and decode the classes into the real value. The proposed demonstrated improved performance on benchmark datasets. ,"This paper analyzes regression problems in which continuous targets are predicted using a set of binary classifiers. There is a large design space including quantization, encoding, decoding, and loss functions. An expected value-based function (D^{GEN-EX}) is proposed as a new decoding function.  The authors apply various combinations of these functions and compare them theoretically and experimentally. It is suggested that no single combination outperforms the others, but the proposed BEL approach has a potential to improve the prediction accuracy of continuous regression problems. ","The paper describes a general framework to solve a regression problem as a binary classification. The labels (targets) are binary encoded and hence the regression problem is solved using a set of binary classifiers. Since the process is dependent by the type of Encoding and Decoding used, the authors compare different type of configurations showing, empirically, the effect of different design parameters on the final accuracy.","The paper tackles regression problems using an error-correcting code (ECOC) approach, i.e., reducing a *regression* problem into multiple binary *classification* problems. ECOC have been so far mainly used for classification tasks, and their application to regression problems in this paper is elegant and seems novel. The paper examines several combinations of (encoding method $\times$ decoding method $\times$ loss function) on several (deep) regression tasks and reports significant improvements over existing methods. Some brief analytic discussion is made.",0.23529411764705882,0.2549019607843137,0.23529411764705882,0.23255813953488372,0.1511627906976744,0.22727272727272727,0.13953488372093023,0.19696969696969696,0.1518987341772152,0.30303030303030304,0.16455696202531644,0.189873417721519,0.17518248175182483,0.2222222222222222,0.18461538461538463,0.2631578947368421,0.15757575757575756,0.2068965517241379
50,SP:09bb1ac6ec98450c2decddd7db64c80a703c8716,"The paper is motivated by group coordination through natural language and creates a goal of developing agents that can communicate effectively with other agents and humans. They build off of work on emergent communication and their method is most related to L2C [Lowe19], which uses a meta-learning approach to learn communication protocols.  Their training method is as follows: ``` Train with some human data while not_converged:     for n_meta steps:         Learn a meta-listener on a population of speakers     for n_selfplay steps:         Learn new speakers through self-play with the meta-listener     for n_supervised:         Fine-tune on human data train  ```  And their evaluations are: 1. Image based referential games 2. A text based referential game that they design where agents are trained to communicate in German about English sentences  Their experiments show that: 1. They outperform baselines on referential accuracy in both modalities 2. They have a high sentence quality (BLEU, caption score) wrt baselines 3. The meta-speaker in the text task works reasonably effectively as an English to German translation system (BLEU) 4. Their speakers and listeners perform better on human evaluation in the image task  Learning to learn to communicate. Lowe et. al., 2019.","In this paper, the authors aim at training agents that can coordinate with seen, unseen/human partners in a multi-agent communication environment. Instead of using static populations, the authors propose a dynamic population-based meta-learning approach that dynamically builds the population. Such an approach enables the trained agents to generalize to seen and unseen/humans partners. On the other hand, the authors try to use a limited amount of human data to fine-tune the agents to coordinate with humans using natural language. They also compare the proposed method and baselines on two referential games.","The current work proposes to use dynamic population based meta-learning in referential games between agents using natural language. In particular, it uses meta-learning to capture the diverse speaker/listener behaviors in a population and then initialize new agents by interacting with these meta-agents across iterations.  They also add supervised learning iterations to ensure there is no language drift. Experiments show that agents trained through the proposed approach have higher accuracy in the referential game interacting within themselves and also with humans, while retaining the naturalness of the language.","This work deals with the problem of multi-agent communication where the agents can be artificial or human. The latter means that the conversation has to be in natural language. New agents can be added or removed from conversation so the environment is not static. The authors propose a dynamic population-based meta-learning approach. They analyse the language skills of agents using BLEU, the quality of interaction in a human trial, as well as cross-play with unseen partners achieving overall good results.",0.11,0.115,0.08,0.18556701030927836,0.24742268041237114,0.16483516483516483,0.2268041237113402,0.25274725274725274,0.19047619047619047,0.1978021978021978,0.2857142857142857,0.17857142857142858,0.14814814814814814,0.15807560137457044,0.11267605633802817,0.1914893617021277,0.26519337016574585,0.17142857142857143
51,SP:09e78591958a3d9f6c27dbc76114c731b3be4108,"This paper develops a method for estimating cumulative effects of treatments when available data is limited to historical data of old treatments and short-term data of new treatments. A key challenge in this setting is that treatment assignments over time are correlated with each other, making it difficult to isolate the effects of individual treatments on the cumulative, long-term outcomes. This paper expands on previous methodology for this problem which make use of ""surrogate indices"" that mediate the effects of treatment on outcomes at each time step. Specifically, the authors estimate the isolated long-term reward of a treatment in the absence of future treatments. The methodology allows for continuous treatments and has double robustness properties (with respect to the surrogate index and surrogate score models). Semi-synthetic experiments show that the proposed dynamic adjustment approach lowers estimation error.","The paper considers the problem of estimating a long-term treatment effect. It considers the setting where there are two datasets: an observational dataset, which contains the long-term outcome, the treatment and the surrogate variable, and an experimental dataset, which contains only the treatment and the surrogate variable. The authors generalize the surrogate index method proposed by Athey et al. (2020), by considering dynamic treatment policies. Their new proposed method is a combination of the surrogate index approach, the double machine learning approach and the dynamic treatment effect estimation approach. ","This paper proposes an estimation strategy for treatment effects on long term outcomes in a difficult setting with sequentially correlated treatments. They consider a certain case where unobserved values of the treatment show up in a small ""new"" dataset without long term outcomes recorded but where there is access to an ""old"" dataset with old treatments that include long term outcomes. Assuming certain nuisance functions (conditional expectations of various parts of the graphical model) can be estimated consistently with [insert favorite ML method here], they use Double ML to derive the asymptotic normality of their treatment effect estimators, as well as double robustness with respect to two of the nuisance functions.",This paper tackles the problem of estimating the long-term effect of some new treatment in the case where the treatment assignment is decided adaptively. Athey et al.(2020) proposed the long-term effect estimation for the stable treatment assignment case. This paper applied the method proposed in Lewis & Sygkanis(2020) to adjust the approach of Athey et al.(2020) for the case where the treatment assignment is decided adaptively.,0.1702127659574468,0.18439716312056736,0.14893617021276595,0.18681318681318682,0.27472527472527475,0.16216216216216217,0.26373626373626374,0.23423423423423423,0.3,0.15315315315315314,0.35714285714285715,0.2571428571428571,0.20689655172413793,0.20634920634920634,0.19905213270142177,0.1683168316831683,0.31055900621118016,0.1988950276243094
52,SP:0a3be0e67b80f13fd2e7fdc523f0a8e78fe289dd,"The paper proposes a new interpretable training procedure for neural networks. The paper argues that the existing model’s prediction explanation through saliency maps uses backpropagation with noisy gradients. The approaches use gradient calculations to assign an importance score to individual features, reflecting their influence on the model prediction. As the importance scoring is dependent on the Gradients, noisy gradients can result in unfaithful feature attributions.  The problem was attempted to tackle by introducing an interpretable training procedure to reduce noisy gradients of the existing saliency methods: Gradient (GRAD), Integrated Gradients (IG), DeepLIFT (DL), SmoothGrad (SG), and Gradient SHAP (GS). The proposed interpretable training procedure (interpretable training) iteratively masks features with small and potentially noisy gradients while maximizing the similarity of the model outputs for both masked and unmasked inputs using a loss function that combines KL divergence and another loss function. ","This paper proposes a method (interpretable training) to improve the interpretability of saliency methods. The interpretable training method iteratively masks the features with small vanilla gradients and maximizes the similarity of the model outputs for both masked and unmasked inputs. They demonstrate this training procedure can improve interpretability in various data sets from CV, NLP, and time series across neural architectures, including RNNs, CNNs, and Transformers. They also find this method can reduce the vanishing saliency issue of RNNs. Lastly, they demonstrate improvements of the explanations produced after training by several gradient-based saliency methods.","The authors propose an interpretable training procedure, in which the input features with the bottom k input gradients are masked. The loss term is comprised of the original task loss, plus a KL divergence between the model outputs on the original and masked inputs. The authors perform experiments on 3 image datasets, 3 language datasets, and a suite of synthetic time series datasets. They use model accuracy drop as a metric to evaluate saliency map quality.","This paper addresses the noise in saliency map techniques, which seek to explain what part of the input is responsible for a model's output decision, by introducing a novel training paradigm in which noise is added to the least important input features and the model is encouraged to produce similar outputs for original and noise-augmented inputs. This paper shows extensive experimental results in a variety of ML domains, such as image, language, and time series data. Additionally, the paper adds interesting commentary on input feature replacement paradigms such as ROAR and OOD as well as analysis that shows how their interpretable training paradigm reduces the vanishing saliency problem in RNNs.",0.22535211267605634,0.1267605633802817,0.14788732394366197,0.2,0.2631578947368421,0.2631578947368421,0.3368421052631579,0.23684210526315788,0.1875,0.25,0.22321428571428573,0.17857142857142858,0.27004219409282704,0.16513761467889906,0.1653543307086614,0.22222222222222224,0.24154589371980675,0.2127659574468085
53,SP:0a79d57d55650ede0193daa2499d0fe3f4d68f4b,"The authors present SAC-I, which is a modified version of SAC designed for retraining of some existing agent on an updated environment or task. Inspired by the concept of inhibitory control from neuroscience, they learn a separate Q functions and optionally a policy for inhibition. They define some inhibitory reward $r_I$ for each environment to enable it, and make comparison with SAC on modified versions of LunarLanderContinuous and BipedalWalkerHardcore-v3.","This paper introduces an agent architecture with two independent policy networks (called ‘go’ and ‘stop’) and an assignment mechanism consisting of a network or a rule that activates one of the two every step. The two policy networks are in principle both soft actor-critic (SAC) agents, although the authors explore several variations on the degree of independence between them, with e.g. hyperparameters being tied in some cases. Each policy network is trained on replayed data from the subset of states that has been assigned to it by the assignment mechanism. ","This paper presents a method for transferring a pre-trained agent to a new task. The authors refer to this method as SAC-I, as they are combining ‘inhibition’ with soft actor-critic (SAC). The basic idea is to train a separate set of Q-networks and potentially an inhibitory policy network. These are then used with the existing policy. Various agents are presented on two Box2D environments from OpenAI gym. SAC-I generally improves over SAC.","The paper proposes inhibitory networks for reinforcement learning. Inhibitory networks choose a behavior among a set of low-level behaviors to be executed at the current state. The inhibitory network can take the form of a handcrafted rule (i.e., execute policy X if some state feature is active, else execute policy Y), or can be learned similarly to hierarchical policies. The method is implemented on top of soft actor-critic and evaluated in two OpenAI Gym environments, LunarLander and BipedalWalkerHardcore with some non-default modifications, and compared to a baseline standard SAC implementation.",0.1388888888888889,0.18055555555555555,0.18055555555555555,0.17391304347826086,0.17391304347826086,0.19480519480519481,0.10869565217391304,0.16883116883116883,0.13829787234042554,0.2077922077922078,0.1702127659574468,0.1595744680851064,0.12195121951219512,0.17449664429530204,0.1566265060240964,0.18934911242603553,0.17204301075268816,0.17543859649122806
54,SP:0a7bad2b9c6d5ef5b21a17593da68605b5befe2c,"- This paper introduces a novel GANsformer-based architecture for two-staged, compositional image generation. The first stage samples a panoptic-segmentation-like instance mask of the scene which is re-painted in the second stage to yield a high-fidelity image consisting of multiple objects. - The explicit architectural biases w.r.t. scene layout, object instances, depth ordering and appearance modelling enable explainable and controllable image generation. - The paper presents a thorough quantitative analysis of its proposed approach conducted on five suitable and challenging datasets and compared to seven adequate baselines models.",This paper proposes a novel method for compositional generation for 2D segment-like elements. It decomposes the task into two stages: a sketching stage for soft layout generation and a painting stage for refining the layout and generating photo-realistic images conditioned on the soft layout. The paper uses different modified versions of GANsformer in each stage.  Experiments are carried out on multiple datasets with better results compared with prior works. The proposed framework is capable of separating style from structure and decomposition of multiple individual object properties.,"This paper proposes SceneGAN that uses a two-stage strategy to perform image generation. The SceneGAN is based on a previous transformer-based GAN model, it firstly uses iterative generation to generate scene layout then based on the layouts to perform image generation. The authors demonstrate their model's performance on clevr and real images and show some good results.","This paper introduces an object-oriented GAN model, which generates an image of a scene in a two stage process. First, it generates a layout, where different masks show what object should be painted in which part of the image. Second, this layout is refined by a second network that converts it into a real-world-looking image. Both stages are based on modified GANformer networks. Finally, the paper introduces two novel losses. These make sure that the generated image reflects the generated layout, and that each portion of the image (as given by the layout) looks realistically. The method is fairly well evaluated, but the paper lacks comparison with two key baselines, that is BlockGAN (Nguyen-Phuoc et al) and GIRAFFE (Niemeyer & Geiger), which are not cited. Modelling decisions are evaluated in an ablation study.  The introduced method is supervised and requires paired (image, segmentation masks; this is in contrast to what the authors claim, see my comments below) inputs.   # UPDATE The authors addressed my questions about unpaired training and using CNNs instead of Transformers. I'm still waiting for details of the CNN experiment, but I am already happy to increase my score to 7.",0.21739130434782608,0.18478260869565216,0.25,0.1590909090909091,0.2159090909090909,0.26666666666666666,0.22727272727272727,0.2833333333333333,0.116751269035533,0.23333333333333334,0.09644670050761421,0.08121827411167512,0.22222222222222224,0.22368421052631576,0.15916955017301038,0.1891891891891892,0.13333333333333333,0.12451361867704279
55,SP:0a9c33711aa4eb18bf5813dc05cc31e75d267c38,"This work focuses on removing residual connection in network via  reserving and merging (RM) operations on the ResBlock. The author has tested the approach on classification tasks on several networks with skip connection, e.g., resnet, mobilenet v2, etc. The experiments demonstrated the good performance on the listed benchmarks. ","Update: I have read the rebuttal, see below. ----------- While residual connections are a key component in today's deep learning architectures, they can be problematic in some settings, e.g., in pruning. This paper presents an improved method (RMNet) for removing residual connections from a ResNet - type neural network after training. It improves over related work and in contrast to those, it works on some typical ResNet variants. The paper also discusses fine-tuning, pruning and more efficient architectures.","This paper proposes to convert residual network architectures to equivalent ""plain"" networks after training.  This is accomplished by augmenting convolutional layers with Dirac initialized filters (folding extraction of the residual signal into the conv layer), and modifying the subsequent Batch Normalization and ReLU layers (converting ReLU to PReLU) accordingly.  Motivations for doing this are to increase inference throughput, and allow additional pruning of the resulting plain network. ","This paper focuses on an important direction of removing residual connections from ResNet-based architectures and proposes RM operation to achieve this goal. RM operation consists of two steps: reserving and merging. The reserving operation allows input feature map passing through the Conv, BN,ReLU layers without changing their values and the merging operation adds the output feature map and the reserved input feature map together with the last Conv layer. Unlike re-parameterization method in RepVGG, RM Operation can remove residual connection across non-linear layers, resulting in equivalent transformation from ResNet to plain models. Authors also design a series of plain models with RM Operation named RMNet, which outperforms previous plain models such as DiracNet, ResDistill and RepVGG.",0.16326530612244897,0.16326530612244897,0.2857142857142857,0.11392405063291139,0.16455696202531644,0.2537313432835821,0.10126582278481013,0.11940298507462686,0.11666666666666667,0.13432835820895522,0.10833333333333334,0.14166666666666666,0.125,0.13793103448275862,0.16568047337278105,0.12328767123287672,0.1306532663316583,0.1818181818181818
56,SP:0af1906dfe307ad9a44548b8235827d647ef9fdc,"In this paper, the authors provide a novel generative model to learn topics using graph modeling based on the word network, following a (now classic) variational encoding approach. The documents are encoded as graphs by modeling an edge between two words if they appear in the same window, which is similar to the embedding methods with the distributional hypothesis. The graph structure is then part of the generative process. They demonstrate that the well-known LDA model is a special case of their approach if we ignore edges. Most of the time, the model outperforms existing topic models (classic such as LDA and neural TM) on 4 English written datasets in term of Cv, NPMI and Topic Diversity. It also outperforms prior works in term of clustering accuracy.  ","This work is based on neural topic modeling where a document is represented by a graph of word notes with edges representing the semantic dependency and the generative process of learning is a function of both the document-graph structure (i.e., word-dependency edges) as well as word sets (i.e., document vocabulary). On top, the AVI approach is introduced to perform learning over the proposed Graph Neural Topic Model (GNTM).   Several past works in topic modeling have investigated dependency relations among words: modeling sequential dependency and syntactic relationships [1] using dependency parse trees. Another thread of research [2, 3] incorporates both: BoW and contextualized information in the generative process of topic modeling, where the contextualized representations introduced modeling word-dependency relations in the generative process of neural topic models. This line of research is missing in quantitative comparison with the proposed topic model. ","Going beyond the bag-of-word assumption in topic modelling is an interesting problem, which has been studied since the introduction of the vanilla LDA, for instance, the syntactic topic model. This paper proposes to represent each document as a directed word graph, which is constructed via moving a window along the word sequence. The main technical contributions of this paper can be 1) the graph neural topic model, where words are generated conditioned on both the topic assignments and the reconstructed word graph; 2) the VAE framework used to learn the posteriors. ","The authors propose a topic model that explicitly captures the semantic dependency among words in the same document, by representing each word as a node in a graph and connecting words appearing closely in the document through an edge in the graph. This approach overcome the limitation of standard topic models as they can only consider word as independent. in this way, inferred topics may be more interpretable and semantically more meaningful. This is validated in the experiments as the proposed model achieves better topic coherence.",0.1875,0.125,0.1640625,0.15862068965517243,0.15862068965517243,0.17204301075268819,0.16551724137931034,0.17204301075268819,0.2441860465116279,0.24731182795698925,0.26744186046511625,0.18604651162790697,0.1758241758241758,0.14479638009049775,0.19626168224299062,0.19327731092436978,0.19913419913419914,0.1787709497206704
57,SP:0b283ef10d97c96319bf75db0ad97fc098c57bf4,"This work presents cross-layer attention (CLA) modules to find informative keys across different CNN layers for each query feature. Furthermore, an adaptive cross-layer attention (ACLA) is also formulated to dynamically select keys from different CNN layers by using a NAS method. After that, the authors embedded the presented CLA or ACLA modules into EDSR to formulate a deep model for image restoration. Experimental results on several image restoration tasks show that the developed deep model outperforms state-of-the-art methods.","This paper proposes a cross-layer attention module for the image restoration tasks. Unlike previous conventional non-local attention approaches that find correlated keys within the same layer, the proposed method selects keys across the layers. In order to prevent expensive computational costs, the authors propose adaptive cross-layer attention modules. The proposed method is validated on various image restoration tasks including SR, denoising, demosaicing, and compression. ","The paper proposes a Cross-Layer Attention(CLA) module in order to capture the correlations. Between features among different layers. Besides, an Adaptive Cross-Layer Attention (ACLA) is proposed to reduce the computational cost of the Cross-Layer Attention(CLA) module. Lastly,a neural architecture search method is used to find the insert positions of ACLA modules to further improve performance. ","This paper proposes a novel Cross Layer Attention (CLA) module for image restoration tasks. The CLA module doesn’t look for correlated key pixels within the same layer as other algorithms do; instead query pixels attend to key pixels at previous layers of the network. To reduce computational complexity deformable convolution is used to reduce the number of sampled keys.  To further reduce computational complexity, the paper proposes a modified CLA called Adaptive Cross Layer Attention (ACLA). In ACLA, the number of collected keys for each query is dynamically selected through a gating mechanism.  Further, a neural architecture search method was used to find the proper positions to insert the ACLA modules in the backbone network.  The paper provides comprehensive experiments on image restoration tasks to validate the effectiveness and efficiency of ACLA. Conducted experiments include single image super resolution, image denoising, image demosaicing and image compression artifact reduction.  Contributions are the CLA and ACLA modules designed to efficiently perform attention across layers in a neural network.",0.20481927710843373,0.25301204819277107,0.3253012048192771,0.26865671641791045,0.5373134328358209,0.5409836065573771,0.2537313432835821,0.3442622950819672,0.16167664670658682,0.29508196721311475,0.2155688622754491,0.19760479041916168,0.22666666666666668,0.29166666666666663,0.216,0.28125,0.30769230769230765,0.2894736842105263
58,SP:0b3fe43c5a747d2adf74f3cb86f2c2bfdaa3219b,"This paper validates the SDE approximation to SGD under finite (non-infinitesimal) learning rate by proposing a novel algorithm SVAG. This paper theoretically proves that SVAG is an order-1 weak approximation to the SDE, and empirical experiments show that SVAG behaves similarly to SGD. Using SVAG as a theoretical tool, this paper shows new theoretical insights in the linear scaling rule and the necessary condition for the SDE approximation.","The authors consider a discrete time algorithm (SVAG) which is indexed by a parameter $l$ and which exactly corresponds to SGD when $l= 1$.  They show that this algorithm weakly converges to SGD's continuous SDE approximation as $l$ goes to $\infty$, and this without having to take the step-size infinitely small. They also provide a setting in which the SDE approximation provably fails to capture SGD's dynamics. Finally they give a testing rule for the SDE approximation to hold. ","The paper has two main contributions. Firstly, it presents a practical simulation algorithm, SVAG, which is shown to converge weakly to the Itô SDE approximation of SGD over finite time horizons (with no restrictions on the learning rate $\eta$). Experimental evidence on deep nets is given to support the claim that SVAG can be used as a diagnostic for empirically testing if SGD and its SDE approximation have similar behaviour. The second main contribution provides sufficient conditions for the failure of the SDE approximation and linear scaling rule when the loss function is scale invariant. This theory is further supported by experiments on deep nets.","SDE has been used as an approximation to help understand the implicit bias of SGD. This paper studies the validity of such approximations, which hasn't been formalized in prior work for realistic choices of learning rates. The contributions are three-folds:   - Proposing ""Stochastic Variance Amplified Gradient"" (SVAG), an efficient numerical method to test whether the trajectories of SGD and the corresponding SDE are close.   - Empirically verifying SVAG can be made to follow SGD. Since SVAG provably follows SDE under certain conditions, this means SDE also tracks SGD well by transitivity.   - Providing necessary conditions for SDE and LSR to track SGD. ",0.2714285714285714,0.3,0.2714285714285714,0.24390243902439024,0.15853658536585366,0.17142857142857143,0.23170731707317074,0.2,0.18811881188118812,0.19047619047619047,0.12871287128712872,0.1782178217821782,0.25,0.24,0.2222222222222222,0.213903743315508,0.14207650273224043,0.17475728155339804
59,SP:0b61336b56382ba8c4dc35750a5c50f4deaed084,The paper studies how to use location information (geographical location) of images to help with image classification tasks. The paper studies use of spherical based location over GPS lat long and argues that the euclidean space does not preserve distances and has distortions whereas a spherical coordinate approach would not have the inherent problem. The approach demonstrates a framework to train the location encoding in an unsupervised way to with losses based on representation learning approaches. The paper studies multiple ways of expanding the spherical coordinates and evaluates their performance and empirically demonstrate the strength in their performance. ,"This paper proposes a positional embedding method for GPS coordinates, which lie on a sphere.  Based on the Double Fourier Sphere, this paper proposes a family of encoding methods and show their effectiveness.  This paper also gives a family of loss functions to enable self-supervised training.","This paper explores developing location encodings for spherical surfaces and their application to tasks in geo-aware image classification. The authors propose a method for encoding spherical locations into a higher dimensional space (based on Double Fourier Sphere) that preserves distances. The proposed method, referred to as Sphere2Vec, is demonstrated for three tasks and evaluated using seven datasets. In addition, a self-supervised method is proposed for using unlabeled data to pretrain the location encoder.  This self-supervised pretraining stage is shown to improve performance, especially when available labeled data is limited.","This paper proposes a strategy for learning a location embedding (i.e., translating lat/lon into a useful high-dimensional vector). The idea is to first encode the location into the double Fourier sphere basis (or a subset) and then pass this through a multilayer perceptron. Several alternatives to the first stage are evaluated.  Two stages of training are proposed: 1) Weakly Supervised Pre-Training: use a contrastive loss to train the embedding by comparing it to features extracted from geotagged images. The idea is that a good location embedding should be sufficient to recognize whether a given image was captured in that location. 2) Supervised Fine-Tuning: given a geotagged image, extract image and location features, combine them to predict the image classification label.  The approach is demonstrated on several geo-aware image classification applications. The idea with these applications is that combining image features with the location embedding is sometimes enough to resolve ambiguity from the image alone.",0.10204081632653061,0.17346938775510204,0.20408163265306123,0.3404255319148936,0.3829787234042553,0.22826086956521738,0.2127659574468085,0.18478260869565216,0.125,0.17391304347826086,0.1125,0.13125,0.13793103448275862,0.1789473684210526,0.15503875968992248,0.2302158273381295,0.17391304347826086,0.16666666666666669
60,SP:0b72f28329c6f2100e523c4e99769731adb34725,"The streaming sparse GP (named O-SGPR in this paper) is a variational GP approximation that supports updating the variational posteriors and hyper-parameters online. This paper reinterprets O-SGPR as compressing the past observations into a small set of ""pseudo-data"". Then updating the posterior based on new observations can be simply treated as a standard GP inference based on the new observation and the pseudo-data. This paper adopts the proposed Online Variational Conditioning (OVC) over Bayesian optimization and active learning, where inferring the updated belief from new observations is important to quantify the contained information. This paper is well-written and conducts extensive experiments to verify the proposed approach.","The authors are interested in the problem of online learning with applications to online Decision making. The focus is on (sparse variational) Gaussian Processes and Bayesian optimization.  The authors 1) propose an alternative formulation of the online learning algorithm of Bui et al, which allow to ‘preserve’ what has been learned from the previous data 2) propose a way to update the posterior when adding observations in the conditioning set for non Gaussian likelihood 3) Thoroughly evaluate the resulting algorithm on various BO tasks. ","The authors propose online variational conditioning (OVC), a procedure for efficiently conditioning Stochastic Variational Gaussian Processes (SVGPs) in an online setting that does not require re-training through the evidence lower bound with the addition of new data. The authors propose to efficiently update a posterior distribution after receiving new data; they rely on the idea of a Sparse Gaussian Process Regression (SGPR) model trained on pseudo-data under a block-diagonal Gaussian likelihood, they also provide an extension to non-Gaussian likelihoods by means of the Laplace approximation. The authors show the application of their method in look-ahead acquisitions for Bayesian Optimisation. ","Authors propose a novel method, named OVC for online variational conditioning of GPs under sparse approximations. Particularly, they are interested in the recursive update of the posterior GP distribution after receiving new data per time-step. The model can be used for both regression and classification tasks, bayesian optimisation and active learning as is shown in the experiments.",0.13392857142857142,0.16071428571428573,0.15178571428571427,0.21428571428571427,0.14285714285714285,0.19230769230769232,0.17857142857142858,0.17307692307692307,0.29310344827586204,0.17307692307692307,0.20689655172413793,0.3448275862068966,0.15306122448979592,0.16666666666666666,0.19999999999999998,0.1914893617021277,0.16901408450704225,0.2469135802469136
61,SP:0b8c864063a0087697908bbe157243d503b6ad67,"This paper presents two modified greedy quasi-Newton updates, originally given by Rodomanov and Nesterov [18]. They first consider using these updates to approximate a positive definite Hessian. In particular, for the greedy SR1 update, they introduce a different progress measure, which is maximized over the coordinate bases to yield the vector used in the update. This new measure allows them to obtain a superlinear convergence rate for the Hessian approxiamtion error, where the rate is independent of the condition number $\kappa$. For the greedy BFGS update, they use the same progress measure as in Rodomanov and Nesterov [18], but preconditions the corresponding greedily selected vector with the current Hessian approximation. This yields a linear convergence rate for the Hessian approximation, although their greedy selection for BFGS is rather impractical. In addition to the modified greedy selection strategies, their analyses and rates also apply to a random selection from a spherically symmetric distribution.  Similar to Rodomanov and Nesterov [18], they then apply these results to analyzing the convergence rate of minimizing strongly-convex quadratics, as well as strongly self-concordant objectives. They obtain similar explicit local rates as the previous work, but without the condition number (although the initial, non-local phase still converges linearly with a dependence on $\kappa$).  Experiments on evaluating the newly introduced Hessian approximations show that their methods are quite effective. On a strongly-convex quadratic minimization problem the conclusions are similar. For minimizing a strongly-convex logistic regression problem, the randomized BFGS (with the preconditioned vector) does not perform better than the version from Rodomanov and Nesterov [18], which does not contain the preconditioning. This is true for the two logistic regression problems with different scales of condition numbers.  ","This  paper proposes a tighter local analysis of two popular variants of quasi-Newton methods: BFGS and SR1 with greedy or random choice of directions. It is heavily based on a recent work of Rodomanov & Nesterov, but the analysis is simpler and, most importantly, the guarantees are strictly better (the final bounds do not contain the conditional number $\kappa$).  I think it is really a good paper. ","This paper introduces new non-asymptotic analysis for quasi-Newton methods, extending recent results on DFP to BFGS and SR1. This is relevant since these two methods produce better Hessian approximations so can converge faster. The authors show that the same rates of convergence can be achieved using either greedy or random updates, which is relevant because greedy updates do not always come for free.","This work considers the topic of the convergence of quasi-Newton and it builds on top of a series of recent results by Rodomanov and Nesterov. The main two contributions of this paper are: 1) improve the bounds for greedy quasi-Newton methods using a new potential function; 2) to propose a new modification of greedy strategy to obtain better results. The authors consider two settings: unconstrained quadratics and general strongly self-concordant functions (the same settings were considered in prior work). All results are local but this is probably inevitable as quasi-Newton methods approximate Newton's method, which itself is a local method.  I find the results to be very solid and promising. The topic of quasi-Newton methods might get a second breath due to the recent results and I would be glad to see this paper accepted. It does have a small number of typos but I think most of them can be fixed if the authors proofread their work (I hope the authors will do at least a partial pass during the discussion stage). ",0.08450704225352113,0.06338028169014084,0.12323943661971831,0.22388059701492538,0.3283582089552239,0.26153846153846155,0.3582089552238806,0.27692307692307694,0.19553072625698323,0.23076923076923078,0.12290502793296089,0.09497206703910614,0.13675213675213677,0.10315186246418337,0.15118790496760257,0.22727272727272727,0.17886178861788615,0.13934426229508196
62,SP:0bb740b01e14743c55708fa6312deff78032c636,"The authors introduce a framework to facilitate the analysis of recurrent neural networks (RNNs) by reverse engineering. An RNN is co-trained on a task together with a so-called Jacobian switching linear dynamical system (JSLDS). The latter is the RNN’s linearization around certain expansion points. The co-training idea results in both networks solving the task, while ensuring that the JSLDS has both similar dynamics and similar fixed points (expansion points) to the RNN. By this construction, the trained JSLDS is easy to analyze while matching the RNN, thereby eliminating many previously needed steps of analysis. This procedure also implicitly regularizes the RNN to exhibit dynamics that can be described by the JSLDS.  The authors show the utility of the framework on three tasks previously studied for reverse engineering RNNs. ","This study describes a new interpretable model of neural activity, consisting of a piecewise linear latent dynamical system coupled to a trained RNN. The manuscript outlines intuitions for the model and first applies it to artificial neural networks trained on two cognitive tasks, a working memory task and a context-dependent evidence integration task, showing that their JSLDS model recovers dynamical mechanisms already described in the literature with sets of linear dynamical systems. The authors also propose using the JSLDS model as a replacement for the complex generator RNN of the LFADS model, and apply this idea to neural recordings of a monkey performing a motor control task, showing that a simple linear dynamical system captures most features of this dataset.","This work concerns the challenging problem of improving the understanding of how RNNs perform computations and was inspired by combining ideas from reverse engineering RNNs and SLDS models. The novel SLDS dynamics are governed based on the first-order Taylor series expansion of the co-trained RNN. It is also equipped with an auxiliary function trained to pick out the fixed points of the RNN. Their results are a trained SLDS variant which closely approximates the RNN, an auxiliary function that can produce a fixed point for each point in state-space, and a trained nonlinear RNN whose dynamics have been regularized such that its first-order terms perform the computation, if possible. Moreover, The presented model removes the post-training fixed point optimization, and allows to study the learned dynamics of the SLDS at any point in state-space. It also generalizes SLDS models to continuous manifolds of switching points while sharing parameters across switches. ","The central idea of this paper is to co-train a switching linear dynamical system (SLDS) together with a nonlinear RNN, such that the SLDS, due to its locally linear dynamics, can be used to gain insights into some aspects of the dynamics of the RNN. The SLDS retains the same Jacobians as the accompanying RNN, which are evaluated at approximate fixed points learned through an auxiliary function. The fixed point condition is imposed via a regularization term, as is the agreement in RNN and SLDS dynamics. The method is evaluated on 2 simple benchmarks and a neuroscience dataset.",0.17424242424242425,0.21212121212121213,0.18181818181818182,0.17355371900826447,0.17355371900826447,0.15384615384615385,0.19008264462809918,0.1794871794871795,0.24242424242424243,0.1346153846153846,0.21212121212121213,0.24242424242424243,0.18181818181818182,0.19444444444444445,0.2077922077922078,0.1516245487364621,0.19090909090909092,0.18823529411764706
63,SP:0bc6b75f871cebe8e2e93c58e67624d8c1f5dba5,"Observing that existing adversarially-trained classifiers have different class-wise model performance, this paper aims to protect the most vulnerable class against an adversary who can smartly pick which class to attack. Built upon on a classical Bandit algorithm and adversarial training method, it proposes an algorithm called “Class Focused Online Learning”, which adaptively reweights the class-wise loss function over the training epochs. In addition, the paper proves a convergence guarantee for the worst class loss and provides empirical evaluations of their method on benchmark datasets. ","In this work, the authors present a method for maximizing minimum class-wise accuracy in the $\ell_p$-adversarial setting. The proposed method uses an online learning algorithm (exponential weights) to choose classes adversarially at train-time to sample examples from (in place of randomly choosing examples from the given data distribution). The authors find their algorithm can effectively increase the class-wise worst-case accuracy on a number of standard datasets (CIFAR-10/100, STL10) in the $\ell_\inf, \epsilon=8/255$ threat model.","This paper points out that there could be applications where the worst class accuracy could be critical -- for example, the class with the worst class accuracy can be the main target for the attacks and that the difference between the worst robust test accuracy of a class and the average robust test accuracy can be large (also pointed out independently in Tian et al. (2021)).   Motivated by this, the paper proposes a solution to use the Exponential-weight algorithm (Auer et al., 2002), where, as the empirical distribution is taken the adversarial distribution over classes, resulting in a method called Class Focused Online Learning (CFOL). The proposed CFOL method is based on the Focused Online Learning (FOL) (Shalev-Shwartz & Wexler, 2016) method, which does not take the class label into account.  The authors show empirically that CFOL consistently improves the worst class accuracy on CIFAR-10, CIFAR-100, and STL10. Moreover, the authors provide high probability convergence guarantees for the worst class accuracy.","The setting is robust learning to perturbations at test time (adversarial examples), where instead of minimizing the average loss, the goal is to minimize the loss of the class with the lowest accuracy. That is, instead of a min-max problem, there is another maximization component over the classes.  This is motivated by an example that shows a larger variance in losses between classes than in the standard setting.  The authors suggest a method from online learning - exponential weight in the bandit setting (that is - Exp3). ",0.16091954022988506,0.28735632183908044,0.12643678160919541,0.29411764705882354,0.2,0.13496932515337423,0.16470588235294117,0.15337423312883436,0.12790697674418605,0.15337423312883436,0.19767441860465115,0.2558139534883721,0.16279069767441862,0.2,0.1271676300578035,0.20161290322580647,0.19883040935672516,0.17670682730923695
64,SP:0c5417b62e933b0e58a50af1b3c7a75e4302b673,"This paper presents an attribute-aware hashing network for generating attribute-aware hash codes. The proposed method includes two keys components: (1) visual feature extraction with attention, (2) learning attribute-specific features with autoencoder.   The authors conducted experiments on many datasets, and it outperforms recent SOTA approaches.","In this paper, authors propose to learn attribute-aware hash codes for fine-grained image retrieval, which establish the explicit correspondences between hash codes and visual attributes.   The attention mechanism is employed to extract the fine-grained tailored visual patterns from both global-level and local-level, and unsupervised reconstruction task is defined to distill the attribute related vectors through an encoder-decoder structure. Finally the hash codes are generated by the attribute vectors.","(1)	This paper aims to address the large-scale fine-grained image retrieval, and it introduces an attention mechanism into a CNN backbone to capture fine-grained local patterns in an image. And it integrates the attentive local-level feature and the extracted global-level feature by the same CNN as the holistic feature of an image, (2)	It proposes an Attribute-Aware Hash Codes Generating module with an encoder-decoder network by encoding the holistic feature to a latent space, and decoding the latent feature to the holistic feature, and projecting the latent feature to a binary hash code.  (3)	It designs the empirical experiments on 5 fine-grained benchmark datasets to verify the performance improvement of the proposed method compared to existing methods. ","This paper proposes a unified framework to generate attribute-aware hash codes for dealing with the large-scale fine-grained image retrieval task. Specifically, it develops an attribute-aware hashing network, termed as A^2-Net, consisting of two main modules including fine-grained representation learning and attribute-aware hash codes generating. In A^2-Net, the proposed reconstruction task realizing by an encoder-decoder structure is proven to be able to unsupervisedly distill high-level attribute-specific vectors from the appearance visual representations without attribute annotations. Thus, the hash codes can be generated by the attribute vectors based on a feature decorrelation constraint and the similarity preserving constraint. Experiments are conducted on several fine-grained benchmarks, which can show outperforming on both qualitative and quantitative aspects.",0.23404255319148937,0.2765957446808511,0.40425531914893614,0.35135135135135137,0.40540540540540543,0.272,0.14864864864864866,0.104,0.14960629921259844,0.208,0.23622047244094488,0.2677165354330709,0.18181818181818182,0.15116279069767444,0.21839080459770116,0.26130653266331655,0.2985074626865672,0.2698412698412699
65,SP:0c5c8540e7636db4bc233cd7dccb81baf3b544b2,"The authors propose a novel way to learn new recommendation deterministic policies from feedback collected under different recommendation policies. The authors start by pointing out that the majority of current approaches, either Counterfactual (Inverse Propensity Weighting) either Domain Adaptation-based,  do not work well when the support of the current and future recommendation policies do not fully overlap (one of the main causes being that the current recommendation is deterministic and therefore not exploring all actions). To this end, the authors propose a novel way of re-weighting and regularizing empirical risk that encourages the search for policies that have better reward (lower risk) and are still overlapping in evidence with the current policy. To escape the support issue that is present in the existing approaches, the authors switch the regularization parameter to an IPM metric (WASSERSTEIN) that can handle non-overlapping supports.  The authors provide both theoretical and empirical evidence that the resulting algorithm outperforms existing SOTA methods.","In this paper, the authors first point out that the requirement of efficiency of IW- and DA-based methods is the overlapping between the source domain and target domain. Meanwhile, theoretical analysis also shows that insufficient overlapping problems can cause the hardness or impossibility of IW- and DA-based learning methods. In light of this, this paper aims to propose a novel method to optimize recommendation results by solving such insufficient overlapping problems. Specifically, a principled transportation-constraint risk minimization objective function is devised to optimize the recommendation results, which is able to transport the learned patterns from the source domain to the intervention domain better. Then a two-layer adversarial model (GDA) is deployed to optimize the transportation-constraint risk minimization objective function, and sufficient analysis of the GDA is provided. Extensive experiments were conducted on both real datasets and semi-synthetic datasets to show the superiority of the proposed method.    ","This paper proposed a new perspective for recommender systems: rephrase optimizing recommendation as finding an intervention that best transports the patterns it learns from the observed domain to its intervention domain. To optimize the recommendation algorithms in this setting, the authors proposed a transportation-constraint risk minimization objective and convert it to a two-player minimax game. Theoretical and empirical studies demonstrated the effectiveness of the proposed method. ","In this paper, the authors study the domain transportation issue in information retrieval (IR) systems. The main challenge is the insufficient overlapping problem. The authors propose a new adversarial learning method based on transportation-constraint risk minimization to address this challenge. The main insight behind this proposed method is that the interventional impact of recommendation is not exclusive to observed data, but also interferes with the target domain of interest. The authors first analyze the generalization bound of IW and DA methods, and then further give the consistency, generalization, and excessive risk bounds of the transportation-constraint risk minimization. The simulation, real-data analysis, and online experimentation show the effectiveness of the proposed method.",0.18238993710691823,0.11949685534591195,0.1509433962264151,0.17763157894736842,0.26973684210526316,0.3088235294117647,0.19078947368421054,0.27941176470588236,0.21052631578947367,0.39705882352941174,0.35964912280701755,0.18421052631578946,0.1864951768488746,0.16740088105726875,0.17582417582417584,0.24545454545454545,0.3082706766917293,0.23076923076923075
66,SP:0cec0fd15252ee470adbb46921aff20683b7d8e1,"The paper studies a bandit problem where the actions need to be monotone. Precisely, given an action space [0, 1], a decision-maker needs to choose actions X_1, ..., X_T, where X_1 <= X_2 <= ... <= X_T. The authors first show that in the worst case, O(T) regret is unavoidable. Under a quasi-concavity assumption, they show that T^{3/4} regret is both sufficient and necessary.  Overall, I am ambivalent about the paper. The paper has been written well and while I have not checked the proofs, the results appear correct in hindsight. However, the setting lacks motivation and the proof techniques, while not straightforward, are not earth-shattering.  This is a fairly subjective assessment, and therefore, I have given a neutral score. ","The paper proposes a continuum-armed bandit variant, where the arm sequence needs to be increasing. For the case of Lipschitz functions a linear lower bound is provided. When the functions are constrained to be quasi-concave, a simple algorithm is shown to achieve T^3/4 regret, which is shown to be log-optimal. ",This paper studies the continuous MAB problem under the constraint that the sequence (X_t)_t of chosen arms is non-decreasing. Obtaining non-linear regrets is obviously impossible without restricting the class of candidate functions: the authors focus on the quasi-concave (or unimodal) Lipschitz case. They show that the monotonicity constraint on the choices of arms causes the regret to grow as O(T^3/4) instead of O(T^2/3) for the Lipschitz case and O(T^1/2) for the quasi-concave case. A rather straightforward algorithm reaches this rate up to logarithmic factors. A T^{3/4}/32 lower bound is proved.  ,"The paper studies the continuum-armed bandit problem with additional requirement that the selected arms/actions must be monotone sequences over the course of the game. With this requirement, the authors first show that simply considering a continuous reward function (namely, an unknown function that maps the action to the reward) will suffer linear regrets, in contrast to the sublinear regret $O(T^{2/3})$ in the literature without the monotonicity requirement.  Then the authors consider a more structured reward function which is quasiconcave. The authors then provide an algorithm and prove that a regret $O(T^{3/4})$ of this algorithm for this class of functions. Furthermore, this regret is tight as authors also provide a matching lower bound. Lastly, the authors conduct numerical studies to compare the regret of proposed algorithm with a few benchmark, validating the analysis.  ",0.1349206349206349,0.18253968253968253,0.2222222222222222,0.32727272727272727,0.36363636363636365,0.28703703703703703,0.3090909090909091,0.21296296296296297,0.2,0.16666666666666666,0.14285714285714285,0.22142857142857142,0.1878453038674033,0.19658119658119658,0.2105263157894737,0.22085889570552147,0.2051282051282051,0.24999999999999994
67,SP:0dea3f71628cdd70483b5e697624e03efd4676c5,"This paper emphasizes the importance of context and nuance when discussing “efficiency”. The authors explain different efficiency metrics and the distinction between training and inference efficiency. They then illustrate how assessments of the “efficiency” of different models can be misleading or even contradictory depending on factors such as the choice of efficiency metric, baseline, architecture, hardware, optimizer, and other experimental design choices. The authors finish with a few suggestions for best practices.","This work addresses the problem of measuring and reporting efficiency of machine learning models. The authors show that there is a series of efficiency metrics which are not fully correlated, but, in many of literature, only few of them are reported making confusing claims on production-level usage of proposed models. The main claim is a proposed list of recommendations for reporting of efficiency of new ML models and approaches.","The paper provides a detailed discussion on various cost indicators for “efficiency” of neural network models. It looks at both training and inference processes and argues, based on several examples and arguments, that no single cost metric is sufficient to provide a complete picture of a models’s efficiency, due to the behavior of cost indicators not necessarily being correlated or highly dependent on the platform. It concludes with recommendations to include multiple cost indicators in papers and to clearly state (restrict) efficiency claims.","The paper address the problem of model efficiency indicators. The paper brings up the risks of reporting only few efficiency indicators and points out this erroneous common practice.   The authors investigate how reporting only few cost indicators might lead i) to partial or incorrect conclusions about the model efficiency and  ii) to unfair model comparison. Finally, the authors give recommendations about how to report efficiency indicators. ",0.2222222222222222,0.16666666666666666,0.20833333333333334,0.21428571428571427,0.17142857142857143,0.15476190476190477,0.22857142857142856,0.14285714285714285,0.22727272727272727,0.17857142857142858,0.18181818181818182,0.19696969696969696,0.22535211267605634,0.15384615384615383,0.21739130434782608,0.19480519480519481,0.1764705882352941,0.17333333333333334
68,SP:0e0ecaf4ea28ace6b6a017a61f519cbab16cba86,"This paper claim that they contribute to the norm-based generalization bound of the Transformer model. The paper uses its Lipschitzness to get the bound of self-attention model. With this bound, the paper analyses the inductive biases of self-attention modules; it investigates which function and the long-range dependencies the self-attention modules represent. This paper aims to answer this new question. It presents a theoretical finding that bounded-norm TransformerTransformer can fit sparse functions of the input sequence with sample complexity scaling only logarithmically with the context size. This paper also empirically validates this finding and the scaling law.  ",The paper provides a theoretical analysis of the functioning of the self-attention modules. The paper shows that the Transformer architectures with some modifications (for ex. bounded norm) can create sparse variables. The paper proposes an experimental protocol to support the analysis.  ,"This paper provides rigorous norm-based bounds on the expressivity and inductive biases attention and self-attention provides when learning complex functions. The authors leverage established analytical tools involving complexity measurements, together with novel derivations for standard attention heads transformations,  to provide novel into generalization properties of Transformers, and into the types of functions they prefer to learn. Finally, the authors provide a set of simple numerical experiments using Boolean functions that serve as basic checks on their theory.","This paper proposes a theoretical bound on inductive biases self-attention module can represent. It gives detailed walkthrough about how such bound can be derived and the result complements conclusion from prior works. The conclusion of this paper is very interesting that the complexity of sparse boolean function that a self-attention module can capture is logarithmically dependent of the context size. It could cast insights on future probing works and dataset designs. A downside, however, is the experiment part which only focused on a synthetic task. Analysis is largely absent. ",0.17647058823529413,0.16666666666666666,0.21568627450980393,0.2619047619047619,0.30952380952380953,0.189873417721519,0.42857142857142855,0.21518987341772153,0.24175824175824176,0.13924050632911392,0.14285714285714285,0.16483516483516483,0.25,0.1878453038674033,0.22797927461139894,0.1818181818181818,0.19548872180451127,0.17647058823529413
69,SP:0e5260915eb4794fb7bc769eccdbb740a2c17b11,"The authors analyze the preferred granularity of the modules at different stages of modular neural networks, which leads to the best systematic generalization. Based on Multi-attribute MNIST, they conclude that the modules are most useful when responsible to a specific semantically meaningful feature (like color, shape, etc), as opposed to having a single module or having modules for specific instances of those features (like green, 2, etc). They also conclude, somewhat surprisingly, that modularity is important already in the early stages of the network, so the feature extractor should also be modular. They show the advantage of the modular feature extractor on the more complex CLEVR dataset as well.","This paper studies neural module networks (NMNs) for systematic generalization on VQA tasks. Of particular interest is understanding how the degree of modularity across various stages of information processing affects the ability to generalize systematically.   To that extent three stages are distinguished: (1) the encoding stage from input image to features, (2) the intermediate stage where modules compare and relate image content according to the correct program layout, (3) and the classifier stage, which processes the final representation to answer the question. In each stage, the responsible networks can be shared across sub-tasks or they can be fully modularized by associating separate networks (having unique weights) for each sub-task. In the former case, this requires conditioning the shared network during the intermediate stage via a sub-task embedding to achieve specific behavior, i.e. as done in prior work (eg. the ""find module"" in [16]). Alternatively, an intermediate degree of modularity can be achieved by organizing sub-tasks according to groups (such as focusing on colors or categories) with sharing weights only among the modules within a particular group and using conditioning within a group to further specialize their functionality.   To evaluate how these different choices affect systematic generalization two families of datasets are proposed: Multi-Attribute MNIST contains attribute extraction tasks (for one or more colored digits) and comparison tasks (between objects and spatial locations). Here, the learner is provided only a few combinations of visual attributes during training and is expected to generalize systematically to all possible combinations. The second family of datasets is based on SQOOP [11] that  involves comparing visual objects spatially and provides only few combinations of objects during training. Here also a modified version is considered where the amount of combinations of objects in the scene is also limited. Towards the end two variants of CLEVR are considered.  On Multi-attribute MNIST it can be observed how additional modularity improves systematic generalization (i.e. using a fully modular setting, or an intermediate degree), and how modularity in the encoding stage (1) is particularly helpful. It is also shown how tree-NMNs [11] can be improved on SQOOP by increasing modularity at the intermediate stage, and how vector-NMNs [5] can be improved on CoGenT by inducing modularity at the encoding stage, although the opposite effect is observed on COGENT.","This paper investigates the effect of the degree of modularity on the systematic generalization of neural module networks (NMNs). Specifically, the authors study how the degree of separation or sharing of the weights across sub-tasks at the image encoder, intermediate modules, and classifier levels affect out-of-distribution generalization on multi-attribute MNIST, SQOOP, and CLEVR-CoGenT with systematic test splits. The sub-tasks are either grouped under a single group called ""all"", or in groups of high-level functional classes, or in groups of 1. Weights within a group are shared, but the module operation depends on a sub-task embedding (this is done through straightforward architectures in the extremes of modularity). The authors explore four combinations of modularity settings across image encoder, intermediate modules, and classifiers. In experiments with multi-attribute MNITST and CLEVR-CoGenT, the authors show the benefit of separating groups in the image encoder. For SQOOP, they show an advantage of using sub-task-level intermediate modules.","This work investigates how the degree and location of modularity in neural module networks (NMN) impacts out-of-domain generalization on visual question answering (VQA) tasks. The authors tested NMNs on various VQA datasets (multi-attribute MNIST, SQOOP, CLEVR). On the first two, they separated the stages of the NMN so they were either shared among all tasks, shared only among groups of sub-tasks (like colours or categories), or shared only for particular subtasks (like individual colours or numbers). Group-level modularity, particularly at the encoder layer, improved out-of-domain test-set performance. Given these results, they showed that using a modular encoder also improved systematic generalization on the CLEVR dataset over a shared-encoder baseline.",0.2636363636363636,0.2636363636363636,0.21818181818181817,0.12403100775193798,0.08785529715762273,0.1901840490797546,0.07493540051679587,0.17791411042944785,0.2033898305084746,0.294478527607362,0.288135593220339,0.2627118644067797,0.11670020120724346,0.21245421245421242,0.2105263157894737,0.17454545454545453,0.13465346534653463,0.2206405693950178
70,SP:0e6d53ed655b341ffb22f5590fa31fcbadc61fa9,"Traditional research on symbolic reasoning assumes input data are already translated into a format that complies with the formalism of the underlying system. A major struggle for symbolic reasoning is to handle the data coming in a natural form (e.g. images/text) and perform reasoning on the same. Inspired by this challenge, this paper undertakes the problem of converting text inputs into the format of a system formalism.   The formalism is also proposed by this paper and is called MetaQNL. The framework of MetaQNL is designed keeping in mind the specific need of operating directly on natural language sentences. This formalism supports the natural language sentences with variables within them. This trick alleviates the need of using a semantic parser to parse natural language sentences for the purpose of logical reasoning. Thus, MetaQNL, by design, is conducive to working with natural language inputs.   Next, this paper proposes an algorithm to induce rules from natural language inputs within the MetaQNL framework. This paper doesn’t worry about performing actual deductive reasoning/theorem proving and instead proposes to use existing provers and instead focus on the more challenging problem of rule induction. MetaInduce algorithm draws inspiration from existing ILP approaches. MetaInduce encodes the rule induction problem as a maximum satisfiability (MAX-SAT) problem, which can be solved efficiently by off-the-shelf solvers. The proposed method consists of 3 steps.   1. Given a training example, a rule proposer proposes a set of concrete rules as candidates. This set can be overcomplete/inaccurate.  2. It generates abstract rules from concrete rules via a symbolic procedure called anti-unification. This is essentially a process of aligning common substring segments across two or more strings. 3. It encodes the proof paths in MAX-SAT and solves for a subset of all rules using a MAX-SAT solver.   This paper benchmarks the proposed method on 2 tasks - learning compositional instructions and logical reasoning. For learning compositional instructions, it works on two standard benchmarks: MiniSCAN and SCAN and recovers precisely the ground truth rules. For logical reasoning, the proposed method achieves SOTA on the RuleTaker dataset.   ","This paper proposes a symbolic system in Quasi-Natural Language, MetaQNL, which is compatible with both logical inference and quasi natural language expressions, and where the basic building blocks are sentences and rules. The authors also propose MetaInduce, which learns to generalize a set of rules that explains the examples in MetaInduce.  MetaInduce consists of three mains steps: 1) a rule proposer proposes a set of concrete rules as candidates for each individual example. This set may not be fully correct and may not be the minimal explanations, and they are used to prove the example using forward/backward chaining. 2) the authors apply a symbolic procedure called anti-unification to generate abstract rules from concrete rules. 3) and finally, proof paths are encoded in MAX-SAT and a subset of all rules are solved using a MAX-SAT solver to find minimal possible explanations  This paper evaluates its methods on two synthetic datasets, and the authors claim to learn compact models with much less data, and produces answers as well as proofs","The work proposes two new concepts: + MetaQNL: a symbolic system in Quasi-Natural Language. Instead of representing rules in a formal symbolic format such as first order logic rules, in MetaQNL, a rule is represented in a Quasi-Natural Language format which includes words, variables and control symbols. Since the rules are represented in an informal representation. An interesting property of the MetaQNL representation is that it allows to perform backward or forward inference by substitution of variables with sentences. The authors assume that texts can be translated into the MetaQNL format and thus solving the reasoning problems with text input is possible via mining rules from text and backward/forward reasoning with the Quasi-Natural Language. + To mine rule from text, the authors proposes an algorithm called MetaInduce. MetaInduce iterates through the training data several time to build a compact set of rules that trades complexity to prediction accuracy. It is a bottom up rule induction approach where it includes a Rule proposer which propose a concrete rule from a training example and an anti-unification module to abstract the concrete rule with more generalised rules. A pruning process based on MAX-SAT is used to prune the set of rules such that it optimise the regularised objective.","This paper proposes an algorithm that learns rules from natural language data, and a symbolic system for manipulating these rules, where existing provers can be applied. The objective is to maximize the number of examples in a test set that are consistent with the proposed mode while minimizing the number of rules in the model. The algorithm consists of three steps - given a training example, it proposes concrete rules, abstracts concrete rules into rules with variables, and prunes the resulting rules. ",0.18857142857142858,0.1457142857142857,0.09714285714285714,0.2658959537572254,0.15606936416184972,0.11961722488038277,0.3815028901734104,0.24401913875598086,0.41975308641975306,0.22009569377990432,0.3333333333333333,0.30864197530864196,0.2523900573613767,0.18246869409660108,0.1577726218097448,0.2408376963350785,0.2125984251968504,0.1724137931034483
71,SP:0f055d5216d2f27ec775d4d8cc1862c012ccb38b,"The paper proposes a new method called ‘PartialFed’ to solve the performance degrading caused by data heterogeneity in the federated learning scenario. The main difference between this work and previous work is that the proposed method partially loads global parameters given by FedAvg, while the previous methods load all of the global parameters for each starting point of the local epoch.   For the sampling strategy for the parameters that load the global parameter, two methods are developed in this paper: “PartialFed-Fixed” and “PartialFed-Adaptive”. For the “PartialFed-Fixed”, the parameters are sampled manually based on the commonly accepted assumptions in deep network e.g. low-level knowledge learned from bottom layers. With the ""ParitalFed-Adaptive"", the sampling strategy is trained data-driven. Gumbel Softmax sampling is used to enable back-propagation through the sampling process.   To validate the proposed methods, cross-domain federated classification and detection are examined. Two partial loading strategies outperform a set of state-of-art methods. Specifically, the “PartialFed-Adaptive” further reduces performance losses due to the extreme distribution heterogeneity and automatically shows the tendency that reflects the assumptions of deep network used for the design of “PartialFed-Fixed”. ","This paper proposed a novel idea PartialFed, which loads a subset of the global model’s parameters rather than loading the entire model. Specifically, they validate the algorithm with manually decided loading strategies inspired by various expert priors, named PartialFed-Fix. Then they develop PartialFed-Adaptive, which automatically selects a personalized loading strategy for each client.  ","This paper proposed an adaptive initialization algorithm for FedAvg, motivated by FedPer in a personalized federated learning scenario. The key contribution lies in the observation of the impact on training accuracy of different initialization strategies and the adaptive initialization algorithm. The empirical result of the algorithm shown significant improvement compared with FedAvg in training with heterogeneous data.","This paper explores a simple but novel modification to FL approaches to better deal with settings involving non-homogeneous clients. They find that by allowing clients to mix local and global parameters for initializing local training, the added flexibility can notably improve the performance of the respective local end-models. They perform both an exploration of natural fixed mixing strategies and also propose an adaptive mixing strategy that allows for further improvements on average.  ",0.08762886597938144,0.09278350515463918,0.09278350515463918,0.17857142857142858,0.21428571428571427,0.19298245614035087,0.30357142857142855,0.3157894736842105,0.24324324324324326,0.17543859649122806,0.16216216216216217,0.14864864864864866,0.136,0.14342629482071712,0.13432835820895522,0.17699115044247785,0.1846153846153846,0.16793893129770993
72,SP:0f618c712e503eb2d53d634db581fb584bd79a28,This paper proposes noise added version of the manifold mixup. The authors verify the theoretical properties of the method in terms of regularization and robustness. The paper verifies the effectiveness of the proposed method on image classification tasks with noisy test environments. ,"This paper proposes a new method for data augmentation, named Noisy Feature Mixup (NFM). This method combines the advantages of both the interpolation based training and noise injection schemes. In particular, this method is simple and easy to implement. Empirically, this paper shows that NFM achieves a favorable trade-off between the test accuracy on the clean set and the model robustness. Theoretically, it is shown that NFM enables smoother decision boundary, and amplifies the regularization effects of manifold mixup and noise injection. Additional theoretical analysis also shows that NFM training is approximately minimizing an upper bound related to the adversarial loss, thereby leading to more robust model.","This paper studies data augmentation methods for improving the robustness of supervised learning. The main contribution is presenting Noisy Feature Mixup, extending input mixup and manifold mixup to all layers of a neural net. The experimental results show that the proposed approach improves the robustness of supervised learning under several noise attacks on the input data set. The theoretical results derive the Taylor's expansion of the Noisy Feature Mixup optimization objective.","The paper proposes a new and inexpensive mixup method named Noisy Feature Mixup (NFM) to mitigate over-fitting and improve generalization. NFM combines mixup and noise injection, which inherits the benefits of these methods. Due to its conciseness, it is convenient to apply NFM in model training. More importantly, the authors prove NFM's regularization effect on model optimization and robustness improvement with mathematical derivation. They build the implicit connection between the NFM empirical loss and the original loss. Then they identify the regularizing effects of the proposed NFM. To demonstrate that NFM helps robustness, the authors relate the NFM loss to the one used for adversarial training, which can be regarded as an example belonging to distributionally robust optimization. With integrated experiments, NFM shows its superiority on various models and datasets. The further discussion shows the interesting tradeoff between predictive accuracy on clean and perturbed test sets. The Supplementary Material provides detailed proof and additional experimental results to show NFM's firmed ground.",0.30952380952380953,0.35714285714285715,0.38095238095238093,0.17592592592592593,0.3055555555555556,0.2361111111111111,0.12037037037037036,0.20833333333333334,0.0975609756097561,0.2638888888888889,0.20121951219512196,0.10365853658536585,0.17333333333333334,0.2631578947368421,0.15533980582524273,0.2111111111111111,0.24264705882352947,0.1440677966101695
73,SP:0f649b3c923ba73008bdd28730f14dbbffde03d1,"This paper considered the problem of compressing the state space representation in a manner that is sufficient for control and of smaller dimension than the original state space. This is done by first running the system to collect data and then learning structural constraints that are sufficient for describing the reward random variable. This is done by an adapted Variational Auto Encoder which trades off reconstructability, reward prediction, action sufficiency, and minimality. The resulting low dimensional MDP can be combined with classic model based RL to potentially improve sample efficiency.","This paper proposes to find action-sufficient representation which might be helpful for downstream tasks. The idea is that states and/or observations often include irrelevant for actions information which can be removed by  extracting only reverent information. The method utilizes the concepts of Identifiability, mutual information and sparsity. It is realized by augmenting VAE by Structured Sequential component.  ","The paper proposes a novel method for learning a compact and sparse generative model of a partially observable environment, such that the sparsity of the model can be analyzed to identify the subset of latent state variables that are relevant to the reward function of the environment. This subset of the state space, called the action sufficient state representation of the task and environment, can then be used to learn a policy from fewer samples than using the entire reconstructed state space or a predictive world model would need. Empirical verification on two difficult decision problems with high-dimensional observation spaces supports the claim that the proposed method is indeed more sample efficient.","The paper proposes a new state representation learning method for RL, called Action-Sufficient State Representations (ASRs) to learn minimal and sufficient state representations for downstream decision-making tasks. Different from other related work, they explicitly characterize structural relationships among variables (i.e., state features, observation, reward and action) in MDP. Such structure and ASRs are learned by constructing a generative environment model (SS-VAE). The proposed algorithm is empirically evaluated in conjunction with the model-free, model-based algorithm in VizDoom and CarRacing. ",0.12222222222222222,0.2111111111111111,0.17777777777777778,0.22033898305084745,0.2033898305084746,0.168141592920354,0.1864406779661017,0.168141592920354,0.19047619047619047,0.11504424778761062,0.14285714285714285,0.2261904761904762,0.1476510067114094,0.187192118226601,0.1839080459770115,0.1511627906976744,0.16783216783216784,0.19289340101522842
74,SP:0f7d9f9bd9d2a56deece8d6b74c64d3f773a6779,"In this paper the authors tackle the problem of ensuring fairness between subgroups in the presence of missing data, as the missingness can be biased between classes. The authors propose upper bound as well as lower bounds on the generalization error when propensity weights have to be estimated. The former is novel but classical, the latter is novel but also greatly original.","The paper presents  theoretical results about fairness and missing data. The authors show theoretical bounds for the fairness metric ""accuracy parity gap"" considering weights. The weights are estimated from the data based on correctly or incorrectly specified propensity score models under a given missing data mechanism.  Some experiments on synthetic and real data (where some parameters are controlled) are useful to demonstrate the validity of  the included theoretical.","This paper provides theoretical guarantees (upper and lower bounds) on the estimation error of fairness, in the setting where missing data is present and only the complete rows are used for estimation. In particular, the authors consider the “accuracy parity gap” as the fairness metric, which can be applied to both classification and regression tasks. The paper also includes some numerical experiments to validate the theoretical findings.","This paper discusses how to estimate fairness measures in the presence of missing values. Especially, it compares the fairness measure for the complete data domain (data distribution assuming no missing values) and the fairness measure estimated with reweighing on the complete case domain (data distribution after dropping all the data points that have missing values). They provide an upper bound and a lower bound on the difference of the two. They show experimental results of estimating the fairness measures on synthetic data and two real-world datasets to evaluate.  ",0.1935483870967742,0.27419354838709675,0.2903225806451613,0.2647058823529412,0.23529411764705882,0.23880597014925373,0.17647058823529413,0.2537313432835821,0.20224719101123595,0.26865671641791045,0.1797752808988764,0.1797752808988764,0.18461538461538463,0.2635658914728682,0.2384105960264901,0.2666666666666666,0.2038216560509554,0.2051282051282051
75,SP:0f80f50532ea9feeb16d6348b4afb12d4f0ab366,"This paper proposes a framework for solving vehicle routing problems by combining the strength of deep learning and dynamic programming. The basic idea is to use a graph neural network to restrict the search space and then boost the DP algorithm. The experiment results for the traveling salesman problem (TSP), the vehicle routing problem (VRP), and TSP with time windows (TSPTW) show that the proposed method can achieve similar performance as LKH and outperform most other neural nets-based algorithms.","The paper proposes a framework for solving vehicle routing problems, based on a combination of supervised learning and dynamic programming. The proposed method is an extension of the restricted dynamic programming algorithm where at each iteration, the selection of the (fixed number of) most promising solutions is done by a learning-based heuristic. The framework is tested on instances of 100 nodes of the TSP, CVRP and TSP with time windows.  ","The paper investigates dynamic programming techniques for routing that relies on auxiliary neural network models. The idea is to predict a ""heatmap"" of edges indicating whether they are promising or not for good-quality solutions, which needs to be generated only once per instance. The problem is then solved by a (forward) dynamic program that expands states based on scores extracted by the heatmap. The methodology is applied to the vehicle routing problem (VRP), the traveling salesperson problem (TSP), and the TSP with time windows (TSPTW).","The present submission is concerned with dynamic programming (DP) methods for  traveling salesperson problems and a capacitated vehicle problem. Since DP for these problems struggles with enormous DP table sizes, the authors propose to heuristically prune the DP table using a neural network.   Essentially, the idea is to leverage recent approaches that use neural network to predict good solutions for combinatorial optimization problems. In this particular case, a neural network is used to predict partial solutions, and these in turn allow the others to compute priority values for the edges. High priority edges are preferred in the solution expansion step of the DP.  The contribution of the paper is the evaluation of the approach (which, according to the authors, has not been tried before); the neural network itself stems from the literature (with few modifications).",0.325,0.3,0.2625,0.323943661971831,0.23943661971830985,0.26744186046511625,0.36619718309859156,0.27906976744186046,0.15555555555555556,0.26744186046511625,0.1259259259259259,0.17037037037037037,0.34437086092715236,0.2891566265060241,0.19534883720930232,0.2929936305732484,0.16504854368932037,0.2081447963800905
76,SP:0f93be9b6eda975c7696474a4df97114d428c31f,"This paper introduces sparse queries for vision transformers to exploit the intrinsic spatial redundancy of natural images and save computational costs. Specifically, the authors propose a Dynamic Grained Encoder for vision transformers, which can adaptively assign a suitable number of queries to each spatial region. Concretely, a reshaped 2D feature is first divided into regions using a fixed window. For each region, the number of patches is decided by a data-dependent routing process, and each patch is average pooled to obtain a 1D token. All the tokens are then concatenated into a sequence as the queries. Finally, the output of the encoder is restored to the input resolution by an un-pooling operation and compensates for detailed information with the input feature.  The Dynamic Grained Router is discrete in nature. The authors use the Gumbel softmax trick to train it. To encourage the efficiency, the authors add a (soft) budget regularization to the loss, which can be trained with gradient-based methods.  The authors apply the Dynamic Grained Router to DEIT and PVT, and show that the proposed Dynamic Grained Router can effectively reduce the FLOPs while nearly keeping the accuracy. However, in terms of actual runtime, the overhead from Dynamic Grained Router diminishes the theoretical FLOP gains in the low-resolution region. ","The authors propose a dynamic grained encoder for vision transformer, which introduces sparse queries to exploit the spatial redundancy. The novel method reduces the computational cost by 40%-60% under comparable performance on image classification. The generalizability of the proposed method is demonstrated on object detection and semantic segmentation. Besides, the authors analyze the intrinsic spatial redundancy of natural images and the properties of the proposed method, which are very interesting.",This paper investigates the spatial redundancy of natural images and proposes a novel Dynamic Grained Encoder(DGE) for vision transformer. Experiments show that simply replacing the transformer encoder block with DGE can reduce half complexity while the performance is competitive. The visualization shows how it works and the results on object detection and segmentation further verify its feasibility.  ,"The paper proposes a dynamic approach to split an patch into different sizes to save computations for vision transformers. In particular, the authors first split an image into different regions. And for each region, the authors propose a dynamic approach to select one of three predefined patch sizes. This is achieved with a gumbel-softmax function, which enables deriving categorical decisisons in a differentiable way.  The authors experimented with DeiT , PVT  and DPVT. ",0.1261682242990654,0.11214953271028037,0.12149532710280374,0.2535211267605634,0.18309859154929578,0.1896551724137931,0.38028169014084506,0.41379310344827586,0.3561643835616438,0.3103448275862069,0.1780821917808219,0.1506849315068493,0.1894736842105263,0.17647058823529413,0.18118466898954702,0.2790697674418604,0.18055555555555552,0.16793893129770993
77,SP:0fe6c2418c05deed8294323df9471fe52409524f,The paper proposes a method for incorporating future trajectory information when training model-free RL methods. Two technical challenges were reducing over-reliance on future information and allowing the policy to make predictions without future trajectory information at deployment time. These are addressed by introducing an information bottleneck. Optimization issues with latent variable models are tackled using two different Z-forcing approaches. The method shows favorable empirical performance in both online and offline settings. ,"This paper presents Policy Gradients Incorporating the Future (PGIF), a novel approach to incorporate future information during training to improve performance of model-free RL agents that must overcome the challenges of planning in environments with sparse rewards. The approach involves relying on recent work in ""Z-forcing"" in which training incorporates information about the future, encoded as a learned latent state that depends on the full not-yet-executed trajectory and state information. The future information helps the agent to learn effectively and overcome challenges associated with credit assignment notorious in partially observed environments; additional losses are added to ensure that the latent sate contains sufficient task-relevant information and that the planner does not rely too heavily on future information. The authors show solid results in a number of challenging environments, outperforming competitive approaches (including PPO) in nearly all of these, showing the effectiveness of the technique in both Online and Offline RL experiments.","The work proposes to run two recurrent LSTM neural networks backwards from the end of an episode, giving the rewards $r$ and states $s$ as input to the networks, and producing two separate latent state ($z$, $u$) distributions for each time step. Such procedure can be done during training after having collected an episode. Once, the $z$ and $u$ are computed, these are given as an input to the Q-function $Q(s,a,u)$, and the policy network $\pi(a|s,z)$ (note that the action was already sampled during the episode, but here the probability is recomputed using the new $z$). And the policy gradient/critic updates are compute as usual together with this augmented state. One remaining question is how to pick the $z$ during the episode when the future states are not known yet. This is done by training a prior network $p(z|s)$ by minimizing the KL divergence to the latent state distribution obtained from the backwards network (then one can just sample a $z$ from this prior network during the episode). To aid the backwards network in learning meaningful latent representations, they use z-forcing, which is a technique where the variable $z$ is used directly to predict some quantity to ensure that it incorporates useful information. They consider two options: 1) predict $b$ the hidden state of the backward RNN, 2) predict the values, rewards, discounts. (Note that this z-forcing is done for both $z$ and $u$).  They perform experiments on Bsuite umbrella-length, Gym-minigrid, custom partially observable MuJoCo environments, Offline RL on MuJoCo D4RL tasks. They compare with PPO, SAC, and other baselines. The experimental results improved in all cases, but their method also requires more computational time due to having to train the LSTMs.","The paper proposes to incorporate future information for more accurate policy and Q-function estimation, and consequently to derive variants of policy gradient based algorithms such as PPO, SAC, and BRAC. The authors argue that the ability to condition on the future information enables better credit assignment. The authors empirically demonstrate that their approach improves performance on a range of tasks that feature various challenges, including sparse rewards credit assignment, partial observability, and offline RL.",0.33783783783783783,0.25675675675675674,0.1891891891891892,0.20512820512820512,0.15384615384615385,0.07432432432432433,0.16025641025641027,0.06418918918918919,0.18666666666666668,0.10810810810810811,0.32,0.29333333333333333,0.2173913043478261,0.1027027027027027,0.1879194630872483,0.1415929203539823,0.2077922077922078,0.11859838274932616
78,SP:1001b5d5d846040d79d4db97b699e1f6837c72f5,"This paper presents an efficient algorithm for variational inference of spatio-temporal processes using Gaussian processes as the variational family. The paper leverages so-called conjugate-computation variational inference to compute/represent distributions in the approximating variational family. By leveraging Markov, separability of the covariance kernel and prior sparsity assumptions on the spatio-temporal process, the paper demonstrates that the cubic computational complexity can be improved to linear in terms of the time variable. ","The paper presents a novel approach to variational inference in Gaussian Processes for spatio-temporal data (including non-gaussian likelihood), using state-space representation for linear scaling in the temporal dimension. The authors show how using natural gradients and conjugate-computation VI approach leads to decomposition of the ELBO which, together with linear-time filtering and smoothing of the state-space representation, results in significant computational gains.   The results are supported experimentally on one synthetic and 2 real datasets (conjugate and non-conjugate data). The proposed models outperform the baselines (sparse variational GP).  Overall this is a very solid paper.","The authors propose a novel combination of existing strategies for scalable inference for spatio-temporal GPs. The techniques utilized were Kernel separability for convenient Kronecker structure, inference by filter-smoother in the  time domain, inducing point methods, and what appear to be 2 original contributions, a mean-field approximation to their spatial latent processes, and some computational strategies leveraging parallelization of the optimization. The authors make some camparisons to other methods in terms of computation time and accuracy.",The paper presents an approach for GP-Regression in the case of spatio-temporal data. The paper discusses a new scalable approach in the temporal domain by utilizing Bayesian filtering and smoothing in conjugation with conjugate-computation variational inference. Further scalability in the spatial domain is established by the use of an inducing point method with and without a mean field assumption.,0.25675675675675674,0.22972972972972974,0.24324324324324326,0.17,0.19,0.19230769230769232,0.19,0.21794871794871795,0.2903225806451613,0.21794871794871795,0.3064516129032258,0.24193548387096775,0.21839080459770116,0.2236842105263158,0.2647058823529412,0.19101123595505617,0.23456790123456792,0.2142857142857143
79,SP:101ad0e1348601c48e98d61867fd41fc2bf5de78,"The paper presents an array of techniques for solving the problem of decomposing a single image of a scene into a set of NeRFs, each of which represents an object in the scene. The decomposition requires accurate estimation of scene depth and the segmentation of the scene into objects. To achieve this, the authors propose an enhanced version of NeRF that operates on RGB-D images where its rendering only requires two evaluations per each camera ray. The authors show that the proposed method works effectively on CLEVR-3D and a custom built dataset named MultiShapeNet.  ","The authors propose a method to synthesize novel views of 3D scenes while inferring a decomposition of the scene into multiple objects.  The compositional reasoning is achieved by a slot-based encoder so that there's no need for specific supervision on the object categories. The authors also contribute with a novel loss function that exploits depth training data to improve the sampling strategy and therefore increasing training speed as well as lowering the reconstruction error. The results out-perform the state-of-the art, and are validated against 2D and 3D datasets.","The authors presented a method to infer a neural radiance field per object in a scene from a single input view.  uses  Object-centric learning with slot attention  The proposed architecture takes the shape of an autoencoder. The encoder side takes a single image and pose as inputs and generates N latent codes for each object in the scene following  Object-centric learning with slot attention (NeurIPS2020). The latent codes condition shared NeRF decoders to define the geometry and appearance of each object. The full scene is reconstructed by superposing the multiple Radiance Fields.  Furthermore, to reduce the computational complexity of NeRFs, the authors propose to use GT depth maps to avoid full ray marching.","This paper is about unsupervised segmentation both of 3D volumes and of images. The idea is to learn a set of Neural Rendering Fields (NeRFs) which explain a given image. To reduce computationally effort, the authors assume RGB-D data and show how that reduces the computational burden by two magnitudes. The proposed algorithm can learn segmentation masks and a set of NeRFs that generate each individual object and the background.",0.1875,0.21875,0.19791666666666666,0.23655913978494625,0.15053763440860216,0.13043478260869565,0.1935483870967742,0.1826086956521739,0.2676056338028169,0.19130434782608696,0.19718309859154928,0.2112676056338028,0.19047619047619047,0.1990521327014218,0.22754491017964074,0.21153846153846156,0.17073170731707316,0.16129032258064516
80,SP:107c3fc3e4ce15641549400935a4a740a34630de,"The paper studies the role of compression operators on the convergence of Distributed SGD with Error Feedback. In particular, via simple observations, the authors conclude that a hard-threshold sparsifier with a carefully tuned threshold parameter minimizes the total error appearing in the analysis because of the presence of compression. Moreover, they show empirically the connection between poor behavior of EF-SGD with Top-k compression and the severe error accumulation.  Motivated by these observations, the authors derive new convergence guarantees for EF-SGD with absolute compressors. This class of compressors covers hard-threshold sparsifier. The derived bounds show that the compression does not affect the slowest terms in the bound. Moreover, the authors derived the first complexity result in the non-convex case for EF-SGD with $\delta$-cnotraction operators without bounded gradient assumption and $n > 1$ (though, under bounded data dissimilarity). Finally, the paper contains a good empirical study of the performance of EF-SGD with the hard-threshold sparsifier. The authors also provide an insight on how to tune the threshold parameter in order to outperform EF-SGD with the Top-k operator.","The paper considers gradient sparsification for learning in distributed setup, and advocates using a hard-threshold sparsifier combined with error-feedback mechanism. The paper shows that such algorithm is _optimal_ in a certain sense, and give several convergence guarantees for the error-feedback algorithms using absolute compressors and relative compressors. The empirical performance of HT and top-k compressors are also compared.","This paper analyzes the hard-threshold sparsifier in distributed SGD with convergence analysis and extensive experiments. Main contributions include: 1. Provides upper bounds of the optimization errors for the hard-threshold sparsified distributed SGD, which improves from top-k compressor with linear speedup and compressor operator parameter dependence. 2. Conducts extensive experiments to demonstrate the benefits of hard-threshold sparsifier, achieving much better performance given the same average compression density (# of used coordinates / # of coordinates of DNN weights). ","In this paper, the authors demonstrate that, in the context of distributed optimization problems with $n$ workers, the hard-threshold sparsifier is the optimal sparsifier for a proposed communication complexity model which where the goal is to minimize the total error for a sequence of responses. This allows the authors to compare the sum of compression errors for various algorithms and to demonstrate that while for the per-iteration $k$-element budget the Top-$k$ sparsifier is optimal, when it comes to total error the hard threshold sparsifier is better.   The authors also compare the convergence rates of the top-$k$ sparsifier vs the hard threshold sparsifer for image classification, language modeling, and recommendation tasks. ",0.12365591397849462,0.12365591397849462,0.1881720430107527,0.1935483870967742,0.27419354838709675,0.2692307692307692,0.3709677419354839,0.2948717948717949,0.30434782608695654,0.15384615384615385,0.14782608695652175,0.1826086956521739,0.18548387096774194,0.17424242424242425,0.23255813953488372,0.17142857142857143,0.19209039548022597,0.21761658031088082
81,SP:10b51527ae0ecf2887fada1328dd9920f5a06e81,"The paper presents a framework called CodeTrek for processing and learning source code. The main idea is to represent code as a graph, manually design analyses to serve as additional relations, and perform a set of random walks on the graph. These walks can also be manually guided. Finally, the set of walks is encoded to represent program elements.","A method to learn to reason about programs is presented. The core novelty is an intermediate program representation as a set of relations (in the logics sense), derived from existing program analysis tools. By translating each tuple into a node, and references between relations into edges, the set of relations is transformed into a graph. Finally, graph learning techniques (from the random walk view of the world) are used to learn to predict the target label from the obtained graphs.","The paper presents a framework for creating task-specific program representations for deep learning based on program analysis and random graph walks. The basic idea is to use program analysis to create a graph-based representation of the program, capturing both syntactic and semantic relationships between elements. This graph-based representation of the program is then encoded by embedding a set of (random) walks in the graph. The paper evaluates the framework using four tasks: three tasks that only require intra-procedural program analysis (var-misuse, infer exception type, predict where a definition is used) and one that required inter-procedural analysis (variable shadowing). CodeTrek is shown to outperform existing techniques in most benchmarks.  ","This paper proposes a new program representation approach CodeTrek, which leverages a program analysis tool (Semmle) to produce the rich representation of the context for a program. Semmle is able to convert the program into a relational database that can capture the semantics behind the program, furthermore, it also supports extracting the task-specific semantics with the query language CodeQL to represent new semantic information. Then CodeTrek utilizes a biased graph-walk mechanism for pruning the context paths and feeds them to the transformer encoder to learn the path representations and followed by deepset to get a vector representation. The extensive experiments on four diver Python tasks: variable misuse, exception prediction, unused definition and variable shadowing prove that CodeTrek can outperform the current baselines i.e., Code2seq, GGNN, GREAT, CuBERT by a significant margin.",0.23728813559322035,0.4067796610169492,0.2542372881355932,0.225,0.2375,0.24561403508771928,0.175,0.21052631578947367,0.11194029850746269,0.15789473684210525,0.1417910447761194,0.208955223880597,0.2014388489208633,0.27745664739884396,0.15544041450777205,0.18556701030927833,0.17757009345794392,0.22580645161290322
82,SP:10b5f7e1a8f2c0a32236d2b675c1a2ce8212a751,"The paper proposes an accelerated MCMC method for sampling, motivated by Nesterov's Accelerated Gradient (NAG) method. Starting from the high resolution ODE of NAG obtained in Shi et al, the paper applies a two stage mechanism to remove the Hessian-dependency. The obtained first order ODE system serves as the backbone of the proposed diffusion process, Hessian-Free High-Resolution (HFHR) dynamics. Discretization the HFHR dynamics leads to the proposed sampling method. Theoretical convergence are provided for both the continuous and the discretized variant, showing acceleration to existing underdamped Langevin method (ULD). ",This paper introduces Hessian-Free High-Resolution SDE inspired by Accelerated Gradient (NAG). The author shows that continuous solution achieves an acceleration over the underdamped Langevin. A discrete algorithm also has speed-up with a constant factor.,"This paper introduces a stochastic process called HFHR. The SDEs of HFHR is derived by rewriting Nesterov’s Accelerated Gradient (NAG) into phase-space representation, formulating it as ODEs, and then injecting noise into both position and momentum variables. HFHR can be used for sampling because it has stationary distribution as the target distribution. A discretization of HFHR is given by operator splitting and Euler integration. A mixing time bound of $\widetilde{O}(\sqrt{d}/\varepsilon)$ is obtained for sampling log-strong-concave-and-smooth target distribution with extra third-order growth condition.","The author proposed an accelerated-gradient-based MCMC method based on Nesterov's accelerated gradient (NAG). In continuous time, the algorithm is able to achieve a tremendous acceleration over the underdamped Langevin algorithm. Numerical schemes can propose a speed-up with a constant factor. ",0.10752688172043011,0.13978494623655913,0.17204301075268819,0.2702702702702703,0.43243243243243246,0.10752688172043011,0.2702702702702703,0.13978494623655913,0.36363636363636365,0.10752688172043011,0.36363636363636365,0.22727272727272727,0.15384615384615385,0.13978494623655913,0.23357664233576644,0.15384615384615385,0.39506172839506176,0.14598540145985403
83,SP:10e833e06ca62549ddb3da7524de4e881cdb60f7,"The paper proposes a method for characterizing the ""meaning"" of the representation learned by a given model (interpretability). Towards this goal, it proposes reverse linear probing, a post-hoc method that aims at predicting a quantized version of the internal representation (as observed in specific examples) from semantic label.  Special emphasis is given in the capability of the proposed method on enabling the interpretation of models learned in a self-supervised manner.  PROs + Code will be released upon acceptance. + Evaluation covers a good variety of models.  CONs  - Missing related literature - Unclear how interpretability is gained.","This paper studies how human-interpretable are the concepts learned within the representation space of self-supervised models. It argues that linear probing methods are unable to identify whether a representation space contains a specific concept because the input might contain multiple confounding concepts (“red apple”) and might encode them in a manner that renders “red” and “apple” individually linearly inseparable. Instead, it proposes a “reverse linear probe” moving from which maps combinations of binary concept labels to the representation space, which is clustered using k-means.  The paper uses this linear probing method to propose a normalized mutual information metric to measure how well human-interpretable concepts are encoded in the representation space. Even though they find that their metric is correlated with linear probe, there are some interesting insights. First, the paper finds that models trained on ImageNet images capture more than just semantic class labels. They capture information about textures, scenes, objects, etc. Second, they found that additional training epochs dont lead to more interpretable representations even though it increases end-task performance. They also identified that OBoW, does really well even with only 200 epochs (though the reasons for why is still left to be investigated). Clustering-based approaches generally produce more interpretable representations. Fourth, ViT is a better architecture. Finally, qualitative evaluation of clusters seem to justify their utility of the reverse probe since NMI increases as when accounting for multiple categories for objects that occur together (ex, french horn and people).  ","This paper presents an approach to investigate the semanticity of representations learned via self-supervision applied to images. The approach measures (via mutual information) how well a linear model can map a vector indicating image attributes (e.g. objects, texture, etc) to clusters within the representation space. The authors apply this method to recent methods from the literature and interpret the resulting ranking, aiming to learn more about which methods produce representations that map well to human judgements of similarity.","The paper aims to measure the interpretability of the visual representations learned by the recent self-supervised models. In doing this, the paper formulates the “interpretability” as the mutual information between the feature clusters and a set of visual concepts that can be interpreted by humans. The mutual information is approximated by the proposed Quantized Reverse Probing, a linear function that maps the concept vectors to the feature clusters. The proposed method is claimed to be complementary to linear probes, with further advantages: (1) a single principled score rather than individual prediction scores for different labels in linear probes (2)  handle combinations of concepts better than linear probes. (3) Faster than linear probes.",0.23157894736842105,0.1368421052631579,0.21052631578947367,0.10526315789473684,0.10121457489878542,0.25,0.08906882591093117,0.1625,0.17699115044247787,0.325,0.22123893805309736,0.17699115044247787,0.1286549707602339,0.14857142857142858,0.1923076923076923,0.15902140672782875,0.13888888888888887,0.20725388601036268
84,SP:118c4b71355b95d468197f92495538987b2018cb,"This paper studies a new multi-armed bandit setting termed ""Gang of Adversarial Bandits"", in which a learner solves an instance of the adversarial MAB problem for each of $N$ different users. Without further assumptions on the $N$ users, essentially one can only solve the instances independently (e.g., running $N$ copies of the EXP3 algorithm), and incur a loss that scales as $\sqrt{KNT}$, where $K$ is the number of actions and $T$ is the time horizon.  In contrast, the authors assumes that the connections between the users are specified by a known weighted graph $G$, and assumes that nearby vertices in the graph (measured in terms of the effective resistance) are more likely to have the same action in the ""benchmark"" mapping $y: [N] \to [K]$. Both algorithms proposed in the paper are instantiations of the ""specialists"" framework, using graph $G$ to define certain initial/prior distributions over a carefully-chosen family of ""specialists"".  This approach involves sampling from the (exponentially large) space of mappings from users to actions, so a naive implementation would be computationally costly. The authors show that by sampling a random spanning tree and then embedding the users to a binary tree, the running time per step can be significantly reduced to either $O(K\log N)$ or $O(\log K\log N)$.","This paper studies the following contextual variant of the adversarial multi-armed bandit problem. Each round t (for T rounds), the adversary begins by choosing a context (“user”) u_t in [N] and a loss function l_t from [K] -> R, and reveals the context to the learning algorithm. Based on this context, the learner must select an action a_t in [K]. The adversary then reveals the value of l_t(a_t) to the learner, which the learner suffers as loss. By running a separate instance of EXP3 for each context, it is possible for the learner to achieve a total loss of O(sqrt(NKT)). This paper asks the question of whether we can do better if we believe that “similar” users should be subject to “similar” losses.  The authors answer this in the following way. They assume there is some given graph structure on the N users (“social network graph”), where users i and j are connected with a link of weight w_{i, j} (and higher weights mean users are more likely to have similar losses). They then give the following non-uniform regret bound: against the fixed policy y : [N] -> [K], their algorithms get regret of the form ~ O(sqrt(KT f(y))), where f(y) is a function that measures how “consistent” y is with the social network graph. For example, if y(s) is a constant function (doesn’t depend on user), then this is very consistent with any graph structure, and f(y) ~= log(N). On the other hand, for arbitrary y, y(s) can be as large as ~O(N), recovering the original ~O(sqrt(KTN)) bound. Formally, f(y) is something like the average effective resistance between users with different values of y(s).   The author’s algorithms are based on a technique they call “predicting with specialists” (extending the work of Freund et al.), where one maintains a distribution over functions mapping contexts to either actions or the decision to abstain. In particular, it’s possible to get a non-uniform regret bound for such an algorithm that only depends on the starting distribution and the policy to which the algorithm is being compared (similar in flavor to the regret bounds above). The authors then define two different starting distributions which lead to two algorithms (GABA-I and GABA-II) with good regret bounds for their setting. The first starting distribution has a higher support size (and larger eventual runtime), but lower overall regret than the second. The second algorithm is based off a clever embedding of the social network graph into a full binary tree that partially captures the connectedness of the original network graph.  Finally, the authors demonstrate how to implement their algorithms efficiently. In particular, even though the first distribution has an exponentially large support size (and thus naively would require exponential time / space to run), they show how to implement it in only O(K log N) time per round via a clever online belief propagation algorithm. Similarly, they show how to implement GABA-II in only O(log K log N) time per round (exponentially faster than GABA-1).   ","This paper considers the problem of contextual bandit problem with adversarial loss and finite number of contexts. Specifically, the authors consider the case where the contexts are related in some sense such that the action generated by a policy seeing one context may be related to the one seeing another similar context. The similarity is measured by a weighted undirected graph G. The authors then design two efficient algorithms which achieves $\widetilde{O}(\sqrt{\Psi(y) K T})$ regret bound in this setting where $y$ is the benchmark policy and $\Psi(y)$ is a dispersion measure of $y$ over the context. This result is better than $\widetilde{O}(\sqrt{NKT})$, where $N$ is the number of contexts.  The algorithm is mainly based on classic EXP3/EXP4 algorithm. As generally EXP4 is inefficient, the authors provide two efficient implementation by using online belief propagation over a complete binary tree for contexts, which is generated from the graph G. The first algorithm GABA-I has $O(K\ln(N))$ computation complexity per round while the second algorithm GABA-II has $O(\ln(K)\ln(N))$ computation complexity but with a factor of $\log(N)$ worse regret bound.","In this paper the authors consider the problem of adversarial contextual bandits, where the contexts (finite) and the reward function are adversarial, but there exists a graph structure among the various context vectors. The goal of the online learning algorithm is to select a mapping from the context to an action that minimizes the total regret. Here, the aim is to achieve results better than naively treating the cross product of contexts and actions as an arm and running a EXP3 type algorithm. The authors present regret bounds that depend on an interesting quantity of the graph: weighted effective resistance. This quantity improves the regret to logartihmic when the max-cut size in the graph is small and recovers the worst-case bound (treating the number of context times the number of actions as the arms).",0.3333333333333333,0.1917808219178082,0.1689497716894977,0.13498098859315588,0.08935361216730038,0.17435897435897435,0.13878326996197718,0.2153846153846154,0.27205882352941174,0.3641025641025641,0.34558823529411764,0.25,0.1959731543624161,0.20289855072463767,0.20845070422535208,0.19694868238557559,0.1419939577039275,0.20543806646525678
85,SP:12039ec26a618f159ff3524b3646b246f8883468,"Given a task hierarchy specified as a dependency graph of milestones, this work employs a two-level hierarchical reinforcement-learning method similar to Kulkarni et al. 2016.  The lower level learns to achieve a given milestone in a classical fashion, while the upper level learns to choose milestones such that ultimately the agent will achieve the overall goal.  The core contribution is a method for pruning candidate milestones (upper-level actions) that are not currently achievable (afforded to the agent by the environment), using an affordance classifier trained on the fly.","This paper incorporates the concept of affordances (the idea that humans and agents perceive the world in terms of relevant action possibilities) from ecological psychology with hierarchical reinforcement learning. While affordance theory has been previously incorporated into agent action-reasoning-perception models and more recently with reinforcement learning, this work specifically looks at incorporating the concept with HRL. This is significant because many complex problems need to be broken down into subtasks. One of the challenges of many RL domains is the large action spaces that agents have to explore. While HRL helps to reduce this burden by breaking down a complex task into sub-tasks the incorporation of an affordance concept can further reduce the action space which the agent needs to consider. This paper combines these two ideas and has the potential to allow researchers to use RL in more challenging domains.  The paper develops a theoretical framework for combining affordances with HRL and conducts a number of experiments in two simple grid world domains allowing for comparison with baseline RL algorithms.","The paper proposes Hierarchical Affordance Learning (HAL) that predicts sub-task affordance grounding on the current state. The affordance prediction can be used to prune impossible subtasks leading to a more efficient exploration.  The paper proposes a trick called ""predicting achievement context, "" which filters out false-negative samples during affordance classification.  The paper presents experiments on CRAFTING and TREASURE that demonstrate the proposed method's effectiveness and provides extensive ablation studies.","This paper is in the realm of hierarchical reinforcement learning, and the aim is to employ a particular kind of affordance model: milestones of subtasks which can be achieved and a coupling between states and ""affordable"" subgoal completion (from that state). Basically, this makes it possible to limit action choices (or more general: bias choices) to actions/subpolicies for which it is known from experience that they are in reach from this state with this action/subpolicy. The paper introduces the HAL method, in which an indicator function for the milestones, a state generalization function (mapping similar affordance abilities to similar abstract states), and low level and hight level Q-functions are all learned in parallel. Using two domains, the method is evaluated and put against competetive baselines and it is shown that there are benefits in general, but also for specific aspects such as inexact milestone knowledge or stochasticity.",0.17582417582417584,0.10989010989010989,0.2087912087912088,0.09195402298850575,0.16666666666666666,0.23943661971830985,0.09195402298850575,0.14084507042253522,0.12666666666666668,0.22535211267605634,0.19333333333333333,0.11333333333333333,0.12075471698113208,0.12345679012345681,0.15767634854771787,0.13061224489795917,0.17901234567901234,0.15384615384615385
86,SP:12253758a6225e0b5fcc40c7d0cd801d023f3761,"The paper proposes Cycle MLP architecture, the idea is to bring spatial context into Channel FC and increase its receptive field.  The main objective of the paper is to address the challenges faced by the current MLP-Architectures. Cycle MLP allows flexible image resolution and avoids quadratic computational complexity in dense architectures. The paper presents results in a variety of tasks and shows demonstrates promising performance.  ","This paper presents a  MLP-like architecture, CycleMLP. The proposed CycleMLP can be used for dense prediction, which is more suitable for object detection or image segmentation tasks.  The experiments show the effectiveness of the proposed model. The proposed Cycle FC can reduce the amount of network parameters and calculation, and is insensitive to image resolution.",This paper proposes an improvement to existing MLP style models for image tasks that extends the general MLP competitive work compared to CNN models while attempting to capture some desirable CNN properties such as applicability to varying image sizes.  In addition the method has linear computational complexity as compared to previous MLP work which have quadratic complexity.  The basic idea is a variant of previous channel and spatial FC ideas by having a node's receptive field sample across spatial and channel domains rather than being fixed to either just channel or spatial aggregation.  The overall architecture uses the previous work on patch embeddings and idea of stages with stacked blocks.  Comprehensive experimental results are show improvement over previous MLP models and competitive results to other methods such as CNNs and transformers on multiple tasks including classification and segmentation.,"This paper presents a simple MLP-like architecture, CycleMLP for image classification, object detection and segmentation. It can cope with various image sizes and achieves linear computational complexity to image size. CycleMLP achieves competitive results on object detection, instance segmentation and semantic segmentation. Further, it outperforms Swin-Tiny on the ADE20K dataset with fewer FLOPs. ",0.19696969696969696,0.2727272727272727,0.15151515151515152,0.25,0.2857142857142857,0.11510791366906475,0.23214285714285715,0.12949640287769784,0.18181818181818182,0.10071942446043165,0.2909090909090909,0.2909090909090909,0.21311475409836064,0.175609756097561,0.16528925619834708,0.1435897435897436,0.28828828828828823,0.16494845360824742
87,SP:1282f86205fb0fa45a27c7c85de35e3124f409a1,"This paper studies the problem of a seller who interacts repeatedly with a buyer.  In each round an item is up for sale, and the buyer draws a value from an unknown (to the seller) distribution.  The buyer is assumed to be less patient than the seller, and the seller's goal is to maximize revenue.  Prior work had established low-regret sales strategies for the seller when the buyer is a utility maximizer.  This paper considers a value-maximizing bidder, who wants to maximize value obtained subject to an interim ROI constraint.  The authors construct a dynamic mechanism with regret O(T^{2/3}), versus the best revenue attainable if the value distribution were known.  At a high level the approach is similar to the utility-maximizing case: run an explore phase that obtains low revenue but learns about the buyer's value distribution, then deploy a mechanism aimed at extracting near-optimal revenue from what was learned.  The key here is to incentivize the buyer not to misreport in the explore phase, which is done by using a strictly truthful per-round auction (and leveraging the assumption that the buyer is less patient).  For a value-maximizing buyer, what is needed for strict truthfulness is different.  So a main technical contribution of the work is to propose a new single-round auction format that is strictly single-stage incentive compatible.  ","This paper considers the dynamic mechanism design problem to maximize total expected revenue against a single bidder, who aims to maximize time-discounted cumulative expected value, while subject to an interim ROI constraint each period. The work proposes a dynamic mechanism consisting of an exploration phase during which the seller runs an incentive-compatible mechanism to estimate the bidder’s value distribution; and also an exploitation phase that approximates the single-stage optimal mechanism against an ROI bidder w.r.t. the distributional estimates. The paper shows that the design limits the magnitude of the bidder’s misreporting, and proves that the proposed dynamic mechanism achieves a $T$-period regret in the order of $T^{2/3}$ against the optimal revenue of a clairvoyant seller who knows the bidder’s value realizations.","The paper studies prior-independent dynamic auctions with a single value-maximizing buyer. In this setting, the buyer's objective is to maximize her obtained value (rather than utility) under ROI constraint (i.e., the value is at least $\tao$ times higher than the price paid). The authors show a dynamic auction mechanism that without having no prior information on the buyer’s value distribution (except from being bound in [0,1]), can extract a near-optimal revenue, i.e., sublinear regret for time horizon T. The work is under the assumption of a patient seller and a buyer has a time discount factor $\lambda$.  The algorithm is divided into three parts:  1. Exploration phase that incentivizes the buyer to report approximately truthfully. To my understanding, this is the main new technique in the work.  2. A buffer phase.  3. Exploitation phase that extracts the revenue using a robust version of the single-stage revenue-optimal mechanism for this problem.  ","This paper studies how a seller should design its auctions to maximize her revenue when facing an impatient buyer optimizing his value under a ROI constraint. It mostly consists in plugging together two previous works, Balseiro 2021 and Amin 2013 to extend Balseiro 2021 to the setting where the value distribution is unknown a priori and needs to be learnt. Following Amin 2013, the proposed algorithm has two phases (an explore then commit schema) and the first phase is designed to make him reveal his value distribution, which allows to implement an optimal mechanism in the second phase. Provided that the buyer is impatient enough, the first phase length can be tuned to incur an overall sub-linear regret.",0.1810344827586207,0.2025862068965517,0.15517241379310345,0.25,0.18181818181818182,0.175,0.3181818181818182,0.29375,0.3025210084033613,0.20625,0.20168067226890757,0.23529411764705882,0.23076923076923075,0.23979591836734693,0.20512820512820512,0.22602739726027396,0.19123505976095617,0.2007168458781362
88,SP:12c230492d2eae7ee35227469262f6c0ac6534e4,"This submission addresses the problem of learning diachronic word representations. A new method is proposed that models words as functions and does not rely (it is claimed) on learning representations for discrete time intervals first and then postprocessing them.  The new method is potentially more powerful than prior work since it can in theory approximate any continuous process.  It uses fewer parameters, can model longterm processes and has improved time complexity.  Good performance on a number of tasks is demonstrated. ","This paper proposed a new paradigm to learn diachronic word representations, which models words as functions of time, so that the word meaning in a different time could be correlated and evolves within a continuous process. The paper adopted various functions such as linear function and sinusoidal functions to approximate the word meaning evolution in the context of distributed representations. The experimental results show that this approach is promising in the time-aware NLP tasks like time-aware word clustering, temporal analogy, and semantic change detection."," This paper presents a new approach to modeling word embedding change over time: each word $w_i$ is represented as a function $g_i(t) : N \rightarrow R^D$ that maps a timestamp $t \in \mathbb{N}$ to an embedding $e_{it} \in \mathbb{R}^D$. The function $g_i(t)$ can then be parameterized in a variety of ways; the paper focuses primarily on sinusoidal parameterizations. The paper motivates this approach via the Weierstrass theorem, from which it follows that a real function on the interval [a,b] can be well approximated by a trigonometric polynomial.  The key contribution of this paper is that it breaks out of the typical paradigm that has been used for diacronic word embeddings: i.e., that a separate word embedding $e_{it}$ is explicitly learned for each word $w_i$ and each timestamp $t$. Doing so comes with several drawbacks that this paper addresses: (1) that an alignment between timestamps must be learned (2) that the alignments are (usually) only dependent in a pairwise Markovian fashion, thus missing out on longer time effects (3) that space required is high.  This paper evaluates its method with different parametric functions (e.g. linear, word/time independent, and various sinusoids) on several benchmarks established in prior work for the evaluation of diacrhonic word embeddings and attains (mostly) better results than strong baselines from prior work.  ","This paper proposes a new paradigm for modeling temporal word embeddings. Instead of the commonly prevalent approaches that train static embeddings for documents belonging to each timestamp followed by an alignment step, this work attempts to learn embeddings as a continuous function of time. This work proposes function parametrizations that use sinusoidal polynomials which can model arbitrary continuous functions. The approach is empirically compared against relevant baselines on time-aware clustering, temporal analogy, and semantic change detection.",0.2,0.1875,0.125,0.29069767441860467,0.29069767441860467,0.09170305676855896,0.18604651162790697,0.06550218340611354,0.12987012987012986,0.1091703056768559,0.3246753246753247,0.2727272727272727,0.19277108433734938,0.09708737864077671,0.12738853503184713,0.15873015873015875,0.3067484662576687,0.13725490196078433
89,SP:12f877aedfc97ccee901c35dea872f72329bf5fc,"The authors developed the evaluation method for the worst-case loss of a given predictor over all the subpopulations. The method is motivated to evaluate how a given predictor is robust against population change of the random variable Z.  The authors theoretically investigate the estimation error of the proposed method and show two high probability bounds on the estimation error. One of the bound depends on the complexity of the model to estimate the conditional mean of loss; however, we can achieve a convergence rate faster than $1/n^{1/4}$ if the model complexity is low. Another one is the data-dependent bound and is independent of the model complexity; however, the convergence rate is $1/n^{1/4}$. They also provide some showcases to utilize the proposed methods.","This paper focuses on evaluating the performance of a supervised learning model $\theta: X \to Y$ over a set of subpopulations $Z \sim Q$. The authors consider a setting with ""subpopulation"" shift -- i.e., where the training data under/over samples data from a particular group. In this setting, they propose to bound the ``worst-case"" group performance defined as: $$W_{\alpha}(\theta) := \max_{Q \in \mathcal{Q}_alpha} \mathbb{E}_Q[l(\theta(X),Y|Z)].$$ The authors present a new technique to estimate this quantity, and characterize its convergence in finite-sample settings. The authors apply their measures to determine the ""worst-case"" group-specific performance of models in 2 real-world prediction tasks: Warfarin dosage prediction, and land use from satellite images.  "," This paper poses the problem of evaluating the worst-case risk over subpopulations, which is an important way to quantify the fairness of a learning algorithm. Unfortunately they do not consider the learning aspect of this problem, but rather treat the model as fixed, however they do make substantial contributions, and show that even evaluating worst-case subpopulation risk is challenging in full generality.  It seems the future work could address this shortcoming. In particular, in their model, they allow for a sophisticated and abstract notion of subpopulations, defined by a function family $\cal H$ applied to a random variable $Z$, usually representing protected information. The authors explore an approach based on localized Rademacher averages, and one using a 2-fold cross-validation strategy, to control for the multiple comparisons aspect of considering many subpopulations in the worst-case. ","This focus of this paper is a method for the evaluation of Machine Learning models with respect to their worst-case performance over all sufficiently large sub-populations. This has important applications in validating the performance of a model from a fairness perspective and ensuring that its predictions do not adversely affect a disadvantaged group. The first contribution is to formalise the notion of “performance under worst-case subpopulation”. Here we have a distribution $P$ over random triples $(X,Y,Z)$ where $X$ is a covariate in $\mathcal{X}$, $Y$ is a label in $\mathcal{Y}$ and $Z$ corresponds to a set of protected variables e.g. income, ethnicity, gender etc. Let  $P_Z$ denote the associated marginal distribution with respect to a covariate $Z$. Given a lower-bound on the population $a \in (0,1]$, the associated collection of sub-populations is then given by, $$\mathcal{Q}_a:=\left\lbrace Q_Z:P_Z= \tilde{a} \cdot Q_Z +(1-\tilde{a}) \cdot Q_Z' \text{ for some }a\geq {a} \text{ and  probability measure }Q_Z'  \right\rbrace$$. Given a predictive model $\theta: \mathcal{Z} \rightarrow \mathcal{Y}$ and a loss function $\ell:\mathcal{Y}^2 \rightarrow [0,\infty)$, the worst-case sub-population performance with lower bound $a$ is then defined by   $$ W_a(\theta):=sup_{Q_Z \sim \mathcal{Q}_{a}} E[E[\ell(\theta(X),Y)| Z] ] $$  where $Z$ is sampled from $Q_Z$ and $(X,Y)$ is sampled from the conditional distribution of $P$ given $Z$. The next goal is to estimate the worst-case sub-population performance $W_a(\theta)$ based on a data sample.  The second contribution of the paper is to develop an estimator $\hat{W}_a(\theta)$. In essence this proceeds by finding an estimate $\hat{h}$ for the conditional loss $h^*(Z):=E[\ell(\theta(X),Y)| Z]$, and then applying a variational formula.  The third contribution is to provide a finite sample guarantee (Theorem 1) which gives a high-probability bound between  $\hat{W}_a(\theta)$ and $W_a(\theta)$. This first result uses two assumptions: 1) A bounded loss function and 2) the conditional expectation  $h^*(Z):=E[\ell(\theta(X),Y)| Z]$ belongs to a known low-complexity class $\mathcal{H}$. The deviation bound is given in terms of local Rademacher complexity.   The fourth intended contribution is Theorem 2 which attempts a to give a tight deviation bound on  $\hat{W}_a(\theta)-{W}_a(\theta)$ without relying upon the complexity of the class $\mathcal{H}$..  Up to this point the lower bound $a$ is effectively viewed as a user-specified parameter. Next the authors turn to an alternative framework where the quantity of interest is not  ${W}_a(\theta)$ for a single value of $a$, but rather $a^*$, the smallest value $a$ for which $ {W}_a(\theta)$ is below a user specified upper bound on the loss. The final theoretical contribution is Theorem 3 which gives an estimator $\hat{a}$ for $a^*$ and a high-probability upper bound on the deviation between $\hat{a}$ and $a^*$. The paper also includes an empirical illustration of the estimator in practice on two data sets for which worst-case performance is of interest. ",0.14615384615384616,0.17692307692307693,0.3769230769230769,0.192,0.384,0.30935251798561153,0.152,0.16546762589928057,0.09158878504672897,0.17266187050359713,0.08971962616822429,0.08037383177570094,0.14901960784313725,0.17100371747211895,0.1473684210526316,0.18181818181818185,0.14545454545454548,0.1275964391691395
90,SP:137d11a5ef308cba0e3cf35ed8bf93e45a49a383,"The paper introduces an analysis of the relationship between RL agent visual attention and human visual attention for Atari games. After training pixel-based RL agents, the work introduces comparisons at several stages of training, showing increasing correlation with a model of human visual attention. The paper then analyzes instances of matched or mismatched correlation, and provides a failure analysis of the RL agent’s behavior based on the visual attention synchronization. Extensive experimental details and further analysis is included in the appendix.","This work proposes to use human gaze data and an attention model derived from it to compare and contrast human attention and attention saliency maps extracted from PPO deep RL agents on Atari Learning Environment games. The authors analyze the effect of learning, discovering that learning improves similarity between agent attention and human attention. The authors evaluate how the discount factor contributes to similarity, finding that attentional similarity and performance peak simultaneously, with fairly high discount factors. The authors also perform some focused analyses of how attention varies in failure states for these artificial agents, and how attention behaves when attempting to generalize to unseen data from the same games. ","- This paper presents an empirical study of machine vs. human attention in Deep RL on Atari.  Specifically they examine how the attention of an CNN agent correlates with human attention - They find that human attention and RL attention correlate well and this correlation improves as agent performance improves.  For attention methods like low-level image feature salience, however correlation with RL attention reduces as performance increases, suggesting that RL attends to higher-level features in the image as training progresses. - The examine how the discount factor changes attention, specifically a lower discount factor helps RL learn to attend to similar things as the human experts. -  Finally they show that in failure states, RL attention often deviates from human attention -- although not always. ","The study aims to compare human and artificial attention in the realm of decision making. This research supports understanding of the function (and potential malfunction) of neural networks, and also aims to shed light on potential mechanisms of improved training of artificial agents. Human attention is operationalized as gaze data (which pixels do the eyes fixate?), and artificial agent attention is operationalized as the blurring of which pixels has the largest effects on behavior policy. ",0.20481927710843373,0.20481927710843373,0.18072289156626506,0.24545454545454545,0.14545454545454545,0.11475409836065574,0.15454545454545454,0.13934426229508196,0.2,0.22131147540983606,0.21333333333333335,0.18666666666666668,0.17616580310880828,0.16585365853658535,0.189873417721519,0.23275862068965517,0.17297297297297298,0.14213197969543148
91,SP:13c18c73f9cb5a28bd19f88f43d2f0b7a2aa983f,"This paper presents Knowledge Infused Decoding (KID), an RL-based approach for grounded decoding by conditioning on external knowledge. For every new example: - retrieve the relevant passages from Wikipedia (using dense passage retrieval DPR) - construct a knowledge trie of triples extracted from the passages and initialize a local trie that tracks all the nouns/verbs mentioned - Generate tokens conditioned on both prior context and by retrieving triples from the knowledge trie, used as demonstrations for policy gradient This approach is model-agnostic and can be used to fine-tune both encoder-decoder and decoder-only models.  Experiments are divided into two parts:   - evaluating the decoding method, on six NLG datasets -- outperforming beam search and (top-k/nucleus) sampling - evaluating the knowledge infusion, on four abstractive QA datasets -- outperforming a number of approaches including retrieval-augmented, graph neural network, and knowledge graph methods.  Ablation studies are presented for swapping out the retriever, different hyperparameters for KID and the size of the underlying LM. ","This paper deals with the well-known but important problem of language models -- enhancing language model with (external) knowledge. To that end, the authors present Knowledge Infused Decoding (KID), a novel decoding algorithm for generative LMs, which infuses external knowledge into each step of the LM decoding. This method maintains a local knowledge memory based on the current context and continuously updates the local memory via reinforcement learning to guide each step of language generation. As a result, it can integrate knowledge on the fly and perform well on various knowledge-intensive tasks such as abstractive QA, logic-centric writing, and dialogue generation.","This paper presents a text decoding strategy for knowledge-intensive text generation tasks with the goal to infuse external knowledge based information at each time-step of decoding process via efficient document retrieval, creation of local and fast global knowledge access data structure and the interaction guided decoding method. The decoding method uses RL based policy gradient for token selection. The method is evaluated using diverse types of knowledge-intensive tasks such as abstractive question answering, logic centric writing and dialog generation. This method works with existing decoder only or encoder-decoder LMs as a plug and play method and outperforms existing knowledge aware task optimized models and correlates well with human evaluation. ","The paper is interested in improving the performance of natural language generation tasks that require external knowledge (which are often called knowledge-intensive tasks). In particular, the paper focuses on how it can leverage existing language model (e.g. GPT-2, BART) without changing its architecture and injecting external knowledge into it. To do so, the paper proposes Knowledge Infused Decoding (KID), which alters the decoding probability at each time step (in vocab space) so that it favors decoding certain words, which are obtained by retrieving a relevant document from a corpus. In order to trains such decoder, the authors also add a KL divergence to the loss that prevents the decoding distribution not to be too different from the original Language Model's decoding distribution. When KID is used in conjunction with GPT2-M or BART-L, they show clear improvements, and in some datasets, they achieve higher accuracy than the state of the art. The paper also shows that KID outperforms Retrieval Augmented Generation (Lewis et al., 2020) which uses the retrieved documents directly into the input (whereas KID uses it indirectly to constrain the decoding in a soft way).",0.15432098765432098,0.1419753086419753,0.16666666666666666,0.2621359223300971,0.2524271844660194,0.19469026548672566,0.24271844660194175,0.20353982300884957,0.140625,0.23893805309734514,0.13541666666666666,0.11458333333333333,0.18867924528301885,0.16727272727272727,0.15254237288135594,0.25,0.17627118644067796,0.1442622950819672
92,SP:13d9ea639b1787c541379d25df52c2090822165c,"This paper first gives an analysis framework for controlling 2-Wasserstein distance of sampling algorithms coming from discretizing a continuous Langevin dynamics. This is achieved by first considering the bias (local weak error) and mse (local strong error) of single step update.  The multi-step error accumulate exponentially, but together with the contraction guarantee, the maximum sampling error after arbitrary steps could be controlled. Next, this paper introduces an assumption on the linear growth of the 3rd-order derivative which is shown to improve the convergence rate in terms of dependency of dimension. The convergence analysis is carried out based on the general analysis framework.","This paper introduces a framework for analyzing the discretization of SDEs such as the Langevin diffusion. Within this framework, once the discretization error incurred by the algorithm in a single iteration is controlled, it automatically yields a mixing time bound in Wasserstein distance for the algorithm. Importantly, this framework only applies to contractive SDEs which essentially requires the target distribution to be strongly log-concave (in particular, it cannot accommodate common weakinings of this condition, such as a log-Sobolev inequality). This framework is then applied to the standard Langevin Monte Carlo algorithm and under strong log-concavity, log-smoothness, and a condition on the third-order derivative of the potential, a mixing time bound of d^{½}/ε is obtained; this bound is shown to be tight on Gaussian targets. Prior bounds (using different third-order conditions) obtained at best d/ε.","This paper introduced a mean-square analysis to study the converge rate of overdamped Langevin dynamics to the Gibbs stationary distribution. In particular, the authors proved that the convergence in $W_2$ norm is of order $\sqrt{d}/\varepsilon$ which is also optimal. This is the same order as the underdamped Langevin equation, and beats all the previous rates. ","This paper derives an improved 2-Wasserstein convergence bound for the Langevin algorithm (ULA). As noted in the paper, the folk wisdom is that the previously known error bound for the ULA is O(d/epsilon), compared to the underdamped MCMC O(sqrt{d}/epsilon). The paper claims to improve the ULA bound in W-2 distance, matching the case of the underdamped bound. See my detailed comments for questions.",0.20952380952380953,0.1523809523809524,0.18095238095238095,0.1357142857142857,0.1357142857142857,0.23728813559322035,0.15714285714285714,0.2711864406779661,0.2714285714285714,0.3220338983050847,0.2714285714285714,0.2,0.17959183673469387,0.1951219512195122,0.21714285714285717,0.1909547738693467,0.18095238095238095,0.21705426356589147
93,SP:13dac3b480cd0581adc50711ff42e5ca88912850,"In this paper, the authors presented an algorithm coined Flinters to Identify Near-Neighbor Groups (FLINNG) to solve the approximate near neighbor search problem. Given a dataset $D$, FLINNG is capable of constructing an {\it index} which, when given a {\it query} $y$, outputs a set of points $x_i \in D$ with high similarity to $y$.  The near neighbor search is a problem with extensive research and a wide range of applications, including recommendation systems, social networks, computer vision, and genome sequencing. While the problem is particularly well-understood in low dimensions, with efficient algorithms to identify exact k-neighbors, but large-scale applications on big datas with high dimentionality often challenges existing algorithms and data structures.  The authors address this need by showing that FLINNG is an index with efficient construction time, query time, simple structure, and low memory requirement. FLINNG utilizes many previously developed techniques, most notably a reduction from approximate near neighbor search to approximate set membership, and solving the latter using distance-sensitive Bloom filters, with a lower bound on the true positive rate and a upper bound on the false positive rate.  To construct the index, FLINNG randomly distributes all data points in $D$ evenly among $B$ groups, and repeats $R$ times to create $R$ such independent instances. For each of the $B \cdot R$ groups created this way, FLINNG constructs a classifier using distance-sensitive Bloom Filters with appropriate error guarantees, which solves the approximate set membership problem on this group.  When a query on point $y$ arrives, FLINNG queries each of the $B \cdot R$ classifiers on whether the corresponding group contains a near neighbor of $y$. If the classifier of a group returns an affirmative result, that group is likely to contain a near neighbor of $y$, and is considered a {\it candidate group}. FLINNG then first takes the union of all candidate groups within the same instance, then eliminates noises and amplifies accuracy exponentially by taking the intersection of the unions across all $R$ instances. The resulting intersection is returned as the near neighbor set of $y$.  The authors show that FLINNG can be highly efficient due to the properties of distance-sensitive bloom filters, which allows queries to have constant memory allocation and sublinear time complexity. An implementation is designed and compared with other status-quo near neighbor search algorithms like FLASH, HNSW, and FAISS, on the PromethION data set. These experiments show that indices constructed with FLINNG exhibit a 2x to 10x speed up in queries, as well as decreased index size and construction time. ","The paper develops a new algorithm for approximate near neighbor search by combining ideas from group testing and bloom filters. The proposed algorithm called FLINNG casts the problem of finding the near neighbor of a query point in a high-dimensional dataset, as the problem of finding a few number of ""positive"" groups in a group testing problem where each group captures the absence or presence of at least one near neighbor of the query in a small random subset of data points. The paper provides theoretical justifications of the sublinear query time of the algorithm in number of data points and provides empirical evidence on real-world data sets which demonstrates comparable to superior near neighbor query performance compared to some to some baseline algorithms.",Near Neighbor Search (NNS) is one of the most fundamental problems in machine learning. This paper proposes an efficient NNS method for high-dimensional data by transforming the original problem into a group testing problem. The authors provide a detailed theoretical analysis of the distance-sensitive Bloom Filters for group testing to demonstrate how the query time and the quality are bounded with proper parameters settings. Experiments validate the performance of FLINNG.,"The paper tackles the problem of near-neighbor (and nearest-neighbor) search in high-dimensional spaces. The authors propose to use group-testing on top of distance-sensitive Bloom filters: to simplify, the returned set is the intersection of the union of small sets, each of these small sets being a hash bucket that the query falls into. The authors derive the theoretical properties of their method FLINNG (sublinear query time), and show the superiority of their method compared to the state of the art (FAISS, HNSW, FLINNG) through extensive experiments on genome, URL and embedding data.",0.09601873536299765,0.06791569086651054,0.09133489461358314,0.18253968253968253,0.23015873015873015,0.3194444444444444,0.3253968253968254,0.4027777777777778,0.4020618556701031,0.3194444444444444,0.29896907216494845,0.23711340206185566,0.14828209764918626,0.11623246492985972,0.1488549618320611,0.2323232323232323,0.2600896860986547,0.272189349112426
94,SP:13f851b9cc0dd377a67efeac462805551f300be2,"The paper proposes a randomized method for estimating the permanent positive matrix. It uses a recursive rejection sampling method but with a more fine-grained upper bound and provides an efficient way to compute it. They provide theoretical analysis on expected running time for random matrices, where the bound is superior for certain regimes of p, and provide compelling empirical experiments to show practical improvement over prior methods.","The authors addressed the efficient approximation of permanent of a matrix with nonnegative entries. The proposed scheme is based on the recursive rejection sampling method by Huber 2008, but the upper bound on the permanent is based on the linear combination of the subproblems with the deep recursion tree. The expected running time is both theoretically and experimentally analyzed. ","The paper gives a new algorithm to approximate permanents of non-negative matrices. The method builds upon the well-known rejection sampling method by Huber-Law (SODA'08). The algorithm is experimentally shown to perform well, and in particular better than the related result by Kuck et al. (NeurIPS'19). Finally, a mathematical analysis of the performance of the algorithm on a family of random matrices is given, where it is shown to run in polynomial time to yield arbitrarily good precision.","Computing the matrix permanent is known as a #P-complete problem and no exact polynomial-time algorithm is known. Several works developed a randomized algorithm for approximating the permanent of a non-negative matrix based on recursive acceptance-rejection (AR) sampling and various upper bounds of permanent. This paper adopts a deep AR sampling scheme that skips some recursive steps. Then, the authors additionally propose an efficient way for computing skipped steps based on dynamic programming of permanent. Empirical results demonstrate order-of-magnitude savings of computational time under random synthetic and real-world datasets.",0.25,0.23529411764705882,0.2647058823529412,0.2711864406779661,0.3050847457627119,0.17073170731707318,0.288135593220339,0.1951219512195122,0.18947368421052632,0.1951219512195122,0.18947368421052632,0.14736842105263157,0.2677165354330709,0.21333333333333335,0.22085889570552147,0.2269503546099291,0.2337662337662338,0.15819209039548024
95,SP:143b3d34b949bc6dd139003b9c9d57996238c55e,"This paper proposes a meta learning approach for time series. Specifically, the approach learns to select the best model and hyper-parameters for unseen test datasets, when trained on performance data of various models applied to training datasets. The approach utilizes various dataset specific features in order to make the predictions. Experimental results show an improved performance over other meta learning baselines. The corpus of time series datasets that was used for the experiments, is also provided as supplementary material.","This paper formulates the problem of automatic and fast selection of the best time-series forecasting model as a meta-learning problem.    Specifically, an AUTOFORECAST model consisting of a temporal meta-learner and a general meta-learner are proposed.     A large benchmark and a collection of models are established to evaluate the problem.     ","The authors address the problem of selecting the best forecasting model for a time series dataset. Datasets are described by a large vector of meta features, then two regression models are learnt to predict the test loss of a forecasting model for such a dataset at a given cutoff time point (called window): one predicts directly on the meta features computed until the cutoff time point, the other predicts autoregressively starting from the first cutoff timepoint until the target cutoff time point is reached. In experiments on two meta datasets (called performance tensors), one univariate, one multivariate, it is shown that esp. the autoregressive model outperforms the best constant model (called global best) as well as some baselines. ","This paper present a novel approach to addressing a fundamental problem in forecasting research: given the plethora of methods available and no clearly superior approach overall, which forecasting method should be chosen for a new dataset/forecasting task combination. The authors propose a combination of a static (not taking the time evolution of accuracy into account) meta-learning model with one that takes the evolution of accuracy over time into account. They apply this methodology using a number of well-chosen forecasting methods on public data sets. Some of these (e.g., Adobe traces), I hadn't see before, and their discovery may be an important contribution in itself.",0.15,0.25,0.1875,0.3018867924528302,0.2641509433962264,0.1440677966101695,0.22641509433962265,0.1694915254237288,0.13761467889908258,0.13559322033898305,0.12844036697247707,0.1559633027522936,0.1804511278195489,0.20202020202020202,0.15873015873015875,0.18713450292397657,0.17283950617283952,0.14977973568281938
96,SP:1442a14895d51d947b2fd2756aeaafd787aa5987,"In trained VAEs, mismatch between the prior and the variational posterior can lead to low-quality generated samples.  To address this issue, the authors suggest the following procedure: 1) given a trained VAE, train a classifier to distinguish images generated from the variational posterior vs images generated from the prior; 2) adjust the prior to minimize the accuracy of the trained classifier from step 1. This is done by reweighting the sampling procedure using the predictions from the classifier.  Main contribution: a practical method to improve the generative quality of several forms of VAE by reweighing the prior to diminish the mismatch from the variational posterior","Update after Response:  The response of the authors resolved some of my concerns and I am therefore improving my score by 1 point, provided that the authors add the additional information mentioned in their reply. I think the work is relevant and presents a potential solution to solve some of the problems that plague autoencoder sample generation.   Old: The authors propose a novel method of adapting the prior p(z) of VAE to the experimentally observed approximate posterior q_{\theta}(z) over the training samples. The modification of the prior is performed as a separate step, after model training. This improves the visual quality of generated samples drawn from the modified prior. The article includes an extensive evaluation and the appendix includes further results with important studies.","The paper proposes to learn an energy based model using noise contrastive estimation on the latent of a VAE to mitigate the prior hole problem. To do so, the paper proposes a very simple method that consists in first training a standard VAE, then, given the frozen VAE, training a discriminator in latent space to discriminate between samples from the prior, and samples from encodings of real images. The discriminator can then be used to reweigh the prior, and obtain a better, but non algebraically computable, prior distribution. The implicit distribution can still be sampled using importance resampling or langevin dynamics. The paper proceeds to show that this technique improves the performance of the model in term of FID on various datasets. ","The authors propose a way to improve the performance of a trained VAE by modifying the learned prior with a reweighting factor. The reweighting factor is computed using a network trained to discriminate between samples from the learned prior and samples from the encoder given real data. To sample from the VAE, the latent variables are sampled from the reweighted prior using MC inference (e.g. sampling-importance-resampling) and fed through the decoder. This leads to better samples (versus not using reweighting) but also greater computational cost due to the MC inference.",0.22641509433962265,0.20754716981132076,0.20754716981132076,0.18110236220472442,0.15748031496062992,0.26229508196721313,0.1889763779527559,0.18032786885245902,0.23655913978494625,0.1885245901639344,0.21505376344086022,0.34408602150537637,0.20600858369098712,0.19298245614035087,0.22110552763819097,0.18473895582329317,0.18181818181818185,0.2976744186046512
97,SP:1451c562101b4b3881b1a767cc1c72f7f8c3cb0d,"The paper addresses the problem of adapting a policy to a novel representation of the environment. Different from most previous work the paper focuses on the problem of a change in the representation of the observation provided by the environment, i.e., assuming that the underlying dynamics P and R remains mostly similar. The idea is to learn source and target models of the environment jointly with the policy. This serves as a model-based regularization that guides policy and representation towards the learned models. This effectively leads to a learned representation that is shared for the policy for the source and the target environment representation.","This paper proposes to leverage models of the environment and rewards as regularizers to perform explicit transfer between different observation spaces in RL.  Being able to quickly adapt to different observations is quite a different problem setting than what the rest of the literature is tackling (and we could argue on how relevant it is, but let’s leave that aside). They provide some theoretical analysis of this setting, especially one interesting result on when their method can converge to a near-optimal solution through approximate policy iteration.  They tackle it using ideas very similar to the recent wave of work leveraging bisimulation metrics, hence novelty / comparison on this aspect will be important. They assess it on simple Mujoco control tasks, where their results are promising but perhaps still a bit early.","The paper considers the problem of RL environments where the observations space can change dramatically, e.g. from proprioceptive to pixel observations. The authors introduce various definitions for this setting and provide proofs regarding convergence (in the sense of Q-values converging to true values). Over 5 environments in continuous control, the authors demonstrate that their method enables transfer learning from proprioceptive to pixel observations and that such transfer learning can be beneficial.","This paper tackles the problem of observation space change for RL problems, i.e., the observation changes while the dynamics remain similar. To deal with this problem, the authors propose to learn a latent dynamics models based on encoded observations. When the observation space is changed, the learnt latent dynamics model can serve as a knowledge prior to transfer known dynamics information when learning with the new observation space.   ",0.1792452830188679,0.1509433962264151,0.18867924528301888,0.10606060606060606,0.10606060606060606,0.2191780821917808,0.14393939393939395,0.2191780821917808,0.2898550724637681,0.1917808219178082,0.2028985507246377,0.2318840579710145,0.15966386554621848,0.17877094972067042,0.22857142857142856,0.13658536585365852,0.13930348258706468,0.22535211267605632
98,SP:14bf7b9e43f66f048df5a657a7fdc4fb22ba6850,"This is a paper on sim-to-real transfer, proposing a new method for combining data from both the sim and real domains in order to obtain a better tradeoff between sample efficiency and quality of learning. This is a natural and good idea, although it appears novel in this proposed form.   The technical contribution is to devise a scheme to store data in multiple replay buffers, corresponding to the different sim and real sources, then using this data in a mixed linear actor critic scheme by suitably sampling the source and data points online. This paper attempts to present an analysis of the mixing scheme to argue convergence despite the difference in environments, as long as the sim and real domains are not too different in a certain sense. Finally, there is an empirical evaluation involving multiple simulations of a simple robotic pushing task.  ","The paper presents a reply-buffer based algorithm to learn simultaneously from multiple environments. The target application is learning in a mixture of simulated and real environments but the theoretical analysis is conducted in the context of multiple environments. The proposed approach is sim-real-mix which maintains a reply buffer for each of the available environments; actor and critic updates are computed with a TD error which results from a random sampling from the available reply buffers according to a static categorial distribution.    The proposed sim-real-mix algorithm (theoretical analysis limited to linear actor-critic) is proved to converge to fixed points for both the actor and the critic. Additionally, bounds on the fixed points are given in case the case of two environments (i.e. sim and real) for which a distance between the transition probabilities is upper bounded.  Additionally, the authors propose some experimental evaluations of their sim-real-mix algorithm, testing it on two slightly different instances of the same simulated environments. The idea is to simulate the situation which occurs when learning from two environments (e.g. one sim environment and one real environment) with the aim of solving a task in one environment (i.e. the real environment) while taking advantage of data produced by both environments, assumed to share some similarities. The experimental evaluation aims at understanding how the proposed sim-real-mix algorithm behaves when varying the categorical distributions used for sampling (Figure 2 and 3). Additionally, authors compare the proposed strategy with a standard sim2real fine tuning (i.e. learn in sim and fine-tuning in real) and with a modified sim2real (i.e. learn and sim and fine-tune with the proposed sim-real-mix strategy). Interestingly it is observed that fine-tuning with sim-real-mix data is better than fine-tuning with real data only. ","The paper addresses the challenge of sim-to-real transfer, i.e., how to use simulations for robot learning in the real world. It proposes the use of information from rollouts in both the simulated and the real world. The underlying technique is reminiscent of importance sampling. While the approach is explained for transfer between two environments (sim and real) it is applicable to any situation that involves K environments. At a basic level, the approach keeps track of K individual Replay Buffers which are used for RL. An agent chooses with some probability an environment and then with yet another probability samples a replay buffer to update the policy. In effect, the data from both the simulated and the real environment are used for training albeit with different mixing probabilities. The core contribution (in my opinion) of this paper is a theoretical proof that guarantees convergence of this mixing scheme. The proof shows that sampling transitions is a Markov process and that under some (reasonably benign) conditions it converges to a fix point. The approach is rigorous and the appendix includes all the individual steps of the proof. Two critical assumptions that are made in the proof is that the real and simulated transitions are close to each other (defined as difference of probabilities is below a threshold). In addition, the current proof assumes a linear approximation of the value function. The approach is evaluated on the Fetch Push environment in OpenAI Gym. In the experiments, however, a nonlinear function approximation is used which is not in line with the theoretical results. In addition, only simulation is used -- no real-robot experiments have been performed. To this end, the difference between the simulated and the ""real"" environment was the friction coefficient.       ","The paper combines simulation and the real world data by maintaining a replay buffer for both. The agent selects both the environment as well as the replay buffer with respective probabilities and uses that to estimate the TD error and update the policy parameters. They test this on a simple and well known `FetchPush` task and provide a theoretical convergence analysis.   On a high level, the paper is about how to mix and balance simulation and real world data to increase the probability of the transfer of policies from simulation to the real world. ",0.2413793103448276,0.27586206896551724,0.16551724137931034,0.18831168831168832,0.10064935064935066,0.11643835616438356,0.11363636363636363,0.136986301369863,0.2553191489361702,0.19863013698630136,0.32978723404255317,0.3617021276595745,0.1545253863134658,0.18306636155606407,0.200836820083682,0.19333333333333333,0.15422885572139303,0.17616580310880828
99,SP:14fc9c4897f9d5e2496b22bbd15a5aec08b4802d,"This paper presents a multi-agent RL algorithm based on CVaR. In the proposed algorithm, the return distribution is learned as in QR-DQN, and CVaR is estimated using the trained model. The centralized training for multi-agent settings is based on QMIX. In the proposed algorithm, Q-values in QMIX is replaced with CVaR.  The proposed method is evaluated with several scenarios in StarCraftII, and the proposed methods outperformed baseline methods.","This is an interesting article introducing risk-averse distributional RL ideas into the multi-agent setting in a new and it seems useful way. It builds on methods like Q-Mix and VDN but replaces normal returns with CVar_alpha, the expected return for the worst alpha-percentile outcomes. They also introduces a method for picking alpha different for different timesteps and different agents.","This work takes risk-sensitive learning to cooperative multiagent learning. It first, learns the conditional value at risk of the return distribution then, a second component is a risk-level predictor which dynamically adapts the risk. Lastly, the paper builds on QMIX to deal with the credit assignment problem since there' is a single global reward.  The problem the authors are tackling are very hard and I see the benefits of their approach.  I think the experimental results are interesting and informative, however the paper needs some clarifications to be more understandable to a bigger audience.  ","The authors present RMIX, which is a value-based MARL method incorporating a risk evaluator to prevent overly optimistic actions in teamwork among agents. In particular, the CVaR metric is chosen for its computational advantage and theoretically sound foundation. Experiments in the SMAC environment show better win rates than do recent baselines.",0.16666666666666666,0.20833333333333334,0.125,0.15625,0.09375,0.09375,0.1875,0.15625,0.17307692307692307,0.10416666666666667,0.11538461538461539,0.17307692307692307,0.17647058823529413,0.17857142857142858,0.14516129032258066,0.125,0.10344827586206896,0.12162162162162161
100,SP:157ee60828688078849be938e0f3f9630d093005,"This paper studied the problem of training the gating network for the mixture-of-experts based model architectures. To make the gating network training more stable and robust, the authors proposed a dense-to-sparse gating training algorithm that uses Gumbel noise and temperature tuning. The authors conducted experiments on large NLP datasets.","The paper proposes an algorithm, DTS-Gate, to warm-up experts in an MoE learning setup. Rather than applying standard top-k selection per input, the algorithm initially applies dense routing (i.e. k = total number of experts), to avoid quick collapse, and gradually relaxes it to become sparser (and cheaper). Experiments and ablations in language tasks are provided to support the advantages of the algorithm.","This paper introduces the idea of using dense-to-sparse (DTS) training gates in mixture-of-experts (MoE) based on a gradual sparsification process. The novel produced method is evaluated on a Transformer model. Overall, it looks to me that the paper has rather limited novelty. While the paper is well-written, in my opinion, the empirical evaluation support is not perfectly aligned with the paper claims.",This work proposes the Dense-to-Sparse Gate or DTS-Gate. It’s a simple idea of starting training with soft (continuous) routing to all experts and then gradually reducing to hard (discrete) routing. The authors claim that this improves the quality of traditional approaches. The work proposes the novel idea and details how to schedule the sparsity adjustment. The authors suggest this improves traditional approaches by 5% FLOPs-efficiency.,0.16981132075471697,0.2641509433962264,0.22641509433962265,0.15151515151515152,0.18181818181818182,0.208955223880597,0.13636363636363635,0.208955223880597,0.17142857142857143,0.14925373134328357,0.17142857142857143,0.2,0.1512605042016807,0.23333333333333334,0.1951219512195122,0.15037593984962405,0.1764705882352941,0.20437956204379562
101,SP:163da91b8828f91fbcaaed7198ed1415ca22df03,"Provides constant approximation algorithms for certain ""robust"" or min-max submodular maximization problems. They consider cardinality and matroid constraints. Also provide good experimental results on partitioning a training dataset. ","This paper studies constrained robust submodular partitioning. Given a submodular function $f$, a constraint set family $\\mathcal{C}$, and the number $m$ of blocks, we are to find an $m$-partition $(X_1, \\dots, X_m)$ of the ground set that maximizes $\\min_{i=1}^m f(X_i)$ subject to $X_i \\in \\mathcal{C}$.  The main contributions of this paper are two algorithms called Min-Block Greedy and Round-Robin Greedy: - Min-Block Greedy achieves $\\alpha/(\\alpha m + 1)$-approximation for a general down-closed $\\mathcal{C}$, given an $\\alpha$-approximation algorithm for submodular function maximization on $\\mathcal{C}$.  Furthermore, the authors proved that the analysis is tight for the unconstrained case. - Round-Robin Greedy achieves $(1-1/e)^2/3$-approximation when $\\mathcal{C}$ is a cardinality constraint. It also achieves $(1-1/e)/5$-approximation when $\\mathcal{C}$ is a matroid.","The focus of this work is constraint robust submodular maximization. In this problem, the goal is to find $m$ sets maximizing the minimum value of a given submodular function $f$ over these sets. This problem has attracted attention recently and  algorithms are designed for offline and streaming setting. In this work, author consider this problem subject to three famous constraint and provide two type of algorithms.  1- Min-block greedy algorithms: This algorithm has been used before for non-constraint version robust submodular maximization and provides a $1/m$ approximate solution.  In this work they extend it to the constrained version and achieve $\frac{\alpha}{\alpha m+1}$ approximation algorithm, where $\alpha$ is the approximation guarantee of the greedy offline algorithm for that constraint. Intuitively this result is very close to being tight with respect to the unconstrained for large $m$ values.  2- Round Robin greedy algorithm: They present a $\frac{(1-e^{-1})^2}{3}$-approximation algorithm for the cardinality constraint and  $\frac{1-e^{-1}}{5}$ for the matroid constraint.","This paper focuses on the problem of finding an allocation of $n$ items in $m$ blocks that maximizes the minimum of the block valuations, where each block has the same monotone submodular objective (homogeneous case) and the subset allocated to each block has to satisfy a feasibility constraint. The model allows allocations that do not necessarily assign every item, since each block allocation needs to be feasible.  The authors first studied the Min-Block Greedy (introduced in [29]) which in each iteration selects the block of minimum value and greedily assigns a new element. They show that the $(1/m)$-guarantee (proved in [29]) of this algorithm is tight. Then, they extended this algorithm to the constrained version and prove a $\alpha/(m\alpha+1)$ factor, where $\alpha$ is the factor for the vanilla constrained submodular maximization problem. In the second part, the authors give a Round-Robin Greedy algorithm (which is adapted from the algorithm in [3]) that achieves a $(1-1/e)^2/3$ factor for the cardinality constrained case. This algorithm is later extended to the matroid case for which it achieves a $(1-1/e)/5$ guarantee. Finally, they present computational experiments to compare the proposed algorithms. ",0.2413793103448276,0.3103448275862069,0.20689655172413793,0.3103448275862069,0.2827586206896552,0.29651162790697677,0.04827586206896552,0.05232558139534884,0.029850746268656716,0.2616279069767442,0.20398009950248755,0.2537313432835821,0.08045977011494251,0.08955223880597014,0.052173913043478265,0.2839116719242903,0.2369942196531792,0.2734584450402145
102,SP:1642a5d14f04c414fc17eca26c6a44a30cca575b,"This paper presents a novel approach to  link prediction methods in graph representation of imbalanced domains using adversarial training. the result is an improved domain-agnostic graph embeddings. The approach takes is similar to few-shot learning that is popular in computer vision. Discussions on how to define shots for graphs, and how to design experiments for addressing imbalanced domains contribute to the novelty of the paper. Results on two Stanford Open Graph Benchmarks and the PPI dataset are given. ",This paper proposed adversarial training for few-shot link prediction problems. The authors introduce a domain discriminator on pairs of graph-level embedding and address the issue of few-shot learning in graph data. The proposed method is tested on 3 benchmark datasets and achieves good performance compared to the prior approaches.,This paper proposes a method for few-shot graph link prediction by using a domain discriminator. The motivation is to learn domain-invariant graph-level representations. The authors also introduce the concept of “shot” in the graph. The proposed method outperforms baselines on three different datasets.,The paper proposes a method to resolve the issue of domain imbalanced dataset in graph link prediction. The proposed method uses adversarial training to generate graph embeddings that are domain agnostic in order to facilitate transfer learning cross domains. The paper uses T-SNE plot of graph embedding to gain insights of the best scenarios for applying the proposed methods. The paper compares the proposed method with heuristics-based and GNN-based domain adaptation methods using experiments. ,0.2,0.2,0.225,0.4230769230769231,0.25,0.3695652173913043,0.3076923076923077,0.34782608695652173,0.23376623376623376,0.4782608695652174,0.16883116883116883,0.22077922077922077,0.24242424242424246,0.253968253968254,0.22929936305732482,0.44897959183673475,0.20155038759689922,0.2764227642276423
103,SP:166a50a278ff3de53768fb4399cc533d7a948fe0,"This paper aims to improve GNNs' expressive power based on bag-of-subgraph representation. The authors develop a Equivariant Subgraph Aggregation Network, which uses equivariant layers to encode subgraphs and aggregate subgraph representations. The authors have performed theoretical analysis of the proposed framework, subgraph selection policies, and their expressive power. The experiments show that such an encoding can lead to a better expressive power on several real and synthetic graph classification datasets. ","This paper proposed a novel way of increasing MPNN expressive power without using higher order node tuples representation, thus proposed method has lower complexity compare to naive powerful GNN methods. The core idea is to create a set of subgraphs for each graph in the given dataset by using single node or edge deleting,...etc policy. They create a virtual graph that adjacency is summation of subgraph's adjacencies and node feature matrix is the summation of subgraph's feature matrix. Each subgraph's next layer representation will be obtained by aggregation of self subgraph's new representation by baseline MPNN and also this virtual graph's representation by another MPNN. In application this virtual graph is replaced by original graph or in another version is basically neglected. After arbitrary number of layers, they apply subgraph level pooling to obtain invariant representations. They see all subgraph's final representation as a set of representations. Thus to obtain final graph level representation, they apply another layer on set of subgraph representations such deepsets.   ","This paper proposes to improve the expressivity of MPNN by using H-Equivariant layers to process bags of subgraphs accounting for their natural symmetry. The proposed DS(S)-GNN is proven to be equivalently powerful as DS(S)-WL, which can be strictly more powerful than 1-WL. It also provides theoretical results about how design choices such as the subgraph selection policy and equivariant neural architecture affect the architecture’s expressive power. In addition, a stochastic sampling scheme is proposed to mitigate the computational overhead. ","The paper addresses known limitations of the expressiveness of message passing neural networks (MPNNs). Key idea is to generate subgraphs from the original graph, to encode each subgraph with a GNN, and to aggregate the resulting set of subgraph-encodings. Since generating all possible subgraphs is computational infeasible for larger graphs, only a smaller subset of subgraphs is used in practice. The paper shows that their approach is more expressive than 1-WL. Moreover, the paper shows that different variants of the proposed method can outperform the base model.",0.2222222222222222,0.25,0.20833333333333334,0.11627906976744186,0.11046511627906977,0.19767441860465115,0.09302325581395349,0.20930232558139536,0.16853932584269662,0.23255813953488372,0.21348314606741572,0.19101123595505617,0.13114754098360654,0.22784810126582278,0.1863354037267081,0.15503875968992248,0.14559386973180077,0.19428571428571428
104,SP:169d25f9a59319f1022a01877b48a8e90a4d7af3,"This paper proposes a meta-learning model that represents stochastic processes through a contrastive representation. The authors mainly addressed the unified framework which could handle the two types of tasks; 1) given a set of data and target covariate (they called it the targeted task) and 2) estimating the label for the set of data (called untargeted task). To do that, they designed the specific encoder for the targeted task and by applying a self-attention mechanism on their encoder, they improved the performance for the tasks. They validated their model on interesting tasks like bi-modal sinusoidal regression and semi-supervised learning. I think that the architectural novelty of their model is the unified design for targeted/untargeted tasks and self-attention layers. However, I don't think their contribution is not limited to them, they validated/analyzed their model and other meta-learning models trained through the contrastive losses (e.g., FCLR and MetaCDE) for a variety of interesting tasks, which is one of the strong points on this paper I thought.",This paper proposed a framework for learning contrastive representation of stochastic processes (CRESP) and designed two variants of models to tackle targeted and untargeted downstream tasks (e.g. label prediction) in a semi-supervised setting.  The models were evaluated on downstream predictive tasks of three stochastic processes and outperform previous contrastive learning methods.,"My understanding of the paper is that authors propose to introduce contrastive methods (Van den Oord 2019, Chen 2020) to improve the learning of stochastic processes with NN architectures. Particularly, they identify the main points where the Neural Process (NP+CNP) family of models fails, e.g. due to high-dimensionality or complex likelihood densities. The advantage of using these contrastive representations for stochastic processes is that it makes the task likelihood-free, performing better when the aforementioned problems appear.","The authors propose a method for representation of an observational context by optimization of a contrastive learning objective. In contrast to (conditional) neural process modeling, this avoids specification of a likelihood, which may be difficult and ultimately unnecessary in tasks that only require prediction of a downstream label. Experimental results illustrate the advantages of this method over related neural process models on such tasks.",0.10919540229885058,0.09195402298850575,0.08045977011494253,0.24528301886792453,0.2641509433962264,0.1375,0.3584905660377358,0.2,0.21875,0.1625,0.21875,0.171875,0.16740088105726875,0.12598425196850394,0.1176470588235294,0.19548872180451124,0.23931623931623933,0.1527777777777778
105,SP:16e0d2970dcc7e5b51efc3d33b7eed10e10457a8,"The paper introduces a new architecture for generative modeling, called Bundle Networks. This is based on the mathematical concept of fiber bundles and allows to explore the networks fibers above the labels (their preimages). Training the network then corresponds to learning the local trivialization of the fiber bundle, i.e., of the labeling function. ",This paper proposes the Bundle Networks as a framework to learn many-to-one maps. The empirical performance is promising on two synthetic datasets and two real datasets.  The main contribustions are three-fold:  1) The problem of learning the fibers of a ML task is formalized. 2) An approach is proposed to learn the fibers. 3) A family of deep generative models called a Bundle Network is designed.  ,"This paper proposes a neural network to model data (or machine learning tasks) with fiber bundle structure. Machine learning tasks like regression or classification are often many to one. They assign a ""label"" (discrete or continuous) $Y \ni y = \pi(x)$ to each $x \in X$. Many different $x$ may receive the same label. It is interesting to explore / parameterize the set of all $x \in X$ that yield a given label $y$, that is to say, the set $\pi^{-1}(y)$. This is nicely modeled by the topological construction of a fiber bundle (a space that is locally a product space). Operationally, it corresponds to a covering of the label space $Y = \bigcup_{i} U_i$ by ""patches"" $U_i$ so that $\pi^{-1}(U_i)$  are homeomorphic to product spaces $U_i \times Z$, with $Z$ being the fiber space.   The proposed network is in essence a conditional generative model with the added twist that the conditioning is also performed on the ""patch"" that a label (or a regression target) belongs to (for training; inference is only performed in the ""reverse"" direction of fiber sampling.)  The authors show interesting results in modeling familiar low-dimensional bundles as well as real-world datasets. Comparisons with conditional generative models suggest that the new architecture may be a better fiber bundle model. Finally, the authors numerically explore the role of ""latent space"" topology, a welcome addition. ","The manuscript introduces Bundle Networks which are neural networks designed with an explicit fiber bundle in the architecture. With this in hand one has an explicit description of the fibers of the fibers associated with a machine learning task. The model is demonstrated on both artificial data, showing that it is capable of learning the fiber bundles where these can be explicitly computed, as well as real world data sets where the fiber bundles are of interest. ",0.24074074074074073,0.37037037037037035,0.2777777777777778,0.34782608695652173,0.21739130434782608,0.11914893617021277,0.18840579710144928,0.0851063829787234,0.19480519480519481,0.10212765957446808,0.19480519480519481,0.36363636363636365,0.2113821138211382,0.13840830449826988,0.22900763358778628,0.15789473684210528,0.2054794520547945,0.17948717948717952
106,SP:16e0e65d06ac6d8d799704c888d2ded52ea7f7df,"This manuscript shows the connection between soft clustering ensemble and the discrete Wasserstein barycenter problem. Based on the connection, a sampling-based algorithm is proposed with some provable quality guarantees. The manuscript provides some theoretical analysis about the quality of the obtained consensus clustering results.","This paper proposes a new approach for soft clustering ensemble, using a new problem formulation (a “geometric prototype”), which is equivalent to a Discrete Wasserstein Barycenter problem.  The proposed algorithm achieves, with a fixed probability 1 – δ, a (1+ ε)-approximate solution to the soft clustering ensemble problem, when the number of clusters k is fixed and small. As the authors state, this is a theoretical result since the runtime is more than exponential in k, 1/ δ, and 1/ε.  In addition, the authors propose an improvement on the existing alternating minimization algorithms in the case of k not being a constant. Then, they prove that the obtained solution converges to the ground-truth clustering as the number m of clustering increases.  Finally, they evaluate the performance of the alternating minimization Wasserstein barycenter algorithm with the proposed improvement, and they compare it to three baselines from the literature. The results show that their algorithm outperforms the baselines in terms of the objective function value and, for the largest datasets, also in terms of runtime. ",Authors connected the soft clustering ensemble and discrete Wasserstein barycenters. They proposed a (1+eps) approximation algorithm and adopted a uniform sampling scheme to reduce the time complexity of computing Wasserstein barycenters. Convergence is proved and verified via experiments. ,"This paper studies the ensemble clustering problem with soft clustering indicators. They reveal an interesting equivalence between the soft clustering ensemble problem and the $k$-sparse discrete Wasserstein barycenter. The authors proved the hardness result of SCE and reported approximation algorithms for two different regimes (fixed $k$ or not). On the modeling side, they provided consensus results for the SCE problem under a weaker assumption on the ground-truth clustering set than prior results. They also conducted numerical experiments to support their results.",0.4222222222222222,0.24444444444444444,0.4,0.08771929824561403,0.16374269005847952,0.28205128205128205,0.1111111111111111,0.28205128205128205,0.21686746987951808,0.38461538461538464,0.3373493975903614,0.13253012048192772,0.1759259259259259,0.2619047619047619,0.28125,0.14285714285714285,0.2204724409448819,0.18032786885245905
107,SP:1700a9e3c3ea516d83f49cb5408070f2c245fcc1,"1. Motivation: The paper focuses on understanding the behavior of networks with attention and investigating how encoder-decoder networks solve different sequence-to-sequence tasks. To this end, the authors hypothesize that representations (i.e., hidden states) can be decomposed into ""temporal"" and ""input"" components.  2. Approach: In detail, the author presents a decomposition of the hidden states of encoder/decoder into temporal (independent of input) and input-driven (independent of sequence position) parts, where the former is used to explain the temporal behavior of the network and the latter is used to describe the input behavior.   3. Experiments: This paper conducts extensive experimental studies on several common sequence-to-sequence frameworks, including vanilla RNN, RNN with attention, and simplified Transformer networks. They study the sequence-to-sequence networks on both synthetic and real-world tasks, i.e., a synthetic task, eSCAN and English-French translation tasks, and find that the temporal components contribute more when computing the attention.","This paper proposes a novel method to decompose hidden representations of encoder-decoder architectures into two kinds of components: temporal and input-driven. The temporal component is the average hidden representation at a given time step thus it only varies with time, while the input-driven component is defined as the representation of a specific input word removing temporal component thus it is input-dependent but time-independent. Based on the decomposition, the authors show how encoder-decoder architectures attend, providing some insights for future research.","The paper introduces a novel method for understanding how attention works in sequence alignment problems by decomposing the hidden states into temporal and input-driven contributions.  This is applied to three different network architectures, with AO only recurrent connection, AED with both recurrent connections and attention, and AO, with only attention.  They then test on some different synthetic dataset and link the attention matrix to the hidden state decomposition and to the temporal and input component dynamics.","In this work, the authors propose a framework to analyze how different encoder-decoder architectures attend in 3 simulated tasks and 2 real-world tasks. The framework relies on isolating the  following components in the network: 1. Temporal - Average hidden state of the encoder/decoder at a specific timestep, across all the test samples. 2. Input - Average deviation of a word's hidden state from the mean state at the timepoint it occurs in, across all its occurrences in the test set. Using these components, the paper factorizes the attention alignment between eh encoder & decoder's hidden state into 9 different products of the temporal and input states.  The usefulness of the two components in understanding attention mechanisms is demonstrated with 3 tasks: 1. Simulated tasks: Matching 3 characters to their numbered IDs | The authors find that the input and read-out states are highly aligned such that the decoder only needs to find its corresponding temporal match in the encoder. This is apparent from the results in the paper. Further, the non-attention based network doesn't demonstrate similar behavior. 2. eSCAN: Extension of the SCAN dataset formed by concatenating commands to generate longer sequences 3. English $\rightarrow$ French translation  For tasks 2 & 3, the authors find that the networks learn mappings between the input tokens and corresponding actions/French words in the readout layer. Further, the networks attend through a combination of temporal information and input components unlike task 1. Since the output in several cases doesn't depend on just 1 timepjoint in the input, these networks also exhibit 'offset' behavior wherein they attend to information at different time points depending on the input token (thus violating their orthogonality)  Overall, by separating the input and temporal components of the network state, the paper reveals if networks use only one mechanism or a combination to attend and solve a task.",0.1320754716981132,0.16352201257861634,0.27044025157232704,0.19767441860465115,0.32558139534883723,0.3116883116883117,0.2441860465116279,0.33766233766233766,0.13782051282051283,0.22077922077922077,0.08974358974358974,0.07692307692307693,0.17142857142857143,0.22033898305084745,0.18259023354564755,0.2085889570552147,0.1407035175879397,0.12339331619537275
108,SP:17061c03e2956c48fbb0b56daba1b3b3010d3aef,"This paper addresses the task of molecular conformer generation, i.e. generate the 3D conformer ensemble conditioned on the molecular graph. The introduced method, Dynamic Graph Score Matching (DGSM), is using score matching to generate 3D structures by explicitly capturing both local and long-range interactions. This is done using dynamic graphs constructed via atom coordinate perturbations using different noise levels as required by the denoising score matching procedure.  The score is estimated using message passing neural networks which operate on these dynamic graphs. At inference time, Langevin dynamics is used to sample conformations from the generated distribution. Empirically, the authors show clear improvements over previous machine learning (ML) models and over one popular open-source non-ML method (RDKit) on molecular conformer generation on two different tasks, as well as improvements on experiments on property prediction based on molecular conformations, protein sidechain conformation generation, and multi-molecular complex conformation.  In short, I think this approach is quite compelling and would be very impactful for future work related to graph conditional 3D generative models of various data in biology, chemistry, etc. The paper is very well written and the method is clean, addressing nicely using dynamic graphs the very important problem of modeling long-range interactions in 3D generation.  ","This paper introduces a new approach for generating molecular conformations, using score-based generative modeling.  The key contribution over prior work is the dynamic addition (during both training and inference) of long-range interactions between non-bonded atoms that are not proximal in the molecular graph.  As these interactions are often critical to determining the underlying molecular conformations, the authors contend that their addition leads to improved performance on a range of tasks.  In particular, they demonstrate improved performance over previous state-of-the-art on the generation of conformations for small molecules, protein side chains, and multi-molecular complexes, as well as small-molecule ensemble property prediction.","The authors develop an improved method for 3D molecular conformation generation from 2D molecular graphs that can more directly incorporate non-bonded, topologically long-range interactions into the generation process. Their starting point is a recent method that leverages score matching with a GNN based on the 2D molecular graph topology, or more specifically, a GNN that is trained to estimate the gradient of the noise-perturbed log density of interatomic distances on the graph edges. They follow the same framework, but add additional edges to the graph topology based on the current perturbed distances when they fall below a certain cutoff (10 Angstroms). Through their experiments, they show that this relatively modest modification signifcantly improves generation performance of small molecule conformers and can be applied to larger and more complex systems such as protein side chain packing and multi-molecular complexes.","This paper proposes to generate molecular conformation by score matching. The proposed method, called DGSM, learns a score network to predict gradient field with respect to a predicted conformation. The score network is parameterized as a message passing network over a dynamic graph between atoms. The graph is constructed based on pairwise distance between atoms in the current conformation. Finally, DGSM uses the learned gradient field along with Langevin dynamics to sample molecular conformations at test time. DGSM is evaluated on two conformation generation benchmarks and achieved new state-of-the-art results.",0.1291866028708134,0.11004784688995216,0.1291866028708134,0.24074074074074073,0.17592592592592593,0.1267605633802817,0.25,0.1619718309859155,0.2903225806451613,0.18309859154929578,0.20430107526881722,0.1935483870967742,0.17034700315457413,0.13105413105413105,0.17880794701986755,0.208,0.1890547263681592,0.15319148936170213
109,SP:17782a55cb3f96ca19a178dfb54c5d0923424751,"There is a good literature for off-line RL. However, this  is reviewed at all. For example, LSPI (model free), LAMAPI (model-based), pseudo-MDPs (model-based) and RKHS embedding (model-based). The paper went straight to discussing the reduction of the value estimation error constraint. However, most of the algorithms use the error to directly learn the parameters. It is not necessarily formalized as constraint. ",This paper provided a model-free algorithm for offline-RL. It provided Theorem 5.1 to justify its uncertainty quantifiers. It also conducted experiments to validate its algorithm.,"The paper agues that methods, which they call ""support constraint"", consisting in assuming that any action outside the training yields negative outcomes, lead the agent's training to be overly pessimistic. In contrast, they propose MSG an algorithm where a lower bound of the value function is estimate from ensembles of networks. They provide some theoretical arguments on infinite-width neural networks and validate their methods on many applications, ranging from illustrative toy examples to classic continuous state-action spaces offline RL benchmarks.","This paper proposes a novel method to utilize ensemble in quantification of uncertainty in offline RL, where the particular structure of dynamic programming and cumulative error are unique and interesting. Some theoretical justification emphasizes the ability of independent TD targets to capture the variance of the prediction, hence provide uncertainty quantification. Simulations show the proposed method gets larger variance in low-confidence regions. In experiments in benchmark environments, the proposed method outperforms those in the literature. ",0.07575757575757576,0.12121212121212122,0.16666666666666666,0.14285714285714285,0.2857142857142857,0.10843373493975904,0.17857142857142858,0.0963855421686747,0.14473684210526316,0.04819277108433735,0.10526315789473684,0.11842105263157894,0.10638297872340424,0.10738255033557047,0.15492957746478875,0.07207207207207209,0.15384615384615385,0.11320754716981131
110,SP:17aea38c6c121ea0eb15076c67eddb58e36cea55,"Several combinatorial optimization problems can be fully specified by one or more matrices specifying the relationships between two groups of parameters. For example, such a matrix can encode inter-city distances in the traveling salesman problem. In the hybrid flow shop problem, a set of matrices D_i encode the time it takes to perform a job on a given machine at each timestep i. the This paper proposes to learn to embed such matrices using a neural network derived from the transformer architecture and use such embedding to find good approximate solutions to the corresponding combinatorial problems. ","The authors propose a family of matrix encoding networks to encode the states of combinatorial optimization problems which can be encoded as a matrix. The proposed matrix encoding network works like a bi-partite graph embedding network. Compared to previous efforts encoding sequences and graphs, the effort of this paper is appealing. Two CO problems are studied: ATSP and FFSP, and the performance on ATSP is inferior to competing methods, and the performance on FFSP is more convincing. ","This work proposes a novel matrix encoding network (MatNet) to solve combinatorial optimization (CO) problems with matrix-style relationship data. This kind of CO problems, such as asymmetric TSP, can be found in many real-world applications, but cannot be solved by the current neural combinatorial optimization approaches. The key contribution is the proposed dual graph attentional layer, along with the multi-head mixed score attention, to efficiently incorporate the matrix data into the well-known Attention Model (AM) architecture for solving CO problems.  ",The paper introduces a neural net model that takes relationship matrices as input for solving combinatorial optimization problems. The proposed network can be considered as a special GNN fed by a complete bipartite graph with weighted edges. The experiment results show that their end-to-end RL framework can achieve a similar performance as LKH3 for asymmetric traveling salesman problems and significantly outperform conventional OR methods for flexible flow shop problems.,0.16326530612244897,0.15306122448979592,0.1326530612244898,0.20512820512820512,0.19230769230769232,0.15476190476190477,0.20512820512820512,0.17857142857142858,0.18309859154929578,0.19047619047619047,0.2112676056338028,0.18309859154929578,0.1818181818181818,0.16483516483516483,0.15384615384615385,0.19753086419753083,0.20134228187919462,0.16774193548387098
111,SP:17c251f85bc1d35991f972f6fc4d2e402d2d2df9,"The paper analyses the multi-head self attention behaviour in CTC based ASR models. They identify the working regimes, namely, phonetic and linguistic localisation.  The paper argues that lower MHSA layers build phonetic attentions patterns while upper layers do that linguistically showing some diagonal pattern similarly to NLP tasks. Based on these findings the authors propose to share attention maps across layers. This yields some modest improvements in terms of WER but obtains larger speeds-up with respect to a basic Conformer model. During the study they introduce new measurements such as cumulative attention diagonality (CAD) and phoneme attention relationship (PAR). ","The paper consists of two parts. First, it conducts the analysis of an ASR model. Specifically, it measures the diagonality of the self-attention map and notes that the diagonality for the top layers is higher than for the lower layers. Based on this, the paper suggests that the lower layers are responsible for the phonetic information and the higher layers are responsible for the linguistic information. Then, the paper confirms this hypothesis by using each layer for a phoneme classifier.  Second, based on the above observations, the paper proposes a method to exploit this division into the phonetic-linguistic layers. The proposed method is to reuse the attention maps across several layers. This modification allows for more efficient architecture both in terms of latency in train and test time. In some cases, the proposed architecture outperforms the baseline.","The paper explores how self attention maps in conformer based ASR models are distributed across layers. The authors note that the lower layers of the model capture phonetic information, and the variance of the attention map distribution for the same class of phones across the utterance goes down. The higher layers do not display this behavior. The authors hypothesize that the higher layers capture linguistic information that likely combines information across phones.  Based on the observation that the variance of the attention map distribution goes down, the authors propose reusing the attention map from one layer across multiple layers. This reduces the amount of computation needed for inference, and reduces the training time. Tying attention maps also results in small improvements in performance. ","This paper develops a mechanism to analyze self-attention for speech recognition, and further use the resulting insight to facilitate attention reuse for reducing inference cost. The analysis identifies a distinct pattern between lower half and upper half layers on the ASR encoder through cumulative attention diagonality and characterizes the lower half as finding phoneme localization and upper half as finding linguistic localization. The paper justifies the characterization of phoneme localization by illustrating an example of such a phoneme localization, and also shows that the output of the layers in the middle provide better phoneme classification accuracy than the upper layers. An additional analysis, called phoneme attention relationship, further quantifies the correlation between phonemes and attention, and shows that the lower layers have higher correlation. These insights are used to design attention reuse across layers, and shown to maintain the quality while improving the speed on Librispeech.  ",0.25742574257425743,0.27722772277227725,0.2079207920792079,0.26618705035971224,0.2302158273381295,0.21951219512195122,0.18705035971223022,0.22764227642276422,0.14285714285714285,0.3008130081300813,0.21768707482993196,0.1836734693877551,0.21666666666666667,0.25,0.1693548387096774,0.2824427480916031,0.22377622377622378,0.2
112,SP:17eb6e3c7260377f7576782ece091d44bd72b9ec,"The authors propose NatPN, a model which can estimate predictive uncertainty for classification and regression tasks. NatPN predicts the parameters of the posterior distribution which belongs to the exponential family. Contrary to other algorithms, NatPN requires no OOD data for training, and is able to evaluate the uncertainty in a single forward pass utilizing normalizing flows for density estimation.","This paper proposes a single-pass uncertainty estimation method for neural networks by predicting the update to the natural parameters of the conjugate prior to the likelihood of the data distribution and optimizing a corresponding ‘Bayesian loss’. In that it generalizes PostNet (Charpentier et al., 2020) beyond classification. The paper reports competitive or improved performance across a range of datasets and against both standard and recent baselines.","The paper introduces a new method Natural Posterior Network (NatPN), an extension to the Posterior Network (Charpentier et al., 2020). NatPN aims to tackle the problem of enabling calibrated uncertainty estimates for in and out-of-distribution inputs for exponential family likelihoods parameterized by deep neural networks. In particular NatPN maps input samples into a latent space on which a normalizing flow is defined. A mapping from this latent space and its corresponding likelihood under the normalizing flow to the parameters of exponential family update rule is defined. The exponential family posterior tends towards the prior parameters for data points far from the training data and prior has little influence for data points within the training distribution.  NatPN makes two significant changes to the Posterior Network 1) it generalizes the method to the exponentially family, allowing for regression and counting tasks beyond just classification as per the Posterior Network and 2) it uses a single normalizing flow rather than class specific normalizing flows as per the Posterior Network, in theory this aids the scalability of the method to datasets with many classes. In addition a minor change is made to the update rule of the posterior parameters.  The paper presents extensive empirical results on classification, regression and counting tasks across several relatively small scale datasets. NatPN is shown to perform on par or slightly better than similar methods from the literature.","This paper proposes Natural Posterior Networks (NatPNs), a technique to provide uncertainty-estimation to tasks where the likelihood is an exponential family, e.g. classification or regression. NatPNs leverage the properties of exponential families, i.e. conjugacy and closed-form posterior predictive for fast and elegant Bayesian Inference. This is combined with a Bayesian treatment of the loss function that allows for end-to-end training. Furthermore, it uses a normalizing flow to account for the epistemic uncertainty. The normalizing flow density is supposed to increase the evidence on the training data and thereby decrease it everywhere else. The posterior distribution is on the outputs not on the weights of the NN which makes it easier to train. The paper provides one theorem for why the posterior uncertainty is meaningful and multiple experiments showing practical improvements for multiple exponential families and datasets.    NatPNs are an extension of Posterior Networks. However, the differences are significant and well explained in the paper. From my perspective, the contributions are clearly novel enough to justify this new paper.  ",0.22033898305084745,0.3728813559322034,0.288135593220339,0.373134328358209,0.31343283582089554,0.17316017316017315,0.19402985074626866,0.09523809523809523,0.09770114942528736,0.10822510822510822,0.1206896551724138,0.22988505747126436,0.20634920634920637,0.15172413793103445,0.1459227467811159,0.16778523489932887,0.17427385892116182,0.19753086419753085
113,SP:1880b91791683d30340ad5a1a2ad9135c45ea067,"Object detection is a fundamental task in computer vision. In this paper a new paradigm of training, MixTraining, is proposed which utilizes augmentation of different strengths. Either standard augmentation and standard processing is used for an image or strong augmentation is applied (mixed data augmentation) and changed ground truth is used (loss is computed only on simple boxes and mixed training targets is used). As soon as human-annotated bounding boxes often are noisy (missing or inaccurate localizations) the other key ingredient when strong augmentation is applied to an image is to use only confident bounding boxes predicted by the model itself (fixes errors in human annotations) in addition to the confident bounding boxes from the ground truth (making simpler task as soon as strong augmentation is used). This key ingredient is obtained via pseudo-labeling by EMA detector (pseudo boxes). The idea is that this pseudo boxes and bootstrapping from the model itself help to detect images where strong augmentation can be expected to be helpful. MixTraining is shown to consistently improve various detectors on the COCO dataset.",This paper introduces a new training paradigm named MixTraining for object detection that can improve the performance of existing detectors for free. The main idea is that MixTraining enhances data augmentation by utilizing augmentations of different strengths. MixTraining is found to bring consistent improvements across various detectors on the COCO dataset. It improves faster R-CNN from 41.7 to 44.0 and Swin-small from 50.9 to 52.8. ,"This paper proposes a method to improve the performance of object detectors with supervised training. The main ideas consist of two parts: (1) The human annotation of bounding boxes is often noisy or incomplete. Adopting a teacher model to generate pseudo boxes and combining them with original annotation can improve the performance of object detectors. (2) Strong augmentation may generate data which degrades the training performance. Adopting a teacher model to select easy targets in the augmented images for training can improve the performance. In this paper, the author adopts an exponential moving average model as the teacher model. The proposed MixTraining strategy improves the performance (1.6~2.3 mAP) of several object detectors in COCO benchmark. ","The study offers MixTraining, an object detection training technique that may be used to improve the performance of existing detectors. The suggested method improves data augmentation by leveraging augmentations of various strengths while eliminating strong augmentations of particular training samples, which could be harmful to training. It addresses localization noise and missing labels in human annotations as well. The efficacy of MixTraining is validated on the COCO dataset using different deep learning models and backbones.",0.12290502793296089,0.15083798882681565,0.11173184357541899,0.22535211267605634,0.3380281690140845,0.16101694915254236,0.30985915492957744,0.2288135593220339,0.26666666666666666,0.13559322033898305,0.32,0.25333333333333335,0.17600000000000002,0.18181818181818182,0.15748031496062992,0.1693121693121693,0.3287671232876712,0.19689119170984457
114,SP:18a1f39daee10ec0649427f27e2fd2d05bae67f0,"The paper attempts to answer the question of how to pre-train encoders in order to get task-relevant representations that are useful for a downstream task in each of three different environments (ViZDoom,  Distracting DMContro and Autonomous Driving). Three main approaches are compared (Behavioral Cloning, Value prediction only or another commonly used offline RL loss), along with a number of additional baselines (VAEs, predicting combinations of observations and rewards, contrastive methods and others), in a setting where an encoder is pre-trained offline on data coming from a certain task and its weights are frozen and used to provide representations to a policy head trained to solve an unseen task, that’s held out in the original dataset. The authors also provide an analysis using saliency maps to explore which parts of the input image are important for decreasing the loss of each method (task relevant vs background) and also show the performance of the representation on a task relevant and a task irrelevant task. Finally, the authors explore how the quality of the data (coming from many tasks vs coming from a single task) during the representation learning phase, and whether they come from an optimal, random or suboptimal policy, affects downstream performance. The main contribution of the paper, in my opinion, is this investigation of the different methods. In particular the comparison of performance in each of the three environments, the saliency analysis and predictability of state vs background information and finally the exploration of the effect of data on performance.","In this work, authors formalized the problem of task-induced representation learning. It basically involves investigating various different ways for learning representations from the raw data (for ex. offline dataset) containing ""privileged"" information (task relevant) from many different tasks. The aim is to leverage task relevant information for learning representations that capture task-relevant information.   The authors evaluate their idea on three different suite of environments (a) Distracting DMControl (b) ViZDoom (c) Autonomous Driving. The authors compare the proposed way of learning task-induced representation to various other methods for representation learning like reconstruction based approaches, mutual information based methods, prediction based methods etc. The results show that the proposed way of learning task-induced representations capture task-relevant information. ",The paper proposes a representation learning approach for multi-task reinforcement learning. The idea is to pre-train a model using multi-task data either with rewards or behavior cloning. Then fine-tune it on a new task hopefully more efficiently. They contrast it to unsupervised representation learning.  Fine-tuning experimentation is performed with reasonable baselines on 3 different visual domains. The authors probe both what the representation looks like and what pretraining tasks give better performance.   ,"This paper discusses the problem of learning meaningful representations that can efficiently focus only on the features that are relevant for downstream tasks. As opposed to unsupervised representation learning approaches that do not differentiate between task-specific and other information in a given dataset, the paper proposes a framework named “Task Induced Representation Learning” which leverages task information to guide the representation learning. Through this framework, the paper suggests three possible ways to induce task-relevance in representations: by predicting the value of a state, maximizing the discounted reward for a group of tasks, or by imitating an expert policy again for a group of tasks. Through a set of experiments in three domains: distracting DMControl, VizDoom and CARLA, the authors show higher task learning efficiency using the proposed framework as compared to standard approaches such as reconstruction, contrastive learning, state-reward prediction etc.  ",0.11023622047244094,0.08661417322834646,0.13385826771653545,0.14166666666666666,0.25,0.22077922077922077,0.23333333333333334,0.2857142857142857,0.2361111111111111,0.22077922077922077,0.20833333333333334,0.11805555555555555,0.14973262032085563,0.13293051359516614,0.1708542713567839,0.17258883248730963,0.22727272727272727,0.15384615384615385
115,SP:18f78b39d4014087420b92acdb84f34d91dc11d7,"I will give the authors credit for introducing a novel problem to study, but they do not do a sufficient job motivating why the problem is an interesting or important one. At a high level I can buy the story they are trying to tell, but this really needs to be tied closely with a compelling (and, ideally, well studied) application  area. Without this, the claim in the introduction that ""it is imperative to develop defense mechanisms"" are unjustified. It is not clear to me that scheduling or TSP are problems where you would typically encounter adversarial inputs, and if the authors have concrete cases to illustrate this, please highlight them. If Fraud Coverage is a well-studied or established problem, provide citations to this effect.  Beyond the motivation of the problem itself, it is also not clear to me why the attack model studied in the paper is the ""right"" one. The authors should spend more time making a convincing case why their model is a plausible one. Again, this justification will need to be tied with a (compelling) application.  Other comments: * p2: ""Combnaotorial"" * The formalism for combinatorial optimization presented on page 3 is unlike any I have seen before. How should we understand what the x variables correspond to: binary indicators on e.g. edges? If so, where are the explicit binary constraints? It is also not clear to me what it means for the constraints to be ""usually encoded in graphs"". * What does it mean to ""loosen part of the constraint h_i""? Please introduce more formality about what h_i are, what loosen means, etc.  * I find it interesting that the FC attack against Gurobi essentially seems to find ways to make the problems extremely difficult to solve, as opposed to degrading the solution quality (though that may also occur). I wonder if the authors can gain any insight into what the attack is doing here.","The authors aim to study the robustness of combinatorial optimization solvers (in particular ones that are based on learning). They treat the task as a graph problem and perform attacks on the underlying graph structure. Additionally, they aim to propose a defense mechanism. Finally, experiments on three different tasks are performed.","This paper presents a framework of adversarial attack and defense for solvers to combinatorial optimization problems. In particular, the attacker can modify the problem instance by removing graph edges, while the defense side seeks to add edges to mitigate the attacking effect. The paper presents attack methods based on reinforcement learning and heuristic methods, and proposes a defense method based on reinforcement learning. Experimental studies are provided on three combinatorial problems.","This paper studies the adversarial attacks and defenses problem of combinatorial optimization problems. The paper presents ROCO framework, to conduct attack and defense by maximizing/minimizing the difference between the calculated optimum values between the perturbed graph and the original graph. Both RL-based and heuristics-based solutions are proposed and implemented on three combinatorial optimization tasks.",0.05,0.053125,0.046875,0.2549019607843137,0.23529411764705882,0.2676056338028169,0.3137254901960784,0.23943661971830985,0.2631578947368421,0.18309859154929578,0.21052631578947367,0.3333333333333333,0.0862533692722372,0.08695652173913043,0.07957559681697614,0.21311475409836067,0.2222222222222222,0.29687499999999994
116,SP:1a823236874340f4f2ec85e93a1942db1a099b11,"This paper studies the latent concept learned in BERT representations. The authors use a simple and effective clustering method to discover latent concepts in an unsupervised way. The contribution is primarily on the empirical side where the learned concepts are labeled and compared with pre-defined concepts. Then a layer-wise comparison is conducted where the results are similar to previous work but is done in an unsupervised way. The authors summarise this paper by organizing the discovered concepts into a BCD dataset. The writing is also clear. While there are many aspects deserving further pursuit, I think this is a solid paper that makes concrete contributions to the understanding of BERT. ","The paper analyses concepts that BERT can capture. The core idea is to use agglomerative hierarchical clustering method to discover latent concepts (i.e. clusters) with hierarchy. Those clusters were manually assigned labels / meanings. The authors then used these labelled clusters to analyse concepts that BERT captures, across several syntactic and semantics aspects. The labelled clusters were also used to build a concept dataset (namely BERT concept dataset - BCD) for 1M instances (i.e. words in contexts).  ",The paper performs a clustering analysis on the contextual representations of BERT on a subset of the News 2018 WMT corpus. These clusters are manually tagged by 3 annotators to connect these clusters to human concepts. The authors additionally release a dataset with these concepts labeled.,"**Summary**    While a large number of previous papers evaluate DNN models on pre-defined concepts, this paper interprets the models (i.e., BERT) on latent concepts that are learned in an unsupervised manner. The analyses in this paper include: (1) Agglomerative hierarchical clustering from ~200k embedded tokens. (2) Comparing the clustered concepts to some hierarchical concept-tagsets. The authors find that lower layers learn many shallow lexical concepts, while higher layers learn more semantic relations. In addition, this paper releases a novel BERT concept dataset (BCD). With cross validation, the authors show that the BCD dataset has high consistency.  **Contributions**  1. Interpret BERT by analyzing latent concepts learned in the network 2. Collect and annotate a hierarchical concept dataset (BCD)",0.16071428571428573,0.11607142857142858,0.20535714285714285,0.16883116883116883,0.24675324675324675,0.2826086956521739,0.23376623376623376,0.2826086956521739,0.19166666666666668,0.2826086956521739,0.15833333333333333,0.10833333333333334,0.1904761904761905,0.16455696202531644,0.1982758620689655,0.2113821138211382,0.19289340101522842,0.1566265060240964
117,SP:1a9806b897efe37df369d36e119742becbaef37d,"This work proposes two novel techniques, an identity matching and a long-short term transformer block. Their first contribution, identity matching, enables them to handle multiple object scenarios as fast as single object ones and process all objects simultaneously. Their second contribution, enables them to handle occlusion better while keeping boundaries accurate. Overall they achieve sota results on 3 main semi supervised video object segmentation tasks.","This submission addresses the task of semi-supervised video object segmentation (VOS), in which given a set of object mask over a frame, these masks must be propagated to the rest of the frames of the sequence.  The main contribution is proposing a method to treat the multiple objects to be segmented in a single pass, by defining an Identification Embedding that encodes all masks simultaneously. Previous works in VOS used independent embeddings for each object, which required multiple decoding passes, one for each object. With the proposed Identification Embedding and Identification Decoding mechanisms which are trained end-to-end.  The submission also introduced a  Long Short-Term Transformer (LSTT) combines a long-term attention related to the first frame, where the object segments are provided as ground truth, together with a short -term attention related to nearby frames.  The experiments on YouTube-VOS and DAVIS set new state of the art results in terms of accuracy and speed at inference time, especially in the case of multiple object in the video scene.","The paper tackles the problem of video object segmentation. Firstly, the authors propose an identification mechanism which allows jointly segmenting multiple objects in a video with constant computational complexity w.r.t. the number of objects. This is achieved by mapping the masks of different objects to a common embedding space, which is then decoded by a single decoder. Secondly, the authors introduce a transformer based architecture with both long term (old frames) and short term (recent frames) matching to obtain accurate segmentation. The proposed approach obtains state-of-the-art results on both YouTubeVOS (83.7 $\mathcal{J}$&$\mathcal{F}$ on val2018) and DAVIS (83.0% $\mathcal{J}$&$\mathcal{F}$ on DAVIS 2017) benchmarks, while being computationally efficient. ","In this paper, the authors study better and efficient way to associate objects in the multi-object video object segmentation. In contrast with previous methods that segment each target object separately, the proposed method matches and decodes multiple objects uniformly and efficiently. For this, an identity bank and a Long Short-Term Transformer is designed. The proposed method AOT become the new state-of-the-art on Youtube-VOS and DAVIS benchmarks.",0.21212121212121213,0.15151515151515152,0.15151515151515152,0.1791907514450867,0.15028901734104047,0.20168067226890757,0.08092485549132948,0.08403361344537816,0.1388888888888889,0.2605042016806723,0.3611111111111111,0.3333333333333333,0.11715481171548117,0.10810810810810813,0.14492753623188404,0.2123287671232877,0.2122448979591837,0.2513089005235602
118,SP:1ad081f4072dc63ec581801e09e203875dde87ed,"This paper explores language emergence properties through two aspects:  - extending the classic Lewis Discrimination Game to a multi-reference/concept one.   - probing language compositionality with a new protocol called ACRe evaluation	  Overall, it is a well-designed paper. It slowly introduces the concept and detail the intuition. I greatly appreciate the didactic aspects of the paper.  Experiments look sounds, and there is a nice balance between quantitative and qualitative results.  Some visualizations -- without being ground-breaking -- are novel and help understanding communication protocols.  The paper does not have substantial weaknesses. Although the ideas are simple (they slightly extend classic ideas), they are correctly explored, and there is no overclaim. My main concern may be that further work could have been done to bridge these ideas to the ML literature.  Indeed, the Lewis Game extension is a form of negative sampling (which recently (re)boom with CPC)  ACRe is a naive approach for language automata. As those ideas already in the literature, the contribution sometimes feels a but too incremental. Therefore, correctly bridging those concepts would be an actual core contribution that is lacking in the community.   In the end, I enjoy the paper, and it may nicely pile up in the emergent communication literature.  Yet, it misses this je-ne-sais-quoi that could make it excellent.  I thus lean toward acceptance (but not clear acceptance).","In this paper, the authors point the limitation of existing reference games and propose different games that require communicating generalizations over sets of objects. In the set reference game, a teacher must communicate to a student a group of objects belonging to a concept. The concept game is more challenging as each agent sees different examples of the concept. The authors also build a teacher model and a student model to play these games. They build two tasks to examine the languages developed for their proposed communication games.    ","In the referential game setup that is widely used in the emergent communication literature, this paper proposes expanding the target into a target set, and studies the effects of different choices for the distractor and target sets. The authors find that choosing suitable sets leads to more consistent communication protocols and higher topographic similarity between concept and message spaces, but at the same time causes a drop in communicative success.  The other contribution of the paper is a composition-based evaluation of the protocols that emerge from the set-based training. Small transformers are trained to mimic operators such as ‘AND’ and ‘NOT’. Evaluating the listener performance on the messages produced by those operator models shows a non-trivial communication performance, albeit significantly reduced relative to the original speaker models. The authors conclude that there is limited evidence for compositionality in the communication protocols based on this experiment. ","The current manuscript proposes to study the emergence of language when agents communicate with each other about generalizations over a set of objects, as part of a referential game. Experiments with object sets yield improved and more interpretable language that evolves between the agents. Additionally, the work also explores finding approximate logical operators in the emergent language.",0.08849557522123894,0.12831858407079647,0.07079646017699115,0.26136363636363635,0.17045454545454544,0.0945945945945946,0.22727272727272727,0.19594594594594594,0.2807017543859649,0.1554054054054054,0.2631578947368421,0.24561403508771928,0.12738853503184713,0.15508021390374335,0.11307420494699646,0.19491525423728814,0.2068965517241379,0.13658536585365852
119,SP:1b752f6a0103f7f78d3ff640e820b17f0d2d11ce,"This paper focuses on the field of meta-reinforcement learning (meta-RL), where the authors propose a method to augment the tasks for meta-reinforcement by applying a mixture of learned reward functions. Specifically, the paper builds on the framework of variBad [1], which uses a VAE to learn a latent dynamics and reward model by encoding the data and decoding the next states and rewards. The authors propose a method that uses a mixture of the encoded latent variables from different tasks during the training time to obtain new latent encodings corresponding to imaginary tasks, and then applies these latent encodings to the reward decoder to obtain new reward functions to train the policy on these imaginary tasks.  The authors then evaluate the proposed method on grid world tasks and several MuJoCo locomotion tasks, where the authors specifically emphasize the performance of adapting to out-of-distribution tasks. The empirical results demonstrate that the proposed method outperforms several baselines.   References  [1] Zintgraf, L., et al. ""VariBAD: a very good method for Bayes-adaptive deep RL via meta-learning."" Proceedings of ICLR 2020 (2020). ","This paper tackles out-of-distribution generalization in meta-reinforcement learning. The proposed method, latent dynamics mixture (LDM), extends meta-RL methods (e.g. variBAD) that support inference and generation of a reward function by interpolating between inferred latents of training tasks to generate new reward functions. The intuition is that ""filling in"" areas of the task space not supported by the training task distribution corresponds to shifting towards the test task distribution. ","The authors attempt to solve a problem in context-based RL, where a model tries to learn and identify the context within which it is behaving. This context can be defined by differences between MDPs, such as differences in the transition dynamics, and the reward function; the agent will make use of such a context in its policy.   This work builds heavily on the variBAD, as is mentioned in the paper. As its main contribution, it introduces a so-called Latent Dynamics Mixture model where during training, latent context values that are seen over the sampled training tasks are used (additively) to generate new, 'imaginary' tasks, to augment the training.   This method aims to improve on the problem of generalising to unseen tasks that are generated from an unseen task distribution.  ","This work studies the partial observability problem of training a policy to adapt to new, unseen out-of-distribution tasks at test time. The authors focus on the setting where the dynamics across all tasks are the same but reward changes. They propose to train an ensemble of latent dynamics models, policies, and reward decoders and train an additional mixture version with dropout. During training, they replace the real reward with an imaginary reward generated by the mixture decoder to create new imaginary tasks. They show that in gridworld and MuJoCo environments, their method generalizes better than other Meta-RL methods.  ",0.1358695652173913,0.1793478260869565,0.15760869565217392,0.2602739726027397,0.1917808219178082,0.16030534351145037,0.3424657534246575,0.25190839694656486,0.2871287128712871,0.1450381679389313,0.13861386138613863,0.2079207920792079,0.19455252918287935,0.2095238095238095,0.20350877192982458,0.18627450980392157,0.16091954022988508,0.18103448275862066
120,SP:1c02f79afdd7b89479c34b0416cfc07492cf85f9,"This paper gives a systematic evaluation on different factors that may cause the high-variance of actor-critic algorithm on various robotic control problems. They found that the main source of variance is from the numerical instability. The authors then propose four training techniques to mitigate the instability, which significantly reduced the variance.","This paper is an empirical study of the issue of high variance in RL. Learning continuous control from pixels is adopted as the setup. The authors demonstrated that the failure of a few runs is responsible for much of the variance. To look for the source of the variance, they separately studied the impact of the initial and training-time randomization as well as variation in representation learning between runs, none of which were found to be majorly responsible for the variance. The authors hypothesized and isolated the exploding activation of the actions to be the main source of the variance, which cause some runs to saturate beyond salvation and fail at performing. The hypothesis is tested by employing several measures to counter this phenomenon. Employing normalization of the penultimate features turned out to be particularly helpful. ","This paper investigates sources of poor outlier runs in a DDPG-based agent on 5 image-based DM control tasks. The empirical analysis suggests that such outlier runs result from saturating tanh nonlinearities due to large-valued activations in the policy network. To prevent saturation of tanh, the paper proposes and evaluates fixes and shows that combining a bunch of them results in better performance with lower variance.","The paper studies the variance of the DDPG algorithm on continuous control tasks. They identify that the variance in rewards arises in part due to the failure of some runs to learn. They argue that the failure to learn is due to saturating tanh nonlinearities, and suggest two methods to address this problem. ",0.33962264150943394,0.20754716981132076,0.2641509433962264,0.11678832116788321,0.1386861313868613,0.19117647058823528,0.13138686131386862,0.16176470588235295,0.2641509433962264,0.23529411764705882,0.3584905660377358,0.24528301886792453,0.18947368421052632,0.18181818181818182,0.2641509433962264,0.15609756097560973,0.19999999999999998,0.21487603305785125
121,SP:1c0c31ebff02affb7f9bdaa20b17b147669190ce,The paper targets navigation and obstacle avoidance using high-frequency event cameras. A new event VAE (eVAE) is proposed to encode inputs from event cameras. An unsupervised learning scheme for eVAE is also proposed. The encoder is first evaluated qualitatively under variations of the input data. The pre-trained eVAE is then used to learn RL navigation policies. Results favorably compare to alternative encoding methods.,"The authors present an event-based variational auto-encoder (eVAE), which learns a latent space representation that is subsequently used for reinforcement learning (RL). The RL task consists of a simulated drone that has to avoid obstacles while flying forward in AirSim. Although the authors themselves do not mention this as a contribution in the paper, I think that the Event Context Network (ECN) they introduce is actually a major contribution of this article. In the ECN they combine an EventNet-like architecture with a Transformer-like position encoding (here temporal encoding). This ECN is a core part of the eVAE and works better than the original EventNet-way of encoding temporal order (as evidenced in the appendix). Finally, the authors show that with the eVAE, a learned policy transfers better to different environments.  ",The author proposed an event based VAE for high speed robot navigation useful for reinforcement learning. They then compare to other image based RL approaches in AirSim and show they outperform other event based RL baselines. They demonstrate that the representation is useful for training visuomotor policies. They show that it reaches SOTA in event based reinforcement learning on the given tasks.,"This paper proposes a method for learning representations of event-based camera percepts which can be used for reinforcement learning. The method is able to digest irregular, streaming camera information using a VAE conditioned on a context vector computed from event data using a PointNet style network. The empirical evaluation shows that using this pretrained representation for PPO is more performant and generalizable than frame-based methods on a simulated UAV obstacle avoidance task. ",0.2153846153846154,0.12307692307692308,0.16923076923076924,0.12686567164179105,0.13432835820895522,0.1935483870967742,0.1044776119402985,0.12903225806451613,0.14864864864864866,0.27419354838709675,0.24324324324324326,0.16216216216216217,0.14070351758793972,0.12598425196850394,0.15827338129496404,0.17346938775510204,0.1730769230769231,0.1764705882352941
122,SP:1c18119277bab10bdc1f2b1e1dcb23dd45387520,"This is an exploratory study of the effects of performing representation learning based on a generative model trained on data, as opposed to the data itself. It is very well written and coherently laid out, and was a pleasure to read. There are no theoretical advances, but the paper provides a useful first foray into an interesting topic, based on well executed empirical studies. ","The paper explores applying a black-box generative model (instead of a fixed training dataset) to learn unsupervised visual representations, mainly using contrastive learning. Standard contrastive learning algorithms generate multiple views of a datapoint using transformations in pixel space. The authors explore using transformations in latent space, either in isolation or in combination with pixel-level transformation methods.  Results are reported for two generative models. * BigBiGan trained on ImageNet and finetuned on ImageNet100 and ImageNet1000 * StyleGan2 trained on LSUN Car and finetuned on ImageNet ","The work investigates **generative models as a data source for self-supervised representation learning**. In particular, the authors propose to form contrastive pairs in the latent space of the generative model and combine it with standard contrastive learning where pairs are formed in image space.  While performance is **slightly inferior** to training with the full dataset, the proposed approach **only requires storing the model weights** of the generator instead of the full dataset.","The paper investigates if synthetic datasets obtained from implicit generative models can be used for representation learning in place of the original dataset. To do so, they compare the performance of multiple popular contrastive learning methods trained on real data against the same methods trained on synthetic data generated by GANs. Notably, for the generated data, in addition to standard pixel transformation, they also evaluate latent space augmentations methods for contrastive learning.",0.15625,0.140625,0.171875,0.14285714285714285,0.16666666666666666,0.2191780821917808,0.11904761904761904,0.1232876712328767,0.1527777777777778,0.1643835616438356,0.19444444444444445,0.2222222222222222,0.13513513513513511,0.13138686131386862,0.16176470588235295,0.15286624203821655,0.1794871794871795,0.2206896551724138
123,SP:1c8f98eeab50e982df3fdb17ba41e7e005765b1d,"The paper considers different aspects of feature-importance-based explanations. In particular, the authors propose a ‘counterfactual training’ approach to deal with observations that are out of the original training distribution (OOD smaples). Moreover, the authors illustrate different approaches to replace input features with a pre-defined baseline, i.e., a common approach to assess the quality of an attribution/explanation method, which however may lead to OOD samples. Finally, the authors propose search algorithms to find binary-valued, feature-based explanations that have been shown to improve relevant test measures.","This paper addresses a core limitation of existing feature importance methods, where importance scores are generated by comparing original examples to unrealistic counterfactuals. To solve this issue, the authors propose a solution where counterfactuals used to compute feature importance are generated by manipulating the attention mask and not by editing the actual text of the original examples. They also propose training models on the generated counterfactuals and show that it improves model robustness. Finally, they introduce a novel method producing model explanations and show that it outperforms strong baselines. ","This paper investigates feature important methods in NLP. The authors argue that counterfactual inputs should be in-distribution, and propose training on counterfactual inputs to reduce this distribution shift. They also investigate the best approach to producing counterfactuals and the best search method for finding FI explanations, finding that one of their proposed methods does best. ","Summary  Explanations methods based on feature importance (FI) assign an importance score to each input feature. This is often evaluated by (1) removing the k most important features and replacing it with a feature that is in some sense defined as uninformative (also referred to by “Replace” function in the paper), (2) measuring how much the confidence in the original prediction drops (= sufficiency metric), (3) a good explanation is determined by still having a high confidence in the original prediction.  The paper argues that this approach is problematic because the explanations, were some features are replaced with uninformative features, represent a counterfactual out-of-distribution (OOD) example that the model is not equipped to handle correctly. As a consequence, the paper argues, the “best” explanation, as determined by the above approach, does not necessarily provide the information the user expects, making this explanation “socially misaligned”.  The paper studies the problem and explores alternative options, which are tested on six text classification tasks.  Contributions  -	Arguments on why the current sufficiency-based evaluation of FI explanation methods is problematic -	Investigation of which “uninformative” features are best for avoid the outlined problem (see second paragraph above) -	Novel search-based methods for finding FI explanations ",0.2087912087912088,0.17582417582417584,0.25274725274725274,0.1797752808988764,0.21348314606741572,0.3392857142857143,0.21348314606741572,0.2857142857142857,0.11442786069651742,0.2857142857142857,0.0945273631840796,0.0945273631840796,0.21111111111111114,0.21768707482993196,0.15753424657534246,0.22068965517241376,0.13103448275862067,0.1478599221789883
124,SP:1cc8be67965c031d72bfda0c3377bec79c8b42ae,"The paper extends the policy gradient theorem from purely on-policy gradients to arbitrary state distributions. The authors provide proofs of convergence and (improved) rate of convergence under milder assumptions than the original policy gradient theorem. They propose a simple agent that separately performs on-policy and off-policy updates, and include a carefully-designed toy experiment that elegantly shows their algorithm achieve globally-optimal results when traditional policy gradient approaches fail. They also demonstrate their approach on a simple deep-RL benchmark. ","In this paper the authors propose an extension to policy gradient theory which considers policy updates under any state distribution density. Moreover, they present the necessary conditions for convergence to optimality under their new formulation. Subsequently, they implement the ideas supported by their theory and construct a new agent which they call J&H (Dr Jekyll and Dr Hyde). The main idea of this agent is to consider two policies one which is fully exploitative (Jekyll) and one which is completely exploratory (Hyde). This way during training they can combine on-policy and off-policy updates. Finally, they test their agent in a series of simple MDPs and show that their proposed agent is superior at  recovering from converging to a suboptimal policy. ","This paper studies the effects of off-policy updates for policy gradient methods in RL. The authors show that under certain conditions on the state update distribution and learning rate, exact PG methods will converge to the optimal solution. Based on this result, the authors propose an RL algorithm where the policy is trained on states from two distributions: 1) the on-policy data distribution and 2) the distribution of an ""exploratory"" policy. This method is demonstrated on two exceedingly simple tasks. ","The paper addresses exploration with policy optimization methods by explicitly separating the exploration and exploitation parts of the policy. First, some theory is developed to support this idea by generalizing the policy gradient to allow arbitrary state distributions. Then, experiments in small MDPs and in deep RL demonstrates its advantages empirically.  ",0.26506024096385544,0.1927710843373494,0.14457831325301204,0.14634146341463414,0.0975609756097561,0.13414634146341464,0.17886178861788618,0.1951219512195122,0.23529411764705882,0.21951219512195122,0.23529411764705882,0.21568627450980393,0.21359223300970873,0.19393939393939394,0.17910447761194032,0.17560975609756097,0.13793103448275862,0.16541353383458648
125,SP:1ce0882cf8ac4e9714a29bf3ea1959d72f1089a6,"This work focuses on the problem of learning predictive models where no ground truth labels are provided.  Instead a set of weak constraints over instances that restrict the space of possible target values are provided.  The proposed method, LLF, is based on the core idea that instead of determining a single true weak label for each instance, a model can be learned by considering the expectation over all possible labels in the feasible set defined by the given constraints.  The authors utilize a conditional normalizing flow model as a generative process for labels and constrain it's output by imposing regularization terms on the training objective that represent different constraints imposed by the weak supervision.  The authors show how their framing of weakly supervised learning can be applied go specific settings in classification, regression, and unpaired point cloud completion.  In their evaluation, the authors compare LLF to a number of weakly supervised baselines and show superior performance on classification and regression tasks, while largely being outperformed by one method in point cloud completion.","This article proposes **label learning flows (LLF)**, a general framework for weakly supervised learning tasks. The modelling framework is primarily driven by 1) conditional normalizing flows (Trippe & Turner, 2018), which is used as a flexible conditional likelihood model, with an affine coupling flow layer, and 2) constrained optimization formulation of maximum likelihood, where weak signals/error rate bounds enter as constraints (Arachie & Huang, 2021). The main practical formulation uses Lagrange multiplier to define an unconstrained stochastic loss function that requires Monte-Carlo samples during training (Equation 6). Ultimately, they obtain a trained inverse flow, which conditional on a covariate, can produce prediction samples.","This paper proposes a new method for weakly supervised learning where the true label of a data sample is unknown but there are known constraints on the label. The proposed method is based on conditional normalising flows, where the inverse flow is used to model the conditional label distribution and optimised with the label constraints. The proposed method is used in three applications including classification, regression, and point cloud completion. The proposed method is shown to have better performance in these applications in the comparison with several baseline methods.  ","Different from many existing weakly supervised learning methods, which learn a deterministic function that estimates labels given the input data and weak signals,  this paper proposes label learning flows (LLF), a general framework for weakly supervised learning problems, and it is a generative model based on normalizing flows. The main idea of LLF is to optimize the conditional likelihoods of all possible labeling of the data within a constrained space defined by weak signals.  The proposed method is applied to three weakly supervised learning problems. Experimental results show that this method outperforms many state-of-the-art alternatives.",0.09826589595375723,0.1791907514450867,0.14450867052023122,0.17475728155339806,0.23300970873786409,0.2808988764044944,0.1650485436893204,0.34831460674157305,0.25510204081632654,0.20224719101123595,0.24489795918367346,0.25510204081632654,0.12318840579710144,0.23664122137404578,0.18450184501845018,0.1875,0.23880597014925373,0.26737967914438504
126,SP:1cefec5bc62b64d7df27ef1c5d70096af1be47ff,"The authors propose a 3-level hierarchical method, one that operates on a feature space, another that operates on a feature-conditioned goal space, and finally a low level policy that outputs actions in the environment conditioned on everything above and the current state. They pretrain the policies in a pretraining environment before applying it to the task at hand. The authors also introduce a new suite of environments to test hierarchical RL. The issues brought up by the authors about current HRL methods are enlightening and the method is a novel contribution. Results are fine, with subtle/no improvements over the baselines on some tasks, and substantive improvements on others. I believe the paper has some issues with baseline comparisons and tasks, which I hope will be addressed in the rebuttal. As such, I am currently learning towards not accepting the paper.",This paper presents a hierarchical framework for training locomotion policies for tasks with sparse reward. Low level policies are trained to achieve a selection of subgoals and high level policies are trained by exploring in the goal spaces. Challenging locomotion tasks are introduced to show the effectiveness of the proposed methods.,"This work proposes a benchmark task with bipedal robots instead of locomotion tasks, where the bipedal robots are low dimensional and perform various movements. It also proposes a hierarchical reinforcement learning method with three levels: a policy that specifies a goal space (a set of features to operate on), a policy to specify the goal configuration given that goal space, and a low level policy to reach the desired goal configuration. It learns the low level policies through unsupervised learning, and then optimizes off policy the high level options by optimizing the value function.","This paper argues that, in the context of pre-trained low level skills, there is a trade-off between generality and learning speed. It makes two contributions: firstly, it proposes a benchmarking suite of sparse reward tasks that require different motor skills to study this trade off. Secondly, the paper proposes a hierarchical skill learning algorithm that attempts to trade off generality and learning speed directly. The proposed algorithm outperforms reasonable baselines.",0.0979020979020979,0.15384615384615385,0.0979020979020979,0.2549019607843137,0.21568627450980393,0.1595744680851064,0.27450980392156865,0.23404255319148937,0.19444444444444445,0.13829787234042554,0.1527777777777778,0.20833333333333334,0.1443298969072165,0.18565400843881857,0.13023255813953488,0.17931034482758618,0.1788617886178862,0.1807228915662651
127,SP:1d3ae28dcd1f679033a9177a5903fda1944e7024,"The paper presents an interesting research problem of ""Learning Inter-Object Functional Relationships"" in virtual 3D indoor environments, where the goal is to make an agent understand the functional effect on an object by interacting with other objects in the scene. Two important (and related) questions arise: (a) How to understand which object(s) affects the functionality of which other object(s) in the room?, and (b) how to learn such inter-object functional priors to build a good knowledge base for reliable inference in unseen environments?  The paper addresses these two questions by presenting a reinforcement-based learning algorithm.   The technical summary of the paper is given below.  ===Goal===:   Learning inter-object functional relationships, represented by scene graphs. That is, an agent is provided with large-scale scenes to explore for learning in the training stage and is asked to predict a functional scene graph, (S, R_s) for a new scene at test time. Here S is the scene and R_s is the relationship scene graph for the scene, S.    ===Assumptions===:  Object segmentation and state changes (by ""state"", the authors refer to the functional outcome of an object) are provided to the agent apriori. Therefore, the functional effect of a relationship is directly observable.    ===Input===:   A 3D scene as a set of objects {O_1, O_2, ..., O_n}, where each object O_i is explicitly modeled as (O_i^hat, c_i, s_i) where O_i^hat represents the point cloud of the object, c_i represents the 3D centroid, and s_i represents the object's isotropic scale.    ===Output===:  Functional scene graph, R_s, i.e., whether there exists a link between two nodes (objects) of the scene graph (scene).     ===Learning Style===:   Self-supervised learning framework     ===Neural Network===:  1) Pretrained PointNet++ and an MLP (these two make up what is called a ""BR-Prior-Net"", which is short for Binary Relationship Prior Net) -- supervised training using observed GT data  2) GCN, 3-layer (this is a part of what is called an ""SR-Prior-Net"", which is short for Scene Relationship Prior Net) -- Partly supervised using data from the above step and some rule-based heuristics  The goal of the above two networks is to propose functionally related objects.  3) SR-Posterior-Net, which has the same architecture and weights as SR-Prior-Net. The only difference is that the input to SR-Posterior-Net, in the beginning, is the refined prediction output from SR-Prior-Net.  4) A GCN trained using Reinforcement Learning (RL) algorithm to output action probability scores for each object that indicates the probability that this object triggers a state change on another object. Essentially, the RL-algorithm-based optimization on GCN is used to obtain supervision ground truth.    ===Dataset Used===:   AI2Thor, PartNet-Mobility  1) A total of 1200 scenes covering 23360 objects from 121 categories are used from the above datasets.  2) Among these, 27 are trigger objects, 29 are responders, where 11 are both trigger and responders (e.g., desk lamp), and 79 are non-interactive background objects    ===Relevance of datasets===:   The above two datasets employed fit well for working on the proposed problem.   ===Loss===:  1) For training the RL algorithm, a Policy Optimization loss is formulated. 2) SR-Priori, BR-Priori, and SR-Posteriori are all simple link prediction tasks. So, standard binary cross entropy loss.    ===Objective Evaluation Metrics===:  Precision, Recall and F1-scores","The paper proposes a framework to study inter-object functional relationships, i.e., where the state change in one of the objects causes functional effects on other 3D objects.  The authors propose a two-stage strategy guided by a self-supervised strategy. The first stage predicts possible functional relationships between the objects. In the second stage, the system performs interactions between the objects in the scene. The authors present the inter-functional relationship with a directed graph.   The binary relationships are represented with a prior network. Processing all the objects results in a scene representation of a functional relationship. Their procedure for training an exploratory policy allows them to generate data to construct a posterior network of the functional relationship in the scene. The authors create a dataset using scenes and objects from AI2Thor, enriched with articulated objects from PartNet.  The authors present an ablation study where baselines are extracted for the binary priors, scene priors, the exploration policy, the selection of objects, and their level of interaction. ","This paper proposes to study inter-object functional relationships such as a switch on the wall turning on or off the light. To tackle this task, this work designs a two-stage approach under a self-supervised interactive learning-from-exploration framework, which can predict functional relationships between objects in new 3D indoor environments. Furthermore, this work creates a dataset based on AI2THOR and AI2THOR to demonstrate the task. ","This paper proposes a method to learn relationships between two objects, e.g., flipping a switch turns a light on. This is formulated as a binary classification task taking two objects as input and outputing a single a value $\in [0,1]$ if the objects are related or not. The paper proposes a method to train this using RL to select the training (object_1, object_2) pairs. The paper also introduces a dataset for evaluation and provides a lot of experiments to study this.",0.1111111111111111,0.047619047619047616,0.054673721340388004,0.16071428571428573,0.1488095238095238,0.2608695652173913,0.375,0.391304347826087,0.36470588235294116,0.391304347826087,0.29411764705882354,0.21176470588235294,0.17142857142857143,0.08490566037735849,0.0950920245398773,0.22784810126582278,0.19762845849802374,0.2337662337662338
128,SP:1d5b33ccc252a7daeb057ea9bae46589a556c284,"The paper considers the offline stochastic matching problem: Each edge e of the graph is equipped by a probability p_e (that it exists) and a weight w_e.  The goal is to find a good probing strategy: an edge can be probed if we can add it to the current matching, if probed and it exists it has to be added to the matching. One difficulty is that vertices have patience constraints stipulating how many adjacent edges the strategy can probe.  The main results of the paper are  - An improve approximation guarantee of 0.383 for general graphs; improving upon the previous best 0.31.  - An improved guarantee of 0.432 if vertices have unbounded patience (it is trivial to get 1/2 by querying the edges in decreasing order; however their result applies in the random edge-arrival online model)  - In the special case of bipartite graphs and one side has unit patience, they get an 1-1/e guarantee improving upon a recent 1/3-guarantee.  The results are nice and are obtained by interesting insights.",They give an improved 0.38 approximation algorithm for a basic stochastic matching problem. They also get improvements for a few other variants. One notable result is improving a recent 0.33 approximation in [Hikima et al] to 0.63.,"The submission focuses on the following “stochastic matching” 1-player game: Nature samples a (hidden) graph, where each edge appears independently with probability p_e. The Algorithm can sequentially probe candidate edges. If an edge belongs to the graph, its vertices are matched and can not participate in later matches. The goal is to maximize the total weight of matched edges. In one of the variants, each vertex also has a (known-to-the-algorithm) patience which upper bounds the number of allowed attempted matches.  There is also a second result which generalizes ordered CRS from independent r.v. to negatively correlated. ","The paper considers a version of the stochastic matching problem. We have a graph with known weights on the edges but where each edge is present with probability p_e (also known). Sequentially, a decision-maker has to probe edges. When DM probes an edge, she can see whether the edge was present or not, and in the former case, she must include it in the matching. The main result in the paper is 0.382 approximation-algorithm for this problem. ",0.05027932960893855,0.1452513966480447,0.16201117318435754,0.175,0.25,0.2647058823529412,0.225,0.2549019607843137,0.35802469135802467,0.06862745098039216,0.12345679012345678,0.3333333333333333,0.08219178082191782,0.18505338078291816,0.2230769230769231,0.09859154929577466,0.1652892561983471,0.29508196721311475
129,SP:1d74a0dd9946ac2715c8259a178e175786fc1c28,"During a network update step all layers need to wait for the feedforward pass (or its temporal RNN analogue) to reach ""the end"" and only then the error is calculated, gradient computed, weights updates. So effectively most of the time the lower layers are just waiting for the feedback to trickle down. This is called ""feedback locking"" and is inefficient. The paper proposes to add a second network to the architecture that learn to predict intermediate layer gradient from their activations, allowing an immediate update. The overall architecture bears hypothesized similarity to the role of cerebellum in primate brain.  I find the idea interesting and the execution faithful and detailed. My main concern is whether the chosen control experiment is doing a good job at eliminating confounding factors potentially responsible for the better performance of the proposed method (see below for detailed explanations and a proposal).  I am reserving my final assessment till after authors' rebuttal.  ---- UPDATE ---- Authors' rebuttal has fully addressed my concerns and resolved them in a positive manner. I am now much more confident in the value of the contribution. Raising the score to ""8: Top 50% of accepted NeurIPS papers, clear accept""","This paper builds on past work that uses synthetic gradients (estimated by small networks) to decouple parts of a network so that they can run in parallel. The current paper proposes that the cerebellum may produce such synthetic gradients to deal with delayed feedback to recurrent networks. To illustrate how this could work, the authors model a cortico-cerebellar network as a LSTM (cortex) and feedforward gradient-estimating network (cerebellum). They show that the system can learn various tasks, including line and digit drawing and caption generation. The further show that disrupting the cerebellar input leads to something resembling ataxic behaviour due to cerebellar damage. ","In this work, the authors propose a neural architecture inspired by the anatomy of the cortico-cerebellar pathways in the brain.  The method relies on a previously proposed framework of decoupling neural interfaces. Here, the authors suggest that the cerebellar circuit calculates the future error of the cortical representation. It allows the implementation of Back Propagation Through Time without the phase-lock problem—where the network has to be wither in a feedforward of a feedback phase to calculate the local weight gradients. ","The authors propose to interpret the function of the cerebellum through the recently developed theory of decoupled neural interfaces, referred to as DNI, allowing for online training of a deep neural network and not requiring locking. After introducing the DNI, the paper extensively explains the possible mapping between the model and observe cortico-cerebellar networks, leading to a clear explanation of how they conceive feedback to be implemented in the brain. Finally, the paper proposes to compare the results obtained with their architecture on a variety of tasks validating their model. ",0.09693877551020408,0.10714285714285714,0.10714285714285714,0.14285714285714285,0.1523809523809524,0.25301204819277107,0.18095238095238095,0.25301204819277107,0.23076923076923078,0.18072289156626506,0.17582417582417584,0.23076923076923078,0.12624584717607973,0.15053763440860213,0.14634146341463414,0.1595744680851064,0.16326530612244897,0.2413793103448276
130,SP:1d7d497f3157744ca0a8a516d04b651b8a9c0c7e,This paper investigates the graph clustering problem with the aid of graph labels. GMM and GCN are used to solve this problem. A new loss is also designed. Extensive experiments are conducted to show the effectiveness of the proposed method.,"This manuscript proposes a problem defined as weakly supervised graph clustering. Given a set of graphs and node features, the problem is to predict node labels in each of the graphs. Some variations can be made depending on selecting the targets whose labels are supposed to be predicted.","This paper studies a new problem named weakly supervised graph clustering (WSGC), where the goal is to cluster nodes better with some graph-level side information, i.e., graph labels.  Most previous studies for graph clustering primarily consider node/edge information and their connections, but ignore graph-level side information, which might be easily accessible in real scenarios. Inspired by this observation, this paper proposed to incorporate graph-level labels into graph clustering, and formulate the new problem as weakly supervised graph clustering. Based on the new problem, the authors further proposed a simple yet effective framework based on Gaussian mixture model (GMM) and graph convolutional network (GCN) named Gaussian Mixture Graph Convolutional Network (GMGCN) for learning node representations. Moreover, they also introduced a consensus loss for graph labels and employed Gaussian Mixture  Layer to infer the category of each node.  Experiments on both synthetic and real-world datasets demonstrate the effectiveness of the proposed GMGCN approach. ","Authors provide the framework that combines Graph Attention Networks (GAT) and Gaussian Mixture Models (GMM) to tackle the weakly supervised graph clustering model. In particular, the consensus loss, a new loss function designed by the authors, is used for the entire framework. Authors also address different kinds of weak supervision settings and show the performance of the proposed framework for each case on synthetic datasets. ",0.25,0.45,0.35,0.3541666666666667,0.22916666666666666,0.14012738853503184,0.20833333333333334,0.11464968152866242,0.2153846153846154,0.10828025477707007,0.16923076923076924,0.3384615384615385,0.22727272727272727,0.18274111675126903,0.26666666666666666,0.16585365853658537,0.19469026548672566,0.1981981981981982
131,SP:1d92b4021b5a9bf3c2372b63a637319a97af53e2,"This work provides theoretical and empirical justifications on why shift-invariant neural networks are less adversarial robust than fully-connected neural networks. Using Neural Tangent Kernels and Convolutional Neural Tangent Kernels, the authors prove that for simple neural networks, shift-invariant neural networks produce separators with decreasing margins, making them not as robust as fully-connected neural networks. Empirically, the authors provide experiments on simple neural networks with MNIST and more complicated networks such as ResNet on bigger datasets such as CIFAR10. ","They show that invariance to circular shifts increase adversarial vulnerability. To motivate this finding, they characterize the margin between classes for a shift-invariant linear classifier. Based on the NTK settings, they show that FC and shift-invariant networks produce linear decision boundaries, which partially explains their adversarial vulnerability. They further empirically verify that shift-invariance reduces adversarial robustness on real datasets and realistic architectures.","The adversarial attack is one of the critical problems in current machine learning research that causes end-users to mistrust models deployed for high-stakes decision-making applications. The paper identifies the ""shift-invariance"" property of neural networks to be a possible reason behind their high sensitivity to adversarial attacks. The paper shows that the margin between the classes for a shift-invariant linear classifier only depends on the DC component of the signals. Further, the authors show that in simple cases,  fully connected and shift-invariant neural networks learn linear decision boundaries. Finally, experimental evaluations on real datasets and architectures show that shift invariance reduces adversarial robustness.","This paper begins with the finding that shows that shift-invariant linear classifiers can have a significantly smaller margin than non-shift-invariant linear classifiers. They then perform a similar analysis on neural tangent kernels (NTK) and find that the linear classifier trained on the convolutional NTK also has a smaller margin. Empirically, they found that some networks, like AlexNet, are less shift-invariant and they seem to be more robust.- The observation that shift Invariance can reduce adversarial robustness is an interesting and important topic. - The experimental results are worth a deeper analysis of why less shift-invariant networks appear more robust. - Code is provided in the supplementary. ",0.14634146341463414,0.1951219512195122,0.2073170731707317,0.47692307692307695,0.3230769230769231,0.18518518518518517,0.18461538461538463,0.14814814814814814,0.1559633027522936,0.28703703703703703,0.1926605504587156,0.1834862385321101,0.16326530612244897,0.16842105263157894,0.17801047120418848,0.3583815028901734,0.2413793103448276,0.18433179723502305
132,SP:1dff5f8767a955f6b952fca45b5b79e1c7efb3b5,"This paper considers the learning of non-canonical Hamiltonian dynamics from time-series data. Canonical Hamiltonian systems in Euclidean spaces correspond to $\dot{x}=J \nabla H(x)$ where $J=[0,  I; -I, 0]$ is a constant matrix corresponding to the canonical symplectic form. Non-canonical Hamiltonian systems are in the same form, however $J$ can be $x$ dependent, and it has to be a skew-symmetric matrix satisfying Leibniz’s rule and Jacobi identity. This paper studies the Euclidean case, which has nice enough topology so that $J:\mathbb{R}^{d}\to \mathbb{R}^{d\times d}$ satisfying these conditions can be expressed using a vector field $Y: \mathbb{R}^{d}\to \mathbb{R}^d$ instead, which is no longer constrained. The proposed method represents $H$ and $Y$ by neural networks and trains them to match the data. Some numerical investigations are provided.","This work addresses the problem of learning Hamiltonian systems from data without assuming that the generalized momentum is given. This assumption limits the applicability of standard Hamiltonian Neural Networks (HNNs) because the generalized momentum is defined in terms of the Hamiltonian itself, which means it cannot typically be provided as training data for unknown Hamiltonians. This work proposes to get around this limitation by exploiting a coordinate-free formulation of the Hamiltonian and incorporating the 2-form from symplectic geometry into the structure of a HNN (either directly as a learned skew-symmetric matrix or via a learned 1-form and an exterior derivative). The resulting architecture performs favorably compared to a standard HNN and Lagrangian Neural Network (LNN) on several simple physical systems.","This paper proposes to learn conservative physical dynamics from data by directly learning a differential 1-form. The dynamics is subsequently determined by computing the exterior derivative of the learned 1-form (yielding a symplectic 2-form) and providing a coordinate-free representation of the dynamics. In addition to inherently preserving the symplectic nature of the dynamics, this procedure does not require *a priori* knowledge of the generalized momentum coordinates, i.e. only velocities need be observed so masses of the degrees of freedom in the data are not needed. The neural symplectic form methodology is compared to an approach in which the symplectic 2-form is learned directly as a skew-symmetric matrix as well the existing Hamiltonian neural network and Lagrangian neural network approaches. ","Authors present a ML-based approach to learning dynamics in the space of Hamiltonian systems. The approach is based on generic formulation of Hamiltonian mechanics in the language of differential geometry, effectively avoiding the requirement of knowing the appropriate generalized coordinates beforehand. The proposed approach is based on modeling the symplectic 2-form (known to be closed) as an exterior derivative of a parametrized (learned) 1-form. This method is termed ""neural symplectic form"" and provides a compact parametrization of the underlying governing equations.  The proposed model is trained by minimizing the MSE between estimated ground truth time derivatives of the dynamical system and predictions of the model on several datasets: mass-spring system; double pendulum; Lotka-Volterra equations (which are not captured by a traditional formulation of Hamiltonian mechanics with fixed symplectic form).  The model is compared to baselines (HNN, LNN and a naive implementation termed skew matrix learning) on test loss and errors of the conserved quantity in the predicted dynamics accumulated during simulation for a fixed time.  Authors find that the proposed model produces best results across most of the comparisons, especially on the Lotka-Volterra test case.  ",0.16666666666666666,0.14583333333333334,0.1527777777777778,0.23387096774193547,0.24193548387096775,0.2777777777777778,0.1935483870967742,0.16666666666666666,0.11518324607329843,0.23015873015873015,0.15706806282722513,0.18324607329842932,0.1791044776119403,0.15555555555555556,0.13134328358208955,0.23199999999999998,0.1904761904761905,0.22082018927444794
133,SP:1e1fcd93f3d5343dc7bc87d5a0aa5974b577a9ac,"The authors consider the relationship between the performance of modern deep neural network architectures trained on standard benchmark natural image datasets and their relationship to stability with respect to smooth deformations of the input image. These issues have been studied previously in the context of (natural) adversarial examples and in theoretically-constructed (i.e. non-trainable) networks -- the motivation and contribution here is to examine them through the lens of trained networks as well. To study this issue empirically, the authors introduce a maximum-entropy distribution on diffeomorphisms, which (I believe) amounts to constructing the deformation vector field as a (gaussian) random combination of periodic Fourier basis functions with frequency-dependent weights; they undertake an empirical investigation of the properties of this distribution in order to find parameter regimes where it produces realistic small-magnitude diffeomorphisms of the image plane (rather than distorting it beyond recognition, or doing nothing). They then use this tool to construct a computational ""relative diffeomorphism stability"" metric, which is the average stability of a predictor with respect to such diffeomorphisms divided by the predictor's average stability with respect to scale-appropriate gaussian noise perturbations, called $R_f$. With $R_f$, several findings are reported: across numerous modern architectures (ResNet18; EfficientNet) and datasets (CIFAR10; ImageNet), randomly-initialized networks have $R_f$ near unity and it decreases after training; $R_f$ decreases with the size of the training set (measured by subsampling); and test set performance correlates well with $R_f$ after training in the context of the CIFAR10 dataset. The authors present a toy theoretical model to give a theoretical corroboration of the $R_f$ parameter's behavior for a shallow network, and provide some ablation studies in the context of presence or not of data augmentation in the training process and the type of interpolation used to apply the diffeomorphisms.  ","This paper introduces a new quantity called ""relative stability"" that strongly correlates with the classification performance of state of the art architectures on standard datasets. This quantity is a ratio between the variation of the features with respect to diffeomorphic transformation of the images and simple additive transformations.  To do so, the authors propose to use a parametrized set of diffemorphisms with bounded L2 norm of the derivative.  The dependence on the training set is highlighted, in particular a given amount of data is necessary to see the ""relative stability"" decrease with the size training set. Last, a simple synthetic experiment is proposed that illustrates the expected behaviour.    ","This work contributes a framework for characterizing the invariance of image classifiers to diffeomorphisms. It is based on the empirical index $R_f$, which measures the stability to random diffeomorphisms relative to stability to additive noise. The authors find that $R_f$ is strongly correlated with classifier performance for a wide array of image classifiers. The results suggest that an image classifier's generalization performance is strongly related to its invariance to certain diffeomorphisms, as opposed to additive noise.","This paper investigates the role of stability to the action of diffeomorphisms in the classification performances of deep neural networks. Surprisingly, it shows that stability measurements are good indicators to compare the performance of deep neural network classifiers even for complex image data bases. The paper introduces stability measurements of diffeomorphism actions over images and shows that these measurement correlate strongly with the classification errors obtained by different CNN architectures over MNIST and CIFAR. The paper also shows that the stability to diffeomorphisms decreases when the training sizes decreases and gives a simple model to explain this phenomenon. ",0.11437908496732026,0.08496732026143791,0.09477124183006536,0.14814814814814814,0.19444444444444445,0.22784810126582278,0.32407407407407407,0.3291139240506329,0.29591836734693877,0.20253164556962025,0.21428571428571427,0.1836734693877551,0.16908212560386474,0.13506493506493505,0.14356435643564355,0.17112299465240643,0.20388349514563106,0.2033898305084746
134,SP:1ead91a87e17e1771bde3b223cb5f94686cb45d6,"This paper introduces a new pipeline for human motion prediction with the help of an action dynamics memory bank. Based on the previous work PHD, the stored action motion dynamics from the training data clustering and action classification task can guide the prediction better. In experiments, the proposed method outperforms its counterpart and several baselines on two widely-used benchmarks.","The paper introduces an external memory component to the baseline method PHD on the task of human action prediction. This external memory component is designed so that it clusters common actions across videos conditioned on the action class. A query-read mechanism is designed to train and retrieve similar actions using attention. This memory component is then used to provide information to the LSTM that predicts future parameters pertaining to action conditioned on the past. A combined loss of different components pertaining to the training of the different parameters is used. This method achieves promising results, and the action memory component is able to cluster relevant actions. An ablation study is also provided to check the contribution of the action memory component.","The authors of the paper proposed to exploit the representative motion dynamics of each action class for 3D human motion prediction. Specifically, they constructed an action-specific memory bank to store representative motion dynamics for each action category, with a query-read process to retrieve relevant motion dynamics from the memory bank, which are consistent with the action in the observed video frames and are used as a prior knowledge to guide motion prediction. The experiment results demonstrated that the proposed approach achieved and exceeded state-of-the-art performance. The ablation analysis demonstrated that each component's contribution to the performance improvements, especially the contribution by the action context captured in the memory bank and the action constraint loss to the motion prediction performance.  ",This paper proposes a system to use predicted action class and action-specific memory to guide human motion prediction. It constructs an action memory bank that consists of observed short-term and long-term human dynamics that can be later queried and used to predict future human motion. The proposed method achieves state-of-the-art human motion forecasting on the H3.6M and PennAction datasets using only videos.,0.26666666666666666,0.3333333333333333,0.26666666666666666,0.2540983606557377,0.13114754098360656,0.2,0.13114754098360656,0.16,0.2318840579710145,0.248,0.2318840579710145,0.36231884057971014,0.17582417582417584,0.21621621621621623,0.24806201550387597,0.2510121457489879,0.16753926701570682,0.2577319587628866
135,SP:1ec817e5e9fb2b129bfde16213686650f8cda1e0,"This paper introduces a method of adaptive space-time tokenization (TokenLearner) for video representation learning. Instead of using fixed/pre-trained tokens for videos, TokenLearner aims to generate multiple spatial attention maps for the architecture to better focus on regions with rich information, thus obtaining better tokens. The overall architecture mimics a combination of X(2+1)D and Bottleneck Transformer while inserting the TokenLearner module. The authors conduct experiments on Charades and AViD datasets, and the proposed method outperforms previous state-of-the-art models. The authors have also carried out ablation study on their design choices.","In this paper, the authors present an adaptive space-time tokenization for video representation learning. In the proposed method, each frame in a video is tokenized into a set of visual tokens through weighted summation over spatial locations. Then, vector transformer operates on a collection of tokens from entire video frames to temporally aggregate the information. The updated tokens are added back to the input tensor after linear pooling and broadcasting. The authors validated the proposed network module (TokenLearner) on two video action recognition benchmarks.","The paper proposes a transformer-based neural network architecture for extracting video representations. Past approaches discretize entire videos into 3D (RGB-T) chunks as input 'tokens' to transformers. Instead in this work, an additional operation is proposed to extract a smaller number of tokens per frame. This leads to a more computationally efficient architecture that outperforms existing methods on the task of video classification. ","The paper introduces a dynamic tokenizer that constructs the tokens using a set of convolutional layers, each selecting regions of interest. This is then combine with a space-time like attention mechanism. The idea is intersecting but important experiments are missing.",0.23469387755102042,0.12244897959183673,0.09183673469387756,0.12941176470588237,0.10588235294117647,0.140625,0.27058823529411763,0.1875,0.21951219512195122,0.171875,0.21951219512195122,0.21951219512195122,0.25136612021857924,0.1481481481481481,0.12949640287769784,0.1476510067114094,0.14285714285714288,0.17142857142857143
136,SP:1f2557bac92907d1b6216ef3af3e30504484f1e4,"In this paper the authors  propose the particle dual averaging (PDA) method, which generalizes the dual averaging method in convex optimization to the optimization over probability distributions with quantitative runtime guarantee. The method in thi paper can thus be interpreted as an extension of the Langevin algorithm to naturally handle nonlinear functional on the probability space. An important application of the proposed method is the optimization of neural network in the mean field regime. By adapting finite-dimensional convex optimization theory into the space of distributions, the athors  establish global convergence of PDA for two-layer mean field neural networks under more general settings and simpler analysis. Numerics support the   theoretical findings. ","The paper studies the convergence of the dual averaging method for the 1-hidden layer neural network in the mean-filed regime. Mean-field perspective allows one to recast original non-convex optimization problem over parameters of neural networks into (non necessarily strictly) convex problem in the space of probability measures.   Similarly to Mei et al. 2018 and Hu et al. 2019, authors consider entropy regularised cases under which the optimal solution to the infinite-dimensional problem over measures admits convenient Boltzman/Gibbs distribution.  Unlike Mei et al. 2018 and Hu et al. 2019,  who studied corresponding Langevin dynamics, while here authors study dual averaging methods in which one approximates the optimal sampling distribution by a sequence of simpler distributions that converge to it (with a rate one over a number of iterations), and use Langevin dynamics to sample from each distribution in the sequence.  If one would be able to quantify the rate of convergence of to each distribution in the sequence, this will imply the overall rate and complexity of the algorithm. ","The paper is motivated by optimizing two-layer neural networks, with entropy and quadratic regularization. The proposed method is an extension of the dual averaging method to the space of probability densities, together with a finite particle approximation based on Langevin dynamics. An error bound is provided, such that to reach an error $\epsilon$, the number of steps and the number of particles required are both polynomial in $1/\epsilon$. ","This paper introduces a Particle Dual Averaging (PDA) algorithm to minimize non-linear functional of probability distributions. One of the main application of this algorithm is the training of two-layer neural networks. The authors obtain quantitative convergence results for PDA under mild assumption on the neural network for the minimization of an entropy-regularized loss function. In particular they show that a solution can be reached with precision $O(\varepsilon)$ after $O(\varepsilon^{-3})$ iterations. The theoretical study is completed with the investigation of the finite particle algorithm and generalization bounds. The authors then present a short experimental study on regression and classification tasks, illustrating the properties of the algorithm.",0.24107142857142858,0.16071428571428573,0.22321428571428573,0.13218390804597702,0.14942528735632185,0.24285714285714285,0.15517241379310345,0.2571428571428571,0.22522522522522523,0.32857142857142857,0.23423423423423423,0.15315315315315314,0.1888111888111888,0.1978021978021978,0.2242152466367713,0.18852459016393444,0.1824561403508772,0.1878453038674033
137,SP:1f89d1e3e1fa6660203d060d1587f34b0dd7811f,"The paper proposed a fast and accurate sketching based ALS algorithm for Tucker decomposition. The method is composed of a sequence of sketched rank-constrained linear least squares subproblems. In addition, the paper proved that the sketch sizes of TensorSketch and leverage score sampling that are sufficient for the relative residual norm error of the problems to be bounded by $O(\epsilon)$ with at least $1-\delta$ probability.  The experiments show that proposed algorithm is more accurate than the existing sketching based randomized algorithm for Tucker decomposition. The algorithm can also be extended to CP decomposition effectively.","The authors propose an Alternating Least Squares (ALS) scheme for fitting the Tucker tensor decomposition, based on sketching techniques. In particular, the proposed strategy is to update one of the factor matrices and the core tensor during each iteration, through the use of sketched rank-constrained linear least squares solvers. The theoretical analysis suggests that leverage score sampling may be more accurate than TensorSketch technique, given the same sketch size budget. However, the authors expose that with standard random initializations for factor matrices, leverage score sampling performs poorly on tensors with high coherence; and introduce the randomized range finder (RRF) algorithm so as to provide a better initialization for the factor matrices, leading to better empirical performance.","This paper studies faster tensor decomposition algorithms using randomization. It studies both Tucker and CP tensor decomposition algorithms, both of which solve a sequence of least squares regression problems. Each of these problems are over-constrained, and thus amenable to randomized dimensionality reduction approaches. These methods were implemented using standard numerical packages (NumPy), and tested on both real and synthetic data sets. They obtain significantly faster convergence, as well as lower costs per iteration.","This is paper presents a new randomized sketching algorithm for computing a low rank Tucker and CP decomposition of tensors. The prospered method is based on using sketching to solve the rank-constrained linear least squares subproblems that appear in the popular alternating least squares (ALS) method. Theoretical analysis is presented that give the sampling complexity needed to solve each rank-constrained subproblems to a certain relative error guarantee. Results are presented for two types of sketching, TensorSketch and leverage score sampling. The rank-constrained linear least squares is solved using the randomized SVD. Many numerical results are presented that illustrate the performance of the proposed method.",0.30927835051546393,0.13402061855670103,0.32989690721649484,0.09401709401709402,0.2222222222222222,0.20270270270270271,0.2564102564102564,0.17567567567567569,0.29906542056074764,0.14864864864864866,0.24299065420560748,0.14018691588785046,0.2803738317757009,0.15204678362573099,0.31372549019607837,0.11518324607329843,0.23214285714285712,0.16574585635359115
138,SP:1fc14e4fee0eccde9be1adb5ddda640875f3dd26,"The authors tackle an underexplored problem they call conditional BO, which is a variant of multi-task BO in which one seeks to identify the optimum of each task, instead of the traditional multi-task BO seeks to identify the optimum of the average over all tasks.   The authors introduce ConBO, which uses the knowledge gradient acquisition function to determine the next point to evaluate, and discuss its efficient computation through a combination of intelligent sampling and discretization.   ConBO is evaluated on a variety of problems, including three good applications: hyperparameter optimization, ambulance base placement, and assemble-to-order policy selection. Results are promising, and suggest that ConBO outperforms a well-chosen set of sensible baselines.","The authors propose a method for conditional optimization problems: finding the optimum of each function in set of objective functions. In this setting, the method jointly selects the parameters to evaluate and the task to evaluate. The authors propose a theoretically-grounded variant of the Knowledge Gradient to address this class of problems and propose a new method of approximating the KG acquisition function. ","This paper proposes a new algorithm for a problem entitled ""conditional Bayesian optimization."" The goal is to search for a *series of input locations* that maximizes the performance of a given set of tasks. Unlike multi-fidelity optimization, the objective is to find good values for all tasks rather than the high-fidelity task. The main contribution is a Hybrid Knowledge Gradient (KG) algorithm that mixes two ways of computing KG: one that discretizes the input space (x, t) and computes KG for a discrete set of points,  and another one that uses the reparametrization trick to perform stochastic gradient descent over Monte Carlo samples. Formal guarantees about this method are also given. Empirical results show that the proposed method effectively reduces computation time and improves the quality of the solution.","This paper presents a framework for conditional optimization, which is called ConBO. Authors also proposed a new acquisition function: hybird knowledge gradient, which is built on KG. Empirical experiments show their proposed method outperform many competing algorithms. ",0.1896551724137931,0.21551724137931033,0.06896551724137931,0.34375,0.140625,0.11450381679389313,0.34375,0.19083969465648856,0.21621621621621623,0.16793893129770993,0.24324324324324326,0.40540540540540543,0.24444444444444444,0.20242914979757085,0.1045751633986928,0.22564102564102562,0.17821782178217824,0.17857142857142858
139,SP:1fc2031b1bb2a0d817fc67404614b3bebb5398e8,This paper examined novel neuron activation pattern analysis on neural network classification models via graph theoretic and entropy-based methods. The authors showed reliability of their work by approving hypotheses on examining the qualitative correlation between model performance and activation patterns. The main technical contribution of this paper comes from explaining the neural classifiers by combining the graph-theoretic and information-theoretic approaches.,An important problem of understanding the performance of deep learning models is studied by this submission. Two graph-theoretic and information-theoretic metrics are checked in some datasets with MLP networks. Preliminary results with expected correlation are observed.,"This paper proposes new metrics based on the activation patterns of a model. It propose an activation pattern entropy metric and a graph theoretic metric of neuron communities. The authors then evaluated models trained on MNIST, Fashion MNIST, Fashion MNIST mixed, CIFAR-10, and plant village using their new metrics. They found that entropy is negatively correlated with training accuracy, modularity is correlated with training accuracy, and that the number of well defined neural communities increase with accuracy.","The authors propose two methods to analyze the behavior of neurons in neural nets. The main idea is to study the neuron activation patterns of classification models and explore if the performance can be explained through neurons' activation behavior. The authors propose two approaches: one that models neurons' activation behavior as a graph and examines whether the neurons form meaningful communities, and the other that examines the predictability of neurons' behavior using entropy.",0.15873015873015872,0.20634920634920634,0.1746031746031746,0.21052631578947367,0.18421052631578946,0.16666666666666666,0.2631578947368421,0.16666666666666666,0.1506849315068493,0.10256410256410256,0.0958904109589041,0.1780821917808219,0.19801980198019803,0.18439716312056736,0.16176470588235295,0.13793103448275865,0.12612612612612614,0.17218543046357615
140,SP:1fd30f14c6a94039f5908a5a980f5795afb0cddf,This paper proposed a conic Blackwell algorithm (CBA+) based on Blackwell approachability for solving convex-concave saddle point problems. The algorithm does not need to select stepsizes and achieves $O(1/\sqrt{T})$ convergence rate/average regret. The paper demonstrates strong empirical performance of CBA+ on matrix game and DRO problems.,"# Summary   In light of successful applications of regret matching (RM) -- an instance of Blackwell approachability -- and its variant regret matching+ (RM+) for saddle point optimization  in zero-sum matrix games with simplex decision sets, the paper proposes generalizations beyond this simple setting to address convex concave problems with compact decisions sets. In the way that RM+ improves RM the conic Blackwell algorithm+ is introduced to improve over the approach of conic blackwell approachability (CBA) from [1]. Furthermore, CBA and CBA+ require non trivial Euclidean projections, in this work explicit closed form or efficient approximations are introduced. CBA and CBA+ are extensively tested on a variety of domains, for which CBA+ seems to demonstrate improvements over CBA similar to how RM+ improves RM. Importantly, like RM and RM+, CBA and CBA+ are parameter-free requiring no tuning.  ","This paper describes an online method for saddle point problems, which uses a sequence of linearizations to minimize for each player. The claim is that this method can accommodate conic domains (as opposed to previous methods which focused on simplex domains) and achieves 1/sqrt(T) average regret (the optimal rate). Discussions of efficient implementations and numerical results are also given. ",This manuscript provides a parameter-free algorithm for solving convex-concave saddle point problems. The main ingredient is that the authors propose an a Conic Blackwell Algorithm^+ (CBA^+) which is able to recover the standard $O(1/\sqrt{T})$ rate but does not require any stepsize choices. Several use cases are presented. Experiments include matrix games on a simplex and distributional robust optimization.,0.27450980392156865,0.35294117647058826,0.39215686274509803,0.10294117647058823,0.13970588235294118,0.2459016393442623,0.10294117647058823,0.29508196721311475,0.31746031746031744,0.22950819672131148,0.30158730158730157,0.23809523809523808,0.14973262032085563,0.32142857142857145,0.3508771929824562,0.14213197969543148,0.19095477386934673,0.24193548387096772
141,SP:1fe46e4e0a6e695daeebc2786082630a39d839a7,"The authors have proposed a model for spatiotemporal reasoning problems using self-attention and self-supervised learning in this work. For object representation, an unsupervised method, MONet (Burgess et al., 2019) is used where recurrent attention mechanisms are used to obtain N attention masks. This self-supervision along with unsupervised auxiliary loss helps in learning better representations. Experiments were performed with datasets like CLEVERER, CATER and ACRE after pretraining the model on individual frames. Through various experiments, authors have demonstrated the importance of self-attention, object-based discretization and auxiliary loss in their model. Overall, the model can perform higher-level reasoning with its simplified architecture.  ","The paper proposes to combine self-attention and object-centric representations to be able to solve complex reasoning tasks. The author(s) also propose a self-supervised loss based on masking of the objects' representations to implicitly learn the dynamics, further enhancing the performance on downstream tasks. The results of the proposed method are very impressive, especially on the challenging counterfactuals reasoning task. ","A transformer achieves state of the art in three video+text reasoning tasks. A pre-trained unsupervised method obtains object representations per frame; object representations across frames, word representations (question+options) and a special token is fed to a transformer and a prediction is made using a linear layer on top of the transformed special token. Question answers provide supervision and masking objects provide self-supervision. Ablation experiments are performed: they show the usefulness of object-centric representations and, albeit less so, self-supervision. Model attention and errors seem sensible. It shows that a fully end-to-end network can solve causal reasoning tasks; which until now required symbolic modules such as program generation and execution engines or physical engines.","This paper builds an end-to-end system for visual reasoning. The videos are first parsed to object-centric representations using an unsupervised object detection method (MONet). After that, they use a transformer on top of the object-centric representations to model the spatio-temporal relationship. The features are then combined with the question embedding and output the answer. The learning process is driven by two losses: 1) self-supervised prediction (the authors examine different forms of such supervision); 2) a supervised-learning loss such as the answer to questions. The proposed method achieves impressive improvement over previous state-of-the-art.",0.14150943396226415,0.16037735849056603,0.14150943396226415,0.19047619047619047,0.31746031746031744,0.15833333333333333,0.23809523809523808,0.14166666666666666,0.14705882352941177,0.1,0.19607843137254902,0.18627450980392157,0.17751479289940827,0.15044247787610618,0.14423076923076925,0.1311475409836066,0.24242424242424243,0.17117117117117117
142,SP:1ff0216366936a6e3113a987b307e28ef04bbd44,"The paper proposes a new structured generative model of multi-view images, representing their latent decomposition into objects. This model extents MulMON -- a spatial mixture model over multi-view images that is suitable for image decomposition but lacks a prior necessary for synthesis -- by adding such a prior (using an autoregressive model), and making some other architectural changes (e.g. adding a normalising flow posterior instead of gaussian). In the proposed model, a global latent variable conditions the sampling of per-object latents, which in turn parameterise a spatial mixture model for each image. The model is evaluated on novel view synthesis (NVS) and a priori scene synthesis, on simple datasets of renderings (e.g. multi-viewpoint CLEVR); it is shown to out-perform a baseline (MulMON) on several metrics for both tasks quantitatively, and qualitative results are also displayed.","The paper tackles object-centric learning from multiple views. It proposes an extension to MulMON. Concretely, it introduces a global latent variable, aiming at modeling global information like spatial relationships, and predicts object-level latent variables upon the global one by iterative amortized inference. It also uses normalizing flows to stabilize training and inference. The authors compare the proposed model with  MulMON on three simple datasets: CLEVR MV, CLEVR AUG, GQN Jaco.",The paper proposes a model for inferring object-centric scene representations from multiple views that explicitly factorizes the representation into global scene properties and objects.  The model builds upon previous object-centric VAEs and is capable of generating novel views of a scene. The main contribution of the paper is the global-local factorization of the scene representation together with the architectural changes and optimization tricks required to train a VAE with the proposed latent structure.,"This paper presents a hierarchical, object centric view conditional generative model for scenes. The latent structure includes a global latent variable which captures general scene structure/configuration and conditions a set of slotted latents, each one capturing a single object in the scene. An inference method is proposed - a product of expert + flow encoder for the global, and a sequential encoder + iterative refinement for the slots. Everything is view conditional so training requires camera information, but novel views may be synthesized at ease. The model is demonstrated to work on CLEVR and a slightly more complex version of CLEVR with more object structures as well as the Jaco arm dataset.",0.11428571428571428,0.12857142857142856,0.17142857142857143,0.20833333333333334,0.2638888888888889,0.2631578947368421,0.2222222222222222,0.23684210526315788,0.21818181818181817,0.19736842105263158,0.17272727272727273,0.18181818181818182,0.1509433962264151,0.16666666666666666,0.192,0.20270270270270271,0.2087912087912088,0.21505376344086022
143,SP:206b97aec56ae93fe6596c32d23b8fcb01998e97,"This paper attempts to describe sequence-based and graph-based one-step retrosynthesis models, which are the current mainstreams of retrosynthesis, in a unified manner with the Energy-Based Model (EBM). This paper reformulates several existing models with the EBM, mainly focusing on the difference in the energy function definitions.  In addition, a new EBM variant is proposed in this paper, and its performance is updated the State-of-the-Art.　  Aside from the mainstreams of the machine learning community, AIzynthfinder recently draws many attention as a free and powerful multi-step retrosynthesis engine for computational chemistry. Some comments on this engine may be beneficial for placing the submitted work in a broader context.   Thakkar A, Kogej T, Reymond J-L, et al (2019) Datasets and their influence on the development of computer assisted synthesis planning tools in the pharmaceutical domain. Chem Sci. https://doi.org/10.1039/C9SC04944D Genheden S, Thakkar A, Chadimova V, et al (2020) AiZynthFinder: a fast, robust and flexible open-source software for retrosynthetic planning. J. Cheminf. https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00472-1 ","The paper summarizes many different models for for retrosynthesis under the EBM framework. It introduces dual training. By performing reaction center annotation, state of the art results are claimed.",This paper presents a unified framework with energy-based models for retrosynthesis prediction. Different methods (sequence and graph methods) can be untied with different energy functions within the framework. A dual variant model is presented and it performs consistent forward and backward training. Experimental results demonstrate this consistent training improves the retrosynthesis prediction significantly. ,"This submission tackles the problem of one-step retrosynthesis, a classic problem in chemistry, through the lens of energy-based models. The framing of the problem in this manner is elegant. The quantitative results may need to be clarified or resolved to ensure that the comparison to prior SOTA is fair (i.e., not using an additional model or leaking data of the ground truth reactants).",0.04918032786885246,0.09289617486338798,0.1092896174863388,0.3103448275862069,0.20689655172413793,0.16666666666666666,0.3103448275862069,0.3148148148148148,0.30303030303030304,0.16666666666666666,0.09090909090909091,0.13636363636363635,0.08490566037735849,0.14345991561181437,0.1606425702811245,0.21686746987951805,0.12631578947368421,0.15
144,SP:207c34918396aa7e34101ef6d7c381773c5358b1,"The authors examine the problem of building multiset-equivariant neural networks, and note that there exists multiset-equivariant functions which are not set-equivariant. Given this observation, using set-equivariant models for multiset prediction tasks may be limiting. The authors propose a strategy to construct multiset-equivariant functions (which are not set-equivariant) through the Deep set prediction network framework. The authors propose to improve upon that framework by making use of implicit differentiation to compute a backwards mode gradient without backpropagating through the optimization process which is memory- and compute- intensive. Some empirical results are provided for a simulated toy dataset, as well as a simulated object property prediction problem. ","The paper ""Multiset-equivariant set prediction with approximate implicit differentiation"" points out a difference between typically looked at set equivariance and multiset equivariance, the latter being able to produce different outputs for identical inputs. It is further shown that a method from the literature (DSPN) is multiset equivariant; to make this method perform better, this method is improved with an implicit differentiation approach. The superiority of the method is demonstrated on toy data, and also on CLEVR.","This paper the discusses the conceptual improvements of exclusive multiset-equivariance over set-equivariance in the context of set prediction networks. Deep Set Prediction Networks (DSPN) are identified as the only used architectures for set prediction that satisfy exclusive multiset-equivariance with a specific choice of encoder that employs sorting. As an answer to DSPN not being the leading architecture in terms of performance on set prediction benchmarks, iDSPN is proposed as an improved version of DSPN, which still satisfies exclusive multiset-equivariance and builds on implicit differentiation to employ better optimizers and reduce memory usage and computation time. The experiments highlight the benefits of exclusive multiset-equivariance in toy tasks and demonstrate that iDSPN significantly outperforms the state-of-the-art on the CLEVR object property prediction benchmark.","The paper proposes the usage of multiset equivariance as a weaker constraint for deep sets. The authors then modify DSPN to be multiset equivariant and introduce the usage of Jacobian-free implicit differentiation to speed up computation. Finally, the authors compare on two test tasks and show general improvements.",0.14414414414414414,0.21621621621621623,0.15315315315315314,0.2597402597402597,0.18181818181818182,0.13953488372093023,0.2077922077922078,0.18604651162790697,0.3469387755102041,0.15503875968992248,0.2857142857142857,0.3673469387755102,0.1702127659574468,0.2,0.21250000000000002,0.1941747572815534,0.2222222222222222,0.20224719101123598
145,SP:20ce1ffae7074c08947894b1f1883fcd1b6b13d2,"This paper considers the problem of selective sampling for best-arm identification. At each time step, nature reveals a potential measurement to the learner and the learner decides to either query (contributes 1 count to $\mathcal{L}$) or abstain. The objective of the leaner is to identify the best arm, w.r.t. the unknown environment parameter $\theta_*$, with probability at least $1-\delta$ at a learner specified stopping time $\mathcal{U}$. The main results of this paper characterize the information-theoretic trade-off between labeled samples $\mathcal{L}$ and stopping time $\mathcal{U}$, and presents an algorithm that nearly matches the lower bound. The framework is also extended to cover binary classification problems.","This paper tackles the problem of best-arm identification under linear assumption. The arms are d-dimensional vectors. In addition, the learner has the possiblity to observe or not the (noisy) value of the arm x_t that is proposed by the environment at each time step. The difficulty of this problem lies in the tradeoff between the number L of labeled observations, and the total number U of labeled or unlabeled one. The goal is to minimize both L and U. In the case of i.i.d. x_t's, theoretical lower bounds on the expectation of L and U are provided for any delta-PAC algorithm ; almost matching upper bounds are given as well. A practical elimination-style algorithm is described: it depends on a subroutine solving an optimization problem by stochastic gradient ascent.",This paper considers an interesting problem of selective-sampling for best-arm identification. The paper provides both lower and upper bounds that capture the trade-off between labeled samples and stopping time. The proposed algorithm is equipped with a clear optimization solver and achieves the near-optimal label complexity given a desired stopping time. ,"This paper considers the problem of best-arm identification in a set Z. The environment samples a vector x from a set X and provides it to the learner. The learner may either choose to observe the reward corresponding to x (given by theta^T x + noise) or ignore it. The authors derive lower and upper bounds on U (number of samples provided by the environment) and L (number of rewards requested by the learner) in terms of a tuning parameter tau and the error probability delta.  The algorithm runs in rounds. In each round, there is an active set of arms that can potentially be the best arm. The learner solves an optimization problem to find a function P: X -> [0, 1], where P(x) is the probability with which it observes the reward for x. The goal of this optimization problem is to minimize the expected number of observations made in tau steps, while ensuring that P(x) does not reject so many samples that the problem of identifying the best arm with probability delta in the first tau steps becomes infeasible.  The authors also show how their approach can be used for binary classification as well, again with selective sampling of outputs. ",0.30701754385964913,0.21052631578947367,0.3333333333333333,0.1386861313868613,0.2846715328467153,0.4074074074074074,0.25547445255474455,0.4444444444444444,0.18536585365853658,0.35185185185185186,0.1902439024390244,0.1073170731707317,0.2788844621513944,0.2857142857142857,0.23824451410658307,0.19895287958115182,0.2280701754385965,0.16988416988416985
146,SP:20d9bf47633e596d6aa9ac4894bae24a495e0d95,"This paper proposes a method to do MLP / LSTM-based inferences/predictions based on incomplete data. While typical methods would impute and then predict, the proposed method predicts directly based on incomplete data. The method learns an “importance” matrix which is multiplied with the first weight matrix of the neural network. The learning is done using reinforcement learning, with the negative prediction error being the reward. This method is compared with new and classical prediction methods for incomplete data on a range of datasets and is shown to be the best most of the time.","This paper targeted at the missing data issue in time series data and proposed a imputation-free method to handle missing data. More specifically, the authors proposed a gradient importance learning method named GIL to weigh the gradients for different parameters using reinforcement learning. Experiments on one tabular dataset and two image datasets demonstrated the effectiveness of the proposed method.","The paper proposes a method to train on data with missing features. It does so with a single model that can handle missing features, not with extra imputation methods. This is done by weighting the gradient update of the first weight matrix of the neural network with a vector a that is produced by an RL agent. This agent's reward in turn is the performance of the model after the update.","The paper proposes gradient importance learning for predicting labels on incomplete data. The problem is: during both training and testing, certain dimensions of the samples are masked randomly and the masks (i.e. the ""missing indicators"") are known for the algorithm. Among existing methods in this problem, the paper claims that it is the first ""imputation-free"" way to solve this problem. The paper focuses on MLP and LSTM and adds an (element-wise) multiplicative parameter matrix on the gradient of the first layer (the one that is mostly closed to the data). The matrix is trained using reinforcement learning. The authors also connect its proposal to visual attention. Experiments are conducted on tabular, time series, and a simple image dataset. ",0.18947368421052632,0.24210526315789474,0.23157894736842105,0.2,0.31666666666666665,0.2777777777777778,0.3,0.3194444444444444,0.18181818181818182,0.16666666666666666,0.15702479338842976,0.1652892561983471,0.23225806451612904,0.2754491017964072,0.2037037037037037,0.1818181818181818,0.20994475138121546,0.2072538860103627
147,SP:20e38efdb3d352b8240e8a103a95bda5f6fa19da,"he paper  provides a solution for multi-cloud configuration problem. Specifically, the paper tries to provide cloud customers with an optimal configuration to minimise runtime and cost. The paper also presents a dataset, for offline benchmarking, comprising of 60 different multi-cloud configuration tasks across 3 cloud service providers. ","This pager presents CloudBandit to solve the multi-cloud selection-configuration problem. While the problem is heavily explored in several domains, this paper leverages the hierarchical aspect of the selection-configuration problem. The main novelty of the paper lies in creating the dataset for the above problem. Such dataset can be immensely useful for academic research as ""executing code"" in cloud can be prohibitively expensive in academic settings; thereby, hindering the academic research.","The paper proposes a method to find the best cloud provider for multi cloud compute selection (time, cost) to avoid vendor lock in. They achieve this using a best-arm identification algorithm where each arm runs an BBO on each provider and eventually choosing the best arm. They test this approach on 60 different configuration across 3 providers using a proposed dataset and show that the algorithm provides cheaper, faster config. given a set of parameters.","In this paper, the authors analyze the similarity between the multi-cloud configuration optimization task and the model selection-configuration problem, and try to solve the multi-cloud configuration optimization task inspired by an algorithm for best-arm identification from the AutoML domain.  They built a benchmark for the multi-cloud configuration task including  60 multi-cloud configuration tasks across 3 public cloud providers. The experimental results show the proposed method can find best cloud provider with best instances and other parameters more cheaper or faster, compared to BBOs.",0.24489795918367346,0.30612244897959184,0.40816326530612246,0.1506849315068493,0.2054794520547945,0.2631578947368421,0.1643835616438356,0.19736842105263158,0.2247191011235955,0.14473684210526316,0.16853932584269662,0.2247191011235955,0.1967213114754098,0.24000000000000002,0.28985507246376807,0.14765100671140938,0.18518518518518515,0.24242424242424243
148,SP:211ca69b10198e0cd3986c7f357285befa4e9fe3,"This paper presents a GAN model for virtual try-on trained from unpaired image data. The main network is based on StyleGAN2. During the training stage, for each image, the extracted 2D skeleton pose is used to convert the parsed garment region into a set of rectified texture patches with the proposed Patch-routed disentanglement Module. These rectified patches are encoded to be a single vector fed into every stage of the generator. The head region and the 2D skeleton pose are encoded to be identity features as input to the generator. Because the detailed texture information is not well preserved in style patches, the generator first synthesize coarse-level image and garment mask. The garment region is further fused in later stage of the generator to synthesize the final output. The model is trained in a reconstruction way without using paired person images. During the inference stage, the garment region is warped by the target pose as input into the texture synthesis stage of the generator. The key idea is its patch-based garment disentangling. The part-wise patch rectification removes pose and shape related information but only keeps low-level texture information. Results are reported on DeepFashion and MPV and self-collected UPT datasets with comparisons to state-of-the-art methods. ","This paper focuses on virtual try-on based on unpaired images. The authors propose to disentangle the style and spatial information of each garment, and generate it under the condition of homography transformed normalized patches. The authors also introduce a new unpaired dataset. Compared with the paired and unpaired state-of-the-art methods, both quantitative and qualitative results prove the effectiveness of the proposed approach.","This paper presents a human garment try-on pipeline. The key point of the proposed approach is to normalize each body part into a regular patch according to the body skeleton, and use this patch representation to extract the style and disentangle the spatial variation due to different body poses. The network is based on StyleGAN architecture, and adapted to accept style and pose conditions. Furthermore, to overcome the loss of the texture details, it also proposes to use the first-stage network to generate the clothed mask, and use a refining network to synthesize/inpaint the texture details based on the deformed patches aligned to the target skeleton.","The paper creates realistic virtual try-on results by modifying the StyleGAN2 architecture and using the garment appearance as the style modulation. In particular, to preserve the details of the target garment, the garment patches are warped and used as reference for the output image. The proposed method significantly outperforms the existing baselines thanks to the photorealism brought by preserving the input patches. The improved quality is backed by FID, user study, and analysis via ablations. ",0.11214953271028037,0.16822429906542055,0.1308411214953271,0.30303030303030304,0.24242424242424243,0.1743119266055046,0.36363636363636365,0.3302752293577982,0.3684210526315789,0.1834862385321101,0.21052631578947367,0.25,0.17142857142857143,0.22291021671826622,0.19310344827586204,0.2285714285714286,0.22535211267605634,0.20540540540540542
149,SP:212a00d35ca64b949d6f756eb1bfc7d2d2a379ff,"The paper extends and modifies the expected improvement (EI) algorithm for contextual bandits. Specifically, LinEI and NeuralEI are proposed for the linear reward function and general reward function (under some assumptions), respectively. The paper claims that LinEI and NeuralEI achieve the state-of-the-art regret bounds and they work well on both synthetic functions and benchmark datasets.",The paper introduces and studies the expected improvement (EI) technique as a way to balance exploration and exploitation for the contextual bandit problem. The authors propose two EI-based algorithms for linear bandits and for neural bandits for a general class of reward functions. The paper presents regret bounds for both methods and shows the experimental results to support their theoretical claims.,"This paper applies expected improvement to contextual bandits to balance exploration and exploitation. The authors proposed LinEL for linear rewards and NeuralEI for general rewards using neural network for approximation. Theoretical analysis claimed that compared with UCB based approaches, LinEL's regret bound reduces a factor of $\sqrt{\log(T)}$. Empirical results validated the effectiveness of LinEL and NeuralEI. ","The authors study regret analysis of the expected improvement (EI), a popular but theoretically understudied technique to handle the tradeoff between exploration and exploitation in bandits.  They propose two novel EI-based algorithms for this problem, one for linear payoff and for deep neural networks. With a linear reward function, we demonstrate that our algorithm achieves a near-optimal regret. In particular, our regret bound reduces a factor of \sqrt{log T} from UCB. They also present numerical studies using the proposed algorithm.",0.3448275862068966,0.25862068965517243,0.2413793103448276,0.3225806451612903,0.3870967741935484,0.3728813559322034,0.3225806451612903,0.2542372881355932,0.1686746987951807,0.3389830508474576,0.2891566265060241,0.26506024096385544,0.33333333333333337,0.25641025641025644,0.19858156028368792,0.3305785123966942,0.3310344827586207,0.30985915492957744
150,SP:217651f7240749a7f782d8ad774c4c456b7b4f32,The paper provides an analysis of priors other than standard isotropic Gaussians for Bayesian neural network. The authors first empirically estimate the posterior distributions of the weights of a BNN and then use these distributions as a prior for these BNNs. They show that those priors allow better performance of BNNs. The paper discusses this analysis in the context of the cold posterior effect.,"This paper presents numerous empirical facts about the distribution of the weights after training a neural network. The authors considered one FCNN, one CNN, and one ResNet, and computed several statistics on their weights.  The authors also propose, in a Bayesian setup, to compare the performance of trained BNNs with different priors.","This paper proposed to use distributions different from isotropic gaussian for Bayesian neural network priors. For fully connected neural networks the heavy-tailed distributions such as laplacian and student-t are used, while for convolutional neural networks multivariate gaussian with spatial correlations are employed. Empirical results show that these alternative priors give better results than isotropic gaussian on several image classification tasks. Cold-posteriors versions of the isotropic gaussian prior and the proposed priors are also provided for comprehensive comparison. Overall this is an interesting paper with novel discoveries.","The work studies prior distributions for Bayesian CNNs. The work report that conventionally used priors e.g., Gaussian poorly fit empirical distributions of the trined weights. The empirical distributions appear to be heavy-tailed and correlated. Thus the paper proposes to use ""heavy-tailed priors"" for FCNNs and correlated Gaussian priors for CNNs that tend to improve classification performance.",0.171875,0.265625,0.203125,0.19230769230769232,0.17307692307692307,0.14606741573033707,0.21153846153846154,0.19101123595505617,0.22033898305084745,0.11235955056179775,0.15254237288135594,0.22033898305084745,0.1896551724137931,0.22222222222222218,0.21138211382113822,0.14184397163120566,0.16216216216216217,0.17567567567567569
151,SP:2188c1f4763015148fe55a2313b70038d46cec24,"This paper presented strong technical results for nonsmooth nonconvex problems. The main lower bound is counter-intuitive; the argument for random smoothing is deep. Meanwhile, the main body gives a very good illustration of the hard technical content. ","### Update  I'd like to thank the authors for their response.   I have decided to increase my score to 9 in light of the other reviews and these comments. I look forward to seeing this work published.  ========  This submission investigates two problems central to non-smooth, non-convex optimization: i) the oracle complexity of computing approximately stationary points and ii) the complexity of smoothing non-convex functions.  To tackle the first problem, the authors propose a new concept of _near approximately-stationary_ points, which the set of points within $\delta$ (in some metric) of an $\epsilon$ stationary point. This differs from previous work [35], which classified $(\epsilon, \delta)$-stationarity as $\epsilon$-stationarity of a convex combination of (generalized) gradients in a local neighbourhood of size $\delta$.  The authors prove that, unlike for $(\epsilon, \delta)$-stationarity, finding near approximately-stationary points is impossible in finite time for the classes of deterministic and randomized, zero-respecting algorithms under standard assumptions and for sufficiently small $\epsilon, \delta$.  For the second problem, the submission formalizes trade-offs between the degree of smoothness in the smooth approximation, as measured in the Lipschitz modulus of the gradient, and the computational complexity of the smoothing operation. In particular, it is shown that for every smoothing algorithm which has polynomial running-time in the problem parameters, there exists a function for which the modulus of the smoothed function is $\tilde O(\sqrt{d})$.  As a consequence, randomized smoothing by convolution with a Gaussian or uniform distribution is seen to obtain the optimal dimension dependence out of polynomial-time smoothing procedures.","The authors consider the problem of nonsmooth nonconvex optimization through an oracle complexity perspective.  The authors propose a precise mathematical statement of the idea that there is no complexity estimate for nonsmooth nonconvex optimization based on first order oracles. This is the first contribution presented in the manuscript. The convergence criterion is a quantitative version of the property of ""being close to an almost critical point"". The authors show that obtaining such an approximate critical point is not possible in a dimension independant finite time, in general.   As a second contribution, the authors consider the possibility to use smoothing techniques for nonsmooth nonconvex optimization. For a smoothed objective, usual optimality criteria, such as small gradient norm, can be used. Yet classical first order methods come with complexity estimates which depend on the Lipschitz constant of the gradient of the objective. It is a known fact that randomized smoothing (formally, convolution with a density) allows to obtain Lischitz constants which have a square root dependency in the dimension. The authors show that this is actually tight in the sense that a large class of computationally implementable local smoothers need to account for such a dimension dependency.   ","This paper investigates the question of finding near $\epsilon$-stationary points for  Lipschitz nonconvex functions. This is a quite challenging question, and the paper addresses it with the following advances:  1. An oracle lower bound for the task of finding near $\epsilon$-stationary points. This is achieved by an intrincate construction that resembles the trick of hiding a cone in a random direction. This result is restricted to deterministic and zero-respecting algorithms.  2. The second contribution is the study of smoothings from an oracle perspective. It is shown in particular that in this oracle model, the Lipschitz gradient bounds obtained by randomized smoothing are essentially optimal, which rules out e.g. efficient implementations of the Lasry-Lions regularization. BTW, I wonder if Corollary 3 readily implies that computing saddle points has exponential oracle complexity with respect to the dimension (these type of results are already known, but this would be a more transparent proof, I believe).  3. A less emphasized third contribution is the observation that the relaxation used in Zhang et al. 2020 (based on the convex hull of a union of subdifferentials around the point) does not lead to anything resembling a nearly stationary point. ",0.3157894736842105,0.3157894736842105,0.42105263157894735,0.18702290076335878,0.1717557251908397,0.18974358974358974,0.04580152671755725,0.06153846153846154,0.08080808080808081,0.2512820512820513,0.22727272727272727,0.18686868686868688,0.08,0.10300429184549358,0.13559322033898308,0.21444201312910283,0.19565217391304346,0.18829516539440203
152,SP:218c8824ba95b728daf9d047571123708cd4404b,"The authors analytically study model, sample, and epochwise descents of the random feature model. Model descent meaning how the test error changes as the model complexity is varied; sample descent meaning how the test error changes as the number of samples are varied; and epochwise descent meaning how the test error changes as the model trains. They derive analytic expressions for the test and train errors and study what happens when the number of neurons $N$, the number of samples $n$, and the sample dimensionality $d$ all tend to infinity. Since this can be done in varying ways, they study when $\frac{N}{d} \rightarrow \psi$ and $\frac{n}{d} \rightarrow \phi$ for fixed values of $\psi$ and $\phi$. They observe double descent which is beneficial (lower test error when $\psi \gg \phi$) if training is performed for a long time. On the other hand, if training is cut short (early stopping) the test error is low and even has a dip where later there develops a peak. The authors also observe triple descent if the training time tends to $\infty$.","This article provides, modulo some important technical assumptions, computationally tractable formulas for the test and train error of a random features model trained by gradient flow on \ell_2-regularized MSE during the entire course of training. The features are those computed by the first layer of a randomly initialized neural network and the regime of interest for this article is a triple scaling limit in which the input dimension, dataset size, and hidden layer width tend to infinity but their ratios remain order 1. Unlike prior work this article provides formulas for the evolution in time of both the training and test error, instead only of only their behaviors are t = \infty. While the results are not completely rigorous (they assume concentration for spectral statistics of certain random matrices), I found them intriguing and am fairly convinced by plots in which the predictions fit empirics quite well. The main weakness for me is a lack of further formulation of takeaways from the theoretical results (see weakness #2). Overall, I thought this was a solid paper. ","This paper presents an analytical derivation of the train and test loss curves of the random feature model. The main novelty lies in the fact that the authors not only capture the behavior at final times, but also the dynamics. This leads to various interesting observations, since such models have been shown to exhibit epoch-wise double descent. ","In this paper, authors proposed an analytical model to understand the multiple-descent curves in generalization errors of over-parameterized models (random feature models optimized over gradient flow). More specifically, they provided exact solutions to interpret the model-wise, epoch-wise and sample-wise double descent phenomenon.   In Section 2., authors define the learning problem of over-parameterized models with random features as a regression problem of Ridge-alike, then define the optimization procedure as an ODE. In Section 2.2., authors represent  training and testing errors with  Cauchy integral representations with assumptions prespecified. They report the result as the solution of ODE under assumptions while providing some simulation results to backup their theoretical results. The sketched proofs are attached. ",0.16022099447513813,0.08839779005524862,0.11602209944751381,0.07954545454545454,0.125,0.29310344827586204,0.16477272727272727,0.27586206896551724,0.175,0.2413793103448276,0.18333333333333332,0.14166666666666666,0.16246498599439776,0.13389121338912133,0.13953488372093023,0.11965811965811965,0.14864864864864863,0.19101123595505617
153,SP:219637f6604b208b375b93ff0f15f4f61a76e89c,"In this paper authors provide a novel architecture for modelling neural dynamics with the aim of separating dynamics that are related to behaviour from the ones that are not. Using amortized inference, observed neural activities are used to set the initial state of two RNNs, one for relevant dynamics and one for irrelevant dynamics, which in turn predict behavioural and neural data. The results show that compared to baseline models the model is indeed able to predict behavioural responses, while maintaining prediction ability for the neural data.","The authors introduce a new sequential VAE that simultaneously models behavioral dynamics and neural activity for experimental neuroscience data. The model includes partitioned latents which map to either neural activity and behavior, or just behavior.  The model uses RNNs to describe the evolution of each of these latent states (similar to LFADS), and the model is trained by optimizing an objective function which contains separate variational distributions for behaviorally relevant and irrelevant latents, as well as a penalty to prevent entanglement of the latent states. The authors test their model on neural data recorded from Monkey V1 during a reaching task.","This paper details a dimensionality reduction and dynamical modeling technique that partitions the neural variability into behaviorally-relevant and behaviorally-irrelevant factors. This idea was introduced and explored in preferential subspace identification (PSID), a reformulation of SSID (subspace identification), which models the latent dynamics as evolving linearly. This paper extends that to the nonlinear regime, using recurrent neural networks to model the evolution of the latent variables. The authors use the LFADS framework to model the temporal evolution, while partitioning the state into behaviorally-relevant and behaviorally-irrelevant factors, and not allowing for the behaviorally-irrelevant state to influence the behaviorally-relevant state.","In this work, the authors propose a state-space model and inference algorithm that aims to recover the latent nonlinear dynamics from sequential data with particular interests in neuroscience. The proposed model inherits the latent factor analysis via dynamical systems (LFADS) and incorporates the advantage of disentangling behavior relevant/irrelevant dynamics in the latent space as preferential subspace identification (PSID) does. It overcomes the limitation of linear dynamics and more faithfully captures the behavior. The proposed method was tested on the data from a monkey reaching task.",0.20689655172413793,0.2413793103448276,0.21839080459770116,0.18811881188118812,0.24752475247524752,0.17475728155339806,0.1782178217821782,0.20388349514563106,0.21839080459770116,0.18446601941747573,0.28735632183908044,0.20689655172413793,0.19148936170212766,0.2210526315789474,0.21839080459770116,0.18627450980392157,0.26595744680851063,0.1894736842105263
154,SP:21a03ab7537d26ac11e09a7db449c0d223f235c9,"This paper proposes a new blind super-resolution method, which can due with arbitrary blur kernels instead of Gaussian kernels. The whole method estimates kernels in the frequency domain. The method involves a kernel generator that takes the frequency amplitude as input and outputs a kernel. A patch discriminator is designed to perform adversarial loss to the predicted kernel. This design is novel and the experiments show good results.","The authors propose a novel framework for solving the problem of blind super resolution in which the low resolution input can be degraded by arbitrary blur kernels. In their framework, the degradation kernel estimation is performed in the frequency domain. The authors show that the frequency domain is better suited for kernel representation and reconstruction than the spatial domain and propose a Spectrum-to-Kernel mapping network to estimate diverse blur kernels. They use a conditional GAN to learn how to 'translate' the spectra of degraded images to the unknown kernels.","The authors make a key observation that degradation kernels are highly correlated to degraded images in the frequency domain. From this, they design a kernel generator that directly generates the degradation kernel from the frequency-domain LR image. Then, a non-blind SR model is employed to utilize the estimated kernels for SR. The proposed method outperforms existing methods on two types of kernels (Gaussian and motion) with two backbone networks (SFTMD and RCAN).","This work proposes a method for blind super-resolution (SR) of images degraded by arbitrary blur kernels. The method makes use of a novel blur kernel estimation technique, that proposes to estimate such kernels in the frequency domain following an end-to-end deep learning approach. The authors claim that the Fourier amplitude spectrum kernel representation is better posed for the learning task of kernel estimation than the spatial representation, under the assumption that the kernels are sparse both in frequency and space domains (this assumption holds form most common kernels in real settings). They support this claim with theoretical and experimental evidence.   The image formation model considered here, that produces the degraded LR image from the HR image, is the following. The clean high-resolution (HR) image first downsampled, then convolved with the blur kernel, and finally contaminated with additive noise.    Based on the proposed frequency domain representation, the authors propose a kernel generator network that allows to translate this representation to the kernel spatial representation. The generator $G$ takes as input the single-channel amplitude spectrum (the FFT) of the degraded LR image bi-cubicly downsampled by two, and outputs the single-channel blur kernel estimate. This generator consists of an encoder-decoder network built on a U-net with mirror connection between each of the seven down-sampling convolutional layers and up-sampling transposed convolutional layers with the same feature size.   To ensure that the kernel generated by $G$ is at the same time close to the ground truth kernel and consistent with the SR results, they propose a three-terms loss for $G$. The first term imposes similarity between the ground truth kernel and the kernel generated by G from the degraded image (in the spatial domain) by means of the $\ell_1$; the second term also imposes similarity between these two kernels but in seen as probability density functions, using an adversarial loss that involves a trainable critic $D$ that has to  discriminate between the ground truth kernel and the output of G (actually, de concatenation of each of them with the input of G are considered instead). This patch discriminator is trained using the loss proposed in LSGAN. Finally, the third term is a regularization loss on the output of G; unless I missed it, the regularizer is not specified. This kernel estimation model is called S2K.   Then, the authors propose a pipeline for blind SR that combines the S2K module with a non-blind SR module. For the non-blind SR module, they consider two methods:  the multiple degradation-based method SFTMD (Gu et al., CVPR 2019; ref 6) and the kernel modeling-based method RCAN (Zhang et al., ECCV 2018; ref. 35). Given a ground truth kernel of size 15x15 and a HR image, a LR degraded image is generated and cropped to 512x512. The input to the S2K module is the ground truth kernel upsampled to 256x256 and the FFT of the LR image, downsampled to 256x256. The estimated kernel of size 256x256 returned by G is then downsampled at size 15x15. Finally, the non-blind SR module is fed with the LR degraded image and the 15x15 kernel estimated with the S2K module.  The experiments are carried out considering Gaussian kernels of size 15x15 of random covariance matrix, and random motion kernels of size 23x23 (instead of 15x15) with Boracchi and Foi’s method (IEEE TIP, 2012; ref 3). The proposed approach is compared to an extensive list of state-of-the-art methods, both for the task of blur kernel estimation and blind SR for 2x, 3x and 4x SR. All methods were trained on the the same degraded dataset (DIV2K), including S2K and the non-blind methods used in the proposed blind SR pipeline. Blind SR methods are compared on synthetic images using the PSNR and SSIM metrics. Kernel estimation performance is evaluated using the $\ell_1$ norm and KL divergence. The results reported by the authors show that the proposed S2K and the blind SR pipeline significantly outperform the others. Finally, an ablation study on the loss components and frequency vs. spatial inputs is conducted, showing the pertinence of the proposed approach.   Finally, blind SR experiments on real images using the dataset proposed by Cai et al. (ICCV 2019; ref. 4), evaluated using two reference-free metrics (PIQE and BRISQUE) show that the proposed pipeline also outperforms its competitors, including the NITRE20 winner (Ji et al., CVPR 2020; ref. 10).  ",0.37681159420289856,0.2463768115942029,0.6086956521739131,0.2087912087912088,0.4945054945054945,0.5135135135135135,0.2857142857142857,0.22972972972972974,0.05652759084791386,0.25675675675675674,0.06056527590847914,0.05114401076716016,0.32499999999999996,0.23776223776223776,0.10344827586206896,0.23030303030303031,0.1079136690647482,0.09302325581395349
155,SP:21dbe4d9e8e7a956a61728e893dbc61429b59669,"This paper considers the rank-1 signal recovery for rotationally invariant noise matrices. It uses PCA to generate an initialization of AMP that is correlated with the signal but independent of the noise. Different from the IID Gaussian noise matrix case, rotationally invariant noise matrix makes the construction and the state evolution of AMP be more difficult. For example, the Onsager term in AMP not only involves the memory in last iteration, but also contains all the memories in preceding iterations. Therefore, the PCA initialization will change the expression of AMP (and its SE) in all the iterations. In a word, this work focuses on a hard rank-1 signal recovery problem with general rotationally invariant noise matrices. The authors propose a modified AMP to solve this problem. Furthermore, the state evolution of the proposed AMP is also rigorously proved. ","The submitted paper addresses estimation of rank-1 matrices from observations (1.1) or (1.2) subject to rotationally invariant noise matrix W. The paper proposes approximate message passing (AMP) with long memory Onsager correction (3.2) in the square case or (3.10) in the rectangular case, inspired by an arXiv preprint [20]. AMP uses the conventional principal component analysis (PCA) estimate as the initial message (3.1) or (3.9). To eliminate dependencies between the initial message and the noise matrix W, non-trivial Onsager correction b_{t,1} in (3.3) or a_{t,1}, b_{t+1,1} given in (3.11) and (3.13) is proposed via state evolution while the other correction b_{t,t-j} in (3.3) or (3.12) and (3.14) is the same as in [20]. Theorems 1 and 2 claim state evolution recursion for the square and rectangular cases, respectively. The theoretical predictions are also verified via numerical simulations in artificial setting. ","The authors consider the problem of estimating a structured rank-1 signal in the presence of rotationally invariant noise, which is a class of perturbations that is more general than Gaussian noise. For this problem, they propose an AMP algorithm that is initialized by PCA, and they derivate a state-evolution characterization that holds in the infinite-dimensional limit. The rationale is that PCA is a near-optimal estimator in the absence of structure, whereas AMP facilitates the exploitation of structure yet allows a rigorous asymptotic analysis. Their work builds upon previous work, such as PCA-initialized AMP with Gaussian noise, but is different in that, to handle rotationally invariant noise, their algorithm uses a PCA correction at every iteration, not just the first one. Their state-evolution analysis uses a two-phase artificial AMP formulation that has previously been used in the context of generalized linear models.  Although their analysis assumes a noise distribution with non-negative free cumulants, they provide empirical simulation results that suggest that derived state evolution is correct with any noise distribution with compact support.",This paper considers the problem of estimating a signal from its noisy rank-1 measurement. PCA is a traditional method that can be used for this task. AMP is an alternative that can additionally leverage statistical priors on the unknown signal. AMP algorithms are traditionally analyzed using a scalar iteration known as state evolution (SE).  The focus of this paper is to theoretically establish SE for AMP under a more general noise matrix starting from an initialization using PCA. Having SE enables the analysis of the error behaviour of AMP in different settings. Empirical results focus on comparing AMP+PCA against PCA on simulated data.  Theoretical contributions: The paper develops a new AMP algorithm initialized using PCA. Theorem 1 and 2 are the main theoretical results in the paper that establish SE for the proposed algorithm for square-symmetric and rectangular matrices.  Empirical contribution: Validation of the superiority of AMP on simulated data over PCA. ,0.21428571428571427,0.2571428571428571,0.22142857142857142,0.1524390243902439,0.1524390243902439,0.18333333333333332,0.18292682926829268,0.2,0.2,0.1388888888888889,0.16129032258064516,0.2129032258064516,0.19736842105263158,0.225,0.21016949152542375,0.14534883720930233,0.1567398119122257,0.19701492537313428
156,SP:21f02403d706919974df3d49952bc98bc32d2a88,"This paper contains two main contributions. First, they provided an improved analysis on the NA-Hutch++ algorithm to bridge the gap between adaptive and non-adaptive algorithm. Secondly, they also provided a nearly matching lower-bound on the complexity of the problem, closing the gap.","The paper studies the problem of estimating the trace of an input PSD matrix with the perspective of improving the query complexity of matrix-vector products required to compute an eps-multiplicative approximation of the trace. There have been two types of randomized algorithms solving this problem: adaptive and non-adaptive.  The former uses samples that depend on previous ones and therefore, by definition, is sequential. The latter doesn't, and is therefore amenable to parallelization, clearly a useful/desirable feature to have.   However, the prior lowest query complexity results for adaptive algorithms were lower than those for non-adaptive algorithms, by a factor of sqrt(log(1/delta)), where delta is the error probability. This paper focuses on non-adaptive algorithms for trace estimation of PSD matrices. Within the framework of this problem, it has two key contributions.   1. The first is an improved analysis of a non-adaptive algorithm proposed in a previous paper. The analysis is an improvement in the sense that it is able to show that this non-adaptive algorithm in fact needs just as many queries as the best adaptive algorithm, thus shaving off a factor of sqrt(log(1/delta)) from the prior analysis.   2. The second contribution is a proof that the new result obtained is in fact optimal, thereby completely resolving the question of non-adaptive query complexity of matrix-vector products for trace estimation of PSD matrices.   In the process of 1., the paper also provides the following technical contribution: a proof that to estimate the k-rank approximation of a PSD matrix with probability 1-delta, it suffices to use O(k + log(1/delta)) queries, down from the previous best query complexity of O(k log (1/delta)). The paper explains how, to achieve k-rank approximation, one needs matrices that satisfy two properties: (1) a subspace embedding property, and (2) a property about orthogonal matrix products. While property (1) was satisfiable by the improved query complexity before, it wasn't known that (2) was. This paper shows, by carefully arguing about tail bounds of chi-squared distributed variables, that (2) is also in fact true for O(k + log(1/delta)) queries. ","This paper presents an improved analysis for Hutch++, a stochastic trace estimation method proposed recently. In particular, the paper presents a new analysis for the non-adaptive variant of the Hutch++ algorithm and show that even the non-adaptive algorithm achieves optimal dependency on the failure probability \delta (i.e., O(\sqrt{\log(1/\delta)}))for the number of samples/matvecs. The paper also presents an improved lower bound for the number of matrix-vector queries required in terms of \delta and error tolerance \epsilon. Some numerical results are also presented to illustrate the performance of the methods.","This paper gives improved bounds for estimating the trace of a matrix using matrix-vector products. It turns what was previous a multiplicative factor of log(1 / failure probability) into an additive one, and shows that this bound is tight up to a loglog factor. These methods are evaluated on a wide range of matrices, several of which are implicitly represented (via the Lanczos method), and a better success probability was observed.",0.4666666666666667,0.4222222222222222,0.2,0.10136986301369863,0.0821917808219178,0.15306122448979592,0.057534246575342465,0.19387755102040816,0.125,0.37755102040816324,0.4166666666666667,0.20833333333333334,0.10243902439024391,0.26573426573426573,0.15384615384615385,0.15982721382289417,0.13729977116704806,0.17647058823529413
157,SP:222c6654e8f2b4461eda7d3fd74a2cdf4d3e94c3,"The authors propose a lossless compression method for graphs. By analogy to patches in images, or words in text, they propose breaking up the graph into sub-graphs, and encoding common sub-graphs using a dictionary. This is then compressed by an entropy encoder. Experiments are provided on a variety of datasets, ranging from molecules to social networks","This article presents a procedure to compress graphs (into a code). It takes the principle of entropy minimization i.e. finding a code that has the lowest possible entropy. In order to do so three steps are considered: 1) partioning the graph into subgraphs ($\operatorname{Part}_{\theta}$ which is parametrized by a NN) 2) finding a dictionary that faithfully describes a prior on the subgraphs structures, and that will be used to encode the different subgraphs coming from PART 3) Graph encoding part: assign a (parametrized) probability to each subgraph (which depends of the dictionary also). From a probability we can derive the length of the code. Overall this defines a description length which can be minimized in order to have the shortest code possible.  ","The authors establish a theoretically well-motivated, general-purpose graph compression framework they call Partition-and-Code (PnC). The framework consists of 3 general parts: partitioning, creating a dictionary of common subgraphs and entropy coding. Crucially, the first two steps are used to map a graph to its isomorphism class, and it is in fact the isomorphism of the input graph that is encoded using entropy coding.  The compression pipeline, given a graph partitioning function and a subgraph dictionary, works as follows: 1) The graph to be encoded is split up into two types of subgraphs: atoms (subgraphs of a fixed size, such that they should be likely to appear in the dictionary), and cuts, which connect the atoms. 2) Then, atoms appearing in the dictionary are encoded using a distribution tailored to the dictionary. Cuts and atoms missing from the dictionary are coded using an appropriate uniform distribution.  Importantly, ML techniques are only used as heuristics for the partitioning part of the encoding process, which means that the decoder does not need to know the model parameters. The authors propose various proof-of-concept ways to learn the partitioning function using an e.g. GNN and propose a relaxed, greedy gradient-based way to learn the atom dictionary. Learning of all parts of the encoder is performed by minimizing an appropriately defined minimum description length objective.  The authors perform experiments on multiple popular graph datasets and achieve good results. ","The paper studies the lossless compression of graphs with  deep learning. The authors propose to compress  the graphs in 3 steps: a partitioning algorithm decomposes the graph into elementary structures, which are mapped to the elements of a small dictionary, and an encoder translates the representation into bits. The partitioning  is parameterized  by a graph neural network output the likelihood of partitioning and optimized with the policy gradient method to achieve as small minimum description length as possible. The minimum description length are decomposed into several parts, the dictionary and encoding etc. A few experiments are conducted on small graph datasets to show 30% space compression  compared to the naive edge list, and 50% compared to the Erdos-Renyi method. ",0.22413793103448276,0.3448275862068966,0.3103448275862069,0.264,0.176,0.15,0.104,0.08333333333333333,0.15,0.1375,0.18333333333333332,0.3,0.14207650273224043,0.1342281879194631,0.20224719101123598,0.18082191780821918,0.1795918367346939,0.2
158,SP:2236133166c5eff1348d91030200b7749a69334a,"The papers proposed a new objective function for comparing two 3D structures, in the molecular conformation context. Specifically, they proposed to compare the bond lengths, angles, and torsions, and design reweights to balance the three components. Furthermore, they improved the efficiency by approximating the comparison of angles and torsions with Taylor expansion. Experiments demonstrate that compared with directly comparing the aligned RMSD and other metrics, the proposed method is both more efficient and effective.","The work  introduces a custom loss function for the molecular conformation similarity. The authors combine this loss function with the PhysNet and GraphDG models to either predict a single conformation or to generate an ensemble of conformations. Other loss functions, including Naive RMSD, Kabsch algorithm, Conn-k and lDDT-k are compared under the same settings.  The main contributions of the paper are as follow: authors proposed a novel loss function for conformation generation, that can be used in *straight-forward* models, also provide Taylor Expansion and Multiplier Truncation techniques to speed up this metric.","This paper proposes a new loss function for conformer generation tasks. These tasks essentially generated 3D point clouds from molecular graphs, and require a geometric similarity metric between generated samples and training data to drive the learning. The traditional metric of distance is RMSD based on aligned poses, but the authors suggest this is too time-consuming and instead propose a heuristic-weighted combination of bond distance between first other neighbors, and a Taylor-expansion of angles between second-order neighbors and dihedrals between 3rd order neighbors.   The metric is empirically found to perform well, based on both statistical metrics used in conformer generation tasks, by visual inspection of pathological cases, and because the timings are excellent","The authors proposed a new method for molecular conformation generation. The authors choose to use bond lengths $d$, bond angles $\phi$ and dihedral angles $\psi$ for molecular conformation generation. The authors use Taylor expansion and multiplier truncation to improve models.  The authors conduct experiments on QM9 to verify the algorithm.  ",0.22972972972972974,0.28378378378378377,0.22972972972972974,0.2,0.18947368421052632,0.11965811965811966,0.17894736842105263,0.1794871794871795,0.34,0.1623931623931624,0.36,0.28,0.20118343195266272,0.21989528795811517,0.27419354838709675,0.1792452830188679,0.24827586206896549,0.16766467065868262
159,SP:2349c5d44697313f66dda84628e2695c15e9c76c,"This paper presents a neural network training strategy that leverages lexicase selection and evolutionary algorithms. Specifically, the training strategy involves two layers of loops to iteratively update model&select candidates, which searches the best optimization direction through greedy search. Experiments are conducted on two image datasets and show the proposed training strategy allows the learned model to reach better performance (accuracy). Further experiments investigate how hyperparameters would impact the final performance.",The paper applies lexicase selection towards training deep neural networks using a hybrid evolutionary-gradient descent optimization method. The mutation operator is replaced by sub-gradient descent on bagged data and lexicase selection is used as the selection procedure in the evolutionary framework. Results in a variety of benchmark image classification tasks demonstrate that the proposed method can improve upon standard stochastic gradient descent methods.,"This paper extends the lexicase selection developed in the genetic programming community to apply the idea of lexicase selection to deep neural network training. In the proposed gradient lexicase selection, a parent network is selected based on the lexicase selection from several candidate networks trained using different subsets of the dataset. The authors apply the proposed method to several well-known convolutional neural networks (CNN) and show that the gradient lexicase selection can improve the performance of trained models.","Authors propose a method to optimize deep networks through a combination of gradient descent and a population-based mechanism. In particular, authors propose an adaptation of ""lexicase"" selection, an exotic selection mechanism proposed in evolutionary computation, in order to be used in the context of deep learning and deep networks optimization. In gross terms, _lexicase selection_ consists of picking individuals for surviving and/or as parents for generating offspring, based on their performance test-case instance-wise, rather than based on some aggregated metric such as MSE or loss, as usually done. Authors provide results from a extensive battery of tests where they compare the performance of their method, against that of regular SGD, optimizing several well-know deep architectures in a set of commonly used too benchmark datasets.",0.14084507042253522,0.2112676056338028,0.15492957746478872,0.27692307692307694,0.24615384615384617,0.24050632911392406,0.15384615384615385,0.189873417721519,0.08527131782945736,0.22784810126582278,0.12403100775193798,0.14728682170542637,0.14705882352941177,0.2,0.10999999999999999,0.25,0.16494845360824742,0.1826923076923077
160,SP:23778834180fe13711c894b9ba8034a0e7248b01,This paper proposes a strategy to _optimally_ sample clients on a given FL global round. The sampling strategy selects more informative updates by their magnitude. This criterion minimizes the variance of the sampling scheme.   Experiments show this method outperforms uniform sampling.  ,"The paper works on strategies of client subsampling for federated learning. Specifically, it proposes an adaptive partial participation strategy, which can carefully sample clients in each round to sending their updates to the server. More importantly, the paper proposes a practical approximation to optimal sampling strategy. Theoretical analyses are given for the proposed sampling scheme, which shows better performance than existing FedAvg with uniform sampling.","The authors propose an efficient client (sub) sampling scheme for federated learning (FL) which relies on the quality of the update of each (sampled) client. The authors justify their construction by designing the (sub) sampling strategy that minimizes the gradient variance over the whole network (or sampled clients). The authors analytically show the convergence of DSGD and FedAvg under their sampling strategy which improves on the uniform sampling strategy. Finally, the authors empirically validate the proposed sampling algorithm by showing considerable communication gains compared to both and uniform client sampling strategies. ",The paper proposes to use importance sampling in client sampling in federated learning to improve convergence behavior and reduce communication.  Contribution: 1. A new importance sampling scheme based on the norm of gradients/updates. 2. Experiments show the proposed sampling scheme outperforms uniform sampling.,0.34146341463414637,0.2926829268292683,0.3170731707317073,0.26153846153846155,0.2153846153846154,0.16483516483516483,0.2153846153846154,0.13186813186813187,0.29545454545454547,0.18681318681318682,0.3181818181818182,0.3409090909090909,0.2641509433962264,0.18181818181818182,0.3058823529411765,0.21794871794871795,0.2568807339449541,0.22222222222222218
161,SP:2378430a7f9df6f12616582052a5e6a6ce9d18ed,"This papers aims to explore the impact of learning rate schedules in neural network training, through   a newly proposed measure denoted the “activation pattern temperature” which examines the probability that ReLU activations will change between the 0 and linear regions. Thus this measure aims to examine the changes in non-linear behavior of the network. Several learning rate schedules are examined through this lens, and a learning rate schedule based on reducing this temperature is proposed and examined.","The paper attempts to understand the change of activation patterns, meaning the post-ReLu sign flip before and after one parameter update, for the same batch of input data. A measure, activation pattern temperature (APT), is then defined on a layer as the self-information of the event that an activation pattern has not changed. The author uses APT to interpret different training methods and training dynamics. Finally, based on the observation that existing learning rate schedule adheres to a linear decrease of temperature, the author propose ActCooLR, a LR scheduler that was tested on a variety of training tasks. ","The paper introduces a measure (Activation Pattern Temperature, APT) of the rate of change in the function modelled by a neural network (with ReLU non-linearities) as training progresses.   This measure is then used to set a learning-rate schedule by regulating the APT, an adaptive learning rate schedule denominated ActCooLR.  The paper compares the performance achieved by ActCooLR to a standard step-wise decaying learning rate and to a schedule called 1-cycle.  ActCooLR did not obtain better results than the alternatives.","This work contributes a analysis of the dynamics of neural network training using a new quantity called the activation pattern temperature, that measures the amount of (ReLU) neuron activations that switch from 0 to positive, or positive to 0.  They measure the APT during training of typical CNNs using different learning rate schedules and show that using these schedulers neural networks typically undergo a transition from a high temperature regime to a small temperature one.  They then design a learning rate scheduler that adapts the learning rate so that the temperature of the optimization dynamics during training follows a predefined curve -- in this case it decreases linearly from 3.17 (why?) to 0.",0.23076923076923078,0.24358974358974358,0.3333333333333333,0.2,0.23,0.24096385542168675,0.18,0.2289156626506024,0.23008849557522124,0.24096385542168675,0.20353982300884957,0.17699115044247787,0.20224719101123595,0.2360248447204969,0.27225130890052357,0.2185792349726776,0.215962441314554,0.20408163265306123
162,SP:245ba0ed73917597ae55102a5ea839b03455fe74,"The report provides a method for decoding stimulus identity (odor) using data from multiple animals. The framework is straightforward and the performance appears respectable. I anticipate that this approach could be of use in neuroscience labs and could help with developing models and BCI. Given its general applicability, it would strengthen the impact if its use was demonstrated on spiking data and/or other types of stimuli or behaviors. This is a well written paper with helpful illustrations. I expect it would support interesting discussions.","Recent advances in recording techniques allow us to record from a large population of neurons - this high dimensional activity can typically be described by low dimensional latent factors. The authors leverage this low dimensional structure to capture the shared dynamical structure across animals in neural recordings from the olfactory bulb as animals are exposed to various odor stimuli. They fit a linear dynamical system (LDS) model with latent parameters and trajectories estimated for each odor that are shared across animals. These latent trajectories are subsequently mapped onto the neural recordings by employing animal specific observation parameters. The LDS model parameters are optimized using expectation maximization (EM). In order to decode the odor identities, the marginal likelihood for all stimulus conditions is computed. They validate the model on simulated data as well as neural recordings from the olfactory bulb. ","The authors develop a method for aligning the neural activity of multiple animals. Specifically, they address the challenge of odor detection, where neural activity from multiple animals is recorded in response to many stimuli (odors), and then the odor needs to be decoded from a given animal’s neural activity. They use a hierarchical and probabilistic approach, in which there is a shared low-dimensional space of neural activity that is shared across animals, with different low->high dimensional mappings for each animal. The authors demonstrate the utility of the approach in simulations and on experimental neuroscience data.","This submission is a very well done application of the probabilistic inference to decoding of odour identity from neural recordings in mice. Interestingly, the latent code is assumed to have temporal dynamics shared across animals. The main benefits (high accuracy, data savings,  robustness to incomplete data) are shown in simulation and neural recordings. The method outperforms the existing (non-probabilistic) neural alignment procedures.  ",0.1411764705882353,0.21176470588235294,0.11764705882352941,0.15942028985507245,0.12318840579710146,0.14285714285714285,0.08695652173913043,0.1836734693877551,0.15873015873015872,0.22448979591836735,0.2698412698412698,0.2222222222222222,0.10762331838565022,0.19672131147540983,0.13513513513513511,0.1864406779661017,0.1691542288557214,0.17391304347826086
163,SP:246f374cfc071e291b12da1a35882a33b2fb838b,"This paper analyzes multi-layer random ReLU networks, and shows theoretical evidence that for constant depth networks with a variety of widths, adversarial examples must necessarily exist. They prove this result based on the key idea that such random networks are nearly linear, and that establishing smoothness accounts for the network’s nonlinearity. Further, they show that adversarial examples will not exist if the depth grows too large.","This paper studies how adversarial examples arise in constant depth ReLU networks with independent Gaussian parameters. The main results of the paper is Theorem 1.1 which determines how the width of different network layers affect the probability of the existence of adversarial examples. Overall, the most important takeaway is that smoothness (or the absence of adversarial examples) requires networks that are polynomially (in the number of input dimensions) deeper and/or wider.","This paper studies some adversarial examples in random ReLu networks, where a smaller perturbation generates a negative label.  The main result of the paper is some theorems. and the main technique used in the paper is mostly some inequality manipulation and concentration. I cannot check all the derivations, but the main results, i.e. the constants are convincible to me.  ","EDIT: I will be keeping my (positive) score.  The paper studies robustness properties of ReLU networks that have random weights. The main finding is that random ReLU nets of constant depth are susceptible to adversarial examples for every input vector. The main theorem relies on some conditions about the widths in the networks' layers. To complement this result, the authors show that in the large depth regime, such a result cannot hold.  The paper builds on prior work and  their main result generalizes results of Daniely and Schacham (2020) for networks with rapidly decreasing width and results of Bubeck et al (2021) for two-layer networks. Some ideas and intuition come actually from prior works:  the main goal is to demonstrate the existence of an adversarial example near input x,  and to do this for a function f, it turns out that it suffices to show the smoothness property: see (1) . This ""smoothness"" property is not in the common sense of the word ""smooth"" function, but basically says that for a deep ReLU network with random parameters and a high-dimensional input vector x, there is a relatively large ball around x where f satisfies this smoothness property.   The main result is Theorem 1.1 that proves how taking a small step away from a point x in the direction of the gradient can flip the sign of the evaluated function f at x thus corresponding to an adversarial example. The most interesting condition is that the min depth of the net $d_\min \ge c_1(\log d_\max)^{c_2} \log 1/\delta$.  The authors also prove a converse result to justify why their results could only be true in the constant depth regime. This complementary result is given in Theorem 3.1 which shows that when depth grows polynomially in the input dimension d, the function computed by a random ReLU network can essentially be constant, which rules out the possibility of adversarial examples.  For theorem 1, an important aspect of the architecture of the network that allows for adversarial examples or not, depends crucially on the width of the narrowest layer before layer j—that  is ""the bottleneck layer"" for layer j as the authors call it. This width determines the dimension of the image at layer j of a ball in the input space. In proving bounds on gradients and function values that hold uniformly over pairs of nearby vectors x and y, this dimension—the width of the bottleneck layer—dictates the size of a discretization that is a crucial ingredient in their proof.    ",0.23529411764705882,0.16176470588235295,0.36764705882352944,0.273972602739726,0.4931506849315068,0.38333333333333336,0.2191780821917808,0.18333333333333332,0.05813953488372093,0.3333333333333333,0.08372093023255814,0.053488372093023255,0.22695035460992907,0.171875,0.10040160642570281,0.30075187969924816,0.14314115308151093,0.09387755102040816
164,SP:24ad820638f28e7a02187f9e509195ab7908f2ca,"This paper introduces a simple technique for target-side data augmentation. The high-level idea is to generate a ""soft token"" embedding by interpolating the target side word embeddings, with the output distribution generated by an initial model being the interpolation weight (after being adjusted by a temperature parameter $T$). The resulting training loss is in three terms: (1) original cross-entropy loss; (2) cross-entropy loss with the soft token embedding as the input; (3) a consistency loss controlling the distribution divergence of (1) and (2).  Results show that the proposed data augmentation method is very helpful for the dialog generation task, while also being somewhat helpful with neural machine translation for abstractive summarization. Ablation studies show that the weight of (1) cannot be too low, and that the optimal temperature parameter has some negative correlation with the vocabulary size.","This paper focuses on sequence generation tasks and proposes to perform data augmentation on the target side. During training, given an input-output pair ($x$, $y$), their model first uses teacher-forcing to get the output probability $p_y$ and then gets an augmented pair ($x$, $\tilde{y}$) by using ""mixup"" to mix the embeddings of $y$ and $p_y$. They also propose to encourage the consistency of predictions between ($x$, $y$) and ($x$, $\tilde{y}$) by minimizing the KL divergence of their output probabilities.  They experiment on three sequence generation tasks and demonstrate improvements over baselines.","Traditionally, the decoder of Seq2Seq model takes ground truth words of previous steps as input during training, while at inference, its input are those generated tokens starting from scratch. Thus there exists discrepancy between training and inference.  This paper presents an approach of data augmentation for the input of the decoder during training.  Specifically, the authors still feed the ground truth to the decoder, and then obtain its output vocabulary distributions that are multiplied with word embedding weights to get ""pseudo tokens"".  The contribution of this work comes from narrowing the gap of decoding procedure between training and testing.","This paper experiments with a target-side sequence-level data-augmentation scheme for sequence-to-sequence generation tasks. The primary contribution of this work is an algorithm that leverages model-outputs to construct pseudo-target-side tokens (and consequently pseudo-sequences) for augmentation. It is done by incorporating soft-embeddings in the encoder-decoder model, a standard cross-entropy loss function supercharged with a consistency loss objective. One of the critical highlights is that this work doesn't require any external model for performing data augmentation. A comprehensive set of experiments on three sequence generation tasks are conducted to highlight the effectiveness of using such an approach for data augmentation.",0.1347517730496454,0.11347517730496454,0.2127659574468085,0.18556701030927836,0.2268041237113402,0.1717171717171717,0.1958762886597938,0.16161616161616163,0.2727272727272727,0.18181818181818182,0.2,0.15454545454545454,0.15966386554621848,0.13333333333333336,0.23904382470119523,0.1836734693877551,0.21256038647342995,0.16267942583732056
165,SP:24bb9b089f95b39921797298abe5e817a1671ef0,"The paper studies personalized federated learning (pFL), where the local models have different structures and sizes. The proposed method enables each client to maintain a personalized soft prediction at the server-side. Empirical results show the strength of the proposed solution.","This paper works on personalized federated learning (FL). The authors specifically studied the situation that clients train different model architectures. The authors leveraged additionally, public unlabeled data and knowledge distillation (KD) to aggregate these heterogenous clients' models specifically for each client, so that each client can learn their personalized model with the help from other clients. Experimental results on several datasets show the effectiveness of the proposed method.",This paper proposes a personalized group knowledge transfer training algorithm (KT-pFL) based on knowledge distillation that deals with model architecture heterogeneity in FL systems.  The authors claim the following contributions: - This paper is the first to study personalized knowledge transfer in FL. - Proposing the ‘knowledge coefficient matrix’ to identify the contribution from one client to others’ local training. - Providing theoretical performance guarantee for KT-pFL and conduct extensive experiments. ,"The work aims to address the problem of creating adequate personalized models in a federated learning regime. The problems stems from a general observation that global models do no generalize well in the Federated Learning (FL) scenarios. The authors propose to use the idea of knowledge distillation to properly transfer knowledge to different clients via soft prediction mechanics. It is based on the knowledge about predictions that can be obtained on a public dataset (also referred as collaborative knowledge). At the server side, the proposed model learns the corresponding knowledge transfer coefficients that align inputs from multiple clients and hence distill the knowledge. The training is performed in an alternating manner by switching between knowledge coefficients (updated at the server side) and the task-specific parameters of the model (updated at the client side). Advantages of the proposed approach are empirically demonstrated in comparison against other FL algorithms w.r.t. both accuracy and communication efficiency on several public datasets.",0.4146341463414634,0.21951219512195122,0.43902439024390244,0.17647058823529413,0.3088235294117647,0.24285714285714285,0.25,0.12857142857142856,0.1125,0.17142857142857143,0.13125,0.10625,0.3119266055045872,0.16216216216216217,0.17910447761194032,0.1739130434782609,0.1842105263157895,0.14782608695652175
166,SP:24bc82496744cc5ea96b33ac8978ceaa8a0dd4ee,"The paper proposes to skip the integration of continuous-time models and directly learn a model that predicts the posterior of the state conditioned on the initial state and the desired time. To learn such a model with 'good' uncertainty estimation the authors mix GP's, neural networks and the wasserstein metric (I did not understand how the authors combine these approaches together). Within the experiments the paper shows that the log-likelihood is better for the proposed model on uncontrolled toy systems, e.g. Lotka Volterra, chaotic Lorenz, double pendulum and quadcopter. ",This paper develops a method called distributional gradient matching for learning unknown differential equations from data. The method is based on a combination of a neural ODE model and a Gaussian process model (with a deep covariance kernel) which is encouraged to produce solutions which follow the neural ODE model by introducing a regularisation term based on Wasserstein loss. Numerical examples show that the proposed method outperforms some existing alternatives.,"This paper presents a novel approach to model and capture the uncertainty of dynamical systems by learning from trajectory data. The method is composed of a Gaussian process smoother model, used for predictions, and a neural dynamics model, used for training. The novelty in the method comes from the use of the distributions over time-derivatives of the state vectors (via a Wasserstein-distance penalty between the distributions from the smoother and the dynamics model) during the training process. Basing the approach on integrals over the state space, instead of over the model parameters space, allows for computational efficiency and more robustness when compared to previous sampling-based approaches, as evidenced by experiments.","The paper tackles the problem of learning nonlinear dynamical systems from historical trajectories, while estimating predictive uncertainties. They propose a new method based on a Gaussian process smoother which is trained with maximum likelihood to fit the data of all training trajectories jointly, while constrained to follow in some sense the underlying dynamics. The latter is performed thanks to a regularizer forcing the smoother’s gradients distribution to match the  distribution of gradients of the dynamics model over a fixed set of support points. The Wasserstein-2 distance is approximated to evaluate the distribution discrepancies. Then, instead of deriving uncertainty estimates over the predicted parameters, as done in standard Bayesian methodology, uncertainty is directly estimated over the predicted states. For this, a Gaussian distribution is used to model  probability of the states derivatives conditioned on the states. Thanks to these tricks and modeling choices, the authors avoid to have to integrate the complex dynamics model, unlike previous Monte Carlo methods. The proposed approach is show-cased and compared to sampling-based methods in several experiments with simulated data, including overparametrized linear dynamics, as well as single and multi-trajectory cases of 4 well-known dynamical systems. An ablation study is also presented showing the benefits of joint smoothing and gradients distribution matching regularization on one of the four simulated datasets.",0.16129032258064516,0.23655913978494625,0.25806451612903225,0.2857142857142857,0.2714285714285714,0.3274336283185841,0.21428571428571427,0.19469026548672566,0.10909090909090909,0.17699115044247787,0.08636363636363636,0.16818181818181818,0.18404907975460122,0.21359223300970875,0.15335463258785942,0.2185792349726776,0.1310344827586207,0.22222222222222224
167,SP:25457741fbc056ddc5d90112742d1fb23bd8419e,"This paper disentangles the effects of adaptive learning rate and momentum in Adam learning dynamics, and proves that adaptive learning rate is good at escaping saddle points but not good at selecting ﬂat minima, while momentum helps escape saddle point and matters little to escaping sharp minima. Based on the analysis, the authors propose a novel optimizer, Adai. Compared to SGDM, Adai parameter-wisely adapts the momentum hyperparameter to the (approximated) Hessians of saddle points, and is proved to fast escape saddle points and sharp minima.","This paper studies the behaviors of some algorithms when the iterate is at a critical point via SDEs. A variant of Adam is given in the end. The paper draws some conclusions about adaptive learning rate and momentum, claiming that   - QUOTE momentum matters little to escaping sharp minima UNQUOTE - QUOTE adaptive learning rate is not good at selecting flat minima UNQUOTE ","This work analyzes the dynamics of momentum SGD and Adam on escaping saddle points and sharp minima, which is based on the diffusion theoretical framework proposed in (Xie et al. 2021b). The authors prove that momentum provides a drift effect around saddle points and does not affect flat minima selection (for SGD), and while Adam escapes saddle points efficiently, it does not favor flat minima as well as SGD. The analysis explains some empirical observation of SGDM and Adam. Motivated by the analysis, the authors propose adaptive inertia (Adai) method, which can approximately achieve Hessian-independent momentum drift (escapes saddle points fast) and favors flat minima as well as (momentum) SGD.  ","The paper focuses on the understanding of the effects of adaptive learning rate and momentum. In particular it proves that the adaptive learning rate can escape saddle points efficiently and cannot select flat minima as SGD does. It also shows that momentum helps the training process by passing through saddle points and without affecting the minima selection.The paper also proposes a new adaptive algorithm, named Adai (Algorithm 2), which uses parameter-wise adaptive intertia to accelerate the training and finds flat minima as well as SGD.  Finally, the paper provides extensive numerical testing showing the benefits of Adai.",0.22093023255813954,0.3023255813953488,0.36046511627906974,0.21311475409836064,0.2786885245901639,0.23423423423423423,0.3114754098360656,0.23423423423423423,0.31313131313131315,0.11711711711711711,0.1717171717171717,0.26262626262626265,0.2585034013605442,0.2639593908629442,0.33513513513513515,0.1511627906976744,0.21249999999999997,0.24761904761904763
168,SP:26000cc4a5b1d89d2005005e364a296a3ca3e680,"This paper introduces LaSynth, a novel neural architecture that jointly learns to synthesize programs and perform a kind of targeted latent program execution. LaSynth is applied to synthesize Karel programs and list processing programs in the C programming language. The paper also finds that one to two iterations of dataset refinement and iterative retraining can improve the dataset quality and model performance. The LaSynth approach, with iterative retraining, is found to outperform baselines both in the Karel and C domains.","The goal of this paper is to perform program synthesis, taking inspiration from recent advances in execution-guided neural program synthesis in situations where execution of partial programs is not immediately possible.  As stated by the paper, the main contributions are 1) combining two types of representations - one which models the mapping from spec to program tokens, as in RobustFill (and other neural synthesis work), and the other which models the “hypothetical input signal for the remaining partial program to execute to get the desired output”  2) demonstrating that iterative retraining can lead to better modeling of code and improve sample efficiency.  3) showing that short C code can be synthesized from IO examples through the above two techniques.","This work improves neural program synthesis of imperative programs with limited control-flow (conditionals and loops) from input-output examples. It extends the established pipelines for the Karel task – recurrent program decoder with attention over partial output and example encodings – with modeling of a latent variable that represents a possible execution state of the program at that location. The latent variable is supervised at the end to match the desired output state. The authors test the approach on Karel (with mixed results), and on a randomly generated dataset of C programs of similar complexity (where it substantially outperforms the baselines).","This paper presents a neural program synthesis system, called LaSynth, that aims to generate programs in non-domain specific languages (DSLs) unlike much of the prior work. In particular, this paper focuses on using LaSynth for the C programming language. A core reason prior work in program synthesis has been generally restricted to DSLs is, as the authors note (and I agree with), because of the linguistic complexity of general-purpose languages and the computational intractability that is often associated with synthesizing programs in such languages.  A core novelty of LaSynth is that it uses two forms of conjoined/parallel representations, one of which uses a hypothetical input to help determine the remaining partial program to execute to get the desired output. This somewhat resembles BUSTLE’s approach of breaking down components into small pieces and then assembling them for a larger program (ICLR ’21), but the actual system-level approach in LaSynth, in my mind, is notably different from BUSTLE and other prior works that I’m familiar (some of which the authors seem to have missed in the prior work – NetSyn (Mandal et al., MLSys ’21), REPL (Ellis et al., NeurIPS ’20), etc.), but we'll get to that later. ",0.2375,0.2125,0.3125,0.16806722689075632,0.2773109243697479,0.26,0.15966386554621848,0.17,0.12376237623762376,0.2,0.16336633663366337,0.12871287128712872,0.19095477386934673,0.1888888888888889,0.1773049645390071,0.18264840182648404,0.205607476635514,0.17218543046357615
169,SP:260b9e6a158ea156772fb80550e035c9da103371,"This paper presents FILM (Following Instructions in Language with Modular methods), a model built for the ALFRED dataset. FILM consists of:   1) A language processing module which uses a BERT backbone to (a) predict 1 out of 7 task types in the ALFRED ontology and (b) predicts slot categories for populating a template (1 pre-defined per task).   2) A semantic mapping module which uses segmentation and depth prediction to populate a 2D spatial map of object class locations.   3) A semantic search policy which uses a CNN to map from the semantic map to subgoal object locations.   4) A deterministic policy which is used to navigate to either (a) a subgoal object location if it exists in the semantic map or (b) a sampled location from the search policy if not, and performs a predefined object interaction based on the current subgoal from the task template.  In my perception the main technical contribution of the work comes in the form of the language processing module, which provides much of the structure used for planning actions by the agent.   The paper also contributes a semantic search policy for reasoning over a semantic map to predict subgoal object locations (instead of randomly picking a new search location).   Experimentally, the paper presents:   1) SOTA results on the task when only the high-level instruction is given (low-level instructions witheld).   2) Ablations showing a bottleneck from errors in the perceptual side and marginal gains from adding low-level language instructions.   3) A taxonomy and distribution of error modes for the presented model, showing most errors come from either the language prediction, goal object location prediction, or goal objects being in closed receptacles (which the deterministic policy does not open).   4)  An analysis showing that task type length does not correlate with performance.","This paper presents a modular system (FILM) for egocentric instruction following in the ALFRED environment. The system does not require expert trajectories and can operate without low-level instruction sequences. The system has the following components:  1. A language processing module that maps high-level instructions ""Drop a clean pan on the table"" to a low-level sub-task sequence. This module makes use of a number of BERT based classifiers that:     a.  classifiy the high level instruction into one of seven possible goal types, each of which is deterministically associated with a templated sub-task sequence     b. identify object IDs to fill in the slots in the template identified in (a)  2. A semantic mapping module that populates a grid with binary classifications that indicate object presence; obstacle presence; and whether the location has been explored. This module is inspired by previous work on egocentric navigation.  3. A novel semantic search policy that predicts the likely location of small sub-goal objects (bowl) on the basis of large receptacle objects (table). This is trained via supervised learning on sampled trajectories and it samples goals at a coarse tiem scale of every 25 steps.  4. A deterministic policy that operates within each 25 step search goal period. This makes use of the Fast Marching Method and is inspired by previous work on this task.  Together, these modules make up a system that generalizes to unseen environments much better than previous work. The new semantic search policy provide small but consistent improvements over a version of FILM that has this module removed.  FILM is evaluated in both the setting where low level instruction sequences are provided, and the setting where only the high-level goal is provided and FILM must map this goal to low-level instructions via module (1). In both settings, FILM significantly outpeforms previous work on unseen environments. When low-level instructions are provided, FILM does worse than some previous work on seen environments. When low-level instructions aren't provided, FILM does marginally better than previous work on seen environments.  Experiments show that the semantic search policy increases path-weighted metrics proportionally more than absolute metrics, suggesting that this policy improves efficiency as well as overall success of the search. Targeted evaluations also show that this search policy is particularly useful in large rooms, where the agent cannot see many target objects, and for the ""Clean and Place"" task that involves a difficult to detect object (sink) contained in a simple to detect object (countertop).","The paper presents a modular approach for solving Vision Language Navigation tasks and test it in ALFRED, a recent benchmark for that task. Compared with other benchmarks, ALFRED features a higher number of actions, objects, and an agent operating with an egoistic view. Solving the task requires multiple high-level actions, combined with low-level actions and exploration. As suggested by the authors of ALFRED, the benchmark requires exploiting the hierarchical structure, reusing skills in multiple contexts, and structured reasoning. The submission uses precisely all these aspects to achieve a significant improvement over the next entry in the ALFRED leaderboard.","The paper proposes a modular framework for solving instruction following tasks from the ALFRED benchmark. The approach uses a language module that takes the high-level instruction, predicts the instruction type and uses the oracle template for the instruction type to fill in the arguments (objects/receptacles) and generate a list of sub-tasks to be executed on those objects and receptacles. The agent builds and maintains a spatial+semantic map to keep track of state and uses it to plan the next waypoint (which object/receptacle to navigate to next) as its intermediate goal. Their approach demonstrates the ability to ground small objects on the top-down semantic map and yields SoTA performance on the ALFRED benchmark. The authors also present a comprehensive analysis of their ablations and failure modes.",0.23333333333333334,0.08666666666666667,0.13,0.07177033492822966,0.10526315789473684,0.25,0.1674641148325359,0.26,0.29770992366412213,0.3,0.33587786259541985,0.19083969465648856,0.1949860724233983,0.13,0.18097447795823665,0.11583011583011582,0.16029143897996356,0.21645021645021648
170,SP:26246b71f2b3804273427bafa8e50aadcb227a68,"This work introduces transductive universal transport for zero-shot action recognition, where no training examples for unseen classes are available. To address the biases of prior approaches towards seen classes during inference, this paper re-positions unseen action embeddings through transduction by using the distribution of the unlabelled test set. Experimental results on several action recognition datasets demonstrate the effectiveness of the proposed method.","The paper targets transductive zero-shot action recognition.  To alleviate models biased to seen categories, the authors propose to re-position unseen action embedding through transduction. There are three steps in the proposed method: first, finding an optimal mapping from unseen actions to the mapped video in the shared hyperspherical space. Second, defining target embeddings as weighted Frechet means with the weight given by the transport couplings. Third, re-positioning unseen action embeddings along the geodesic between the original and target. The zero-shot classification performance of the proposed method is tested on the UCF-101 and HMDB datasets. The zero-shot spatio-temporal localization performance is tested on the UCF Sport and J-HMDB datasets.  ","This work tries to address the problem of zero-shot action recognition. Particularly, the paper aims at preventing the case that many unseen action categories in the target domain are simply never being selected during inference. Using the distribution of the unlabelled test set, the embeddings of unseen actions in the target domain are reweighted and repositioned along the geodesic such that they are better aligned with embeddings of training actions in the source domain. In experiments, Empirically, the proposed method has been evaluated on  benchmark datasets for tasks zero-shot action classification and spatio-temporal action localization.","The paper presents a zero-shot action recognition framework by learning an universal mapping from video to semantic space. The unseen action embeddings are re-positioned through leveraging the distribution of unlabelled test set. The universal mappings from unseen action to test videos are first defined and the target embeddings are treated as weighted Frechet means. The unseen action embeddings are re-positioned as a semantic regularization. The results on UCF101 and HMDB-51, UCF Sports and J-HMDB validates the proposed method.",0.3125,0.359375,0.328125,0.25,0.29310344827586204,0.2653061224489796,0.1724137931034483,0.23469387755102042,0.25301204819277107,0.29591836734693877,0.40963855421686746,0.3132530120481928,0.22222222222222224,0.2839506172839506,0.28571428571428564,0.27102803738317754,0.3417085427135678,0.28729281767955805
171,SP:2660c15f0389a5faeebec5fffb945b2170ddc21d,"Vision Transformers (ViT) have attracted much attention recently; however their large model sizes and high training cost pose practical problems for computer vision tasks. This paper introduces sparse ViT exploration algorithms ($SViTE$, $S^2ViTE$, $SViTE+$) to explore sparse patterns in the ViT architectures. The goal is to alleviate the training memory bottleneck and achieving high inference efficiency. The authors conducted comprehensive experiments on ImageNet dataset to validate their approach.",This work pioneered to study of various sparse training and pruning techniques to the recently proposed vision transformers (ViT). It shows the effectiveness of pruning/sparse training/data selection in such settings. This is a  interesting work for both ViT and sparse learning communities. ,"This work proposes sparse ViT algorithms, named as SViTE and its variants S^2ViTE, and SViTE+, to explore high-quality sparse patterns in both ViT’s architecture and input token embeddings. The proposed methods can jointly optimize model parameters and explore connectivity throughout training. Experiments are conducted on ImageNet to validate the effectiveness of the proposed methods and find that SViTE+ can mine crucial connections and input tokens.","The author proposes three methods for achieving sparse vision transformers to reduce the number of parameters and FLOP. By using a dynamic structured sparsity injecting method, the authors are able to achieve promising speedup without loss of accuracy. For an example, the author claims to achieve 14.7% run time reduction with 0.42% accuracy improvement. However, the training settings of the author's experiments and the DeiT baseline are different. Thus, there are some ambiguities exits.",0.11594202898550725,0.30434782608695654,0.14492753623188406,0.25,0.20454545454545456,0.17647058823529413,0.18181818181818182,0.3088235294117647,0.12987012987012986,0.16176470588235295,0.11688311688311688,0.15584415584415584,0.1415929203539823,0.30656934306569344,0.13698630136986298,0.19642857142857145,0.1487603305785124,0.16551724137931037
172,SP:2661e7246e313c6b37f30192240844730e8885fb,"The authors investigate why networks with highly different architectures and objectives seem to produce similar decision boundaries. By analyzing the model behavior of each sample of the validation set during training, they find that over half of the samples of the ImageNet validation set are either trivial or impossible for almost all analyzed models, which they name ""dichotomous data difficulty"". They then show that the model agreement is almost only caused by these trivial and impossible samples, by measuring the agreement over all models with and without the trivial examples. Finally, they show through human trial that humans can with a relatively high accuracy (~81.36%) predict which samples are easier for neural networks to predict, showing that there is a level of difficulty to the image samples.","The authors demonstrate that, irrespective of model architecture or hyperparameters, many modern image classification models are always correct on 46.0% “trivial” and 11.5% “impossible” images. The authors describe the remaining 42.5% of images as ""difficult"", and claim that by focusing on these difficult images, it is possible to see pronounced differences between models. They also find that humans can predict which images are “trivial” and “impossible” for CNNs at 81.4% accuracy.","This paper analyzes the effect of dichotomous dataset difficulty (DDD) on model predictions; it has three key findings. First, it shows that a large fraction of imagenet images are either trivial (most models classify such images correctly) or impossible (most models classify such images correctly) for several state-of-the-art models. Second, it shows that differences between model predictions get pronounced when models are trained on ""in-between"" images only (i.e., in the absence of DDD). Third, it suggests that humans and CNNs have similar notions of image difficulty by showing that humans are highly accurate at predicting which images are difficult for CNNs","This paper shows the high consistency between the decisions of CNNs trained on the same dataset, regardless of the algorithms, architectures, hyperparameters in training, and optimizers. The authors further show that the validation set of ImageNet contains a large part that’s “trivial” for all CNNs and a small part that’s almost “impossible”. Removing these parts in the training set indeed makes models more different. Finally, the authors state that humans can easily tell what images are “trivial” and what images are “impossible”.",0.1484375,0.171875,0.1796875,0.25333333333333335,0.26666666666666666,0.16981132075471697,0.25333333333333335,0.20754716981132076,0.27380952380952384,0.1792452830188679,0.23809523809523808,0.21428571428571427,0.18719211822660098,0.18803418803418803,0.2169811320754717,0.20994475138121546,0.25157232704402516,0.1894736842105263
173,SP:2672c9a54271f87fa7bc10fa3645595ed5ef62ea,"The main contribution of this paper is to characterise the eigenvalue decay of the Neural Tangent Kernels associated with fully-connected neural networks evaluated on the hypersphere when the activation functions of the form $Relu^{s}$ where $s$ is a parameter that regulates the smoothness of the activation functions. The results generalise existing results for the particular case $s=1$, i.e. the case of ReLU activations [1,2]. The idea of the proof is to rely on a recent result from [1] which bounds the eigenvalye decay of an arbitrary rotationally invariant kernel based on smoothness and end-point asymptotic properties of the kernel written as a function from $[-1,1]$  to $\mathbb{R}$ where $[-1,1]$ corresponds to the inner product between the two arguments of the kernel function. The computation of the  asymptotics relies on an elegant use of Stein's Lemma as well as the recurrent relation between the NTK and RFK (random feature kernel) of various layers. Results for the eigenvalue decay of the RFK are also provided. Similarly to other works, once eigenvalue decay is obtained, various results can be obtained which further characterise the kernel and the learning problem: (1) it is shown that the kernel is equivalent to a Matérn family of kernels (which itself is equivalent to a sobolev space approach), (2) data-dependent generalisation bounds are shown where the distance between the empirical estimate and the groudn truth is bounded by a term that depends on the relationship between the test point and the training points (3) generalization bounds in $L^\infty$ norm with some strong assumption on the training points (they essentially have to be chosen in a ""perfect"" way). Applications to reinforcement learning are hinted at in a discussion at the end.      / ================== References ==================  [1] Alberto Bietti and Francis Bach. Deep equals shallow for ReLU networks in kernel regimes. ICLR 2021.   [2] Amnon Geifman, Abhay Yadav, Yoni Kasten, Meirav Galun, David Jacobs, and Basri Ronen. On the similarity between the Laplace and neural tangent kernels. NeurIPS 2020.    ","The paper provides a uniform generalization bound for overparameterized neural networks by assuming that the target function resides in the reproducing kernel Hilbert spaces generated by Neural Tangent (NT) kernels associated with general ReLU activation functions.  The approach is based on so-called maximum information gain, which is neat. The equivalence of the RKHSs generated by NT kernels and Matern kernels are discussed. ","The authors consider neural tangent kernels (NTKs) kappa_{NT, s}^l of the neural networks of depth l associated with ReLU a and its powers a_s with a positive integer s. Theorem 1 gives relations between the RKHSs generated by the NTKs and those generated by Matern kernels. Theorem 2 gives bounds for the maximal information gain of the NTKs. Theorem 3 provides some uniform generalization bounds for kernel ridge regression with the NTKs. ","The paper studies over-parametrized neural networks when the size of the network goes to infinity. In this regime, it is known that training the network via gradient descent type algorithms is equivalent to learning via the Neural Tangent Kernel (NTK) and the main focus is on this problem.   Previous results have shown that when one takes ReLU as the activation function, the RKHS associated with the NTK is equivalent to the RKHS of the Laplace kernel. The current paper generalizes this result to powers of ReLU and shows that as the activation function becomes smoother, so do the functions in the RKHS. Moreover, the exact decay of the eigenfunctions is also identified, in this extended case.  Using the above results the authors establish uniform bounds on the generalization error when learning a function from the appropriate RKHS with kernel regression.  ",0.07079646017699115,0.08259587020648967,0.12979351032448377,0.2222222222222222,0.30158730158730157,0.24,0.38095238095238093,0.37333333333333335,0.3120567375886525,0.18666666666666668,0.1347517730496454,0.1276595744680851,0.11940298507462686,0.13526570048309178,0.18333333333333332,0.2028985507246377,0.18627450980392155,0.16666666666666666
174,SP:2684bcc1f38a0e05bb38560dd33852b646febe6e,"This paper tackles the problem of in-situ trade-off between model robustness and accuracy, which is of great importance in real-world applications. The proposed method, FLOAT, makes improvements on the previous state-of-the-art method OAT (NeurIPS'20) by using a more efficient model conditioning method. Experimental results show that FLOAT outperforms OAT in terms of accuracy, robustness and model size. The FLOPs of FLOAT, OAT, and the PGDAT baseline are almost the same. ","This paper presents an algorithm named FLOAT(S), which adds conditional perturbation to weights during training (add noise when training with adversarial examples). This method immediately improves over its predecessor OAT by removing extra FiLM layers. As demonstrated in the experiments, FLOAT achieves better training efficiency than OAT, and perhaps more surprisingly, it has better robustness over OAT.","This paper argues feature-wise linear modulation (FiLM) methods to perform in-situ calibration are too computationally expensive. Motivated by this, the authors propose the FLOAT method which adds scaled binary noise to the weight tensor instead of using extra layers. This paper also extends FLOAT to balance the 3-way-tradeoff between robustness / accuracy / complexity. The included experiments showcase the improvement in resolving the tradeoff between robustness and accuracy. ","This paper generalized the conventional once-for-all adversarial training (OAT) approach [Wang et al., 2020] to FLOAT, the fast learnable once-for-all adversarial training method. The key technical innovation lies at the incorporation of $\lambda$-conditioned noise into model weights so as to achieve the purpose of OAT but not request additional layers to perform conditioning. To demonstrate the effectiveness of the proposed approach, extensive experiments are provided across datasets and model architectures.  ",0.14285714285714285,0.18181818181818182,0.18181818181818182,0.1896551724137931,0.1724137931034483,0.21428571428571427,0.1896551724137931,0.2,0.18666666666666668,0.15714285714285714,0.13333333333333333,0.2,0.16296296296296295,0.1904761904761905,0.18421052631578946,0.171875,0.15037593984962408,0.20689655172413796
175,SP:26f01f1d045f1bb1b7ef9a94d972f3eccdee2fc8,"The paper presents a generalization error bound for graph embeddings in the defined dissimilarity space, including the linear space and the hyperbolic space. The main conclusion is that error is polynomial and exponential with respect to the embedding space's radius in linear and hyperbolic spaces respectively. However, hyperbolic graph embeddings (HGE) with sufficiently larger number of data represents hierarchical data more effectively than linear graph embeddings (LGE).","This paper presents a generalization error bound for graph embedding with negative sampling. The theoretical results are general and can be applied to various settings, e.g., the embedding space, data distribution, and loss function. The theoretical results show the radius of embedding space plays a key role in graph embedding error, and show the imbalanced data distribution can worsen the error. This paper provides specific error bounds for many negative sampling strategies.","This paper studies the question of the generalization error incurred when embedding graphs in linear vs. hyperbolic space. Recent literature (e.g. work of Nickel and Kiela, ref. [20]) has shown that graph data with hierarchical structure is often better embedded in hyperbolic space--due to its higher representation capacity--than in linear space (for which it becomes necessary to increase the dimensionality of the embedding). However, this choice may come with a tradeoff in terms of generalization error, and the current paper studies this phenomenon in the context of embeddings obtained via negative sampling. Specifically, the authors show that the generalization errors of such embedding into linear and hyperbolic space incur polynomial and exponential growth, respectively, and are also affected by imbalance in how edges are distributed (i.e. regular graphs are balanced, graphs with hubs are more imbalanced). Some of the difficulty of this problem arises due to dependency in the negative sampling procedure, such that iid assumptions and related results cannot be applied. Towards the end, the authors also show that for the special case of tree data, their theory suggests that the generalization error incurred by a hyperbolic embedding is better than that of a linear embedding. Finally, the authors discuss the limitation that their bound does not depend on the embedding dimension, so is likely not tight. The authors explain their setup through numerous examples of negative sampling strategies, notably the Skip-gram model (ref. [1]), as well as detailed explanations of the different terms appearing in their main result.","Graph embedding has already been a common and effective technique in many applications, while the corresponding theoretical analysis is limited to ideal noiseless settings. This work intends to derive the unified generalization error bound for negative-sampling-based graph embedding regarding the existence of noise. However, the assumption made in this work to drive the theoretical analysis is somewhat impractical making the derived bound a little bit vacuous.",0.27941176470588236,0.3382352941176471,0.16176470588235295,0.3698630136986301,0.1780821917808219,0.07450980392156863,0.2602739726027397,0.09019607843137255,0.16176470588235295,0.10588235294117647,0.19117647058823528,0.27941176470588236,0.2695035460992908,0.14241486068111453,0.16176470588235295,0.16463414634146342,0.18439716312056736,0.11764705882352941
176,SP:2708358aa39b175dc894196a9ef26c9da5d752a9,This paper introduces a framework to address issues in the leakage of information through gradients. This paper claims to provide Bayes optimal adversary as an optimization problem which other attacks can be considered as an approximation of this framework. Some experiments are provided to support their claims. ,"This paper provides a gradient leakage attack via Bayes optimal adversary. The authors also demonstrate that existing attacks can be seen as approximations of Bayes optimal adversary. Empirically, this paper shows some heuristic defenses fail under the proposed attack.","The paper proposes a theoretical Bayesian framework for the problem of gradient leakage in federated learning. It shows that recent gradient leakage attacks are approximations of the Bayesian framework with different prior distributions for the input and conditional distributions of the gradients given the input. The paper also claims that recent defense mechanisms are not good enough, especially during the early stages of training. ","The submission ""Bayesian Framework for Gradient Leakage"" discusses privacy attacks against federated learning based on gradient inversion. In the first part of this work, a range of existing attacks is unified in a Bayesian setting. In the middle part of this work, three existing defenses are analyzed and broken by adaptive attacks. In the final part of this work, an attack based on estimation of the optimal Bayesian adversary is evaluated against several classical defenses and compared to other objectives.",0.2978723404255319,0.2127659574468085,0.2127659574468085,0.2564102564102564,0.2564102564102564,0.25,0.358974358974359,0.15625,0.125,0.15625,0.125,0.2,0.3255813953488372,0.1801801801801802,0.15748031496062995,0.1941747572815534,0.1680672268907563,0.22222222222222224
177,SP:274126a1a1c83701640697c36b8ec1e0eefd13f1,"This paper proposes an unsupervised (or rather, self-supervised) training method for RL, named active pre-training. This is based on using contrastive learning from pixels, then maximizing state entropy in the abstract space using a k-nearest neighbors density estimate. The authors combine their method with DrQ, demonstrating sample efficiency and asymptotic improvements on DeepMind control suite and Atari, along with several useful ablations.","The authors introduce Active Pretraining (APT), wherein an agent is encouraged to maximization the entropy of the state distribution represented in its replay buffer. The is accomplished by having rewards that are proportional to distance to nearest neighbours in some embedding space. This space is constructed via a separate contrastive learning objective.  This method is tested in a 2 stage process where the agent initially maximizes this state entropy entropy for a long time, and then finetunes on a down-stream time for a much shorter time, with the final performance on this task being the key metric. By this methodology, APT surpasses both data-efficient methods that train from scratch, as well as other methods that use the same 2 stage training scheme.","* This paper introduces APT (Active Pre-Training), a method for unsupervised pre-training in RL environments.  * During the pertaining phase, APT explores an environment using an intrinsic reward that incentivizes maximizing entropy in a contrastive representation space.  * APT is evaluated on DM-Control and Atari to test whether pre-training improves data-efficiency. APT outperforms training RL methods from scratch in DMControl and Atari, and the gains are more noticeable in sparse reward environments.   ","This paper explores the idea of doing unsupervised pre-training in a reinforcement learning context. Before giving task rewards to the agent, it is given many orders of magnitude more experience for unsupervised exploration.   The key idea is to use a non parametric entropy based reward function to efficiently explore the space and amortize it in a Q function. This Q function is then fine tuned on the task reward as a follow up stage.   Compared to previous unsupervised RL papers, this paper explores a new and interesting particle based formulation instead of count/density based model for intrinsic motivation. The performance metrics are shown on the atari and DMControl suite environments. ",0.16923076923076924,0.24615384615384617,0.2153846153846154,0.11290322580645161,0.1774193548387097,0.25675675675675674,0.08870967741935484,0.21621621621621623,0.125,0.1891891891891892,0.19642857142857142,0.16964285714285715,0.11640211640211642,0.2302158273381295,0.15819209039548024,0.1414141414141414,0.1864406779661017,0.20430107526881722
178,SP:275245ade2fd6f1224f75457a9d94f703fed44b4,"The paper presents results of a user study that was designed to study the effectiveness of feature attribution methods in assisting human decision makers. Experiments are conducted on Stanford dog dataset and imageNet dataset. The authors mention that feature attribution methods are not any better than 3 nearest neighbor, and that especially for fine grained classification task ( on Stanford dog dataset), feature attribution map in fact hurts. ","The main contribution of this paper is a thorough and well-designed user study towards using feature attribution maps, i.e. GradCAM, in practice to help human users understand prediction results of neural networks. Following the user study, the author finds several existing metrics in evaluating the goodness of feature attributions do not align with the user’s gains in having feature attributions and conclude that in the evaluated scenarios features attributions are not more useful than K nearest neighbor explanations. ","The paper investigates if current widely used evaluation metrics for saliency maps or explainable visual AI like pointing game, weakly supervised localization are trustable by conducting a very extensive user study. Some interesting conclusions and observations are given. In this sense, this is a good paper.","This paper performs a human evaluation study on feature attribution style explanations.  The main conclusions include:  1. Attribution methods (AMs) are not more effective than an ""explanation"" which consists of three nearest neighbor images.  In some settings (adversarial setting on Stanford Dogs) presenting confidence scores to humans is most helpful.  2.  Current automatic metrics to measure attribution methods don't correspond well with this human evaluation.  3.  When testing on AI-expert users, nearest neighbor explanations are more helpful.  ",0.26865671641791045,0.11940298507462686,0.23880597014925373,0.08641975308641975,0.1728395061728395,0.15217391304347827,0.2222222222222222,0.17391304347826086,0.20253164556962025,0.15217391304347827,0.17721518987341772,0.08860759493670886,0.24324324324324326,0.1415929203539823,0.2191780821917808,0.11023622047244094,0.175,0.112
179,SP:278a3702ec83b43493131cbac67c7b177e9d0c0f,"In this work, the authors study the problem and structure of memoryless stochastic policies in POMDPs. The focus is on the discounted infinite-horizon problem over finite state-action spaces. For this problem, the authors show theoretical properties on the the problem critical points and provide methods to compute their number.","This paper studies several properties of the landscape of memoryless planning in infinite-horizon POMDPs. The primary focus of this paper is to provide formal expression of (a) state-action frequencies and (b) cumulative reward functions given a set of memoryless stochastic policies. Specifically, they show that both quantities can be expressed as rational functions of polynomial equations. The degree of polynomials used is proportional to the number of possible internal states given a set of observations (Theorem 4). Then, the authors describes the set of feasible state-action frequencies as the solution set to a system of polynomial equations and inequalities (Theorem 16). Finally, they provide the upper bound for the number of critical points in the landscape of policy optimization (Section 5). ","This paper studies the problem of finding the best memoryless stochastic policy for an infinite-horizon partially observable Markov decision process (POMDP). It is shown that the (discounted) state-action frequencies and the expected cumulative reward are rational functions of the policy, and the degree is determined by the degree of partial observability. The problem is then formulated as a linear optimization with polynomial constraints. The authors then demonstrate how the partial observability constraints can lead to multiple smooth and non-smooth local optimizers and we estimate the number of critical points. ","This paper studies the geometric property of memoryless policy optimization problem for POMDP. The main idea is to formulate the state-action visitation frequency constraint as a polynomial constraint. Then, the degrees on the polynomials are bounded, based on which the number of critical points is obtained.  ",0.35294117647058826,0.35294117647058826,0.29411764705882354,0.27419354838709675,0.1693548387096774,0.25,0.14516129032258066,0.1956521739130435,0.3191489361702128,0.3695652173913043,0.44680851063829785,0.48936170212765956,0.2057142857142857,0.2517482517482518,0.3061224489795919,0.3148148148148148,0.24561403508771928,0.33093525179856115
180,SP:27976b4bfb6844ad0a4750ebc1a5e8cbdcfcbe72,The paper makes the first attempt to handle symmetric super heavy-tailed noise in a linear  bandit setting. A new estimator called mean-of-medians is proposed to estimate parameters of a  linear model under heavy-tailed noise. A generic algorithmic framework for solving linear bandit  problems is proposed where the mean-of-medians estimator can be plugged in and existing  high-probability regret bounds for sub-gaussian arms can be used to get high-probability regret bounds for the proposed setting. Numerical experiments are also provided.,"This paper addresses Multi-Armed Bandit (MAB) problems when rewards are ""super heavy-tailed"", meaning when the mean (first order moment) might not exist. Recall that previous works [1] usually require the existence of a moment of order $1 + \epsilon$, with $\epsilon > 0$. To address this problem, the authors introduce the Mean-of-Medians estimator, which is shown to concentrate well even in the super heavy-tailed framework (Theorem 3.2). This estimator can then be plugged in existing algorithm (Theorem 4.1). Numerical experiments are provided, showing the advantage of the proposed method over previous works (Section 5).  [1] ""Bandits with heavy tail"" Bubeck et al. 2013","The paper gives a novel robust estimator -- mean of medians -- for heavy-tailed noise satisfying $Pr( |Y| > |y|) \leq |y|^{-\alpha}$ for any fixed $\alpha > 0$. In particular, given access to i.i.d. samples $X_1,\ldots,X_n$, the unbiased estimator returns $X_{\sf mom}$ satisfying (i) $E [X_{\sf mom}] = 0$ and (ii) $|X_{\sf mom}| \leq \sqrt{4^{2/\alpha}/n^{1 - \epsilon} \log(4/\delta)}$ with probability at least $1 - \delta$. The paper then uses this as a black box to give a $\tilde{O}(d\sqrt{T})$-regret algorithm for linear bandits in $d$-dimension.  ","The paper addresses the problem where the noise has super-heavy tail, in the framework of online learning. In the previous works, it is assumed that the 1+\epsilon moment of the noise exists, where $\epsilon >0$. However, in this paper, a polynomially decaying tail bound is only assumed, which coves the setting, when the first moment does not exist. Hence, the previous methods like median of means cannot be used. Instead, here the authors develop mean of medians algorithm. With this, the paper first converts the super-heavy tailed noise to a bounded one, and then use this noise to play any online (ex. linear bandit) algorithm. Even when 1+\epsilon moment exists, a comparison study is also provided. Furthermore, the theory is corroborated with experiments. ",0.26436781609195403,0.20689655172413793,0.2413793103448276,0.1111111111111111,0.25925925925925924,0.1485148514851485,0.21296296296296297,0.1782178217821782,0.16535433070866143,0.1188118811881188,0.2204724409448819,0.11811023622047244,0.2358974358974359,0.19148936170212766,0.19626168224299068,0.11483253588516745,0.23829787234042552,0.13157894736842105
181,SP:27a31e9884477ac794c98ff3e95f8652f2f538e7,"This paper presents a biologically plausible learning method, called global error-vector broadcasting (GEVB). The authors also propose vectorized nonnegative networks (VNNs) on which the GEVB method operates. The key property of the GEVB method is that the gradient has the same sign as the ground truth gradient returned by back-propagation (BP) on VNNs. Experiments on MNIST and CIFAR are conducted to show that the proposed method can sometimes match the performance of BP and sometimes outperform competitors like direct feedback alignment. ","This paper proposes to overcome an important biological implausibility of neural networks trained by backpropagation using two novelties: 1) to train neural networks using a new globally available error signal, 2) a new type of neural network with vector-valued units and nonnegative weights. Interestingly, the model shows that the proposed learning rules lead to update rules with the same sign as those obtained by gradient-descent.   This work justifies the biological plausibility of the learning rule by relating it to the growing literature on three-factor Hebbian rules and justifies the choice of nonnegative weights to observed cortical properties, as well as the presence of ON/OFF cells.   The performance of the proposed network is compared, on MNIST and CIFAR-10 datasets, against a very standard set network trained with BP, as well as more recent models trained using direct feedback alignment. The model outperforms competing DFA-trained models for fully connected and convolutional architectures but not for locally connected architectures. ","The paper proposes to circumvent the need of backpropagation in deep networks by introducing two architectural changes. First, the weights are restricted to be non-negative so the chain rule simplifies significantly. Second, each unit in a network is represented by a vector to match the dimensionality of the error signal in the top layer, further simplifying the chain rule. The resulting weight update approximates backpropagation similar to the sign symmetry method.      ","This paper proposes (i) a learning rule called global error-vector broadcasting (GEVB), and (ii) a class of DNNs called vectorized nonnegative networks (VNNs) in which this rule operates - to solve the credit assignment problem. While Backpropgation (BP) is remarkably effective at training deep neural networks, it is biologically implausible because of the weight transport problem. In this paper, the weight transport problem is circumvented by globally broadcasting the error signal to all hidden units in the network. However, in order to accommodate the vector error signal a new network model with vector units is proposed.   Further, by deriving gradients using a BP like approach, the authors demonstrate gradient alignment with the GEVB learning rule. The performance of VNNs trained with GEVB is evaluated on MNIST and CIFAR-10 and compared with BP on VNNs and conventional networks.  ",0.3132530120481928,0.1566265060240964,0.3855421686746988,0.11728395061728394,0.21604938271604937,0.2777777777777778,0.16049382716049382,0.18055555555555555,0.2318840579710145,0.2638888888888889,0.2536231884057971,0.14492753623188406,0.21224489795918366,0.16774193548387098,0.2895927601809955,0.16239316239316237,0.23333333333333336,0.1904761904761905
182,SP:27f384ab3bd33d9ce76bd2323236e7110d86b465,"This paper studies online convex optimization with continuous switching constraints. The objective of the online agent is to minimize the static regret while keeping the total switching cost within some budget $S$. Under some standard assumptions, the authors establish a lower bound of the regret for all online algorithms by designing the adversary’s policy. They also show that the classical online gradient descent algorithm can match this lower bound if the step sizes are chosen appropriately. In addition, under stronger assumptions on the loss functions, the authors show the regret can be further improved to $O(\log{T})$.","The paper investigates the problem of online convex optimization under metric switching constraints in the non-stochastic setting. In particular, the goal is to minimize a convex loss function where the long-term cumulative switching cost is subject to a stringent budget constraint. A lower bound which accounts for switching constraint is obtained. It is shown that gradient-descent achieves the minimax optimal regret bound.","The authors study online convex optimization with an L2 switching budget (S) on the actions chosen by the online algorithm. The authors propose a modification of the orthogonal technqiue of Abernathy et al to incorporate the switching constraint and propose a new lower bound on the regret achievable for this problem. The authors show a phase transition in the achievable regret for different values of S. The authors show that by setting the step size appropriately, vanilla OGD can match the lower bounds they present.","This work studies the well-known Online Convex Optimization (OCO) problem under an additional $\ell_2$-norm hard switching constraint of $\sum_{t=2}^T |w_t - w_{t-1}| \leq S$ where $w_t$'s are the actions of the algorithm and $S$ is the switching budget. They first provide hardness results to show that the regimes $S=\Omega (\sqrt{T})$ and $S=O(\sqrt{T})$ are different and have varying regret lower bounds. Then, they show that Online Gradient Descent (OGD) with particular choices of step size is able to obtain matching regret upper bounds in both regimes. Finally, the analysis is extended to the strongly convex setting. ",0.24242424242424243,0.26262626262626265,0.25252525252525254,0.26153846153846155,0.26153846153846155,0.24705882352941178,0.36923076923076925,0.3058823529411765,0.22727272727272727,0.2,0.15454545454545454,0.19090909090909092,0.29268292682926833,0.28260869565217395,0.23923444976076555,0.22666666666666668,0.19428571428571428,0.2153846153846154
183,SP:2810a14ddd1b4bc411745262dc0f785c9ccab2c0,An approach vor variational inference in large-scale hierarchical models is presented that is more efficient than the base lines. This is achieved using shared parameters (amortization) and a feature pooling network. Experiments confirm the efficiency of the approach on synthetic data as well as the MovieLens dataset.,The paper deals with scalable approximate inference for hierarchical models and proposes a framework for hierarchical approximations that exhibit the same factorization as the exact posterior of the hierarchical model. This efficiently enables subsampling the data when evaluating the evidence lower bound and its gradients. The authors further propose a pooling-based strategy for amortized inference to avoid the usual linear scaling in the number of approximation parameters.  The paper is concluded with a set of numerical experiments with both synthetic and real data demonstrating the merits of the proposed framework.   ,"This work proposes a Gaussian naive bayes approximate posterior for a 2-level hierarchical naive bayes model. Various parameterizations for the variational posterior are explored, including a fully coupled joint posterior, a factored branch posterior, and an amortized version of the latter. Results and theory indicate that having an approximate posterior that makes the same independence assumptions as the true posterior does not hurt, and that parameter sharing improves performance at scale.",This paper makes an attempt to address the problem of large-scale inference for hierarchical probabilistic graphical models that contain both global and local latent variables by proposing to use Amortized VI instead of VI in [1] for more efficient and scalable inference. The authors first define a general class of hierarchical models which they call Hierarchical Branch Distributions (HBDs). They then propose a variational family matching this class of models for performing amortized inference. The authors then perform a series of experiments on synthetic datasets and the MoveLens dataset where they evaluate different inference models in terms of the final ELBO and convergence.  ,0.3125,0.14583333333333334,0.3333333333333333,0.14285714285714285,0.26373626373626374,0.18055555555555555,0.16483516483516483,0.09722222222222222,0.15384615384615385,0.18055555555555555,0.23076923076923078,0.125,0.2158273381294964,0.11666666666666665,0.21052631578947367,0.15950920245398775,0.24615384615384614,0.1477272727272727
184,SP:282ce4ede77a149687dababdeeefa082879f5117,"This paper develops a nonparametric statistical framework to learn optimal strategies in multi-stage decentralized matching markets, where agents with limited capacity are matched with arms with uncertain preferences in several stages. They propose an algorithm relying on the concepts of lower uncertainty bound and calibrated decentralized matching to maximize agents’ expected payoff. The algorithm uses a data-driven approach to calibrate the arms’ uncertainty and penalize agents for exceeding capacity. They show several implications on welfare and fairness based on their model. First, they show that agents favor arms with low uncertainty in the levels of acceptance to maximize their welfare, leading to unfair outcomes for arms since they might not be picked by their favorite agents even though those rank below them are picked. Then, they show that agents do better in a multi-stage matching instead of a centralized one. They demonstrate the effectiveness of their algorithms through experiments on synthetic and college admissions data.","The paper studies matching in decentralized markets. A collection of agents (schools) and a collection of arms (students) form the two sides of the market. The matching is done over stages where agents select a collection of arms, an arm would choose only one agent with some probability and once matched would exit the market. Further, the agents have quotas which they cannot exceed. An important point is that the market is decentralized and that the probabilities are not known in advance and hence they have to be estimated. The paper thus involves both a learning step and a maximization step. Some theoretical characterizations of the fairness and welfare are also obtained. ","The authors study a multi sided matching market under a statistical model, with the goal of finding the optimal strategies. The model uses agents and arms, but due to its applications the intuition is easier to capture assuming the agents are universities and the arms students. Specifically, there are $n$ agents and $m$ arms, which are pulled over $K$ rounds. Each agent has a quota $q_i \ge 1$ of arms they can use, and the sum of all quotas is at most $n$. The market is two-sided because the arms (students) have preferences over the agents (universities) (and these preferences can be fuzzy) and the agents (universities) have preferences over the arms (students): each arm $j$ has an intrinsic value $v_j$ and an extra value $e_{ij}$, specific to agent $i$.  At every step, each agent can pull any number of arms. However, each arm can only respond to at most one agent, or reject all of them. The arms don’t have such explicit preferences: their interaction with the agents is captures by the probability $\pi_{i,k}(s_{i,k}, v_j)$ that arm $i$ will accept agent $j$ at time $k$, where $s_{i,k}$ is the state at time $k$. This probability is generally not known but is inferred from data. Overall, agent $i$’s utility is the sum of $(v_j + e_{ij}) \cdot \pi_{i,k}(s_{i,k}, v_j) $ over every turn $k$ and all arms they tried using, minus a penalty if the quota $q_i$ is exceeded. Clearly, this induces a game between the agents, which have to decide how to use their quotas to select arms that are good enough, but not so good that they might be snatched by other agents.  The authors provide a variety of results:  1. Three algorithms (one of which is their main result) on the side of learning the optimal strategy from the perspective of agent $i$. 2. Fairness and Welfare trade-offs. 3. Experiments.   In particular, the first algorithm yields an optimal strategy for maximising the welfare of agent $i$ by minimising their *variational loss*, which is (roughly described) a modification of their utility with an added factor depending on $\max_{s_{i,k}} \pi_{i,k}(s_{i,k}, v_j)  - \min_{s_{i,k}} \pi_{i,k}(s_{i,k}, v_j)$ (the ‘variance’) and weighted by $\eta_{i, k}$, which reflects that subsequent rounds are typically less valuable than the first ones. Intuitively this makes sense: minimising the variance would maximise the way the quota is used. This strategy is NP-hard to solve for though, which leads to the development of a *greedy* approximation: rather than immediately solving for variance, the arms are chosen greedily based on their expected utility per unit of acceptance probability. The second algorithm (and arguably main result of the paper) uses historic data to estimate the $\pi_{i,k}(s_{i,k}, v_j)$’s. The actual algorithm has many details, but I hope is fairly represent an abstraction as follows: 1. Use an off the shelf estimator for the $\pi_{i,k}(s_{i,k}, v_j)$. 2. Modify them by subtracting a weighted variational term (as in the first algorithm), or set to 1 if the arm has not been used. 3. Balance exploration and exploitation of the arms used.  The fairness results revolve around *justified envy*: in a nutshell, an arm would have justified envy if it is pulled by agent $i’$, but agent $i$ (which the arm prefers) is trying a different arm on the same turn. This would reflect the real world situation where highly skilled individual might miss an offer because a potential employer might think they have a high chance of rejecting it. The authors show that higher uncertainly results in higher probability of an arm being envious. Moreover, the higher the $\eta_{i,k}$ terms are, the higher the number of envious arms. Finally, they compare with the single-stage matching markets (which do not produce envy in their optimal outcome) and show that multi-stage markets provide increased welfare, at the expense of fairness.  The authors also have experiments on synthetic data that support their learning procedure, comparing to a simpler strategy, where every agent selects their favourite arms within their remaining quota, at every step. As expected, both strategies perform well for popular agents, but have very different performance for less desirable ones. Again, this reflects that minimising variance (which is easier for popular agents: imagine a top university offering positions to students, knowing most would accept) is the key, which is further supported by extrapolating from real data sets of admissions and university rankings. ","The paper studies learning in multi-stage decentralized markets. Motivated by applications such as college admissions and the academic job market, the paper studies the classic matching problem under the assumptions that agents do not know their preferences and matching decisions can take place in multiple stages. As in the classic matching problem, a constraint is that colleges and applicants have quotas (e.g., students cannot accept more than one offer).  The paper models this problem as a multi-armed bandit problem with K>1 stages and quota constraints. Colleges are the agents and students are the arms. The arms’ preferences have no restrictions and are modeled through a probability function. Each agent values both a common quality aspect (the “score”) and a “fit” which is agent-specific. In this model, the paper proposes an algorithm that is based on the notions of lower uncertainty bound and calibrated decentralized matching; this algorithm learns the optimal strategy at each stage. Further, the paper investigates the economic implications of the model. For example, they show that strategic behavior may lead to unfairness (measured in terms of no justified envy) and that agents are better off with multi-stage compared to single-stage matching. Finally, they include simulations using data from college admissions. ",0.1518987341772152,0.2911392405063291,0.26582278481012656,0.44642857142857145,0.2767857142857143,0.07643312101910828,0.21428571428571427,0.05859872611464968,0.2,0.06369426751592357,0.14761904761904762,0.2857142857142857,0.17777777777777778,0.09756097560975611,0.2282608695652174,0.11148272017837235,0.19254658385093168,0.12060301507537688
185,SP:28761977409a3801af7e4d3f160ec6600c01ac5e,"The paper proposed two methods to learn categorical distributions.   Method 1: ArgMax Flows. The sampling procedure is simply taking argmax of a continuous random variable. The learning procedure involves a variational distribution that satisfies the argmax constraint and optimizing ELBO. Several variation distributions are introduced. In addition, the Cartesian product of argmax flows is introduced to trade-off between symmetry and dimension.  Method 2: Multinomial DDPM. The diffusion process is defined by categorical distributions. The paper derived ELBO for these models, and introduced a special parameterization of the reverse process.  The paper evaluated the methods in several experiments including text generation and image segmentation. They show better performances than baseline methods.","This paper proposes two methods for generative modeling of categorical data, one based on normalizing flows and the other on diffusions. The first method models observed data as the result of a continuous normalizing flow, followed by an argmax operation rather than a rounding one sometimes used for ordinal data. The model is then trained by introducing a carefully constructed approximate posterior (of the continuous variable given the categorical observation), and maximizing the ELBO. The second method is a fairly straightforward extension of commonly used diffusions to the categorical setting.","The paper proposes two new generative models for non-ordinal discrete random variables.   First, the paper proposes Argmax Flows, which apply argmax operation on continuous random variables, modeled by flow-based models. Note that estimating the likelihood of a model with a surjection requires a valid stochastic inverse, which should satisfy right-inverse (argmax constraint in the paper) and absolute continuity. For stochastic inverses of the argmax operation, the authors propose three novel methods: (1) thresholding, (2) Gumbel, and (3) Gumbel thresholding. The proposed stochastic inverses and the Argmax Flows are jointly trained by maximizing the ELBO.  Second, the authors propose a diffusion-based generative model for non-ordinal discrete data. Consider a pre-defined Markov chain, by which a data distribution is transformed to a prior distribution. The model learns a reverse Markov chain from the prior to the data. In this context, the paper proposes to diffuse non-ordinal discrete data to a uniform categorical distribution. For each transition, with a small probability, every value is independently decided to be either resampled or remaining the same; when we resample, a new value will be drawn from a uniform categorical distribution. For the reverse Markov chain, the authors acknowledge that a closed-form (conditional) reverse transition exists for the proposed diffusion, and they follow categorical distributions. Based on that, the paper proposes an efficient parameterization of the reverse transitions, which reduces the variance of the ELBO estimation.  In the experiments, the paper demonstrates that the proposed methods improve modeling language and segmentation maps (of cityscapes images) compared to previous generative models designed for ordinal data.",This paper proposes two distinct methods for learning categorical distributions. The first is based on an extension to variational dequantization with argmax operators which allows normalizing flow models to learn categorical distributions. The second is based on an extension to Bernoulli diffusion probabilistic models so that it can handle larger than 2 categories. Proof-of-concept experiments demonstrate that the proposed methods can successfully handle high dimensional categorical distributions.,0.18018018018018017,0.3153153153153153,0.16216216216216217,0.3111111111111111,0.2111111111111111,0.07894736842105263,0.2222222222222222,0.13157894736842105,0.2608695652173913,0.10526315789473684,0.2753623188405797,0.30434782608695654,0.1990049751243781,0.1856763925729443,0.2,0.15730337078651682,0.23899371069182387,0.1253731343283582
186,SP:28db5e1a6a533ab62b0fe402629fd9cebea2cc00,"This paper presents a new analysis for solving smooth, unconstrained variational inequalities, particularly focused on applications to minimax optimization. The primary theory innovation in this work is the observation that the very standard cocoercivity assumption can be replaced by cocoercivity holding in expectation. For such problems, convergence guarantees for the Stochastic Gradient Descent-Ascent and Stochastic Consensus Optimization are given. In both cases, O(1/T) convergence rates are proven. Numerics are given showing the proposed stepsize schemes perform reasonably.","This paper makes multiple contributions to the convergence analysis of unconstrained stochastic variational inequality problem:   (1) This paper proposes the expected co-coercivity assumption for operators that is equivalent to expected smoothness for functions, and weaker than many prevalent assumptions for variational inequality problem such as bounded gradient, bounded variance and growth condition.  (2) This paper generalizes the unbiased stochastic gradient with mini-batch into unbiased stochastic gradient with continuous spectrum, and applies it to generalize the two prevalent algorithms for unconstrained stochastic variational inequality problem, namely, stochastic gradient descent ascent analysis (SGDA) and Consensus Optimization (CO). The latter is further generalized into Stochastic Consensus Optimization (SCO) by applying to variational inequality problem of finite-sum structure, which contains Combines and generalizes several related algorithms including SGDA, deterministic CO and stochastic Hamiltonian gradient descent (SHGD) as special cases.   (3) This paper gives the first last-iterate convergence analysis of this generalized SGDA that only relies on the expected co-coercivity assumption and the quasi-strongly monotone assumption that are weaker than the previous analysis of SGDA, and recovers the results of deterministic GDA. Effects of hyper-parameters are well studied including constant step-size, diminishing step-size and the optimal batch-size.  (4) This paper gives the first last-iterate convergence analysis of SCO that recovers the results of SGDA, deterministic CO and stochastic Hamiltonian gradient descent (SHGD), and the convergence is even faster than an existing analysis of deterministic CO. ","This paper introduces the expected co-coercivity condition, explain its benefits, and provide the first-iterate convergence guarantees of stochastic gradient descent ascent (SGDA) and stochastic consensus optimization (SCO) under this condition for solving a class of stochastic variational inequality problems that are potentially non-monotone. The authors prove linear convergence of both methods to a neighborhood of the solution when the algorithms use constant step-size, and propose insightful stepsize-switching rules to guarantee convergence to the exact solution. One of the appealing features is that the convergence guarantees hold under the arbitrary sampling paradigm, and as such, the authors give insights into the complexity of mini-batching. ","This paper proposes convergence analysis on the optimization algorithms of smooth games, where the problem is formulated as a stochastic variational inequality, under the novel assumptions called expected co-coercivity and quasi-strong monotonicity. These assumptions serve the role of smoothness and strong convexity in the classical analyses of SGD for convex minimization. The authors show the last-iterate convergence rates for stochastic gradient descent-ascent (SGDA) and stochastic consensus optimization (SCO). The authors also relax the assumptions on the coordinate sampling schemes and on the noise. ",0.3,0.225,0.2875,0.15352697095435686,0.13692946058091288,0.24770642201834864,0.0995850622406639,0.1651376146788991,0.26436781609195403,0.3394495412844037,0.3793103448275862,0.3103448275862069,0.14953271028037382,0.1904761904761905,0.2754491017964072,0.21142857142857144,0.20121951219512196,0.2755102040816327
187,SP:28ea0605236fa28e3c848880e1e129338258c241,This paper proposes a new optimization method for min-max optimization (That could actually be generalized to any variational inequality problem) based on Anderson Mixing. They prove the convergence of their algorithm in the case of bilinear min-max games for both alternated and simultaneous updates operator. They eventually try their algorithm on toy bilnear problems and GANs.,Minimax optimization is a central problem in machine learning. The gradient descent ascent method is the most commonly used algorithm to solve this problem. This paper views the gradient descent ascent method as a fixed-point iteration and solves it using Anderson Mixing to converge to the local minimax. They show theoretically that the algorithm can achieve global convergence for bilinear problems under mild conditions. Some numerical experiments have been conducted. ,"This paper focuses on solving minimax optimization using Anderson mixing. Anderson mixing is a framework to accelerate fixpoint iteration. The authors tried to use  Anderson mixing for the minimax optimization problem. In their framework, they use a weighted average of the solution of last p iterates and optimized over the weights to come up with a new point.  They show that for the bilinear case, this algorithm converges to the optimal stationary point. Finally, the authors simulated their method on synthetic examples as well as image generation on CIFAR10. ","The authors propose to use the Anderson Acceleration on min max problem. They show two theoretical results: convergence rate on bilinear problems when using simultaneous gradient decent-ascent, and convergence rate on bilinear problems when using alternating gradient decent-ascent. Finally, they present numerical results in favor of their approach.",0.1896551724137931,0.2413793103448276,0.15517241379310345,0.22535211267605634,0.14084507042253522,0.12359550561797752,0.15492957746478872,0.15730337078651685,0.18,0.1797752808988764,0.2,0.22,0.17054263565891473,0.1904761904761905,0.16666666666666666,0.2,0.16528925619834714,0.15827338129496402
188,SP:28fa21ea7b18aea4b9ebec3584f59234f8085801,"This work proposes a novel architecture for quadrupedal locomotion that fuses proprioceptive and visual information with a transformer-based model to enable an agent to proactively maneuver environments with obstacles and uneven terrain by anticipating changes in the environment many steps ahead. The method is extensively evaluated in simulation and on a sim to real transfer tasks. The method is shown to both achieve higher reward  but also better capacity to generalise in the context of sim to real. Overall, the paper is well written and the provided evaluation is conducted fairly and well.","This paper proposes an approach to legged locomotion which leverages a Transformer-based model and is trained via end-to-end reinforcement learning. It provides extensive experimental evaluation of the approach in terms of performance and safety metrics, both in simulation and using real-world experiments. The code is expected to be open-sourced.","This paper proposes to incorporate the proprioceptive and visual information together for quadrupedal locomotion. The authors introduce a new model architecture named LocoTransformer that consists of separate modality encoders for proprioceptive and visual inputs, the output of which is fed through a shared Transformer encoder to predict actions and values.  Through experiments, the authors demonstrate that the robot, with the help of both the proprioceptive and visual inputs, can walk through different sizes of obstacles and even moving obstacles. They have also transferred the learned policy from simulation to a real robot by running it indoors and in the wild with unseen obstacles and terrain.","In this paper, the authors proposed a transformer based architecture that combines both visual (depth) and proprioceptive inputs (i.e. IMU and joint angles) to solve visual locomotion tasks. The authors demonstrated that their approach can solve challenging visual navigation tasks and locomotions task on uneven terrains. The proposed method out perform proprioceptive only, visual only, and HRL baselines. The sim trained policy has been demonstrated on the real A1 hardware.",0.1702127659574468,0.2553191489361702,0.1595744680851064,0.3148148148148148,0.2222222222222222,0.19047619047619047,0.2962962962962963,0.22857142857142856,0.2112676056338028,0.1619047619047619,0.16901408450704225,0.28169014084507044,0.2162162162162162,0.24120603015075376,0.18181818181818182,0.2138364779874214,0.19199999999999998,0.2272727272727273
189,SP:29051dfbba7fe095fbdcc426485033549fd6fbc9,"This paper extends recent information-theoretic approaches for computing model generalization to the meta-learning setting. They prove algorithm/data-dependent bounds for both the joint-training and support-query meta-learning strategies and instantiate the results for variants of gradient-based meta-learning with noise injection. Finally, they compute generalization bounds on synthetic and Omniglot data.","The paper extends information-theoretic bounds (Xu and Raginsky [15]) to provide generalization bounds for two meta-learning settings: Joint Training (classical setting of Baxter) and Alternate Training (that includes the MAML algorithm). The general bounds are based on the mutual information between the learning algorithm output and input. The bound is given a more explicit form for the case of a meta-SGLD algorithm, that can be described as a stochastic variant of the popular MAML algorithm. Experimental results show bounds that are orders of magnitudes smaller than norm-based bounds.","This paper provides information theoretic generalization bounds for meta-learning that 1) are algorithm and data dependent 2) don't depend on the norm of the gradients, but their sensitivity to the training data. This leads to stronger bounds than previous literature. Experiments on simulated data and Omniglot with the SGLD optimization algorithm confirm the theory.","The authors derive two bounds on the generalization property of meta-learning algorithms using an information-theoretic analysis. The first one considers algorithms based on a *joint-training*, i.e.  when all the data from the tasks is used to update the parameters. The second bound is specific to algorithms learned through an *alternate training*, i.e. the tasks data are split into training and validation, training part is used to update the task-specific parameters and the validation part is used to update the meta-parameters. Then, they analyze two concrete algorithms based on SGLD, a joint-training version and an alternate training version, to derive the bounds for these specific cases. Finally, they empirically estimate the values of their bounds and compare with an empirical estimate of a previous bound based on the lipschitz constant of the network. The empirical estimation is done on synthetic data and the omniglot dataset.",0.3157894736842105,0.2807017543859649,0.3157894736842105,0.1956521739130435,0.25,0.30357142857142855,0.1956521739130435,0.2857142857142857,0.11842105263157894,0.32142857142857145,0.1513157894736842,0.1118421052631579,0.2416107382550336,0.2831858407079646,0.1722488038277512,0.24324324324324326,0.18852459016393444,0.16346153846153844
190,SP:290c3632739e3be79593b45f9028a90aba6f1438,"The authors present an approach for lifelong learning where each task is processed in a single pass. They propose to adapt the learning rate of the training algorithm depending on the current task's similarity to the previously observed tasks. The learning rate is decreased when there are many dissimilar tasks to avoid catastrophic forgetting. The paper presents an adapted RMSProp algorithm, but the procedure can be adjusted to Adagrad and Adam as well. The presented evaluation on computer vision datasets shows that the proposed modification helps to increase the final accuracy of those methods while keeping the forgetting low. When compared to baselines from literature under the conditions of the single pass setting, the proposed method achieves better final accuracy and keeps approximately the same forgetting rate. ","This paper proposes a  task-aware adaptive learning rate method, TAG, for continual learning. The optimizer TAG is to promote the learning rate if they are similar to previous tasks, while decreasing the learning rates if they are dissimilar to previous tasks without storing previous examples. The authors combined the proposed method with existing optimizers, such as Adam, SGD, etc to demonstrate the effectiveness of the proposed method. Experimental results on several datasets show the improvements over naive optimizers.   ","This paper proposes TAG, a method for continual learning in the task-incremental setting. This method relies on storing and using task gradients while learning a set of supervisd tasks sequentially. The influence task-based accumulated gradients is regulated through a learning rate that is adaptive according to the relatedness of the current task with the previously observed ones. The authors report results on benchmark datasets for continual learning of up to 20 disjoint tasks. Comparisons against naive non-continual optimizers such as SGD, Adam and RMSProp are reported, along with results in the continual learning setting with some state-of-the-art methods such as EWC, A-GEM and ER. The authors report performance in terms of overall accuracy, forward transfer and learning accuracy (LA).","In this paper, the authors propose a new optimization method for continual learning. The authors propose a task-aware optimizer to adapt the learning rate for each task. The proposed method is evaluated on several datasets to show its effectiveness. ",0.1875,0.1953125,0.125,0.31645569620253167,0.25316455696202533,0.1349206349206349,0.3037974683544304,0.1984126984126984,0.4,0.1984126984126984,0.5,0.425,0.23188405797101447,0.1968503937007874,0.19047619047619047,0.24390243902439024,0.33613445378151263,0.20481927710843373
191,SP:29576a856e3e235b33bb46af539c9d5a9a9257cd,"The paper presents a convolutional neural network method for panoptic (multi-class, multi-instance) image segmentation. Semantic segmentation is addressed in the standard fashion with pre-trained model weights (convolutional kernels). Novelty is introduced in the instance segmentation component where the parameters of the convolutional kernels responsible for producing activation maps, one for each instance, are adapted at test-time to fit the given image. Overall this is a neat idea and the experimental results are compelling. However, I found the paper difficult to understand on first readings and have several suggestions for improvement of the presentation. ","This paper propose a unified framework named K-Net which segments both instances and semantic categories consistently by a group of learnable kernels, where each kernel is responsible for generating a mask for either a potential instance or a stuff class. The authors propose a refined kernel update strategy to group categories efficiently and effectively. Extensive experiments on three different segmentation tasks prove that such simple design can achieve STOA results while running faster than many previous hand-crafted segmentation methods.  ","This paper presents a unified approach for Semantic, instance, and panoptic segmentations tasks.  Proposed method segments both instances and semantic categories consistently by a group of learnable kernels, where each kernel is responsible for generating a mask for either a potential instance or a stuff class. The model is training using bipartite matching, and the training and inference are naturally NMS-free and box-free.  ","This paper tries to unify semantic and instance segmentation with a group of learnable kernels that is conditioned on image features. Although the idea of using dynamic kernels (aka dynamic convolutions) has been already explored in instance segmentation [44, 47] and panoptic segmentation [30], this paper proposes to use sparse dynamic kernels instead of using kernels that are generated on dense grid in [30, 44, 37]. This paper further borrows the idea of bipartite matching loss in DETR [4] to make their model NMS free. The experiments show competitive results compared to state-of-the-art methods on multiple segmentation tasks.",0.13402061855670103,0.18556701030927836,0.1958762886597938,0.4567901234567901,0.19753086419753085,0.24615384615384617,0.16049382716049382,0.27692307692307694,0.18811881188118812,0.5692307692307692,0.15841584158415842,0.15841584158415842,0.14606741573033705,0.22222222222222224,0.1919191919191919,0.5068493150684931,0.17582417582417578,0.1927710843373494
192,SP:298180ab8495fa742fe9055976f264ca04dfb5c5,"The paper studies whether real natural images are necessary to train deep neural networks for computer vision applications. To that end, the authors create datasets from a large variety of synthetic image generation processes and evaluate the performance of neural networks trained using these synthetic images. In a comprehensive analysis the authors demonstrate that neural networks trained on synthetic images obtain surprisingly good results and also identify key properties of synthetic datasets that lead to good downstream performance.","This paper provides a comprehensive study of using not naturally looking, synthetic images to train image feature extractors. Most of the synthetic images can be generated easily either in closed form or using a randomly initialized model. The obtained feature extractors result in significantly better models on natural images than randomly initialized counterparts after fine-tuning. Various types of synthetic images are evaluated in this paper to reveal the properties of images that can be used to train better feature extractors. In general, I feel this paper is studying a very interesting problem and a solution to the problem can be used in many practical scenarios where training data is difficult to collect or even not available, but the paper might need more technical contributions to be accepted. A potential way for improvement is to utilize the observations to generate better synthetic images from some random process that can improve the accuracy.  Strengths: 1. Evaluates a wide range of synthetic image types.  2. Comparing the statistical properties of synthetic and natural images, in an effort to uncover the important factors for good synthetic images.  Suggestions and Questions: 1. I feel the paper spends too many pages (3 pages) enumerating different image generation methods. These are important details, but maybe they should not take up 30%+ of the paper. The objective for training the models are ignored (I did not find it in the supplementary neither), though references to the paper of these methods are given. I feel it is important to clearly state the training objective in the main paper, and some details of image generation can be moved to the appendix.   2. I feel the observations in Sec. 5.1 are not conclusive. The accuracy do not seem to correlate well with the statistics considered. Maybe a more convincing way to verify the conclusions is to generate images that produce the stats that will likely produce highest accuracy according to the observations, and verify if it is the case. The correlations seem much stronger in Sec. 5.2, but the results are not surprising. For example, it is quite intuitive that if the training images have small FID to test images, thus more similar to test images, then the test accuracy will be higher.    ## Summary after discussions  In general, I feel the findings in this paper is interesting and could potentially invoke investigations into the necessary conditions for the success of contrastive unsupervised learning for images, and would like to raise my score. However, I still encourage the authors to brand this paper as a scientific discovery rather than something that will have immediate impact on applications (addressing the privacy and bias concerns etc.). From the new results of mixing with real data, it seems adding images from random processes does not improve the results of only using real images. As to 3-Real unlabeled data, It also remains unclear whether pretraining with real images generalizes better than random images in the decentralized setting; it seems pretraining with real data will transfer better from the results of 4-Mixing with real data. ","The paper investigates the effectiveness of synthetic data generated via procedural noise processes as training data for deep neural networks. Specifically, the authors consider fractals, computer graphics, dead leave models, statistical image models, and untrained GAN generators, as well as combinations thereof, as mechanisms to generate data, and train a network in self-supervised fashion on these different types of data. They find that deep networks trained in this fashion significantly outperform randomly initialized networks, and are competitive with deep networks that are trained on large natural image data sets on specialized domains (not necessarily natural images). The paper further presents a suite of experiments that aims to relate the properties of the synthetic data (e.g. color distribution) with the downstream performance of models trained on it.","The paper proposes to learn visual representations from procedurally generated images, drawing insights from natural image statistics. Non-trivial test accuracy was demonstrated on standard image classification datasets. The paper also considers what constitutes a good dataset for training wrt the statistics of individual images as well as the whole dataset highlighting precision-recall trade-offs when promoting naturalism vs diversity.",0.358974358974359,0.28205128205128205,0.11538461538461539,0.06627680311890838,0.037037037037037035,0.09375,0.05458089668615984,0.171875,0.14754098360655737,0.265625,0.3114754098360656,0.19672131147540983,0.09475465313028765,0.21359223300970875,0.12949640287769787,0.1060842433697348,0.06620209059233449,0.12698412698412698
193,SP:29aa453541e7433a1a8a2447391bab3ae87cbd9e,"This paper proposes an input-convex neural networks (ICNNs) based method to approximate the JKO scheme for discretizing Wasserstein gradient flows. Following a previous work [1], the key idea is to reformulate the distribution optimization problem in each JKO iteration as optimization over convex functions due to the well-known Brenner’s polar factorization theorem [2]. ICNNs are used to parameterize these convex functions from computational considerations. The proposed method is applied to simulate Wasserstein gradient flows for sampling from unnormalized density and nonlinear filtering. [1] Discretization of functionals involving the Monge-Ampere operator. [2] Polar factorization and monotone rearrangement of vector-valued functions. ","In this work the authors introduce an innovative discretisation of the Jordan Kinderleher Otto scheme (which describes the evolution of the gradient flow of relative entropy in the Wasserstein metric).  To do so the authors iteratively construct a transformation map as a composition of gradients of input-convex neural networks, which satisfy a universal approximation theory among convex functions.  Using Brenier's theorem, an iterative variational formulation for the density at discrete time-steps is obtained, from which one can generate samples and or evaluate an unnormalised density.  A variety of applications are presented, including finding the stationary distribution, as well as nonlinear filtering.   ","The Fokker-Planck equation can be treated as gradient descent over entropy functionals in Wasserstein space. To solve this problem, Jordan Kinderlehrer and Otto proposed the JKO scheme. Later, through the connection between the Brenier theorem and this scheme, [10] treat it as an optimization problem which aims at finding the convex Brenier functional $\psi$ in equation (7). However, the solution given by [10] is usually intractable in high dimensional space. Thus, this paper proposes to use the input convex neural networks (ICNNs) proposed by [5] to replace $\psi$. And then use SGD to optimize the model parameter. This forms the core contribution of this work. Additionally, the experiments are interesting.","This paper is on the computation of Wasserstein gradient flow, a type of gradient flow based on the optimal transport theory. The proposed algorithm is built on the famous JKO scheme. To implement the JKO scheme, input-convex neural networks (ICNNs) are used to model the optimal transport map between the distributions at consecutive steps. ",0.18269230769230768,0.16346153846153846,0.17307692307692307,0.17307692307692307,0.15384615384615385,0.15315315315315314,0.18269230769230768,0.15315315315315314,0.32727272727272727,0.16216216216216217,0.2909090909090909,0.3090909090909091,0.18269230769230765,0.15813953488372096,0.22641509433962267,0.16744186046511628,0.20125786163522014,0.20481927710843373
194,SP:29dacf8c14cc91478cf0eae140e5f64282e6f407,"This paper presents the complexity analysis of two problems: robust optimal transport and robust barycenter problem. For both these problems, the paper analyzes a sinkhorn-based algorithm and obtains a complexity of O(n^2 / \epsilon). Finally, the authors show some numerical analysis to support the theoretical claims.  ","This paper proposes two robust formulation of the optimal transport problem (namely RSOT and ROT) based on the relaxation of the marginal constraints of the optimal transport plan using Kullback-Leibler divergence. The authors introduces algorithms based on Sinkhorn iterations in order to solve RSOT and ROT problems, and analyse their computational complexities. The fixed-support Wasserstein barycenter problem is also tackled using the proposed RSOT as a divergence, and the algorithm shows a better complexity than the one based on entropy regularised OT. Numerical experiments are also provided to support the theoretical results and applied task are performed (color transfer and generative modeling).","This paper focuses on the computational complexity of robust optimal transport between discrete probability measures. The authors propose two Sinkhorn-based algorithms with near-optimal complexity to approximate the optimal cost. Furthermore, for solving a robust barycenter problem, they propose an algorithm based on iterative Bregman projections (IBP) and show that it also has near-optimal complexity between two discrete probability distributions. Experiments verify the presented complexities for the above algorithms. The main limitations and future work directions are also discussed.","The paper proposes the Robust Optimal Transport(ROT), Robust Semi-constrained Optimal Transport (RSOT), Robust Unconstrained Optimal Transport (RUOT) and Robust Semi-constrained Barycenter Problem (RSBP). In these problems, the equality constraints in the original OT problem is replaced by the soft KL divergence. Then the Sinkhorn-like algorithms are proposed to solve the corresponding dual problems. Finally, the detailed convergence analysis is given. ",0.4166666666666667,0.3333333333333333,0.3333333333333333,0.20192307692307693,0.19230769230769232,0.1728395061728395,0.19230769230769232,0.19753086419753085,0.25,0.25925925925925924,0.3125,0.21875,0.2631578947368421,0.24806201550387597,0.28571428571428575,0.22702702702702704,0.2380952380952381,0.1931034482758621
195,SP:29ec4b40dd62d7f26102c65c788b095b8881e358,This paper studies the problem of choosing a justified regularization parameter for unsupervised domain adaptation. The authors make a link between ill-posed inverse problems and approaches based on a weighted combination of the source error and a distance between source and target feature representations. They propose a framework based on balancing approximations and sampling errors to obtain a target error bound that takes into account a balance between learning errors domain distance. This leads to develop a founded rule for the choice of the regularization parameter which can be applied with source and target distributions with disjoint supports.  A bound on the target risk is provided.  An experimental evaluation of the approach is provided using the Moons toy dataset and the Amazon reviews and DomainNet–2019 benchmark.,"The paper proposes the  balancing principle to tackle hyper-parameter selection in unsupervised domain adaptation without requiring target labelled data. By viewing the target error bound as the combination of ""  learning errors""  and the distance between source and target domains, the paper designs an algorithm that balances both terms to estimate the regularization parameter.  Generalization guarantee  of the corresponding selected  model is provided. Empirical  evaluations on a simulation dataset and two classical domain adaptation datasets highlight the performances achieved by the proposed method over existing approaches.","This paper introduces a new error bound for the target domain motivated from Tikhonov-regularized inverse problem. This adoption gives an optimal selection scheme for the error bound of target domain based on distance-regularized domain adaptation, which is a theoretical principle for most domain adaptation methods. The proposed method is evaluated on the three benchmarks on domain adaptation.","In this paper, the authors study the hyperparameter selection problem in the context of unsupervised domain adaptation. While hyperparameter selection is not trivial at all due to the absence of the labeled data in the target domain, the authors employ the balancing principle to draw a connection between unsupervised domain adaptation and ill-posed inverse problems. The proposed method is based on balancing the discrepancy term and the error term, and the joint minimizer is chosen as a good hyperparameter. Notably, this method does not require any assumptions such as covariate shift and support matching as supposed in the existing researches.",0.2109375,0.1328125,0.2109375,0.1744186046511628,0.2558139534883721,0.3050847457627119,0.313953488372093,0.288135593220339,0.26732673267326734,0.2542372881355932,0.21782178217821782,0.1782178217821782,0.2523364485981308,0.18181818181818182,0.23580786026200873,0.20689655172413793,0.23529411764705885,0.225
196,SP:2a764cf6092ddf027257a38305798cdfbd0d5acf,This work considers constrained multi-objective Markov decision processes (CMOMDPs). The authors propose an algorithm to obtain the optimal policy in the offline setting. Optimality and constraint violation bounds are provided.,"The paper proposes and analyzes the setting of off-policy constrained multi-objective Markov decision process.  This is a generalized of the constrained MDP setting.  An algorithm to learn the solution is proposed based on a minimax formulation with a pessimistic penalty.  Theoretical analysis defines upper bounds on the suboptimality of the objective and constraint violation. Efficient learning is also demonstrated, under the assumption that the off-policy data sufficiently covers the trajectories induced by the optimal policy.","The paper considers policy optimization in a difficult setting involving multiple challenges: (1) multi-objective -- rather than simply optimizing a scalar reward, the policy must optimize a convex function g on vectors of cumulative costs; (2) constrained -- while optimizing g the policy must also satisfy a constraint on the cumulative costs; (3) offline -- the policy learning algorithm cannot actively query the environment, and only has offline access to experience.  For this setting, the paper presents an algorithm with corresponding theoretical analysis. The algorithm follows a primal-dual optimization to handle (1) and (2) above while proposing pessimistic value iteration to handle (3) above. Theoretical analysis provides guarantees on the convergence of the algorithm to an optimal policy under idealized conditions in both general MDPs and kernelized MDPs.","This paper considers the problem of offline RL in a constrained multi-objective MDP (CMOMDP) environment, where the goal is to compute a policy that achieves the best performance with respect to the given preference function while satisfying the given cost constraints, only using a fixed offline dataset. Pessimism Dual Iteration (PEDI) is proposed, which is the first provably efficient offline RL algorithm for CMOMDP that exploits dual gradient ascent and does not make any assumption on dataset coverage. First, the constrained optimization problem is converted into its dual problem by the convex conjugate. Then, for fixed dual variables, the overall optimization is reduced to an unconstrained planning problem, where pessimistic planning that penalizes uncertainty for each state-action is performed. The dual variables are updated via projected subgradients that are computed by the value of primal variable obtained during planning. Finally, the output policy is a uniform mixture of policies that are obtained during overall iterations. An instantiation of PEDI for linear kernel CMOMDP is introduced. Theoretical results are presented, which bound the suboptimality and constraint violation, and it is minimax optimal for linear kernel CMOMDPs.  ",0.45161290322580644,0.3870967741935484,0.45161290322580644,0.28205128205128205,0.2948717948717949,0.2125984251968504,0.1794871794871795,0.09448818897637795,0.0748663101604278,0.1732283464566929,0.12299465240641712,0.1443850267379679,0.25688073394495414,0.1518987341772152,0.12844036697247704,0.21463414634146338,0.17358490566037738,0.17197452229299365
197,SP:2a89c564256eae71a6d28904f928e345334fc218,"This paper considers the memorization of data using deep neural networks and aims to minimize both the number of neurons and the number of connections in the network, where the activation function is a step function.  The main result is a construction of a deep and relatively sparse network that memorizes arbitrary labels of 'well-separated' data sets. If the size of the data set is $n$, the number of neurons in the network is proportional to $\sqrt{n}$ plus a linear dependence on the well-separateness parameter. This is an exponential improvement over a recent result of Vershynin (which itself generalized a classical result of Baum from the 80's), which had an exponential dependence on the well-separateness parameter. The authors also complement their work by proving some lower bounds, which, in particular, show that some polynomial dependence on the well-separateness parameter is necessary.","This paper studies the memorization capacity of deep threshold networks, and provides a significant improvement over previous work. For $n$ points in the $d$-dimensional unit ball with at least $\delta$ separation between any two points, it is proved that a threshold network with $\tilde{O} (\frac1\delta + \sqrt n)$ neurons and $\tilde{O} (\frac d\delta + n)$ weights can memorize them. The previous best result (Vershynin, 2020) has an $e^{1/\delta^2}$ dependence in these bounds, and thus the current paper provides an exponential improvement. This paper also provides a lower bound saying that for $n$ in a certain range, the required number of neurons in the first layer must be $\tilde{\Omega}(1/\sqrt\delta)$, though this does not match the upper bound in the paper.","The paper considers a feed-forward neural network that takes inputs in $\mathbb{R}^d$ and outputs binary labels. The neural network has $L$ layers, with each layers consisting of a bias term and the activation function is the threshold function. The paper considers the following question: What is the minimum size of a threshold network so that it can memorize a dataset of $n$ samples in $\mathbb{R}^d$. Memorization refers to learning the parameters of the threshold network so that it can accurately output labels of the $n$ samples, for any arbitrary labeling. The paper shows that a deep threshold network can memorize $n$ points in $d$ dimension using $O(\frac{1}{\delta} + \sqrt{n})$ neurons $O(\frac{d}{\delta} +n)$ weights (up to log factors), where $\delta$ is the minimum distance between the points (considered in angular metric and $\ell_2$-metric). ","This paper considers the problem of memorizing n data examples in R^d using deep threshold networks. It is proved that, if the dataset is delta-separated (either in angle or in l2 distance), then for any labels, there exists a threshold network with O( (log(n)^2/delta) + sqrt{n}log(n)^2) neurons and O( (d+log(n))log(n)/delta + nlog(n)^2) weights that can memorize the dataset. This improves prior rates which either depend linearly on n or exponentially on 1/delta. Additionally, this paper also studies bit and neuron complexity, and particularly it is proved that there exists a delta-separated dataset with n examples which requires Omega(log(n)/sqrt{delta}) neurons to memorize.",0.19047619047619047,0.2108843537414966,0.16326530612244897,0.21705426356589147,0.3023255813953488,0.19310344827586207,0.21705426356589147,0.21379310344827587,0.19834710743801653,0.19310344827586207,0.32231404958677684,0.23140495867768596,0.20289855072463767,0.2123287671232877,0.1791044776119403,0.20437956204379562,0.312,0.2105263157894737
198,SP:2b246de2c999f1e4fe0918f8b3e4b8cfb8bc6474,"The authors build on the work of Chen, Valiant and Valiant from NeurIPS 2020 on estimators minimizing the worst-case expected error by providing two new algorithms for the same problem formulation, but with explicit runtime guarantees. The first algorithm uses a standard semidefinite program within a gradient descent loop to arrive at a runtime that is (with some abusive simplification) $\tilde O(mn^5 \log n/\epsilon^6)$. The second relaxes the setting so that data values are bounded in $\ell_2$ norm (rather than $\ell_\infty$), and uses this new setting to reduce the runtime to $O(mn^3 \log n/\epsilon^3)$. As far as I can tell, these algorithms don't offer any guarantees on sample complexity, but the newly explicit runtime guarantee is (subject to questions below) an improvement. As an impossibility result, the authors provide a combinatorial condition on the sample-target distribution under which the worst-case expected error for all estimators is at least a constant. Finally, the authors verify their new algorithm experimentally, showing comparable performance to [4] but faster runtime.","This paper studies the _worst-case expected error framework_ of statistical estimation proposed by Chen, Valiant, and Valiant, where data values are worst-case, but the estimation algorithm exploits randomness in the data collection process. In this worst-case expected error framework, there are $n$ data values $x_1$ to $x_n$ and a probability distribution $P$ over pairs of subsets $A, B \subseteq {1, \ldots, n}$, where the variables $x_A$ over the sample set $A$ are revealed, and we want to estimate the value of some fixed function $g(X_B)$ of the variables $x_B$ over the target set $B$. Restricting attention to _semilinear_ estimators, which are linear combinations of the sample values whose weights could depend on the sample-target distribution $P$ or the indices $x_A$ and $x_B$ of the sample and target variables, Chen, Valiant, and Valiant gave a complicated semilinear estimator for the mean of the target set $B$ which is at most $\pi/2$ factor larger than the optimal semilinear estimator, when the data values are $\ell_\infty$-bounded.  This paper presents the following three results: 1. a simpler algorithm to compute a semilinear estimator within a multiplicative $\pi/2$ factor and an additive $\epsilon$ factor for $\ell_\infty$-bounded values, with explicit efficient runtime guarantees. 2. an efficient algorithm to compute a semilinear estimator within an additive $\epsilon$ error for $\ell_2$-bounded values. 3. a family of lower bound instances where all estimators (not necessarily semilinear) must have at least constant error, which generalizes the trivial lower bounds of disjoint $A$ and $B$.","This paper studies the recently introduced *worst-case expected error* framework of Chen-Valiant-Valiant [4]. In this setting the data values are worst case, but the statistician has knowledge of the data-generating process. The authors provide new efficient algorithms based on alternating minimization-maximization for estimating the mean in worst-case under the previously studied $\ell_\infty$ assumption [4] on the dataset as well as under a new $\ell_2$ assumption. The authors also provide a simple combinatorial condition denoting when the worst-case error is $\Omega(1)$, and thus consistent estimation is not possible in this setting. Experiments compare their $\ell_2$ algorithm with the algorithm proposed by [4]  on synthetic data in the settings of importance sampling, snowball sampling, and selective prediction. ","This work builds on a recent statistical estimation framework of Chen, Valiant, and Valiant [NeurIPS`20] that leverages knowledge about how samples are collected but makes no distributional assumptions on the data values. The authors explore three important questions in this space:  (1) Is is possible to compute a semilinear estimator for the $\ell_\infty$ case that achieves the same approximation quality as that of Chen-Valiant-Valiant, while also emitting explicit running time bounds?  (2) Is there an algorithm to compute a high-quality semilinear estimator in the $\ell_2$-bounded case?  (3) Are there simple properties of a data collection process that guarantees no estimator can achieve sub-constant worst-case expected error?  The results answer all three questions affirmatively. The authors show that online gradient descent can be used to design simpler SDP-based algorithms, answering (1) and (2). They also quantify a combinatorial property of joint sample-target distributions that, for any estimator, necessarily incur at least a constant error. The paper then presents experiments based on those in [Chen-Valiant-Valiant, NeurIPS`20] to demonstrate the effectiveness of their gradient descent-based approach.",0.2222222222222222,0.17777777777777778,0.2,0.1590909090909091,0.17424242424242425,0.2619047619047619,0.15151515151515152,0.25396825396825395,0.19148936170212766,0.3333333333333333,0.24468085106382978,0.17553191489361702,0.1801801801801802,0.2091503267973856,0.19565217391304346,0.21538461538461537,0.20353982300884954,0.21019108280254778
199,SP:2b3e055a1f17fbfb8c030e923be7d71178fc8e83,"This paper considers the analysis of randomized (online) experiments (A/B tests) through a linear regression that adjusts for pre-treatment covariates, which are independent of the treatment assignment. It is well-known that this type of estimate of the treatment effect can be more efficient than the difference-in-means estimator when the covariates are predictive of the outcome.   The authors propose to incorporate the covariates into the linear regression through a predictor of the outcome, which itself is estimated with data-adaptive, machine learning methods. Cross-fitting is used to control the bias introduced by the data-adaptive estimation. It is shown that the resulting estimator is asymptotically normal with a limiting variance that is no larger than that of the difference-in-mean estimator, under relatively weak conditions. In particular, the method does not require the consistency of the nuisance predictor function. This model-agnosticism is clearly an advantage when compared to influence-function-based methods (e.g., double ML), which typically require the convergence of the nuisance to the truth at a certain rate.   The method developed is shown to deliver correct coverage from simulations and A/A testing data, and to typically outperform competing methods.","This paper proposes what the authors call a machine learning regression-adjusted treatment effect estimator to reduce the variance when estimating the average treatment effect in randomized controlled trials, by using covariates correlated with the outcome but independent of the treatment. The algorithm first trains a machine learning model to predict the outcome from the covariates, and then runs a regression of the outcome on the treatment, predicted model and their cross-product, giving the regression coefficient of the treatment as the estimator. The authors prove consistency and asymptotic normality for their estimator under general conditions. As a main strength, the proposed estimator offers variance reduction when the machine learning prediction is good, and at the same time is robust to poor predictions - in the sense that its asymptotic behavior is no worse than the baseline difference-in-means estimator even if the prediction is bad.  The authors' main contributions are: 1) From the theoretical perspective, they propose an estimator with lower variance which is robust and easy to implement, and prove the consistency and asymptotic normality. 2) From the practical perspective, their estimator performs better than the baseline difference-in-means estimator and the univariate linear regression adjustment procedure in Deng et al. (2013) when applying to 48 outcome metrics commonly monitored in Facebook.","The setting of this paper is that of a randomized controlled trial with data $(Y_i, X_i, T_i)$ for $i=1,...,N$. $Y_i$ is the response of interest, $T_i \in \\{0,1\\}$ is the treatment indicator and $X_i$ is a vector of covariates. The goal is to estimate and provide confidence intervals for the causal effect,  $$ \tau = E[Y_i \mid T_i = 1] - E[Y_i \mid T_i = 0].$$  The simplest estimator here is the difference in means estimator  $$ \hat{\tau}_{\text{diff. means}} = \text{average}(Y_i: T_i=1) - \text{average}(Y_i: T_i=0).$$  Confidence intervals can be constructed using a simple central limit theorem. One can do substantially better (shorter confidence intervals, better point estimator) by utilizing information in $X_i$ to reduce variance and there have been many approaches to do so. This paper proposes an approach that uses cross-fitting (splitting all $i$'s into $K$ folds) and arbitrary predictive models of $E[Y_i \mid X_i]$ (learned out-of-fold) and combines the results using a linear regression adjustment. The authors also show how to compute confidence intervals for the method and prove they are asymptotic valid even if inconsistent predictive models are used.  The asymptotic variance is never worse than the difference in means estimator. The authors also show substantial gains for the analysis of randomized controlled experiments at Facebook.","This paper proposes a new approach for variance reduction in randomized controlled trials. The proposed method first predicts the outcome variable Y from a set of covariates X that are independent of the treatment assignment T, and measures the average treatment effect by regressing the difference between this prediction and the observed Y against the predicted $\hat{Y}$. The method avoids the potential pitfall of invalid confidence intervals by training the predictor for a given $Y_i$ using data from $j \neq i$ (""cross-fitting""), and avoids the potential pitfall of over-reliance on poor estimates of Y by including the predictions as part of a linear regression (instead of directly subtracting the predictions from the observed Y). The consistency and asymptotic normality of the estimator are proven, and experiments on both simulated and real data are presented.",0.235,0.19,0.19,0.19534883720930232,0.19069767441860466,0.16033755274261605,0.2186046511627907,0.16033755274261605,0.2753623188405797,0.17721518987341772,0.2971014492753623,0.2753623188405797,0.22650602409638554,0.1739130434782609,0.22485207100591717,0.18584070796460178,0.2322946175637394,0.20266666666666666
200,SP:2b4f75458e0df94c986ae3bc95274345418382ae,"The paper proposes a framework to generate high resolution images of outdoor scenes with any desired style. The approach extends image synthesis techniques and proposes a system that uses both global and local information. This ensures that the model can be trained using low resolution patches (e.g. 101 x 101) and, at inference time, it can be used to generate high resolution renderings: images up to 4096 x 4096 are showed.  The combination of the global and local appearance, coupled with interesting modules such as the padding-free generator, ensure high quality results that are consistently outperforming other approaches.","This paper proposed a new model called InfinityGAN, for generating images of arbitrary sizes. These images are generated holistically: the authors proposed using a structure synthesizer first to generate a latent structural representation. Then a modified StyleGAN based texture synthesizer generates the image patches at the corresponding locations. The positional encoding and PFG enable generating patches in an arbitrary large image and aggregating these patches seamlessly. The experiments on the Flickr-Landscape dataset show promising large image synthesis performance. On the Place365 and Flickr-Scenery datasets, the experiment results show superior image outpainting performance. The authors also demonstrated other applications including spatial style fusion and image inbetweening.","This paper proposes a novel framework, InfinityGAN, which could generate arbitrary-sized images with infinite pixels.  By disentangling global appearance, local structures and textures, InfinityGAN generates local and global consistent large images with a seamless patch-by-patch paradigm.   Experiments validate its superiority compared to some other baselines, and several applications are raised based on their approaches.","This paper proposes a novel framework, InfinityGAN, for infinitely-wide landscape image generation, combining recent advances on style-based generative adversarial networks and implicit neural representations. Leveraging parallelizable inference, patch-based generative modelling and hand-crafted landscape image priors, the proposed framework can synthesize landscape images of seemingly infinite width and a large height. The paper further proposes a new evaluation metric, ScaleInv FID, and a set of applications for their framework, including image outpainting and inbetweening, and spatial style fusion. The results of InfinityGAN are validated using user studies and some image-space metrics. ",0.15,0.16,0.16,0.11214953271028037,0.18691588785046728,0.2631578947368421,0.14018691588785046,0.2807017543859649,0.16842105263157894,0.21052631578947367,0.21052631578947367,0.15789473684210525,0.14492753623188406,0.2038216560509554,0.1641025641025641,0.14634146341463414,0.198019801980198,0.19736842105263158
201,SP:2b60878cd1bcf20dcec7c4b48043b5f5a4d7d5a5,"In this paper, the authors proposed a new directed graph representation model. The proposed model is developed from a Bayesian viewpoint and is implemented in the framework of the variational autoencoder. Additionally, the authors justified the interpretability of the model. They indicate that the proposed model quantitatively captures the influence of each node on its neighbors. Experimental results show that the proposed model performs well in commonly-used datasets. ","This paper proposes a VAE-based graph representation learning model for directed graphs. The model is composed of a GCN encoder and a hierarchical latent space model decoder. The decoder explicitly models three kinds of node representations: the node latent position vectors $z$, the community membership vectors $s$ and node random factors $\gamma,\delta$. Specifically, the community membership vectors $s$ are used to gate control which of $z$ and $\gamma,\delta$ pass to the next layer. And finally the adjacency matrix is reconstructed by the distance between $z$'s filtered by node influences $\gamma,\delta$. Experiments show better link prediction and community detection performance than plain VGAE and other VGAE methods.","This paper proposed a directed graphical model called deep latent space model for modeling the graph adjacency matrix generation process, where the latent variables are organized in a hierarchical way with three meaningful components: “latent position”, “community membership” and “node random factors” for each node. The learning leverages the variational framework originated from VAE, with GCN as the core component for posterior parameterization. Experiments on link prediction and community detection show that the proposed method is able to achieve better empirical results.  ","This paper studies a deep latent space model for directed graph representation. The model is basically a hierarchical variational auto-encoder architecture and some latent variables like social activity and popularity factors of node account for the edge directions. Some latent variables are interpreted as positions in the latent space or community membership. The model, therefore, is claimed to be interpretable and superior in experiments.  ----Update after rebuttal  The authors basically answered my questions and I'd like to increase the recommendation score. I still think this submission has incremental technical contributions so my new recommendation score is 6 (marginally above the acceptance threshold).",0.3188405797101449,0.2608695652173913,0.2753623188405797,0.24324324324324326,0.25225225225225223,0.2804878048780488,0.1981981981981982,0.21951219512195122,0.18269230769230768,0.32926829268292684,0.2692307692307692,0.22115384615384615,0.24444444444444444,0.23841059602649006,0.21965317919075142,0.27979274611398963,0.2604651162790697,0.24731182795698925
202,SP:2b6dcf39f883ca5585f3af1862efc1b22d6d2862,"In this paper, the authors focus on reducing the computation cost caused by the space-time attention modeling in the Transformer framework for video recognition. Specifically, the proposal includes 1) a space-time mixing attention module that mixes channels from neighboring frames within a local window as key-value tokens, and 2) a temporal attention layer at the end of the model to aggregate global temporal information. The authors provide promising results on Kinetics-400 and SSv2 in the paper.","The authors propose a more efficient self-attention operation, specifically for video models, such that the compute requirements increase linearly with the length of the video. The author's contributions consist of two main things: 1) Self-attention along the temporal axis is limited to a smaller window instead of the whole sequence (with multiple transformer layers, the temporal ""receptive field"" grows to cover the whole video) and 2) The authors efficiently perform ""space-time mixing"" by constructing the keys and values for attention by concatenating features from different frames. This operation requires 0 floating point operations, but is still able to combine temporal information from adjacent frames effectively.  The authors show good experimental results. They achieve similar results to ViViT [1] and TimeSFormer [3] whilst using far fewer GFLOPs, on Kinetics, SSv2 and Epic Kitchens. The paper is also well written in general.","This paper presents a video Transformer model for video recognition. It limits the temporal window to the local area and proposes efficient space-time mixing to reduce computational costs. On Kinetics, Something-Something v2, and Epic Kitchens datasets,  the proposed video transformer model is more effective than other transformer-based models yet has the same computational cost as spatial-only attention models.","In this paper, the authors proposed a Video Transformer model the complexity of which scales linearly (instead of quadratic) with the number of frames in the video sequence. The key ideas are 1) restricting time attention to a local temporal window, and 2) using efficient space-time mixing to attend jointly spatial and temporal locations. They empirically verified that the proposed method outperforms existing state-of-the-art models in popular action recognition datasets.",0.325,0.1875,0.3,0.1388888888888889,0.1875,0.3387096774193548,0.18055555555555555,0.24193548387096775,0.32432432432432434,0.3225806451612903,0.36486486486486486,0.28378378378378377,0.23214285714285715,0.2112676056338028,0.3116883116883117,0.1941747572815534,0.24770642201834858,0.3088235294117647
203,SP:2b8757927fb4fb8b2327122624a5b8d19ca22087,"This work aims at improving the sample efficiency in terms of expert demonstrations in IL. In order to do so, it transforms the usual max-min objective of IRL into a single maximization objective by expressing both the reward and the policy in terms of the Q-function. The methods are supported with proofs and tested on both discrete and continuous action environments for the online setting and on simpler environments for the offline setting.","This paper proposes to get around the typical two-stage inverse RL pipeline of (1) learn a reward model and (2) optimize it, by encapsulating both rewards and dynamics in a single Q function, learned by imitation. This reframing of IRL preserves the standard properties of IRL but is simpler to optimize and avoids the need for adversarial training used by other methods. This results in a simple imitation learning method that is competitive with or outperforms reasonable baselines on a few standard benchmarks. The proposed algorithm has other nice features, including the ability to easily recover the learned reward and to be trained both offline and online, and it can be easily rephrased to obtain a method for imitation from observations.","The paper presents an IL/IRL method that avoids estimating the reward during optimization by alternating between updates of the policy and Q-function instead, which builds on recent insights from ValueDice. Indeed, the optimization problem for learning the Q-function is very similar to the one used by ValueDice, when optimizing the reverse KL. Notable differences compared to ValueDice are the generalization to a large class of divergences (including f-divergences), non-adversarial formulation, and a procedure to recover rewards. In particular, I think that the modification for recovering a state-only reward function is very interesting. The algorithm only requires mild modifications of SAC (mainly by using a different loss for the Q-function) and shows promising results in experiment evaluations.","This paper suggests an alternative approach, equivalent to adversarial IRL (that learns both the policy and the reward in an adversarial fashion), but in a direct way (learning the Q-function that encompasses both the reward and the policy). As in adversarial methods (GAIL, AIRL etc) they use the maximum entropy assumption and more explicitly, they assume that the optimal policy is greedy wrt an optimal soft-Q-function.   Using this relation between the optimal policy and the Q-function, they restrict the set of solutions to a manifold in which the objective only depends on the Q-function. They just have to minimize this objective (ensuring on the side that the policy stays in the manifold, i.e. it stays greedy) to obtain an optimal policy and Q-function. Their solution, in theory equivalent to the ones from an adversarial setting, is expected to outperform them as they avoid the learning difficulties of adversarial learning. Optionally, they can recover a reward function from the Q-function by simply inverting the Bellman operator.  ",0.26666666666666666,0.22666666666666666,0.29333333333333333,0.1721311475409836,0.21311475409836064,0.2601626016260163,0.16393442622950818,0.13821138211382114,0.12716763005780346,0.17073170731707318,0.15028901734104047,0.18497109826589594,0.2030456852791878,0.1717171717171717,0.1774193548387097,0.17142857142857143,0.17627118644067796,0.21621621621621623
204,SP:2ba027f0e9e41cfc868125e914e5a4ed9e50581b,"In this paper, the authors perform an empirical analysis of the effectiveness of Invariant Risk Minimization (IRM). They focus on the natural language inference (NLI) task, and while prior work examining the efficacy of IRM has focused on synthetic benchmarks, results from this paper support the hypothesis that models trained with IRM are generally less sensitive to spurious correlations even in naturalistic settings. The authors conduct a rigorous ablation study with various random seeds, dataset size, strength of spurious correlations, and the presence of spurious correlations and study the influence of each on the efficacy of IRM as applied to NLI. In general the paper presents a good analysis, however, certain generalizations made in the paper may not stand up to scrutiny, as I point below. I think this paper could be a good analysis paper but needs significant improvements to be accepted.","This paper designs natural language inference (NLI) experiments to investigate the properties of the bias mitigation method, Invariant Risk Minimization (IRM). The authors design three out-of-domain setups: synthetic data and synthetic bias, natural data and synthetic bias, and natural data with real bias, to provide detailed analysis on IRM and ERM, under different conditions such as changing bias strength, prevalence, data sizes, and random seeds. The paper advocates the need for more naturalistic settings to study the state-of-the-art bias mitigation models.","The paper performs a case study of applying Invariant Risk Minimization (IRM) training to the task of recognizing Natural Language Inference (NLI). The goal is to test whether IRM can be helpful for training bias-free models on biased data. The considered biases include the hypothesis bias, the word overlap bias and a special synthetic bias whereby a noisy gold label is directly appended to the input. The training environments are constructed to feature different high levels of bias strength, i.e. the degree to which the biased feature is predictive of the label B(x). The test environments are constructed in such a way that the biased feature is a bad predictor of the label. The experiments show that IRM brings a significant but small improvement on top of ERM. The paper also features extensive analysis on the impact of bias strength, bias prevalence and training data size on the performance difference between IRM and ERM.  ","The paper presents an empirical study of IRM on NLI focusing on hypothesis and overlap bias. To do this, the paper defines the notion of bias ""strength"" p_e (probability of the bias label, given that the instance is indeed biased) and ""prevalence"" alpha_e (proportion of biased samples) for environment e. In practice this is controlled by training a biased predictor that only uses the (known) bias of the input to predict the label (e.g., for hypothesis bias we can finetune BERT on hypotheses only). Training environments correspond to different values of p_e and alpha_e (e.g., (0.7, 0.82) and (0.9, 0.82)) which is necessary for IRM training. The model is then tested on in-domain/bias aligned (e.g., alpha=1 and p=1), out-of-domain/bias misaligned (alpha=1 and p=0), and unbiased (alpha=0) data. The main finding of the paper is that: (1) IRM can outperform ERM even on natural data if the test set is bias misaligned, (2) the performance of IRM is heavily influenced by p, alpha, and also data size in training.",0.15384615384615385,0.2097902097902098,0.18181818181818182,0.27906976744186046,0.2558139534883721,0.267515923566879,0.2558139534883721,0.1910828025477707,0.13756613756613756,0.15286624203821655,0.1164021164021164,0.2222222222222222,0.19213973799126638,0.19999999999999998,0.15662650602409636,0.19753086419753083,0.16,0.24277456647398843
205,SP:2be08bd3ff055015aefe58eb886d2dce74dd3550,"This paper focuses on analysis on whether the existing trajectory prediction methods indeed capture the interactions between multiple agents and uses two example methods on multiple benchmark datasets to obtain supportive results for their claims. More specifically, the contributions are three folds: 1. This paper addresses feature attribution for trajectory prediction to gain insights about the actual cues contemporary methods use to make predictions. The authors designed a variant of Shapley values that is applicable to a large set of trajectory prediction models. 2. This paper quantifies feature attributions both locally and globally and studies the robustness of given models. 3. This paper claims that existing models do not use interaction features, which is contrary to the statements made by the authors of the original papers.","This manuscript addresses the difficulty in interpreting what information a model learns from human trajectory prediction datasets. The authors propose a feature attribution method for trajectory prediction based on Shapley values. The authors also show that for commonly used datasets such as ETH/UCY, SDD, and nuScenes, models Trajectron++ and PECNet do not learn interaction information. However, on datasets featuring more interaction, such as SportVU, these models are able to learn social interactions. ","This work discusses the features that attribute to the prediction results in trajectory prediction models. They apply a Shapley values to attribute the features and results, attempting to answer a question if the social models are really social or not. The meaning of a model being social in this case is that the model can use neighbour features in order to predict the trajectory. The work defines a measurement metric called the social interaction score, in which it measures how well the interaction between agents attributes to the prediction of the other agent trajectory.  ","Different from attributing the trajectory prediction power with the success of modeling agent-agent interactions, this paper presents a new perspective that the past trajectory of the target is the only feature used for predicting its future for state-of-the-art trajectory prediction methods on standard benchmark datasets.  To analyze the contribution of different clues, this paper applies to compute the Shapley values. The proposed method modifies the value of the adjacency matrix as zero to drop features and analyze. Besides, a local-global contribution aggregation method is proposed for the problem of variable features in different scenarios.   This paper analyzes many state-of-the-art trajectory prediction models (Trajectorn++, PECNet, and Social-STGCNN) on many datasets (ETH-UCY, SDD, nuScenes, and SportVU). The experimental results on SportVU show the trajectory prediction models have the capability to learn the interactions on the datasets where interactions are more important. ",0.15873015873015872,0.18253968253968253,0.23809523809523808,0.2054794520547945,0.3561643835616438,0.2553191489361702,0.273972602739726,0.24468085106382978,0.20134228187919462,0.1595744680851064,0.174496644295302,0.1610738255033557,0.20100502512562815,0.20909090909090908,0.21818181818181817,0.17964071856287422,0.23423423423423426,0.19753086419753085
206,SP:2c776256612be33d9ae8f4da3b38810797aa4177,"In this paper, the authors intend to automatically search for the optimal auxiliary loss. To achieve this goal, the authors propose Automated Auxiliary loss for Reinforcement Learning (AARL), which uses an evolutionary search strategy to explore the very large loss space. The paper conducts extensive experiments on the DeepMind Control Suite and shows that the searched auxiliary losses have significantly improved RL performance in both pixel-based and state-based settings. ","The paper introduces an approach to automatically discover optimal auxiliary loss for RL, the method is called (AARL). The authors claim that AARL outperform baselines on both pixel-based and state-based task on the DeepMind Control Suite. Also using their method the author analyse different auxiliary losses to identify common patterns.","Reinforcement learning agents can benefit from auxiliary tasks. This paper introduces an evolutionary algorithm to automate the design of auxiliary loss functions, based on prior approaches like forward dynamics, inverse dynamics, contrastive state representation learning, etc. The evolution solves a bilevel optimization problem in which the inner loop is regular RL training while the outer loop evolves the loss function. Experiments are conducted on the Deepmind Control Suite for both pixel and state-based observations. ","This paper introduces a method for searching for the best auxiliary loss function from a huge search space automatically, while the best auxiliary loss function is defined as the one that encourages the agent to get a higher return. The searching space of auxiliary loss functions is finite and discrete. It is a combination of (1) manually designed similarity measures, which is used for encouraging the prediction and target to be similar in the auxiliary task, and (2) the binary mask, which is used for selecting auxiliary loss inputs. In this work, the size of the search space is around $4.6 \times 10^{19}$. The space is pruned firstly by random sampling from the similarity measure space and choosing the one showing the highest averaged performance, then the evolutionary algorithm is applied to select top candidates for loss inputs, which is a method in the literature. The chosen auxiliary losses are empirically shown to be helpful with increasing the learning efficiency and have generalization ability to new tasks. ",0.23943661971830985,0.29577464788732394,0.323943661971831,0.28846153846153844,0.3076923076923077,0.26666666666666666,0.3269230769230769,0.28,0.13609467455621302,0.2,0.09467455621301775,0.11834319526627218,0.2764227642276423,0.2876712328767123,0.19166666666666668,0.23622047244094488,0.14479638009049775,0.16393442622950818
207,SP:2cb8d20c32fb6bca10b66de088482fbd281b2e4b,"This paper reports several empirical findings.  1. Replacing the [CLS] tokens after the fine-tuning does not change the output representation a lot.  2. By using UMAP to analyze the patterns of nearest neighbors in different Transformer layers, it finds that the topological structure in layer 1 is most similar to the structure in layer 2 and less similar to that in layer 24. This shows that each Transformer layer would change the topological structure of the context embeddings. 3. Let's denote the embeddings in the Transformer in this way (H1) -> ATT -> (A1) -> FFN -> (H2) -> ATT -> (A2) -> FFN -> (H3) ..., where (Hk) is the hidden states, ATT is the multi-head attention, (Ak) is the output of the attention layer, FFN is the feed-forward network. Figure 3 (c) shows that (Hk) and (Ak) are very different when k is small, but become similar when k is large. Compared to (Hk) and (Ak), (Hk) and (Hk-1) are more similar when k is small. Similarly, (Ak) and (Ak-1) are more similar than (Hk) and (Ak) when k is small. ","This paper aims for providing a theory for understanding the context representation in pretrained language models. It presents a category theory then some analysis of representations from pretrained LMs. This inspiration is good.   However, this paper is severely flawed in multiple aspects: the basic notations and understanding of LMs seem to be wrong; the definitions of concepts are unclear and ambiguous; many symbols are used without definition or explanation; equations are not explained; there is barely any connections between the category theory and the language model; important related work is missed, and the experiments are quite simple and insufficient. I think this paper needs significant improvements in terms of basic understanding, writing, explanation, and experiments.","This paper intends to understand “how does a language representation model represent contexts” which is different from a more popular direction of understanding “how does the context affect the representation of the tokens”. The paper formulates the contextualized language representation problem using category theory. Authors claim that previous works that focus on substituting tokens by keeping the context fixed miss a lot of useful information. According to their proposed framework, representation of the context contains information about the representation of tokens that get substituted while keeping the surrounding conditions unchanged as well as information about the transitions (morphisms) in the representations as a result of these substitutions. However, quantifying the above information is challenging, and this paper intends to analyze this information from a functional (task-related projection) and topological (UMAP) perspective.","This paper proposes a framework, using concepts from category theory, to analyze the mechanisms through which Transformer based language models perform contextualize representations. In contrast to prior work, which analyzes contextualization of representations through “token-centric” probing, the authors first cast the language representation problem as a functor from an input category (encoding of observed context) to an output category (d-dimensional space). Next, a manifold learning approach is analyzed in the case of a fixed representation model and treating the outputs at each layer as a different representer. Lastly, the authors perform a range of experiments to better understand various aspects in transformer models including how contexts are represented for [CLS] and the change in context representations by layer.",0.11666666666666667,0.1388888888888889,0.12777777777777777,0.19130434782608696,0.1826086956521739,0.17424242424242425,0.1826086956521739,0.1893939393939394,0.19166666666666668,0.16666666666666666,0.175,0.19166666666666668,0.1423728813559322,0.16025641025641027,0.15333333333333335,0.17813765182186236,0.17872340425531913,0.18253968253968256
208,SP:2d2657c059217603c92088de7211eb1de2d5fdd8,"The authors introduce a framework to evaluate the picture-word interference in CLIP which is known to exist in human cognition. It is designed to test whether language-biased decisions occur across different category levels, and the extent to which picture-word interference in CLIP depends on the semantic similarity between superimposed words and images. Experimental results show that presenting words disturbed the CLIP image classification even across different category levels, the effect did not depend on the semantic relationship between images and words, and the superimposed word representation in the CLIP image encoder is not shared with the image representation.","This paper proposes a testbed for picture-word interference in image classification models. The authors specifically investigate the performance of CLIP to understand whether such language-biased model shows similar interferences to the ones observed in humans. The dataset consists of images superimposed with words, representing two different category levels (basic and superordinate). The experiments on CLIP show that the model is affected by superimposed words, but independently of their semantic relationship with the underlying image. Further analysis shows that CLIP image representations are different from the ones of superimposed words, while this does not happen in two ImageNet-based CNNs.","The authors investigate the behavior of CLIP when its handed images superimposed with text. The authors define a label-switched image as one for which the word being superimposed changes the classification prediction of the model. They construct a corpus of {12 superordinate} x {138 basic} x {~150 images with FMRI data} by superimposing labels over each image and then measuring CLIP's response. The authors conduct an error analysis of CLIP on these images, arguing that because CLIP's errors aren't predictable by the semantic or spelling similarity of the word/pictured object (as they are in a set of human psychology experiments), CLIP doesn't appear to process images in a similar fashion to humans.","This paper introduced a benchmark task for evaluating how semantic processing of visual word form interfere with the recognition processing of an object image in word-embedded images. By placing category word (e.g., superordinate category like 'electronic' or basic category like 'laptop') in the center of an image, the authors evaluated the 1) the misclassification rates of CLIP; 2) the semantic/spelling similarity between original and superimposed labels under different conditions; 3) the change of model prediction probability; 4) RSA on images with different types of picture-word interference. In general, this work provided interesting new insights on how language information bias the image classification in visual-language models like CLIP. The authors concluded that CLIP can recognize the visual word form as a word but fails to encode word form and object image that share the same category (e.g. word 'dog' and picture 'dog') with similar visual representations.",0.26732673267326734,0.1782178217821782,0.21782178217821782,0.2079207920792079,0.25742574257425743,0.2033898305084746,0.26732673267326734,0.15254237288135594,0.1456953642384106,0.17796610169491525,0.17218543046357615,0.15894039735099338,0.26732673267326734,0.16438356164383564,0.1746031746031746,0.19178082191780824,0.20634920634920634,0.1784386617100372
209,SP:2d6b52cdbe4309c33f610715f959b6d05eae0a46,This paper proposes a new method called Adversarial Reweighting (AR) for partial domain adaptation (PDA) where the target label space is a subset of the source label space. AR aims to estimate the weight of source instances via an adversarial training manner and Wasserstein distance and then exploits the weight to train a simple objective (re-weighted cross-entropy on source + entropy minimization on target). Results on several PDA benchmarks verify the effectiveness of AR.,"This paper presents a novel approach for partial domain adaptation (PDA) based on the adversarial reweighting of source samples. The motivation of this paper is quite clear that the source private sample may cause negative transfer in such a partial domain adaptation setting. Reweighting is a common practice to address such challenges. In this paper, the authors propose an improved version that the sample weights can be updated with the help of adversarial learning based on a discriminator. It is a smart design that shows promising results.","The paper aims to solve the challenge of 'noise' in the source domain weight for partial domain adaptation.  The paper proposes an adversarial reweighting approach to adversarially learn to reweight the source domain data for aligning the distributions of the source and target domains.  The paper conduct experiments on five benchmark datasets: Office-31, ImageNet-Caltech, Office-Home, VisDA-2017, and DomainNet and show that the proposed method outperforms the prior partial domain adaptation methods. ","The paper introduces the adversarial reweighting (AR) technique for estimating weights in the source domain. Compared to the previous methods on partial domain adaptation, the proposed method seems robust on the effect of negative domain transfer during the adaptation. Authors demonstrate the effectiveness of AR in several partial domain adaptation benchmarks.",0.24,0.2,0.21333333333333335,0.19540229885057472,0.16091954022988506,0.24,0.20689655172413793,0.2,0.3137254901960784,0.22666666666666666,0.27450980392156865,0.35294117647058826,0.22222222222222224,0.20000000000000004,0.25396825396825395,0.20987654320987656,0.20289855072463767,0.28571428571428564
210,SP:2d6f7feec8cae58420be67b2c514574c997cb995,"The paper proposes a feature selection mechanism that aims to preserve the structure of the neighborhood graph of the data. The mechanism is based on the use of stochastic gates that is trained via a regularizer, which balances (i) the goodness of fit of the selected features to the neighborhood graph Laplacian with (ii) the expected value of the number of features preserved. The numerical results show improvements in performance vs. the literature of varying levels and in most tested datasets.","This paper studies the problem of unsupervised feature selection for clustering. It proposes adding stochastic gates to the features such that those that minimize the Laplacian score are learned, and the algorithm iteratively updates the stochastic gate parameters and the random walk graph Laplacian matrix (that is computed only using the features selected by the gates). Experimental results on both simulated and real-world data (e.g. image data, single cell RNA-sequence data) suggest improvement over the naive Laplacian score and other unsupervised feature selection methods.","This paper proposes a novel unsupervised feature selection method to efficiently remove nuisance features. The model combines a graph Laplacian score with stochastic gates, which adds a re-evaluation of the graph Laplacian to improve the performance in downstream applications. Experiment results are provided to validate the theoretical claims.","This paper proposes an unsupervised feature selection method that introduces learnable Bernoulli gates into a Laplacian score. Specifically, The proposed method utilizes stochastic input gates, trained to select features with high correlation with the leading eigenvectors of a graph Laplacian computed based on these features. Finally, compared with baselines, the proposed method can improve cluster assignments using some real datasets.",0.2222222222222222,0.2222222222222222,0.19753086419753085,0.19540229885057472,0.19540229885057472,0.32653061224489793,0.20689655172413793,0.3673469387755102,0.26666666666666666,0.3469387755102041,0.2833333333333333,0.26666666666666666,0.2142857142857143,0.2769230769230769,0.22695035460992907,0.25,0.23129251700680276,0.29357798165137616
211,SP:2d8ddfdc38c54370e55a01aeaa6e2c5e4d29209b,"That paper proposes a multivariate quantile network that may be used for generative purposes. Its contribution is to generalize some scalar characterization of quantile functions (correlation maximization) to the multivariate case and propose a dual formulation of the problem that is advocated as generalizing well. Experiments are very limited, although somewhat convincing. ",This paper proposes a conditional quantile generative model using optimal transport. Theoretical results show that the optimal is convex and a conditional dual objective is further introduced. Existing results shows the efficacy and versatility of this method. ,"This paper estimates conditional quantile contours of a multivariate random vector. As shown by previous papers, his problem can be reformulated as a correlation maximization problem, and its solution exists and can be represented as the differential of a potential function by Brenier’s theorem. Solution can be found by considering the dual problem, as also shown by previous papers. This paper claims to parameterize the potential function by neural networks, which leads to a new method.","Quantile regression is frequently used as an alternative to conventional regression. An important advantage of quantile regression is that provides enough flexibility to capture the whole conditional distribution, rather than the conditional mean, of the response variable for given predictor variables. Standard approaches to tackle the estimation problem of quantile regression are based on the so-called ""pinball loss"" (quantile loss) in the univariate case, or an $L_p$-norm of the vector quantile loss in the multivariate case. This paper builds conditional generative quantile modelling to the multivariate setting. The authors propose convex potential quantile (CPQ), which is an optimal transport approach through a Kantorovih dual objective functional, which maximizes the correlation between the target distribution (dependent variable in the regression case) and the source distribution (uniform distribution over the unit hypercube). Various experiments are conducted on synthetic and real datasets, including generative modelling (MNIST and CelebA) and probabilistic forecasting.",0.19230769230769232,0.21153846153846154,0.28846153846153844,0.24324324324324326,0.2972972972972973,0.16883116883116883,0.2702702702702703,0.14285714285714285,0.09933774834437085,0.11688311688311688,0.0728476821192053,0.08609271523178808,0.22471910112359553,0.17054263565891473,0.14778325123152708,0.15789473684210528,0.11702127659574468,0.11403508771929824
212,SP:2decec4fd3ebc9f328602004e74e8079e0d07803,This paper proposes to train an energy-based model with a normalizing flow to achieve rapid and high-quality MCMC sampling.  It shows the trained CoopFlow is capable of synthesizing toy data and realistic images (reasonable compared to many baseline methods in terms of common evaluation metrics).  Applications to image reconstruction and interpolation are also included. ,"This paper develops a new generative model, called CoopFlow, by combining the energy-based model (EBM), normalizing flow (NF) and Langevin dynamics (LD). The CoopFlow updates all the parts in an cooperative manner: Parameters of NF are updated by using samples modified by LD, LD is performed based on the potential function provided by the EBM, and the gradient of log-likelihood of EBM are estimated according to samples generated by NF + LD. Here, LD can overcome the limitation of the model capacity of NF, and NF + LD can provide a more accurate estimation of the gradient of EBM loss. After learning, a powerful sampler and an accurate EBM can be obtained.","The paper proposes a generative model that consists of two components: a normalizing flow and an energy-based model with short-run MCMC as proposed in (Nijkamp, 2019). The whole procedure operates by pushing some simple distribution through the NF and then running the short-run chain on these samples. Both models are learned jointly as follows. The normalizing flow tries to capture the limiting distribution of the EBM by minimizing  $$\text{KL}(K_\theta q_\alpha \Vert q_\alpha),$$  where $q_\alpha$ is the density of the NF with parameters $\alpha$ and $K_\theta$ is the transition kernel of the short-run chain with parameters $\theta$. Simultaneously, the EBM is learned to capture the data by minimizing  $$\text{KL}(p_{\text{data}}\Vert p_\theta) - \text{KL}(K_\theta q_\alpha\Vert p_\theta).$$  In Section 4.2, the authors provide some analysis of the proposed procedure. Namely, the authors consider the perfect scenario (where the NF matches the EBM density and they both match data) and give some intuition for the case when the models are not expressive enough to perfectly match the data.  Finally, the authors provide an empirical study of the proposed procedure. They report metrics for image generation on CIFAR10, SVHN, Celeba (downsampled to 32x32); give several examples of image inpainting and latent space interpolation.","The authors propose a cooperative learning framework of normalizing flow and energy-based Langevin MCMC models (CoopFlow). Specifically, the normalizing flow suggests a better initialization for the Langevin flow, then the latter generates synthetic samples via short-run MCMC. While the Langevin flow is trained with a standard MCMC likelihood toward the data distribution, the normalizing flow chases the Langevin one by maximizing the likelihood of the synthetic samples. The proposed CoopFlow seems to outperform each of its components.",0.26785714285714285,0.4107142857142857,0.17857142857142858,0.2857142857142857,0.16071428571428573,0.13636363636363635,0.13392857142857142,0.10454545454545454,0.12658227848101267,0.14545454545454545,0.22784810126582278,0.379746835443038,0.17857142857142855,0.16666666666666669,0.14814814814814814,0.19277108433734938,0.1884816753926702,0.20066889632107024
213,SP:2df4c8e3090bfb99b40e9cdc0b0a798bbfae3988,"The paper propose to use the per-sample gradients from the biased classifier to tackle the dataset bias. Following prior work, this work adopts a three-step approach: 1) train a classifier on biased dataset 2) identify which samples are from the minority and which are not 3) train a classifier with re-sampling. The proposed per-sample gradient-based scores are used in step 2. The motivation is purely from the empirical observation that samples from the majority group tend to have similar gradient magnitude and direction. From the observation, they derive a scoring function for re-sampling. The proposed method work pretty well empirically compared to various baselines.  Aside from the novel scoring function, since the noisy labeled data might be accidentally considered as minority group, the paper introduces a de-noising step to tackle the potential noise in the dataset. The de-noising step further improves the performances.","This paper proposes a gradient-based resampling scheme for de-biasing. The authors construct two types of scores that leverage gradients of the biased model: the magnitude and the direction of the gradients. In addition, to mitigate the side effects from the noisy labels, the authors also propose a de-noising module, which can be easily applied to any de-biasing algorithms. Extensive experimentation validates the proposed method against other state-of-the-art methods. ","The paper proposes a de-biasing method for neural network without requiring explicit annotations of biases. The authors propose a resampling-based approach that automatically detects underrepresented samples based on the direction and magnitude of their loss gradients, then balances the dataset by rejection sampling. The paper further proposes a denoising module to identify and filter out noisy samples under the presence of label noise. The gradient-based de-biasing method is empirically shown to outperform prior work on both controlled synthetic and real-world datasets.","The paper proposes a new bias-mitigation technique. The main hypothesis of the paper is that there are differences in gradients for ""biased"" samples (or samples that are ""rare"") compared to majority patterns in the training data. Using this, the paper devises a rejection sampling method that tries to balance samples in a minibatch. However, a sample with a noisy label can appear to be a ""biased"" sample (with a correct label) which can affect the proposed method (as well as many other bias mitigation methods). To this end, the paper also proposes a denoising module that successfully eliminates the effects of noisy labels on the debiasing algorithm proposed.",0.1456953642384106,0.13245033112582782,0.16556291390728478,0.26666666666666666,0.28,0.2441860465116279,0.29333333333333333,0.23255813953488372,0.22935779816513763,0.23255813953488372,0.1926605504587156,0.1926605504587156,0.19469026548672566,0.1687763713080169,0.19230769230769232,0.2484472049689441,0.2282608695652174,0.2153846153846154
214,SP:2e2d4921dd27bc1bc73a680f0dac9f626fd86d02,"This paper proposes a Parameterized AP Loss to better align the network training and evaluation in object detection. The main technical contributions include: 1) It proposes a parameterized loss function by reformulating the AP metric and replacing the non-differentiable component with parameterized functions. 2) It automatically searches for the optimal parameters of the proposed loss function through a reinforcement-learning-based search process. 3) Finally, it demonstrates competitive results on COCO. ","Authors propose a loss for object detection, that is a relaxation of mAP suitable for use in gradient ascent. Where AP includes Heaviside (step) functions, the authors replace these by learnt piecewise-linear functions. These functions are trained to maximise AP on a held-out set achieved by a detection model that was itself trained using them as the loss (so they do not aim to approximate AP closely, but to derive a surrogate loss that is 'structurally similar' to AP and achieves good training performance). Experiments on COCO 2017 show accuracy improvements over baselines (trained with focal/XE + regression losses) and other techniques approximating AP.",This paper proposes a parameterized AP loss. Parameterizd functions are introduced to substitute the non-differentiable components in the AP calculation. Then automatic parameter search algorithms can be employed to search the optimal parameters.,"This paper describes a technique for estimating a loss for object detection that is well-aligned with the AP loss used at evaluation time. The work builds on a prior formulation of the AP loss as a function on classification scores, but converts the formulation to account for per-bounding box results, and also replaces the Heaviside function with parametrized, searchable functions. Experiments show the gain from the proposed technique using three backbone detectors, and compares to different prior losses.  Post-rebuttal: The rebuttal addressed my concerns.",0.18055555555555555,0.20833333333333334,0.25,0.07547169811320754,0.1792452830188679,0.29411764705882354,0.12264150943396226,0.4411764705882353,0.20689655172413793,0.23529411764705882,0.21839080459770116,0.11494252873563218,0.14606741573033707,0.2830188679245283,0.22641509433962265,0.1142857142857143,0.19689119170984457,0.16528925619834708
215,SP:2e5368621bb60469988f5e56b4b27e085137af0e,The paper studies selective prediction where abstention incurs a fixed and known cost. It extends the Fundamental Theorem of Statistical Learning to selective classification and linear regression under distributional shift. The paper also proposes min-max abstention reduction with optimal guarantees in test loss.,"The paper considers prediction with the abstention option, with a reduced loss for abstention (Chow's model), in a setting where the test distribution can be different from the training distribution. The setting is very similar to GKKM20 except that the loss here is different (GKKM separately bounds standard risk and amount of abstention, current paper minimizes a weighted combination), and current work considers linear regression in addition to binary classification. The paper presents efficient algorithms given ERM oracles and bounds the Chow's loss of their procedures for both binary classification and linear regression. The technique involves setting abstention to minimize the abstention-equipped loss, by using a separation oracle that needs an approximate maximizer of the loss. The upper bounds depend on the variational distance between the test and training distributions plus a term which decreases with sample size.","This paper considers the abstention model in which a classifier is able to abstain from predicting (and thus paying a fixed cost) as opposed to being forced to give a prediction.   They consider a formalism in which the algorithm is given a classifer $f$ with 0 training loss, and it must make predictions for an additional $n$ test points. The classifier incurs a loss of 1 for each error it makes on the test points, but incurs loss $\alpha$ if it chooses to abstain (where $\alpha \leq 1$). Furthermore, the points in the test set are drawn from some distribution that differs from the original training distribution.   In this formalism, they give a generalization bound of a polynomial time algorithm that uses empirical risk minimization as a subroutine. The basic gist is that the overall expected lost can be bounded by the total variation between the training and test distributions along with a $\frac{O(d\logn}{n}$ term where $d$ is the VC-dimension of the classifiers.   They then apply their ideas to the setting of adversarial classification, in which adversarial examples can be thought of as being drawn from a distribution $Q$ that is very close to the original data disribution $P$. They also apply their theorem to linear regression giving a bound on how well a linear classifier can be learned. ","This paper studies agnostic transductive learning when train and test distributions have covariate shift. The objective follows Chow 57', where abstention suffers a fixed cost, and the goal is to minimize total costs of prediction error and abstentions. The paper proposes efficient algorithms for linear regression and binary classification, based on ideas from Goldwasser et al. The paper also proves lower bounds showing that the guarantee is near-optimal.",0.36363636363636365,0.2727272727272727,0.36363636363636365,0.24113475177304963,0.1347517730496454,0.08035714285714286,0.11347517730496454,0.05357142857142857,0.2318840579710145,0.15178571428571427,0.2753623188405797,0.2608695652173913,0.17297297297297298,0.08955223880597014,0.2831858407079646,0.18630136986301368,0.18095238095238098,0.12286689419795223
216,SP:2eb855e384207458ad5edcf308f4164b0c7746fd,"This paper proposes a new approach to improve the performance of non-linear predictors (DNN models) on binary labels when the observations are endogenously selected. The issue arising in traditional setups is that the algorithm will only select observations with high positive outcome (reward). This may lead to sub-optimal and unfair models generating a large proportion of false negatives. The solution proposed in this paper is to add optimistic labels (with positive rewards) to a subset of points that the model would normally reject. Doing so optimistically bias the model, and stimulate the exploration of the more uncertain regions. The suggested algorithm and paper focus is on online learning methods. Several experiments highlight the competitiveness of this approach compared to similar ones.","The paper studies class of problems, called bank loan problem (BLP), where the learner only observes whether a customer will repay a loan if the loan is issued to begin with. To solve this problem, the paper proposes a simple and efficient algorithm, Pseudo-Labels for Optimism (PLOT), which adds an optimistic label to a subset of data points rejected by the model and trains the model on all the data seen so far including the optimistically added points. Empirical evidence shows that PLOT achieves competitive performance in comparison to known storng baselines such as NeuralUCB.","This paper first motivates the ""bank loan problem"", which is a scenario where the learner only observes whether a customer will repay a loan if the loan is issued to begin with. The paper moves on to discussions of this problem and introduces contextual linear bandit formulations. This is extended to the case where the model is a deep neural network, and a Theorem shows theoretical guarantees under the case where data is separable. Experiments show improvement over baseline methods, including the recent NeuralUCB method.","Authors propose neural pseudo-label optimism for solving the bank loan problem. This is an interesting setting and I judge the work in the paper (theory & experiments) to be mostly correct. I consider this paper to be sufficiently novel, although there are certainly a large number of papers on the bank loan problem. I am not an expert on the bank loan problem in particular, so I hope another review can provide more clarity as to this paper’s novelty. I am tentatively voting for a rating of 7, but I would like the authors to address some of my concerns below.",0.17886178861788618,0.17073170731707318,0.14634146341463414,0.3854166666666667,0.1875,0.17647058823529413,0.22916666666666666,0.24705882352941178,0.17647058823529413,0.43529411764705883,0.17647058823529413,0.14705882352941177,0.20091324200913244,0.2019230769230769,0.16,0.4088397790055249,0.1818181818181818,0.16042780748663102
217,SP:2f0594e69fa4f06f3ab3a6797406e838ae365dcb,"This paper aims to improve the quality of the parametric bootstrap when the number of particles is limited. It does so by constructing a small collection of ""centroids"" that well-approximate the ideal bootstrap distribution on parameter space in a certain Wasserstein distance. Practically, the centroids are constructed by gradient descent with respect to a loss function involving the negative log-likelihood. Theoretically, this paper shows that the surrogate loss converges to the Wasserstein distance under some assumptions. Empirically, this paper applies the centroid methodology to confidence interval construction, contextual bandits, the bootstrap deep q-network, and ensemble learning .","This paper proposes an acceleration technique for bootstrap. Here a bootstrap particle corresponds to a machine learning model and for a large number of bootstrap particles, a huge computational cost is incurred. The authors propose using a k-means-like centroid approximation which optimizes a set of centroids (each of which corresponds to a machine learning model) minimizing a data-dependent Wasserstein distance.","The paper proposes a way to optimise the choice of Bootstrap samples so as to obtain efficient inference with smaller number of such samples. While traditional Bootstrap requires the need to use a large number of samples to approximate the target distribution, the authors propose a combination of k-means type algorithm is association with Bootstrapping to achieve the goal. In addition, they provide a theoretical justification for the method along with providing some experimental evaluation .","Bootstrap is a versatile technique for uncertainty quantification and a viable alternative to Bayesian inference, but it can be expensive in terms of memory and computations. With a small number of particles/samples, bootstrap may perform poorly. As a consequence, bootstrap can be quite expensive and even unaffordable for deep learning problems with huge models. The paper proposes to address the question of improving the accuracy of bootstrap inference when the number of particles is limited.   The approach is to minimize the Wasserstein distance between the ideal bootstrap distribution (denoted by $\rho_{\pi}$ in the paper) and a probability mass mass function supported on a small number of particles (called ""centroids"" in the paper). The exposition given around equations (3) and (4) plausible decomposes this problem into a coupled two-stage optimization problem. Unfortunately, $\rho_{\pi}$ is not observe, which is a major challenge to this conceptual development. The paper then relies on the fact that in standard problems when training data size is large, the bootstrap obtains properties similar (or identical) to the MLE or Bayes estimators, and the algorithmic steps are base don this framework. ",0.1414141414141414,0.18181818181818182,0.31313131313131315,0.2698412698412698,0.31746031746031744,0.3026315789473684,0.2222222222222222,0.23684210526315788,0.1657754010695187,0.2236842105263158,0.10695187165775401,0.12299465240641712,0.17283950617283947,0.20571428571428568,0.2167832167832168,0.2446043165467626,0.16,0.17490494296577946
218,SP:2f4ea4ae923024fcf12fd672468e4fe5f13e8432,"The paper presents a general Blackwell approachability based framework for online fair learning in the presence of stochastic (sensitive and non-sensitive) contexts. The framework allows to easily obtain both positive and negative fair learnability results. The authors work out their framework in the standard cases of no regret or calibrated online learning, paired with demographic parity constraints. They also present novel optimal tradeoffs between the  constraints in the case of group-calibrated learning subject to demographic parity. ","The authors aim to provide a framework, extended from Blackwell approachability, to study online learning with fairness constraints, in the presence of stochastic contexts. The authors assume that the distribution of contexts is unknown and needs to be estimated over time. They use the approachability framework to provide conditions under which certain learning objectives are compatible with certain definitions of fairness.","The paper studies an online setting formulated as a game between Player and Nature. On each round, a context is drawn stochastically, consisting of non-sensitive and sensitive parts. Player may only observe the non-sensitive part before making a decision, while both cases of Nature observing the entire context or only the non-sensitive part are discussed. After Nature’s decision, a reward based on the context and two decisions is given to Player. Using this formulation, the authors wish to unify previously suggested approaches to fair online learning, adapting Blackwell’s Approachability theory to handle unknown contexts’ distribution. The suggested framework assists them then in studying the compatibility of previously considered objectives [(vanilla/group-wise) no regret, (vanilla/group-wise) calibration] and fairness constraints (statistical parity, equalized average payoffs), where they provide a general sufficient and necessary condition for such compatibility, and study several specific instantiations. ","The paper looks at the online learning problem with different fairness objectives/constraints from the lens of Blackwell Approachability. Doing so allows one to see what fairness objectives (e.g. group-wise calibration/regret) with possibly additional fairness constraints (e.g. statistical parity) are compatible and immediately give an algorithm, if compatible.   Also, they have given a pretty tight characterization of the pareto frontier for group-wise calibration under demographic parity constraint.  Finally, they give how to try to approach a set where some of the parameters characterizing the set is unknown.",0.24358974358974358,0.2564102564102564,0.20512820512820512,0.21311475409836064,0.22950819672131148,0.12837837837837837,0.3114754098360656,0.13513513513513514,0.17391304347826086,0.08783783783783784,0.15217391304347827,0.20652173913043478,0.27338129496402874,0.17699115044247787,0.18823529411764706,0.12440191387559808,0.18300653594771243,0.15833333333333333
219,SP:2f7d032dfdfd7d4c509afa2e4bc2042dbceb6885,"This paper presents a new evaluation metric for natural language generation models based on the discriminator that distinguishes between human-written texts and models generated texts, OUMG. To train OUMG, the authors bring two different generation models and treat them as negative samples. After the training OUMG, the authors suggest an evaluation method by computing the high score rate difference and upgrade rate among the two models. The authors also suggest guiding text generation by OUMG. Experiments show that trained OUMG can identify human-written texts against model-generated texts. And it can identify texts that a poor model generates. ","The paper describes OUMG, a model-based metric for evaluating text generator systems which does not rely on reference standards or gold-standard data. The model is trained similar to a conventional GAN or Turing test-like setup: a discriminator is trained with human-generated and artificially generated sentences to produce a soft-output (probability) to judge if an input text is human-generated or not. After which, the model can be applied as a guide for generating text autoregressively in the hopes of producing texts of better quality. The metric is tested only on a Chinese poetry generation task with one dataset and a single model architecture (Bi-LSTM) both coming from the SongNet study (Li et al, 2020). ","The paper proposes a new evaluation metric, OUMG, to measure human-likeness of machine-generated text without reference. The key idea of the method is to train a discriminator on both human-generated texts (as positive) and machine-generated texts (as negative) and use the discriminator to assign a score to a given sentence, which represents the similarity between the sentence and a set of sentences written by humans provided at training time.  The paper considers a pairwise comparison setting: any pair of models are compared by training such a discriminator on the outputs of those two specific models, and proposed metrics are used to measure the *relative* goodness of one model’s outputs with respect to the other model: (1) which model’s outputs have higher ratio of sentences with scores > 0.5 (i.e. HSRD) and (2) which model’s outputs have higher scores compared to the other model (i.e. UR).  The experiment considers the task of Chinese poetry generation and uses SongNet (Li et al., 2020) as a default model. The first half of the experiment intentionally makes the default model worse and demonstrates that OUMG metrics can show the default model performs better than the intentionally degraded model. The second half of the experiment uses the scores from the discriminator to adjust logits from the default model in order to improve the human-likeness of generation. The reduced accuracy of the discriminator after training on such outputs is interpreted as the evidence that the guided generation produces more human-like texts.  The main contribution of this paper is to frame text evaluation as pairwise comparison between two models and use relative metrics to measure the performance gap between two models. The authors claim that this approach addresses some of the limitations of previous metrics based on string matching, word embedding, and language models such as superficial matching, reliance on reference, narrow focus on fluency (Section 2).","An NLG metric for problems where there is many plausible options for the generated text (e.g. story telling, poetry generation, dialog continuation) is proposed.  Results are reported on one one dataset, but the paper is very poorly written and makes a lot of claims which are not correct. ",0.19,0.3,0.09,0.2833333333333333,0.09166666666666666,0.037383177570093455,0.15833333333333333,0.09345794392523364,0.1836734693877551,0.1059190031152648,0.22448979591836735,0.24489795918367346,0.17272727272727273,0.14251781472684086,0.12080536912751678,0.15419501133786848,0.1301775147928994,0.06486486486486485
220,SP:2fd1a61d9cd658ce7a5d5c33f083593906dc2f3c,"Data parallel distributed NN training often depends on homogenous, well-provisioned networks and reliable end hosts.  This paper presents a scheme, Moshpit All-Reduce, that leverages a hybrid approach to communicating parameters (gradients) that combines elements of gossip with all-reduce to better withstand heterogeneous network environments and node failures.  The authors present their technique, and integrate it into a distributed SGD scheme (Moshpit SGD).  Microbenchmarks validate the extensive theoretical exploration of MAR, and the authors compare/contrast the system's effectiveness on both well-connected and distributed/heterogenous infrastructure.  They find that the scheme can tolerate varying network latencies and often more quickly converges by shortening training during individual epochs.   ","This paper presents a new collective algorithm and associated implementation of SGD training that is optimized for distributed systems where some ranks will not participate in each training step.  The observation is that parameter servers are not scalable, and that most collective based (all-reduce) distributed algorithms are either not fault tolerant, or have significant performance impacts when there is load imbalance or system noise.  The proposed Moshpit all-reduce and SGD strive to overcome these challenges by breaking down the collective communication domains into many more ""sub-domains"" that can tolerate delays or absences of its participants.  However, it does so in a structured manner that is more communication efficient than a standard gossip, random, algorithm.  They show that despite stale gradients for each participant, the algorithm still converges and does so faster than SOTA algorithms.","This paper introduces Moshpit All-Reduce, a fully decentralized iterative averaging protocol for large-scale training under unstable communication conditions and/or with unreliable workers. The proposed protocol allows an exponential convergence to the global average. It is shown that Moshpit SGD, built on top of Moshpit All-Reduce, is equivalent to the centralized SGD in terms of iteration complexity.","The paper introduces a decentralized averaging scheme and uses it for distributed learning. The averaging scheme falls somewhere in between typical gossip averaging and uniform all-reduce averaging. It inherits some of the robustness to node failures from typical gossip averaging and some of the communication efficiency from all-reduce collectives.  For each communication step, nodes are divided into groups using a decentralized hash table, and perform all-reduce averaging within their group. If a node in one group fails or takes too long to respond, other communication groups can still complete their averaging reliably.  In the light of prior work, this method could be classified as a linear time-varying averaging scheme, with block-uniform averaging matrices.  What I personally consider the most interesting contribution of the paper, is the match-making mechanism used to group the workers. By numbering the nodes in each group at timestep $t$, and using those numbers to group the nodes in the next round, workers are never grouped with the same peers in two consecutive communication rounds. I believe this is what gives the algorithm its name 'MoshPit' (although an explanation of this is missing from the paper).  The paper contains convergence rates for the algorithm with random grouping (not with moshpit match-making), and experiments show that the algorithm can reduce training time and cost, mainly in heterogeneous environments (bandwidth or compute capabilities).",0.15315315315315314,0.13513513513513514,0.21621621621621623,0.10948905109489052,0.19708029197080293,0.3333333333333333,0.12408759124087591,0.25,0.1038961038961039,0.25,0.11688311688311688,0.08658008658008658,0.13709677419354838,0.1754385964912281,0.14035087719298245,0.15228426395939088,0.1467391304347826,0.1374570446735395
221,SP:2feff475074743c25080ebf58e27bbaf80549cca,"This submissions introduces RASF (Representation Agnostic Shape Fields), an embedding layer that encodes local geometry and can be used for 3D deep learning with different input domains: point clouds, meshes or voxels. RASF is inspired by the idea of embedding layers in language models, where representations of tokens are indexed by one-hot vectors.   In this submissions the authors propose to implement RASF as volumetric latent embedding that is indexed via tri-linear sampling. For example, given an input pointcloud the process first extracts a local neighborhood around a point p and normalizes it, so that the point p becomes the origin. This pointcloud is then used to sample the volumetric latent embedding and afterwards a max-pooling operation is applied. The max-pooled feature becomes the representation of point p.  The experimental results show improvements when RASF is used on top of several baselines while incurring on a small additional computational complexity.","This paper presents a method for 3D shape analysis by means of local shape embedding.  The key idea is to use a learnable multi-channel 3D grid to embed local shapes in the input 3D object, similar to word embedding in NLP; hence, the method can be pre-trained and applied for various downstream tasks.  The procedure is: (1) If the input is mesh-based or voxel-based, first generate sample points on mesh or in voxels; if input is point-based, no need to have this pre-processing; (2) For each point p, find a set of K nearest neighbors points, i.e., P_neigh; and (3) normalize { p, P_neigh } and take the result as an index to retrieve shape features from the 3D grid, etc.  The paper claims that the proposed method is generalizable (i.e., could be used in different 3D representations, backbones and downstream tasks) and computation-efficient shape embedding layer for 3D deep learning.","This paper presents a generalizable shape embedding layer for 3D deep learning. The proposed shape embedding can be used for a number of 3D shape representations including point cloud, mesh, and voxel. The shape embeddings can be obtained via self-supervised learning, where the paper has experimented with shape reconstruction and normal estimation as the pre-training tasks. The pre-training would enable the proposed method to learn general shape embeddings. At deploy time, the 3D priors are retrieved using coordinate indexing. The experimental results have shown that the proposed method can provide a boost on various representations in different applications.","This paper proposes a method for learning a latent spatial embedding of points in space to a feature space by pre-training on a pretext task (e.g., reconstruction). This embedding can the be used as part of any deep learning pipeline that inputs point coordinates as part of its architecture by first mapping the coordinates to their embedding. The authors demonstrate that utilizing these embeddings improves results of state-of-the-art learning methods on point clouds, meshes, and voxel grids.",0.1568627450980392,0.1503267973856209,0.11764705882352941,0.18125,0.125,0.1782178217821782,0.15,0.22772277227722773,0.21951219512195122,0.2871287128712871,0.24390243902439024,0.21951219512195122,0.15335463258785942,0.1811023622047244,0.15319148936170213,0.22222222222222224,0.1652892561983471,0.19672131147540983
222,SP:2ff61c45468a6d128fc76ee6372656a85e951857,"This paper proposes a way to share data across different tasks in meta-reinforcement learning (meta-RL), where the data from one task is reused in another task by relabeling the rewards. Based on the HIPI method ([1]), the authors construct a relabeling distribution to relabel the pre-adaptation trajectories from one task to be used for another task. The relabeling probability of a trajectory is chosen to be proportional to the exponentiated utility function, which is defined as the expected return after the agent uses that trajectory to adapt. In practice, the post-adaptation return is approximated using the learned Q function. The authors apply this relabeling distribution to PEARL, an existing off-policy actor-critic style meta-learning algorithm.  The authors conduct experiments on simulated robotics experiments. The results suggest that the proposed method outperforms prior methods on sparse reward tasks, while performing roughly the same on dense reward tasks.    References  [1] Eysenbach, Benjamin, et al. ""Rewriting history with inverse rl: Hindsight inference for policy improvement."" arXiv preprint arXiv:2002.11089 (2020). ","The paper proposes a trajectory relabeling method for meta Reinforcement Learning (meta-RL), aiming to share some of the collected trajectories to improve sample efficiency during meta-training. The relabeling method is built on HIPI (Eysenbach et al., 2020). Instead of relabeling the trajectory based on the total reward as in HIPI, the paper argues that in meta-RL, the metric of interest for trajectories from different tasks is their usefulness for task-identification rather than returns. The paper further proposes a meta-RL algorithm based on PEARL (Rakelly et al. 2019). The experimental results on several sparse-reward tasks show that the method outperforms other relabeling methods as well as PEARL.","This paper studies task relabelling in hindsight to increase the efficiency of meta-reinforcement-learning. The authors propose a strategy for calculating a distribution of tasks for which a particular batch of data would be useful for adaptation, and sample from this distribution to construct a relabelled batch which augments the training data. The authors show empirically that this improves sample efficiency over more naive relabelling schemes, particularly for sparse reward tasks. A series of ablations further justifies several design decisions or investigates robustness to hyperparameters.","The paper proposes an approach for using data relabelling for meta-RL for better sample efficiency and to enable training on sparse reward environments. Specifically the proposed method combines Pearl[1] with a modified version of HIPI[2], where the trajectories chosen for relabelling are effective for adaptation, and not necessarily high in reward themselves.  [1] : Efficient Off-Policy Meta-RL vis Probabilistic Context Variables (Rakelly et al.) [2] : Rewriting History with Inverse RL (Esyenbach et al.)      ",0.19540229885057472,0.13218390804597702,0.13793103448275862,0.13392857142857142,0.1875,0.12790697674418605,0.30357142857142855,0.26744186046511625,0.3116883116883117,0.1744186046511628,0.2727272727272727,0.14285714285714285,0.23776223776223773,0.1769230769230769,0.19123505976095617,0.15151515151515152,0.2222222222222222,0.13496932515337423
223,SP:308ae10248303c404bac425849e56fc344e17a84,This paper proposes a model to facilitate the few-shot semantic segmentation task by utilizing pixel-wise relationships between support and query images. The authors use a cycle-consistent attention mechanism to perform cross-attention between features from support and query images. Experiments on the two commonly used benchmarks validate the effectiveness of the proposed method.,The paper addresses the few-shot semantic segmentation problem with cycle-consistent transformer (CyCTR) that utilizes pixel-wise support information for query prediction. The CytCTR estimate the cross-attention between support and query images with the cross-consistent attention mechanism. The proposed method achieves the start-of-the-art performance on multiple datasets.,"The paper introduces a new transformer architecture designed to tackle image segmentation in the few-shot setting. The new architecture seems to make it easier for the pre-trained model to generalise to new classes and produce more accurate segmentation masks. The comparison with benchmarks shows incremental improvements on PASCAL-5i and COCO-20i in 1-shot and 5-shot setting, with two different convolutional backbones.  Due to minor yet numerous issues the paper is currently below the acceptance bar, but I am willing to improve my score if these are addressed.",The paper introduces a novel module called Cycle-Consistent Transformer (CyCTR) that aggregates pixel-level support features into query features via cross-attention. Cycle consistency filters out irrelevant pixel-level support features. CyCTR achieves SOTA results on Pascal-5 and COCO-20 by an appreciable margin.,0.39285714285714285,0.23214285714285715,0.16071428571428573,0.22641509433962265,0.2830188679245283,0.09782608695652174,0.41509433962264153,0.14130434782608695,0.1956521739130435,0.13043478260869565,0.32608695652173914,0.1956521739130435,0.4036697247706422,0.17567567567567563,0.17647058823529413,0.16551724137931034,0.30303030303030304,0.13043478260869565
224,SP:313ee5210d425d3d2420a098669d32234190469a,"The authors studied model stitching as a methodology to examine the internal representations of neural networks. Particularly, they use model stitching to verify several statements such as “good networks learn similar representations” and “representations learned with (1) more data, (2) bigger width, or (3) more training time can be “plugged in” to weaker models to improve performance"". In addition, They showed that SGD has a structural property akin to mode-connectivity: typical minima reached by layers trained via SGD can all be stitched to each other with minimal change in accuracy, termed as  “stitching connectivity”. ","The work proposes to utilize model stitching as a tool to compare the quality of learned neural representations. Authors show that model stitching based comparison can avoid many problems often observed in other representation comparison methods, such as CCA/CKA. Using this tool, under image classification setting, authors empirically show that (1) networks with different random initializations would converge to ""similar"" representations, (2) networks trained via different methods also have ""similar"" representations given these networks have close performances, and (3) networks trained with more data consistently have better quality. ","Understanding how the representations of neural networks, trained in diverse settings, compare and can be combined is an important problem in representation learning. This papers addresses this problem by studying model stitching (Lenc & Vedaldi 2015) in the following experimental setting: connect the bottom-layers of A to the top-layers of B with an inexpensive trainable layer, where A and B are trained neural networks. The contributions are:  1) Empirical confirmation of inutitive statements: such as:   * representations trained with more data and training time or with larger width can be stitched to representations trained with less resources;  * representations trained with different objectives (supervised vs. self-supervised) can be stitched together with minimal penalty;  * early layers of representations are similar even when the labels used for training are different.  2) Observation of a new structural property of SGD, defined as ""stitching connectivity"" by the authors: typical minima reched by SGD can be stiched together without changing accuracy much.      ","The paper use model stitching as a way to examine the hidden states of neural network models. The proposed method stitches the top and bottom of two similar models together with a linear transformation (if you don't take BatchNorm into account). Experiment results show that the hidden states, learned from different initializations, different subsets of the training set and even different losses can be translated from one to another. Such that the top of one model can use the hidden state given by another model to predict the same label as the original top model. Based on this observation the paper proposed stitching connectivity, a measurement of similarity between semantic space of hidden states.",0.21052631578947367,0.3368421052631579,0.23157894736842105,0.19101123595505617,0.16853932584269662,0.1337579617834395,0.2247191011235955,0.20382165605095542,0.19130434782608696,0.10828025477707007,0.13043478260869565,0.1826086956521739,0.21739130434782608,0.25396825396825395,0.20952380952380953,0.13821138211382114,0.14705882352941177,0.15441176470588236
225,SP:315cac0ca081d8ecedbdfa8b5200c924decf8f5e,"This paper proposes a new setting called weak-shot fine-grained classification, which contains base classes with clean labels and novel classes with noisy labels. To boost the performance of the noisy novel classes, the paper adopts sample weighting and similarity regularization for better tackling the noisy issue. The experiments show the superiority of the proposed method.","This paper proposes an interesting setting, dubbed as weak-shot learning, using limited label resources to recognize novel categories (weakly labeled with certain noises) on the web.  The key challenge in weak-shot learning is label noise in the novel category set. This paper proposes to transfer pairwise similarities learned from a clean base set to the novel category set. Such similarities are class-agnostic and easy-to-learn, which is believed to be highly transferable between sets. To further augment the proposed method with fewer noise perturbations, two strategies are designed: 1) sample weighting and 2) graph regularization, which are also evaluated with ablation studies and show significant advances. The proposed method is well evaluated with extensive experiments, and the results verify the effectiveness. ","This paper addresses a new task called weak-shot learning. It is to learn novel categories by using the knowledge learned in the clean set of base categories. The clean set of base categories refers to the clean, well-labeled image classification dataset.   The proposed method can be summarized into two components. First, the authors train a SimNet on the clean base dataset. It is to learn to capture the semantic concepts from the images. Secondly, the authors conduct supervised classification training on the novel images using similarity labels, where the similarity labels are obtained from the similarity matrix computed by the pre-trained SimNet. To address the label noise in the unseen dataset, sample weighting and graph regularization are employed to better leverage the similarity matrix in training.","The paper proposes a simple approach using sample weighting and graph regularization for weak-shot fine-grained classification and demonstrates promising results on CUB, CAR and FGVC datasets. In weak-shot classification, training samples for novel classes are harvested from web search. The paper presents includes results from various experiments most of which give intuitive conclusions such as (1) more categories or images per category in the training data is better to get improved performance metric on novel set (2) Similarity model is better when trained on clean data with known base classes and gets worse when we use noisy data with novel categories.",0.3508771929824561,0.3508771929824561,0.3157894736842105,0.24,0.144,0.12403100775193798,0.16,0.15503875968992248,0.17307692307692307,0.23255813953488372,0.17307692307692307,0.15384615384615385,0.21978021978021978,0.2150537634408602,0.22360248447204967,0.2362204724409449,0.1572052401746725,0.13733905579399144
226,SP:315f2489ecb71d26cd5b31e7a127b4d6c580be7f,"In this paper, the authors study benign overfitting in multiclass linear classification. They show the equivalence between one-vs-all SVM and multiclass SVM with either cross entropy or squared loss under a sufficient condition. Furthermore they show that when overparametrization occurs, the sufficient condition holds with high probability. ","“Benign overfitting” is a recently revealed phenomenon that high-dimensional models achieving zero training loss can still have good generalization. Existing works studying benign overfitting are mostly focused on the regression setting or the binary classification setting. In this paper, the authors establish benign overfitting guarantees for multi-class classification problems. It is shown that empirical risk minimization with cross-entropy loss is equivalent to the min-norm interpolating (MNI) solutions and the one-vs-all SVM classifier, and then derive error bounds on the accuracy of the MNI classifier. ","The authors aim to establish the equivalence of multiple loss functions in training and to analyse the conditions for benign overfitting in the rigime of multiclass classification. The paper proposes a crucial condition under which the solutions of multiclass SVM, one-vs-all SVM and min-norm interpolating (MNI) classifier are equivalent. It is proven that this condition holds in high probability in both Gaussian mixture model and multinomial logistic model. Finally, the paper gives the genalization bound of the MNI classifier based on its equivalence with SVM, as well as the condition for benign overfitting.","The paper studies the problem of multiclass classification in the overparameterized regimes. The work derives a sufficient condition under which multiple multiclass SVM classifiers converge to the min-norm interpolating (MNI) solution, extending a recent result ([MNS+ 20]) beyond the binary classification setting. The paper then shows that for certain configurations of the Gaussian mixture model and multinomial logistic model, this condition holds under high enough effective overparameterization. Finally, the paper derives non-asymptotic bounds on the error of the MNI classifier with multinomial logistic data and discusses sufficient conditions for the multiclass SVM to satisfy benign overfitting. ",0.30612244897959184,0.3673469387755102,0.2653061224489796,0.2222222222222222,0.2222222222222222,0.375,0.16666666666666666,0.1875,0.1326530612244898,0.20833333333333334,0.20408163265306123,0.3673469387755102,0.21582733812949642,0.24827586206896557,0.17687074829931973,0.2150537634408602,0.2127659574468085,0.37113402061855677
227,SP:3178e1c996adddb6255206a895a95463ba5e1e8e,"This papers proposes new conditions for identification of latent temporal causal processes, for both non parametric and parametric processes. They then propose an architecture emboyding the derived conditions such as to recover the latent temporal processes. The approach is shown to give better performance than compared baselines on synthetic data and to show good performance on non-synthetic datasets.",This paper propose two new conditions for nonlinear identifiability of temporal processes. The idea is very promising: discovery of conditional independent sources with time-delayed causal relations in between.  The proof and the code seem good.,"The authors proposed a method to recover time-delayed latent causal variables and identify their relations from measured temporal data called Latent tEmporally cAusal Processes estimation (LEAP). This method leverages VAEs by enforcing our conditions through proper constraints in the causal process prior for recovering time-delayed latent processes from nonlinear mixtures without sparsity assumptions. The experimental results on synthetic datasets show that the proposed method outperformed baselines that do not leverage history or nonstationarity information. The results on public datasets including KiTTiMask, Mass-Spring System, and CMU MoCap demonstrate that temporally causal latent processes were reliably identiﬁed from observed variables under different dependency structures. ","The paper considers the learning of a temporal latent causal process: x_t=g(z_t), where x_t are observations, z_t are the underlying latent variables, and g is unknown. The paper makes two contributions: 1) presenting conditions under which the process can be identified (up to a permutation of the latent variables and their componentwise invertible transformations), and 2) developing a training framework that enforces the assumed conditions. The first contribution is theoretical and the second methodological. In addition, the paper presents empirical evidence for the ability of the method to recover the underlying causal process using synthetic experiments and real-world data.  Two cases are considered: 1) non-parametric, where a non-parametric transition is assumed in the latent space, and 2) parametric, where linear additive transitions are assumed. The main assumption for identifiability is the independent noise (IN) condition, which states that noise for the dimensions of z_t are independent of each other and across time steps t. This assumption is enforced through a contrastive approach during training. In addition, in the non-parametric case the noise is assumed non-stationary, such there exists sufficiently many auxiliary variables that affect the distribution of the non-stationary noise. ",0.1864406779661017,0.23728813559322035,0.2711864406779661,0.2222222222222222,0.3611111111111111,0.1792452830188679,0.3055555555555556,0.1320754716981132,0.07881773399014778,0.07547169811320754,0.06403940886699508,0.09359605911330049,0.23157894736842105,0.16969696969696968,0.12213740458015267,0.11267605633802816,0.10878661087866111,0.12297734627831715
228,SP:31ba7f0d204f8a4eae4d3705a9d2f413179d2f5c,This paper analyzes the reason behind the social posterior collapse caused in typical VAEs for multi-agent modeling and proposes improvement schemes. The paper test the proposed model on real-world multi-agent trajectory prediction datasets. This paper also proposes a novel sparse graph attention message-passing.,"This paper focuses on analyzing VAE-based frameworks for multi-agent trajectory forecasting from a relationship-modeling perspective. Specifically, it argues that standard VAE frameworks can potentially lead to a situation where the interactions between entities are ignored when predicting future trajectories, which they call “social posterior collapse.” To address this, they augment this framework in two ways: first, they convert the model to a conditional VAE (CVAE) where the prior on latent variables is conditioned on input trajectories; second, they add an auxiliary training task wherein samples from the prior are used to train a second trajectory decoder. To analyze the “social posterior collapse” issue, a graph attention layer that uses sparse attention is developed (titled “Sparse-GAMP”), and a metric called Agent Ratio (AR) is defined which consists of the proportion of surrounding agents to which the target agent is attending. Experiments are run on traffic and pedestrian trajectory prediction datasets, where it is shown that, in situations where social posterior collapse is shown to be present, their model is able to alleviate this issue. Furthermore, the auxiliary prediction task can lead to better model performance.",This work discuss the problem of posterior collapse in VAE/CVAE problems and extend this when it is used in the concept of multi-agent  interaction modelling and trajectory prediction problem. They argues that the model designed naively based on VAE for this task is prone to ignore agent interaction when predicting the future trajectory of an agent. The framework proposes a novel sparse graph attention message-passing (sparse-GAMP) layer to address this problem. ,"The proposed paper introduces the concept of social posterior collapse in variational autoencoder (VAE) architectures trained for trajectory prediction in interactive environments. Social posterior collapse refers to the VAE ignoring interaction information during training, thus making trajectory predictions solely from the predicted agent's history and map information. The authors present a means to quantify the extent of social posterior collapse through a sparse attention mechanism and propose architectural changes to alleviate the issue.",0.3191489361702128,0.44680851063829785,0.2978723404255319,0.11170212765957446,0.13829787234042554,0.24,0.0797872340425532,0.28,0.1891891891891892,0.28,0.35135135135135137,0.24324324324324326,0.12765957446808512,0.34426229508196726,0.23140495867768596,0.15969581749049433,0.19847328244274812,0.24161073825503357
229,SP:31e45243f78e00f69e63abf1afe2d06b74fca4ca,"This paper presents an approach for risk-averse Bayesian Optimization in the case that the function to be optimized is subject to heteroskedastic observation noise. For a mean-variance model, the authors propose an acquisition function and provide bound on the cumulative regret for the case when the variance proxy is known. They then extend this to model the unknown variance proxy (assuming strict sub-gaussianity) and provide regret bounds also for this case. The algorithm is empirically evaluated on a set of synthetic and real problems, where it is shown to improve upon non-robust BO approaches in terms of the regret defined.",The paper proposes a Bayesian optimization method for robust optimization of black-box functions under heteroskedastic observation noise. The method uses an objective that trades off mean and noise variance to identify solutions with high mean and low noise variance. The authors propose modeling the black-box function and the black-box noise variance function using independent GPs and using a UCB-style acquisition function. The authors prove a sublinear cumulative regret bound for the method.,"This paper proposes a new UCB strategy for Bayesian optimization that focuses on mean-variance optimization instead of expected value. The authors argue that this setting is relevant for heteroscedastic settings where one seeks high mean values with lower risk. Regret bounds are given to provide theoretical results about the convergence of the proposed algorithm for this setting. Empirical results on two synthetic problems and two ""real-world"" problems show that the proposed approach returns solutions with a higher mean and lower risk than standard risk-neutral GP-UCB.","This paper proposes a risk-averse Bayesian optimization algorithm, which aims to find an input with both large expected return and small noise-dependent noise variance. Both the mean function and input-dependent noise variance are assumed to be unknown functions from a reproducing kernel Hilbert space, and hence both need to be estimated online. Both theoretical guarantees and empirical evaluations are given.",0.18269230769230768,0.20192307692307693,0.17307692307692307,0.2236842105263158,0.21052631578947367,0.14606741573033707,0.25,0.23595505617977527,0.2857142857142857,0.19101123595505617,0.25396825396825395,0.20634920634920634,0.21111111111111108,0.21761658031088082,0.2155688622754491,0.20606060606060608,0.2302158273381295,0.17105263157894737
230,SP:31f451cf964da1151159daa5fe43bd4332398b2c,"The paper presents a new bound on the generalization error of models trained with stochastic gradient descent. The new bound builds on the work of Neu, but is tighter. Based on the insights obtained from the bounds, the authors propose two approaches to improve generalization. On the one hand, they show that gradient clipping helps preventing overfitting. On the other hand, the authors propose a regularization term that adds Gaussian noise to the network weights (Gaussian model perturbation) and show that this type of regularization performs on par with or even better than other contemporary regularization schemes. Aside from these technical contributions, the authors provide ample insight into the mathematical terms they derive and discuss several phenomena observed during learning. In the outlook, the authors discuss how their bound can be further improved by utilizing the strong data processing inequality.","This paper proposed a new generalization bound which is based on the recent construction by Neu (2021). The authors made further assumptions on the randomness of the batch sequence, and improved the results by Neu by achieving tighter bounds by tuning the trajectory term. It is verified empirically, and the flatness term gives a regularization technique for better classification on VGG16 on MNIST. ","This paper derives an improved stability-based generalization bound for SGD upon Neu et. al.’s work. In particular, they drop a term called local gradient sensitivity, leading to significantly tighter bound in practical models. Based on this generalization bound, they study both linear model and two layer ReLU neural networks. During their analysis, they obtain several interesting observations, which is consistent to the previous ones. Finally, based on their bound, they propose a new training scheme, named Gaussian model perturbation, having good performance on popular datasets.","This paper presents novel and tighter information-theoretic upper bounds for the generalization error of machine learning models, such as neural networks, trained with SGD. Following the same construction of the auxiliary weight process in Neu (2021), the upper bounds proposed in this paper improve upon Neu (2021) in two ways. One improvement is mainly due to removing an unnecessary term in the bound of Neu (2021), which the author refers to as “local gradient sensitivity”, by invoking the HWI inequality (Raginsky & Sason, 2018). Additionally, the improvement also benefits from replacing a sample-level mutual information term in Neu (2021) with an individual sample mutual information term, exploiting a recent result of Bu et al. (2020). The bounds obtained here decompose into two terms, one measuring the impact of training trajectories (“the trajectory term”) and the other measuring the impact of the flatness of the found solution (“the flatness term”).  The authors also apply these bounds to analyze the generalization behavior of linear and two-layer ReLU networks. Experimental studies based on these bounds provide some insights into the SGD training of neural networks. Some simple regularization schemes, including dynamical gradient clipping and Gaussian model perturbation, are proposed, which are shown to perform comparably to the current state of the art. ",0.1357142857142857,0.12142857142857143,0.24285714285714285,0.1746031746031746,0.3492063492063492,0.28735632183908044,0.30158730158730157,0.19540229885057472,0.16113744075829384,0.12643678160919541,0.10426540284360189,0.11848341232227488,0.18719211822660098,0.14977973568281938,0.19373219373219372,0.14666666666666667,0.16058394160583941,0.16778523489932887
231,SP:31f8d61dbd760daf4c9772f18b94d9b62365fed1,"The paper studies the effect of discretization issues when using different point-wise non-linearities (applied on the regular representation) in SO(2) equivariant neural networks. Unfortunately, the composition of band-limited SO(2) functions (in the features of an equivariant network) with popular point-wise non-linearities (such as ReLU, ELU or tanh) results in SO(2) functions whose spectrum is decaying but is not generally band-limited. A band-limited spectrum can be reconstructed up to desired precision using a larger number of samples but will always include some non-equivariant approximation error.  To guarantee perfect equivariance, the authors propose the use of polynomial activation functions, which guarantee band-limited output funtions and therefore enable perfect reconstruction of the spectrum using a finite number of samples as prescribed by the sampling theorem.  The authors compare different types of non-linearities on MNIST-rot, a classical benchmark for rotation equivariant models, and ModelNet40. Their models achieve results comparable with the State-of-the-Art while achieving more stable equivariance.",The authors develop an SO(2)-equivariant convolution based on the Fourier transform. They show that non-linear activations create high frequencies and hence they clip those high frequencies in their convolution. They apply their method to both 2D data (rotated MNIST) and 3D data (rotated ModelNet40). ,"The paper investigates the effect of nonlinearities in SO(2)-equivariant networks. When being applied in the spatial domain, such nonlinearities may lead to harmonic distortions, creating higher frequency harmonics in the signal. In a naive implementation this leads to aliasing effects, reducing the equivariance to a finite subgroup C_N < SO(2). Due to their analytical tractability, the authors start by assuming nonlinearities that are polynomials of finite degree D. By employing the convolution theorem, they show that the Fourier coefficients of the nonlinearly transformed signal can be expressed in terms of convolutions of the input Fourier spectrum. This result implies in particular that inputs which are band limited to frequency K result in outputs that are band-limited to KD. Fully SO(2)-equivariant polynomial nonlinearities require therefore a D-fold oversampling of the input, to which the nonlinearity is applied pointwise. For non-polynomial nonlinearities, the authors propose a D-fold oversampling, where D is a user-parameter. Equivariance is in this case approximate, but will decrease with increasing D.  The experimental section evaluates different nonlinearities and the effect of their hyperparameters. The authors implement a G-CNN for G=SE(2), whose features can be seen as functions on fibers SO(2) over a base space R^2. As usual, the nonlinearities are acting on each fiber individually. A second experiment implements SE(3)-equivariant Surfel networks, which are again relying on features which are functions on SO(2) at each point of a surface. Figure 5 evaluates the equivariance of ReLU nonlinearities, which are non-polynomial. The relative equivariance error is hereby found to decay exponentially with the number of oversampled Fourier coefficients. The equivariance error of polynomial nonlinearities is in figure 6 shown to drop at the theoretically predicted cutoff to machine precision, which shows that they are indeed continuously SO(2)-equivariant. Classification results on the rotated MNIST dataset are approximately similar to those of previous work, however, the models require less parameters. The surfel network achieves SOTA results on ModelNet-40, consuming a significantly reduced runtime in comparison to prior work.",The paper describes a focussed analysis of non-linearities to be used with steerable (Fourier based) implementations of equivariant neural networks. The authors show how to get exact equivariance with polynomial activation functions and provide guidelines on how to use general activation functions via the route [Inverse Fourier > activation func > Fourier]. Their theoretical insights are confirmed by experimentation.,0.06470588235294118,0.2529411764705882,0.1,0.2978723404255319,0.1276595744680851,0.04871060171919771,0.23404255319148937,0.12320916905444126,0.29310344827586204,0.04011461318051576,0.10344827586206896,0.29310344827586204,0.10138248847926269,0.16570327552986513,0.14912280701754385,0.0707070707070707,0.11428571428571427,0.08353808353808354
232,SP:31fc4cc48b20bb192ebbb50c54758402eea6f573,"This submission is a technical paper, which is concerned with obtaining bounds for the Empirical Risk Minimization (ERM) procedure for regression, when the data is both heavy-tailed and dependent (non-iid). The performance of ERM in the heavy-tailed context is by now fairly well-understood, in part thanks to the observation that two-sided concentration for the empirical risk is not needed to establish excess risk bounds for ERM, and that one-sided guarantee suffice (functions with large population risk have large empirical risk). The authors' contribution is to extend previous results in the iid setting to the dependent (specifically, mixing or exponentially-mixing) setting. While both the heavy-tailed and dependent settings have been considered separately, this works combines these two cases.  As in the iid case, the analysis involves both obtaining lower isomorphic (small ball) estimates for the function class, and controlling the interaction between the noise and the functions in the class (the so-called multiplier inequality).  The main applications of the generic results obtained for square and convex loss (Section 3) are described in Section 4. For regression over L^1 balls, the authors obtain bounds under square loss for both exponential and polynomial tails of the noise, as well as bounds for Huber loss under polynomially-tailed noise (but sub-Gaussian data).","The authors derive risk bounds for empirical risk minimization under heavy-tailed $\beta$-mixing observations. They provide general bounds for convex, locally strongly convex, loss functions and tighter bounds for the squared loss. They give three illustrative models for which they provide explicit bounds, the most important one being the Huber loss for which they find a justification of its usage as a robust lost in presence of sub-Weibull noise distributions.","The paper analyses empirical risk minimization over a locally strongly convex function class in the context where the data generating process returned non-IID data, and can be potentially heavy-tailed. The authors give specific realizations of their analysis to sparse linear function class.  This work aims to bridge the gap between analyses that handle IID data and heavy-tailed data separately.",The paper considers the performance of ERM on heavy-tailed and dependent data. It proves general convergence rates with a few illustrative examples.  Update  ---- I have read the author(s)' response to my review and maintain my rating. ,0.0958904109589041,0.091324200913242,0.0593607305936073,0.1527777777777778,0.08333333333333333,0.12903225806451613,0.2916666666666667,0.3225806451612903,0.34210526315789475,0.1774193548387097,0.15789473684210525,0.21052631578947367,0.1443298969072165,0.14234875444839856,0.10116731517509726,0.16417910447761194,0.1090909090909091,0.15999999999999998
233,SP:32040641c0cbdc186c2db90470bec7856c89cb38,"The paper approaches the problem of imitation gap. In situations where the teacher (an expert) has access to privileged information, the student may not have such comfort. Therefore, blindly imitating the teacher may lead to inefficient, unwanted or incorrect behavior of the student. The paper provides examples of such situations. Actually, sometimes the student may not even be able to imitate the teacher in some states as not all the information is available.   This leads to an imitation gap. Performing classic imitation learning in states where the imitation gap is significant can be pointless. The authors propose to try existing RL methods in such situations.  Specifically, the imitation loss and the RL loss are weighted by a normalized coefficient. One of the main contributions of the paper is how to dynamically determine the value of the coefficient. The idea is to in some sense 'measure' or 'quantify' the imitation gap in a given state. The authors follow the intuition that for states where the imitation gap is substantial, learning well the teacher's distribution over actions may not be achievable. It means the divergence between the teacher's distribution and the imitation distribution of actions will be relatively large. This quantity is used as an estimation of imitation gap.  To compute that quantity an approximation to the imitation policy is introduced. The divergence between the teacher and this approximation is used to estimate the imitation gap. ","This work introduces ADVISOR, a simple yet effective approach to adaptively combining RL with supervision from imitation learning. The approach is motivated by the issue of *information gaps,* which describes the case where the policy generating the supervision has more information about the state than does the policy being trained. The authors motivate their approach as a way to address this issue and contribute a thorough set of empirical analyses to demonstrate its effectiveness. In addition, this work provides a concrete demonstration that the information gap is an obstacle.","This paper identifies a problem in imitation learning when an expert has access to privileged information that is not available to the learner. When a decision has to be made based on the privileged information, the learner tends to choose average or uniformly random actions of the expert due to the lack of important information, which is called the ""imitation gap"". Therefore, in such cases, learning from the expert can actually harm the training of the learner.","This paper points out the existence of ""imitation gap"" when a teacher and student policy has different observations. Due to this imitation gap, the student policy cannot determine which action leads to a higher reward, and thus ends up outputting a uniformly random action or average actions of the teacher policy.  The proposed method, ADVISOR, tackles such imitation gap by balancing between learning from reward and expert. When the limited observation is enough to predict the optimal action, the student policy imitates the demonstrations; otherwise, it learns from reward. As the weight function, ADVISOR uses the divergence between the teacher policy and the auxiliary policy, which solely learns from the demonstrations.",0.09745762711864407,0.13983050847457626,0.15254237288135594,0.1797752808988764,0.1797752808988764,0.2857142857142857,0.25842696629213485,0.42857142857142855,0.32432432432432434,0.2077922077922078,0.14414414414414414,0.1981981981981982,0.14153846153846153,0.2108626198083067,0.207492795389049,0.1927710843373494,0.16000000000000003,0.23404255319148937
234,SP:3216e523f5a2e8e1f0f79214348bfdfd665f685c,"The paper proposes to use intra-class mixup to train OOD detectors. Adding intra-class mixup, the separability between in-distribution and out-of-distribution data is improved. The methods are evaluated in multiple OOD benchmark datasets, showing improvement of 4-6% over the methods without intra-class mixup.   The paper also interestingly shows that the cosine of the angular margin is also a useful measure for OOD detection. It can be added to other regular OOD measures to further improve the performance.   ",The authors propose to use intra-class mixup (i.e. linearly interpolating images from the same class) as a form of data augmentation. They further define the angular margin and propose to add it to out-of-distribution detection scores in order to improve the performance. They evaluate their method's OOD performance on three different OOD scores and show improvements.,"The authors propose adding angular distance in the OoD score and additional training samples via intra-class mixup.  Angular margin/distance is defined as the angle between the weight vector from a class and an instance (ie. arccos of the dot product of the unit weight vector and the unit vector for a instance.  Intra-class mixup generates samples by interpolating between two samples of the same class.   Angular spread is the standard deviation of the angular margin for a given dataset.  Intra-class mixup reduces angular spread in in-distribution data and increases angular spread between in-distribution and OoD data.   Angular separability is defined as the squared difference of in-distribution angular margin and OoD angular margin, normalized by the sum of their standard deviation.  With four datasets, they show that intra-class mixup increases angular separability in 3 datasets.  With four datasets and 3 different OoD techniques, intra-mixup is more accurate in AUROC and FPR95, but not AUPR.   Adding the cosine of angular margin in the OoD scores generally improves performance over using OoD scores alone.    ","Inspired from inter-class mixup (Zhang et al, ICLR 2018), where data augmentation is used to train models more robust to adversarial samples, this work proposes intra-class mixup to reduce the variance of in-distribution samples, i.e. the training set, and improve the capacity of a trained model to detect out-of-distribution samples at inference time. The key difference between the two works is that the here proposed does the mixup within samples belonging to the same class.   In addition to this, the work propose to use the angular margin, i.e. the angle between the normal of the decision boundary of a neural net (obtained from the weights of the last layer) and an unmixed sample, to detect OOD samples. The cosine of such angle shall be coupled with an OOD method to perform OOD detection. ",0.21686746987951808,0.3373493975903614,0.3373493975903614,0.3770491803278688,0.2786885245901639,0.18333333333333332,0.29508196721311475,0.15555555555555556,0.2,0.12777777777777777,0.12142857142857143,0.2357142857142857,0.25,0.21292775665399238,0.2511210762331839,0.1908713692946058,0.16915422885572137,0.20625000000000002
235,SP:32329e9dbfc30a59682eaaa21774081fa41adc02,This paper proposes NeuroSEED to embed strings into vectors for edit distance approximation. The key insight is that hyperbolic space is more suitable for embedding the hierarchical of biological sequences. Extensive experiments are conducted to show that NeuroSEED significantly improves the efficiency of hierarchical clustering and multiple sequence alignment.               ,"This paper presents an encoder-based framework  (called NeuroSEED) to learn the embeddings of DNA sequences in geometric vector spaces that preserve edit distance between them. Learning an efficient representation that captures the underlying hierarchical relationships allows faster applications of downstream applications like Edit distance Approximation (EDA), Closest String Retrieval (CSR), Multiple Sequence Alignment (MSA), and Hierarchical clustering (HA). The paper explores a variety of neural network models for encoding the sequences as well as different geometric vector spaces. The tasks are formulated as unsupervised (with general MSE loss) and supervised (with tailored loss functions).  It reports hyperbolic space to give the state-of-the-art performance for all the downstream tasks and claims a faster compute time (in terms of time complexity) when using these embeddings. The results are reported on two relevant microbiome analysis datasets - Qiita and RT988. ","This paper offers two contributions - the first is a formalization of a general encoder-decoder framework for learning representations of biological sequences, where the distance in the representation space corresponds to the edit distance between the sequences (NeuroSEED). The second is the use of hyperbolic geometry to embed the sequences. The authors then demonstrate the efficacy and runtime improvements of their method on a variety of tasks, including approximating edit distance, MSA and hierarchical clustering.","The paper tackles the problem of approximating edit distances between biological sequences. For this purpose, the authors propose an encoder that maps sequences to a vector space, and is trained in a way such that the distance between encoded representations matches the edit distance between underlying sequences. The authors show multiple experiments with different architectures (Linear, Transformer, CNN, GRU, etc.) and different geometries (cosine, euclidean, hyperbolic, etc.). Overall, the approach is able to approximate well the edit distance while being much faster to compute. ",0.32653061224489793,0.2857142857142857,0.22448979591836735,0.16428571428571428,0.14285714285714285,0.25333333333333335,0.11428571428571428,0.18666666666666668,0.13095238095238096,0.30666666666666664,0.23809523809523808,0.2261904761904762,0.1693121693121693,0.22580645161290322,0.16541353383458646,0.21395348837209302,0.17857142857142855,0.23899371069182387
236,SP:324702901d2fb744b24a5282fed57977638db754,"This paper proposes a generic method HCL to solve the source data-free domain adaptation problem across various computer vision tasks. Based on the memory mechanism, the method involves two designs to learn discriminative features while preserving the source hypothesis. The first design HCID leverages the contrastive instance discrimination where positive pairs are made of the feature from the current model and features from historical models including the original source model and model in previous epochs. The second design HCCD leverages the consistency between the current model and previous models to select confident pseudo labels for task-specific self-training. Experiments across various vision tasks and domain adaptation settings validate the effectiveness of both designs and the generic method.","The authors address unsupervised model adaptation (UMA). While unsupervised domain adaptation (UDA) lets the model access the source domain data during an adaptation to a target domain, such access could raise multiple concerns such as privacy and portability. UMA does not allow the model to access the source domain data during adaptation. The proposed method, historical contrastive learning (HCL), utilizes the historical source hypothesis to remedy the lack of source data. The experimental results show that the proposed method achieves state-of-the-art performance for multiple visual recognition tasks.","In this paper, the authors propose to solve the problem of unsupervised model adaptation. In this task, the aim is that given a classifier trained on a source dataset and a set of unlabelled target examples, an unsupervised model adaptation is performed to improve the performance of the model on the target dataset without requiring access to source examples. The method is evaluated on standard domain adaptation datasets for classification, detection and segmentation. The approach is related to EM and some theoretical insights provided. ","This paper studies the unsupervised model adaptation problem, where only the trained model, but not the source data, is available to use for target domain adaptation. The authors propose historical contrastive learning (HCL) technique to help this source-free setting. HCL is composed of two components, HCID and HCCD. HCID learns from target instances by contrasting the features generated from the adapted model and historical models. HCCD explores self-training to pseudo label target instances to learn category discriminative representations. Experiments on three different vision applications are conducted. The paper is generally easy to follow, despite some unclear points. The major concern is the novelty.  ",0.15966386554621848,0.16806722689075632,0.21008403361344538,0.2111111111111111,0.26666666666666666,0.2619047619047619,0.2111111111111111,0.23809523809523808,0.23809523809523808,0.2261904761904762,0.22857142857142856,0.20952380952380953,0.1818181818181818,0.19704433497536947,0.22321428571428573,0.21839080459770116,0.24615384615384614,0.23280423280423285
237,SP:3251f004f83f4aa24b6c8882596f878e89abe15e,"This paper mainly proposed a new dataset named ParT Reasoning (PTR) dataset for reasoning the part of the objects in an image, which data is synthetic from the simulator rather than the real world. The dataset is a form of visual question answering that containing five types of questions, including concept, relation, analogy, arithmetic, and physics. The authors conducted several popular methods on their PTR dataset to validate the challenge and characteristics of the dataset.","This paper proposed a new benchmark for visual reasoning on part-whole hierarchies, where the dataset contains 80k RGBD synthetic images with ground truth object and part level annotations. It contains five types of questions including, concept, relation, analogy, arithmetic, and physic. The dataset generation procedure is described in a detailed way. Several vanilla baselines, CNN-based baselines, and neuron symbolic baseline are included and compared.","This paper proposes a benchmark for part-based commonsense reasoning. The dataset consists of realistic-looking scenes with various objects in the scene. Several queries are designed to test a model's reasoning capability which includes concepts, relations, physics, analogy, and arithmetic. The author(s) run several baselines spanning from a random picking to an oracle model (NS-VQA) which utilizes ground truths for vision model and language programs.","The paper introduces a novel benchmark for part-based reasoning, featuring 80k indoor scenes contraining pieces of furniture from the PartNet dataset. Scenes are annotated with object and part labels, as well as geometric relations. Based on these features, a dataset of questions are generated as a VQA benchmark. Several existing models are evaluated on this benchmark, and are found to underperform  human subjects, sometimes substantially.",0.30666666666666664,0.22666666666666666,0.18666666666666668,0.22727272727272727,0.25757575757575757,0.21739130434782608,0.3484848484848485,0.2463768115942029,0.21212121212121213,0.21739130434782608,0.25757575757575757,0.22727272727272727,0.326241134751773,0.2361111111111111,0.19858156028368795,0.22222222222222224,0.25757575757575757,0.22222222222222224
238,SP:33894d9650135a2f4ec686065c4431019f4bdb33,The paper propose a neural architecture with a backbone and side branches. These side branches leads to classification decisions. An ensemble method is use to combine the output of the side branches accumulatively (see Eq. 2). This is an interesting approach and its impact can go beyond early exit applications.,"This paper focuses on improving the computational efficiency of multi-exit networks. The authors aim to reuse the computation performed in previous internal classiﬁers (ICs) at latter ICs. The proposed method, Zero Time Waste (ZTW), includes two main components: (1) adding direct connections between ICs and (2) involving the previous predictions in an ensemble-like manner. Some results on CIFAR, Tiny ImageNet, and Atari games are provided.","This paper proposes to use all the outputs from the internal classifiers in an early-exit network. The paper presents two novel ways of making use of the previous prediction and concludes with the weighted geometric mean as a better choice empirically. By taking care of the details, i.e., stop gradient and handling numerical issues during training, the authors report superior results compared to early-exit networks that do not make use of previous classification results in the later classifiers. The paper is well-written with comprehensive empirical results on CIFAR, TinyImageNet with some ImageNet, and Atari.","This paper proposes Zero Time Waste (ZTW), a novel approach to leverage previous intermediate classifiers (ICs) for early-exit neural networks. ZTW consists of two simple extensions of early-exits NNs. The first is to add cascaded connections from the result of previous ICs to the next IC. The second is to ensemble previous results for confidence measurement. The author conducts extensive experiments on image classification benchmarks and shows improved performance compared to vanilla early-exit neural networks and PABEE, a previous work investigating using results from previous ICs for confidence measurement.",0.14,0.22,0.24,0.2647058823529412,0.22058823529411764,0.22448979591836735,0.10294117647058823,0.11224489795918367,0.13043478260869565,0.1836734693877551,0.16304347826086957,0.2391304347826087,0.11864406779661017,0.14864864864864866,0.16901408450704222,0.21686746987951805,0.1875,0.23157894736842108
239,SP:339246c159fb15b25b777722f52d3108cf64b005,"The paper proposes Discrete Denoising Diffusion Probabilistic Models (*D3PMs*), a type of generative diffusion model for discrete and categorical data. It largely builds on previous frameworks but defines new transition kernels for the discrete Markov chain that perturbs the data. On top of the usual variational lower bound diffusion model objective, the paper also suggests to use an additional loss component in its training objective that corresponds to an additional reconstruction objective given perturbed data. Quantitatively, the proposed *D3PM* improves upon previous discrete denoising diffusion models, but its performance remains lower than standard autoregressive models in text generation and lower than continuous denoising diffusion models in image generation.","This paper introduces diffusion models for discrete data which generalizes multinomial diffusion model (Hoogeboom et al. 2021) by going beyond corruption processes with uniform transition probabilities. To achieve that, it presents a set of corruption processes with structured transition matrices. The authors have also drawn a connection between their discrete diffusion models and autoregressive / mask-based language models. The proposed model outperforms previous non-autoregressive generative model in text domain. On the image dataset CIFAR-10, it achieves comparable results as Gaussian DDPM model.",This paper generalizes diffusion models from Bernoulli distributions to categorical distributions. The proposed method supersedes masked training in BERTS and autoregressive learning. Authors examined different designs of the transition matrices and demonstrated the proposed method in both language and image modeling. Additional tricks like auxiliary cross entropy loss function and carefully designed noise schedules further improve the performance.,"The paper introduce a novel method for discrete state space. The main contribution is as follows:  1. propose new discrete diffusion process based on Markov transition matrices Qt.   2. proposed multiple transition matrices  - Uniform, Absorbing state, Discretized Gaussian, Token embedding distance  3. proposed new noise schedule for the forward process based on the mutual information between x_t to x_0  4. introduce new auxiliary denoising objective, which improve the prediction of x_0  The paper also discuss the connection to other probabilistic models for text - BERT, AR, MLM. The paper check the performance on two domains - text and images and show significant improvement over the previous diffusion model by Hoogeboom et al.   ",0.17592592592592593,0.1111111111111111,0.19444444444444445,0.15476190476190477,0.17857142857142858,0.1724137931034483,0.2261904761904762,0.20689655172413793,0.18584070796460178,0.22413793103448276,0.13274336283185842,0.08849557522123894,0.19791666666666669,0.14457831325301204,0.19004524886877827,0.18309859154929575,0.15228426395939088,0.11695906432748537
240,SP:33d9ed48ce72f65860ffb34e77ed8b79b95b4869,"The paper presents a study about the representations learned by neural networks trained using robust optimization — a type of optimization that requires the model to be robust to small perturbations in the data. Specifically, the paper presents results of ResNet-50 trained on ImageNet with standard optimization and robust optimization. The paper draws three main insights from studying the learned representations of the standard and robust networks. First, the representation of the robust network is approximately invertible. In other words, when recovering an image by matching the representation of a random image to the representation of a target image by adding noise, the recovered images are semantically similar to the target image; the recovered images look similar to a human. Moreover, this is also demonstrated with images from outside of the distribution of the training data. Second, the representation of the robust network, unlike the representation of standard network, shows semantically meaningful high level features without any preprocessing or regularization. This leads to the final insight, feature manipulation is easier in robust networks. This is demonstrated by adding noise to an initial image in order to maximize the activation of a specific higher level feature and stopping early to preserve most of the other features of the original image. ","The paper shows that the learnt representations of robustly trained models align more closely with features that the human perceive as meaningful. They propose that robust optimization can be viewed as inducing a human prior over learnt features. Extensive experiments demonstrate that robust representations are approximately invertible, can be visualized yielding more human-interpretable features, and enable direct modes of input manipulations.","This paper empirically demonstrates that robust optimization encourages deep neural networks to learn a high-level encoding of inputs. Specifically, this paper first utilize $\ell_2$- norm adversarial training to train robust neural networks. Then, this paper leverage two visualization techniques, i.e., *representation inversion* and *feature visualization*, to demonstrate that features identified by the robust neural network are more discernable.   To summary, the main contributions of this paper are as follows:  - A comprehensive literature review about findings related to features of standard and robust models; - Train a $\ell_2$-norm robust neural network and visualize its features; - Leverage *representation inversion* and *feature visualization* to demonstrate the features in robust neural networks are more discernable.","The paper looks at  favorable properties of feature representations of an adversarially robust model. In particular, the authors look at a model trained with PGD training with an $\ell_p$ adversary. In terms of favourable properties, the authors look at representation inversion and feature manipulation and with experimental evidences claim that adversarially robust models are naturally better at it,",0.08133971291866028,0.1291866028708134,0.0861244019138756,0.20967741935483872,0.16129032258064516,0.11304347826086956,0.27419354838709675,0.23478260869565218,0.3050847457627119,0.11304347826086956,0.1694915254237288,0.22033898305084745,0.12546125461254612,0.16666666666666669,0.13432835820895522,0.14689265536723164,0.1652892561983471,0.14942528735632182
241,SP:33fdd2dc77cf2c841a8ee122dfa101568bd15179,"This paper proposes a function-space variational inference method for classification tasks. The method is based on the recently proposed functional variational inference (fVI), which directly approximates the posterior over functions instead of the model parameters. The authors use a variational implicit process as a parameterized posterior and adopt Dirichlet predictive priors for the categorical predictive distributions. Since the predictive posterior is defined implicitly, the authors further propose an iterative approach to estimate the Dirichlet predictive posterior, and end up with a wake-sleep style inference procedure. Finally, the proposed method is verified on several standard classification tasks.","The authors propose Function space variational inference for deep Bayesian classification, an inference framework which extends ideas around evidential learning for classification by applying function space Variational Inference and marginalizing over latent functions. Concretely, this leads to a method where the estimated Dirichlet is not directly regressed from a forward pass of a function, but is estimated as a parameter that would match the empirical distribution over sampled logits given multiple samples from the function posterior (using a nested optimization step), and hence can be plugged into the fVI framework. The authors show this is a flexible way to model uncertainty for classification and can be combined with diverse models (ensembles, MCDropout and various Bayesian models). The method produces good results.","This paper considers a functional variational inference approach to Bayesian deep learning for multi-class classification.  This paradigm enables the use of a Dirichlet predictive prior in function-space, which would otherwise be cumbersome under the classical weight-space treatment. The authors approximate the posterior in function space through the broad family of variational implicit processes, which are able to subsume popular BNN paradigms such as deep ensembles and Monte Carlo dropout. Experiments are provided to support the claim that this combination of function-space regularization with a Dirichlet predictive prior improves adversarial robustness and uncertainty quantification over its weight-space counterparts.","The paper introduces a function-space variational inference algorithm (""fVI"") that uses a Dirichlet predictive prior, and approximates the output of a (weight-space-Bayesian) neural network as a Dirichlet distribution. Inference is achieved by viewing the network as a variational impolicit process, and optimising the functional ELBO from Sun et al., 2019. This view allows them to impose priors in function-space to many existing algorithms, such as MAP, MC Dropout, ensembles, Radial BNNs, and so on. The authors show results on a toy problem, and focus on out-of-distribution performance on larger problems such as MNIST, CIFAR10 and CIFAR100, as well as performance under a fast gradient sign method adversarial attack. The fVI variants almost always perform better when the test distribution is not the same as the training distribution. ",0.24489795918367346,0.21428571428571427,0.22448979591836735,0.1652892561983471,0.19008264462809918,0.24509803921568626,0.19834710743801653,0.20588235294117646,0.16541353383458646,0.19607843137254902,0.17293233082706766,0.18796992481203006,0.2191780821917808,0.21,0.19047619047619047,0.17937219730941706,0.18110236220472442,0.2127659574468085
242,SP:33fe73ddcd33933bc970916bf46fcb59af7ff869,"This paper studies few-shot learning from the viewpoint of transferring feature representations. The authors investigate few-shot performance with varying complexities of source tasks together with a few empirical tricks for improvements. The main finding in the paper is that transferring from more complex source tasks tend to result in better performance. Accordingly, using multiple source tasks is also found to be useful.  Strengths:   (+) The paper is clearly motivated and well-written.   (+) Reported scores could be useful to server as a baseline in this direction of research.    Weaknesses:   (-) No technical novelty.   (-) Results and conclusions seem rather obvious.  ",The authors study impact of self-supervised methods on feature representation in few-shot learning task. They analyze the way features are transferred for MAML method and 3 self-supervised methods. They introduced a method combining self-supervised representation training methods with voting system in order to improve performance.,"This paper conducts an empirical study on the usefulness of different representations for downstream few-shot learning tasks, using the transfer strategy of training a new linear layer on top of the frozen pre-trained network. They also propose to utilize auxiliary data during this downstream learning phase: instead of learning an N-way readout layer for an N-way classification task, they learn a (N+T)-way one, where T is the number of training classes, using a subset of the training dataset for this. To perform inference on the query set, though, they only consider the N classes appearing in the given task. They hypothesize that this approach prevents the linear classifier from paying attention to spurious features, and it sometimes yields a gain in performance. They also propose to combine different training objectives, including the supervised classification objective and different self-supervised ones in a multi-task manner, and propose to do so by keeping the input the same for all objectives involved, meaning that to train the supervised loss together with the rotation, the former will also be computed on rotated images. Finally, they propose a voting scheme, where the predictions for different augmentations of each input image are aggregated to form the final prediction. All experiments are conducted on mini-ImageNet and tiered-ImageNet, and gains are reported mostly on the former. ","The paper provides a study on different approaches to train a backbone network used in a transfer fashion for Few Shot Learning.  Representation learned from MAML, supervised classification, and self-supervised tasks are considered here and their performance are compared. From this analysis it is confirmed that the use of multi-task trained representation (based on supervised classification and self-supervised auxilary tasks) leads to the most performing backbone.  Furthermore it is proposed to improve the classifier part using two additional tricks:  - use of auxiliary classes to help eliminate irrelevant features - use of voting with image transformations (rotation, location splitting) This additionnal trick allows to boost performance over traditionnal simple classifier.   Final results show performance on par with various state of the art solutions.",0.09183673469387756,0.2755102040816326,0.16326530612244897,0.3469387755102041,0.32653061224489793,0.1277533039647577,0.1836734693877551,0.11894273127753303,0.12903225806451613,0.07488986784140969,0.12903225806451613,0.23387096774193547,0.12244897959183672,0.16615384615384615,0.14414414414414417,0.12318840579710146,0.18497109826589594,0.16524216524216523
243,SP:34177dc9d2e81610d167b996c3f106327c666f94,"This paper investigates the relationship between calibration and adversarial robustness, showing that calibration error is larger among less robust images. Based on this, a training procedure is proposed where the labels of the images are smoothed based on the robustness level, to produce a better calibrated model.   The proposed method is evaluated on CIFAR-10/CIFAR-100, and their corrupted counterparts, comparing with other calibration and label smoothing approaches, as well as ensemble methods.","This paper studies the connection between adversarial robustness (measured as the distance to the decision boundary using some adversarial attack) and model calibration (i.e how well the predicted probability indicates how much we can truth the model predictions).   1. The authors show that there is a significant correlation between adversarial robustness and model calibration. That is, inputs that have smaller distance to the decision boundary are more likely to have poorly calibrated predictions.  2. Based on this insight, the authors propose an algorithm called AR-AdaLS to learn how much to smooth the labels of the training data based on their adversarial robustness. They also discuss how this technique can be extended to an ensemble model.  3. The authors thoroughly compare their results against many previous calibration techniques and show that AR-AdaLS results in improved performance (for the single-model based methods). The results are superior also under distribution shift. They show results on CIFAR-10, CIFAR-100 and Imagenet datasets. For results under distribution shift, they show results on CIFAR-10-C, CIFAR-100-C and Imagenet-C datasets.","This paper proposes a new method (AR-AdaLS) for label smoothing to improve deep network calibration. In particular, the authors draw a connection between lack of calibration (overconfidence) and examples which are prone to adversarial attacks. They show that by generating smoothed targets based on the adversarial robustness of an example, they can further improve model calibration beyond traditional label smoothing.","The authors first expose a link between robustness and expected calibration error (ECE), the less robust a data point is, the larger the ECE. They then propose to exploit this link by introducing an adaptive label smoothing method that improves the expected calibration error of less robust data points. They benchmark their new method showing better calibration metrics on standard datasets, as well on corrupted datasets and out-of-distribution data. ",0.3918918918918919,0.22972972972972974,0.22972972972972974,0.10989010989010989,0.12087912087912088,0.18032786885245902,0.15934065934065933,0.2786885245901639,0.23943661971830985,0.32786885245901637,0.30985915492957744,0.15492957746478872,0.22656249999999997,0.2518518518518519,0.23448275862068968,0.1646090534979424,0.17391304347826086,0.16666666666666666
244,SP:34795101680741226020706a6278d571a868bd3e,"This paper studies how to make data unlearnable towards adversarial training. In prior works, including dataset poisoning, no methods can prevent data from being learned in adversarial training. This paper proposed a robust error-minimizing noise to solve this task. ","Error minimization noise was first proposed so that when a dataset is released to the public, conventional empirical risk minimization cannot learn a good model from the perturbed dataset. However, error minimizing noise is only effective to the extent that adversarial training is not used. Adversarial training was identified as an effective method of overcoming the error minimization noise in prior work. In this paper, the authors proposed a variant of the error minimization noise, which continues to be robust even in the presence of adversarial training. Specifically, they generat the adversarial noise with a similar method as the original error minimization paper, but calculate the gradients with respect to the attacked input as opposed to the clean input. ","The paper proposes a min-min-max formulation to generate robust unlearnable examples in order to protect data privacy in adversarial learning. The basic idea of this paper builds upon a previous paper ""Unlearnable examples: Making personal data unexploitable"" where the error-minimization noise is generated to reduce the training loss such that the model performs badly on the clean data. This paper finds that such unlearnability is fragile in adversarial training and fragile to minor data augmentations. Therefore, it proposes to generate more robust unlearnable samples by considering adversarial loss and data augmentation in the noise generation process, and it shows the generated noise can effectively reduce the quality of the trained model through extensive empirical study.  ","Based on a previous work (Huang et al. 2021), this paper proposed a method, called REM, that generates makes a dataset ""unlearnable"" to adversarial training. Experiments are conducted to validate the effectiveness of REM under various settings, including (1) different adversarial training perturbation radiuses, (2) different proportions of unlearnable data, (3) different model architectures, on different common datasets, including CIFAR-10, CIFAR-100, and ImageNet.",0.325,0.35,0.2,0.18487394957983194,0.10084033613445378,0.11016949152542373,0.1092436974789916,0.11864406779661017,0.12307692307692308,0.1864406779661017,0.18461538461538463,0.2,0.16352201257861634,0.17721518987341772,0.1523809523809524,0.18565400843881857,0.13043478260869565,0.14207650273224043
245,SP:34d274ef04ac5d7da4b6a8706697e908e19fe9bb,"This paper investigates whether input gradients (of robust and normally trained models) highlight discriminative features. As part of this investigation, it introduces DiffROAR (a modified version of ROAR for evaluating the accuracy of feature discriminativeness rankings), as well as the BlockMNIST dataset (plus a simplified version where they perform theoretical analysis). It introduces and discusses the idea of feature leakage as an explanation for why non-robust model gradients sometimes highlight non-discriminative features.","The authors test the current assumption for gradient-based explanation methods, i.e., a high magnitude of vanilla gradients highlight more discriminative task-relevant features and vice-versa. The authors theoretically and empirically test the above assumption using DiffROAR, a new evaluation framework, and BlockMNIST, a new semi-real dataset. Empirical analysis using DiffROAR concludes that gradients of standard models violate the above assumption. whereas gradients of adversarially trained models don't. The BlockMNIST dataset separates the signal and noise part of a given sample by encoding a priori knowledge of discriminative and non-discriminative features in the samples. The work tackles one of the existing problems of gradient-based explanation methods and suggests that the proposed frameworks and datasets can be used as sanity checks to audit instance-specific interpretability methods.","The authors present a procedure to check the relative influence on model prediction between pixels with largest input gradient magnitudes versus pixels with lowest input gradient magnitudes. This is an extension of ROAR, which trains a model on data with certain pixels masked out. The authors evaluate DiffROAR on 4 image datasets and 1 synthetic dataset, showing that more robust models are more likely to have input gradients on relevant pixels than non-robust models. They also present a theoretical analysis showing that a standard MLP with one hidden layer can violate the assumption that higher input gradients mean more relevant pixels.","This paper studies the validity of a key assumption (i.e. Assumption A) made in popular post-hoc attribution methods empirically and theoretically. Comprehensive experiments are conducted with specifically designed BlockMNIST. Moreover, the authors constructs a simplified setting for theoretical analysis to demonstrate feature leakage.",0.22972972972972974,0.14864864864864866,0.14864864864864866,0.14393939393939395,0.08333333333333333,0.08823529411764706,0.12878787878787878,0.10784313725490197,0.24444444444444444,0.18627450980392157,0.24444444444444444,0.2,0.1650485436893204,0.12500000000000003,0.18487394957983194,0.16239316239316243,0.12429378531073446,0.12244897959183675
246,SP:3514a7e473397a4c1bbb26e3804336d559452018,"This paper studies ""fairwashed"" explanation of black-box ML models -- interpretable models that achieve high fidelity with respect to the black-box predictions but substantially better scores on various fairness metrics. The authors empirically demonstrate how fairwashing can be achieved with very small decreases in fidelity, and how a fairwashed explanation of one type of model (say, random forests) can be successfully repurposed to explain a different model (eg AdaBoost). Because of this, they argue, auditors need to be very careful about how they use interpretable models in fairness assessments.",The paper presents an empirical study to better understand the ability to fairwash unfair black-box models. They illustrate trade-offs between fidelity and fairness in explanation models. They also demonstrate that fairwashing can generalize beyond the suing group and transfer across models.,"This paper addresses the problem of fairwashing, i.e., the process in which an adversary builds a manipulated explanation of the unfair black-box model. The manipulated explanations should ideally hide the fact that the block-box model is unfair. The authors conduct a series of experimental evaluations and conclude: -	Manipulated explanations can be up to 99.2 % as accurate as the original model, while being much less (up to 50% less) unfair -	Fairwashed explanations can generalize beyond the data points they were trained on. ","This paper attempts to empirically characterize the risk of ""fair washing"" attacks, in which an unfair model can be described with high fidelity by a global explainer that exhibits substantially less unfairness. The authors determine the fidelity-unfairness Pareto curve for several black box models, explanation models, datasets, and fairness criteria and find that across all tested groups, it is generally possible to reduce apparent unfairness by 50% with a minimal reduction in fidelity to the black box model. The authors additionally show that fair washing attacks trained on a specific subgroup (the suing group) extrapolate well to an unseen test group and that fairwashed explanations trained on one black box model may still have high fidelity on other black box models. Finally, the authors determined the range of unfairness metric values that can be achieved at various levels of fidelity to the black box in order to quantify how susceptible different black box-dataset-explainer combinations may be to fair washing. ",0.12222222222222222,0.2111111111111111,0.26666666666666666,0.2558139534883721,0.37209302325581395,0.3058823529411765,0.2558139534883721,0.2235294117647059,0.14814814814814814,0.12941176470588237,0.09876543209876543,0.16049382716049382,0.16541353383458646,0.21714285714285717,0.19047619047619047,0.17187500000000003,0.15609756097560973,0.2105263157894737
247,SP:355208e83bbfa7e59308d9857d556224a6557b76,"The paper tackles the problem of curriculum learning and presents a new technique (boosted CRL) that provides a tighter bound on the approximation error of the action-value function than standard curriculum learning. The authors show both theoretically and empirically the effectiveness of the approach over standard CRL. The presented methodology learns a function approximation based on the sum of `residuals'. The evaluation was done on 3 environments that had scope for the generation of a curriculum by varying the reward and exploration factors -- car on hill, maze, linear system control. Results show that BCRL performs significantly better than CRL in all environments.","The paper proposes a new curriculum for RL method. The basic idea is to ""learn"" a residual for each task, modeling in this way how the tasks differ. The research topic is highly relevant and contemporary and the theoretical analysis of the method is interesting. The major drawback is the empirical evaluation. ","This paper studies a fundamental issue in curriculum design for a complex reinforcement learning problem (target task). When the target task is complex to solve with direct training on it, the learner can be trained over a sequence of tasks with increasing difficulty. Here, the tasks in the sequence vary in either the transition dynamics or reward function. The central question of the paper: developing a principled method for transferring the knowledge (Q-value function or residuals) gained in the current task of the curricula to the next task. The authors propose to use the sum of residuals trained on each task for knowledge transfer and develop a novel curriculum RL method called Boosted Curriculum Reinforcement learning. By leveraging techniques from approximate value iteration literature, they theoretically justify their choice for knowledge transfer and their algorithmic proposal. The authors also conduct interesting empirical investigations on fleshing out the importance of boosting and the curriculum components of their algorithm.","This paper proposed a curriculum value-based reinforcement learning algorithm. Inspired by boosting methods, the algorithm learns on the Bellman residuals between the value function of the current task and that of the previous task. This paper provides analysis on the convergence of the algorithm and some finite-sample guarantee on the output value function. The method is tested on four different problems and is compared with two 3 baseline methods.",0.13592233009708737,0.23300970873786409,0.1941747572815534,0.34615384615384615,0.2692307692307692,0.15822784810126583,0.2692307692307692,0.1518987341772152,0.28169014084507044,0.11392405063291139,0.19718309859154928,0.352112676056338,0.18064516129032257,0.1839080459770115,0.22988505747126436,0.17142857142857143,0.2276422764227642,0.2183406113537118
248,SP:35723794a475f8b2aad0791586ffd8fed72407d5,"The paper address the the problem of continual learning for image classification. This approach focuses on how to reuse, add, fix or update neurons to learn tasks incrementally. The method is based on an existing method name SpaceNET.",The paper proposed a class incremental learning method that can exploit the similarity of classes for knowledge transfer. The previous work CAT did this only in the task incremental learning setting. The paper did some analysis in the experiment section to show how the transfer can happen in some limited experiments. ,"This work aims at addressing the stability-plasiticity dilemma in the class incremental learning scenario by exploring which model components shold be reused, added, fixed or updated. Authors proposed to make use of existing knowledge in previous tasks, including identifying and adding similar knowledge that could be reusable for forward transfer and preventing dissimilar knowledge from being transferred to avoid forgetting, hence achieving better balance between catastrophic forgetting and forward transfer. Extensive ablation studies are conducted to demonstrate the effectiveness of the proposed method.","### Summary: The paper studies replay-free continual learning with the focus on the plasticity-stability dilemma. More specifically, the paper proposes the KAN method for class-incremental learning where the forward transfer is achieved by detecting similar knowledge and reusing the first few layers, and negative backward transfer can be alleviated sparse connection allocations for different classes of different tasks. The paper also studies a limitation of the softmax layer, which is an interesting contribution.  ",0.21052631578947367,0.21052631578947367,0.2631578947368421,0.19607843137254902,0.23529411764705882,0.19047619047619047,0.1568627450980392,0.09523809523809523,0.13333333333333333,0.11904761904761904,0.16,0.21333333333333335,0.1797752808988764,0.13114754098360656,0.17699115044247787,0.14814814814814814,0.19047619047619052,0.2012578616352201
249,SP:35b62e1a679906de35e75acecef9c39f1d5571ec,"The paper studies distributed mean estimation in the $d$-dimensional space where each client can use only $d(1+o(1))$ bits for communication. A crucial assumption is that the server and the $n$ clients can have shared randomness while each client can independently communicate with the server. Under such an assumption, the authors proposed two schemes and demonstrated the efficiency of their new approaches. ","The paper considers the problem of communication-efficient distributed mean estimation. This paper proposes a novel algorithm DRIVE which uses as little as almost 1 bit for each coordinate of the vector and estimate the distributed mean with bounded error. The idea is to use a uniform random rotation to minimize the vNMSE. Besides, DRIVE can use structured random rotation to accelerate the compression for high dimensional vectors. The empirical evaluation of DRIVE shows that federated learning with gradients compressed by DRIVE can achieve faster convergence than other compression schemes.","This paper studies the distributed mean estimation. In this problem, there are n clients,  each of which has a d-dimensional real-valued; the goal is to estimate their mean on the sever side using minimum communication cost. The paper focuses on 1-bit quantization methods, i.e., each client sends one bit per coordinate and d(1+o(1)) bits in total. The algorithms are standard (including the Hadamard transformation based method); the main difference is new scaling factors S. Theoretical analysis on the expectation and variance are provided. Empirical results also show some advantages of the new estimate. ",This paper studies the problem of distributed mean estimation under a single bit (per coordinate) communication constraint. It proposes a novel compression scheme based on random rotation and sign quantization. The suggested method is applied to distributed and federated machine learning problems and shows improved results over previously studied quantization methods.,0.23076923076923078,0.3384615384615385,0.15384615384615385,0.2,0.18888888888888888,0.17,0.16666666666666666,0.22,0.19607843137254902,0.18,0.3333333333333333,0.3333333333333333,0.1935483870967742,0.2666666666666667,0.1724137931034483,0.1894736842105263,0.24113475177304966,0.22516556291390732
250,SP:3638aad87081bfb75b6ad588294965b2327f8b0f,"The paper considers the speed on learning to route an equilibrium for in classical network routing models with congestion. The main application could be routing suggestions made by an application such as google maps.   They consider two versions:   (1) static: when the congestion function is fixed throughout the learning process, called static process. In this case, equilibrium is the minimizer of the potential function, so algorithms minimizing the potential can lead to fast convergence to equilibrium. This leads to a $O(1/T^2)$ error rate as a function of the time.   (2) which is called stochastic, but appears to be worse case instead, where routers can use multiplicative weight, leading to a $O(1/\sqrt{T})$ error rate.   Under some smoothness assumption on the congestion functions (bounded and Lipschitz) the paper proposes a ""best-of-both worlds"" algorithm called ADAWeight, that claims to achieve the $O(1/T^2)$ regret in the static case and the $O(1/\sqrt{T})$ regret in case (2).","Regret-minimizing routing in nonatomic congestion games is a well-studied topic and it is known that simple, classical algorithms, such as multiplicative weights, fare well in this context. Specifically, such algorithms can converge to an equilibrium at a rate of O(1/√T) when cost functions are noisy. However, this leaves something to be desired; when the environment is static (no perturbations to cost functions are possible), this convergence rate is not optimal, motivating the search for algorithms with faster convergence rates. Such algorithms do exist but might not be robust when perturbations do occur. The key contribution of the paper is an adaptive exponential weights algorithm (ADAWEIGHT) that effectively interpolates between the two algorithmic approaches. An appealing property of ADAWEIGHT is that it does not require knowing the specifics of the environment and is scalable (in the size of the network).",The paper analyzes the Adaptive Exponential Weight Updates (AdaWeight) in the learning algorithm for dynamic path recommendations in networks. It shows that it reaches Wardrop Equilibrium at a rate that is a polynomial in the number of paths of the network and the number of epochs. The main contributions of this paper are to display that this algorithm reaches Wardrop Equilibrium in static and stochastic networks without the use of an optimizer. ,"This paper studies the equilibrium computation of congestion games and proposes a new algorithm, named ADAWEIGHT, that achieves $O(1/\sqrt{T})$ convergence in the stochastic regime and $O(1/T^2)$ in the static regime. The algorithm has several desirable properties:   1. it attains the best rates in the stochastic and static regimes,  2. the rates depend on $\log P$, not on $P$,  3. it is parameter-agnostic (no knowledge of parameters is needed),  4. it enjoys any-time guarantee (i.e., last-iterate convergence).   The results are obtained using some tools: primal and dual extrapolation, weighted-average analysis, and an adaptive learning rate. Experiments confirm the fast convergence of ADAWEIGHT in both regimes, while the existing ACCELEWEIGHT fails to converge in the stochastic regime.   --- After rebuttal ---  I appreciate the authors' kind response. I think the paper provides solid technical results, while the contribution to the ML community is somewhat marginal. Taking those into account, I keep my score of 6. I think the paper would be strengthened if the authors could work more on the smooth trade-off between the stochastic and static regimes. ",0.1696969696969697,0.12727272727272726,0.15757575757575756,0.1258741258741259,0.17482517482517482,0.2777777777777778,0.1958041958041958,0.2916666666666667,0.13978494623655913,0.25,0.13440860215053763,0.10752688172043011,0.18181818181818182,0.1772151898734177,0.1481481481481481,0.16744186046511628,0.15197568389057753,0.15503875968992248
251,SP:363b11cdcf8e0589505f53b25f3f391f1460e06a,"The paper proposes a method for hybrid model-based/machine-leanring learning, wherein a model is decomposed into an interpretable parametric prior and a residual (typically represented by a neural net). The paper demonstrates that prediction error minimization does not accurately identify the parametric component and proposes an alternating optimization method that augments prediction error loss with component-specific losses. The paper also introduces a variant where auxiliary data can be used to define the loss.  Theoretical guarantees are provided where auxiliary losses are given as upper-bound constraints and exact minimization is performed. Also, uniqueness and convergence guarantees are provided for the linear approximation case using a more practical algorithm.  Experiments show the efficacy of the method in linear and non-linear settings, in both identification and prediction.","Authors propose a general framework for learning and ensuring identifiability with hybrid models. They derive strong theoretical guarantees as well as a proof of convergence in a simple affine setting. They validate their framework in several simulated experiments and the Natl dataset, and draw comparisons with state-of-the-art hybrid model Aphynity as well as Neural ODEs. Experiments confirm that the proposed framework increase interpretability and maintain high prediction performances","This paper studies the identification and prediction of dynamical systems from observed data by combining statistical approaches with physical priors. The main idea is to decompose the approximation of unknown dynamics into two parts: a parametric physics-based model $h_k$ (used to incorporate prior knowledge, or to identify parameters), and a non-parametric statistical model $h_u$ (used to fit the unknown, or unobserved degrees of freedom).   Since the statistical part has a priori no restrictions, the overall problem is ill-posed in the sense that the statistical part can fit all of the dynamics. To ensure well-posedness, the authors propose to convert the joint learning problem to learning $h_k$ and $h_u$ subject to constraints: $h_k$ is given an allowed error tolerance and $h_u$ is given an allowed deviation from 0. The authors show that this modification implies well-posedness, and subsequently design an alternative minimization algorithms to solve for $h_k$ and $h_u$ iteratively. It is proved that this algorithm converges in the affine setting (via sequential projection to find intersection of convex sets) and it is demonstrated experimentally in low and high dimensional systems that this method works well in practice outside of the convex setting.","This paper investigate a novel method to recover well-posedness and interpretability for the problem of learning a physical and machine-learning hybrid model, by controlling the ML component and the MB hypothesis. The authors also study the case where auxiliary data are introduced. Experiments show the performance of the proposed method in both synthetic data and real-world ocean dynamics.",0.12403100775193798,0.20155038759689922,0.20155038759689922,0.19718309859154928,0.16901408450704225,0.08780487804878048,0.22535211267605634,0.12682926829268293,0.4262295081967213,0.06829268292682927,0.19672131147540983,0.29508196721311475,0.16,0.15568862275449102,0.2736842105263158,0.10144927536231883,0.18181818181818182,0.13533834586466165
252,SP:367d15d18e0e17571b39a85b8f7b74857bab6d0d,"This paper proposes ImageBART, an improved approach built on a recent success of an adversarial quantized representation learning model (e.g. VQGAN with autoregressive transformer) by replacing the single-scale autoregressive model with a multinomial diffusion process with a diffusion step-wise autoregressive encoder-decoder architecture. The (diffusion step-wise) encoder combined with a global cross attention mechanism from the adjacent diffusion step enables modeling of bidirectional context, which is not considered from the previous work (Taming transformers, TT).","The paper addresses image generation problem and proposes a combination of 3 existing methods: VQ-VAE/GAN, multinomial diffusion and BART. The system works as follows: the images are first tokenized with VQ-VAE/GAN, then 3-5 steps of multinomial diffusion are run on discrete tokens, then those steps are inverted with BART at each step (starting from random noise). Quantitatively, paper evaluates FIDs on the LSUN-{Churches,Beds,Cats} and on the FFHQ datasets, choosing 6 methods as baselines (VDVAE, DDPM, StyleGAN2, BigGAN, DCT, TT). Qualitatively, same datasets + additional ImageNet results are presented. Also, the paper contains qualitative results for inpainting and/or text conditioning. A few ablations are presented: joint/independent training of diffusion steps, number of decoder/encoder layers.","The paper proposed a coarse-to-fine image synthesis method which aims to incorporate global context into autoregressive models by a diffusion process. The authors divided the whole task into two subtasks: 1) discrete representation learning task 2) learning a Markov chain to reverse a fixed multinomial diffusion process. Experiment on unconditional image synthesis, class/text-conditional image synthesis and image synthesis with mask were shown.","This paper presents ImageBART, a generative model of images that uses multinomial diffusion to model the distribution of discrete latent codes extracted from a VQ-GAN encoder. Unlike prior diffusion based models, the reverse diffusion processes are modelled autoregressive, which enables a large reduction in the number of diffusion steps required. The model can condition on contexts such as a caption or semantic segmentation map, and unlike typical autoregressive models, is capable of image completion for arbitrary masked regions.",0.17721518987341772,0.12658227848101267,0.189873417721519,0.08943089430894309,0.13008130081300814,0.18181818181818182,0.11382113821138211,0.15151515151515152,0.189873417721519,0.16666666666666666,0.20253164556962025,0.1518987341772152,0.1386138613861386,0.13793103448275865,0.189873417721519,0.1164021164021164,0.15841584158415845,0.16551724137931037
253,SP:36865f44206f70bae377b585b4d60f38d5bf492f,"In this paper, the authors study issues on regulating algorithmic filtering in social networks. They describe a system involving three actors — user, auditor, and social media platform. They prove a result claiming that a regulator only needs to test regulation for the ""most gullible user""; then, for a specific class of regulations, they give an algorithm that they claim tests the regulation for said user. Finally, they give some technical conditions on when regulations do not affect the reward function of the social media platform.","This paper studies the problem of auditing whether algorithmic filtering implementations respect a given regulation. The authors propose a simple hypothesis test that regulators could use to determine compliance given black-box access to a filtering algorithm. They show that their algorithm has desirable asymptotic properties. Finally, the authors show that under their framework platforms are incentivized to provide diverse content to their users.","This paper focuses on the issue of (legally) regulating filtering of content in algorithmic feeds. It assumes that feeds containing certain sets of items violate a regulation, and develops a statistical estimator which allows an external auditor to determine whether a platform complies with the regulation by comparing user feeds. Is shows theoretically that, under certain conditions, it is possible to find feeds that comply with the regulation while minimising the platform's loss (e.g. in terms of the revenue). If further shows that platforms that diversify user feeds might have an easier time complying with the regulation. "," In this paper, the author studies the adjustment of content filtering algorithms on social platforms and proposes a system of ""one platform, one user and one auditor"" to study these problems in response to two problems. The author proposes a Regulatory procedure, and on this basis, studies the impact of content on users, supervision costs, and content diversity. Finally, analysis shows that content diversity plays an important role in coordinating the interests of regulatory agencies and platforms.",0.16470588235294117,0.21176470588235294,0.2,0.234375,0.203125,0.1717171717171717,0.21875,0.18181818181818182,0.22077922077922077,0.15151515151515152,0.16883116883116883,0.22077922077922077,0.18791946308724833,0.19565217391304351,0.20987654320987653,0.18404907975460125,0.18439716312056736,0.1931818181818182
254,SP:36c995ffad9240ec7e074a9e6b1a7b3e22cd19ca,"In this work, the authors explore the use of language as auxiliary supervision for long-horizon imitation learning tasks. The authors show that using instruction modeling is able to improve the performance in planning environments when training with a limited number of demonstrations on the BabyAI and Crafter environments. They further show that instruction modeling is most important for tasks that require complex reasoning in the experiments. ",This paper investigates the usage of language prediction for imitation learning. The authors distinguish between a goal (a short description of the task) and an instruction (a detailed description of subtasks that span a sequence of observations). They show that predicting instructions at each step as an auxiliary loss improves the performance using lesser number of demonstrations. They experiment with minigrid and crafting environments to show the efficacy of the proposed approach.,"This paper explores a simple, elegant idea: the use of natural language as supervision when performing instruction following tasks. Centered around an evaluation of various tasks in the BabyAI complex grid world suite, as well as the Crafting environment introduced by Chen et. al (2021), the proposed approach takes an easy-to-digest tact: eat a sequence of observations from the environment with a Transformer, and predict two things: first, given a goal, predict the action to take (a traditional behavioral cloning objective). Second (and a core contribution of the approach) treat language prediction as an auxiliary task, predicting a sequence of language instructions that specify how to perform the task (which I liken to “subtasks” or some compact language describing an immediate outcome to optimize for, though the authors should please correct me if I’m misinterpreting!).  These language assumptions are assumed to come from an oracle (concretely, in the implementation, generated synthetically exploiting the simulated nature of the environments), though the authors argue that this sort of “guidance instructions” appears naturally in the wild; videos have captions, humans could provide this feedback cheaply, etc.  The bulk of the evaluation evaluates sample efficiency and performance on the long-horizon tasks in BabyAI/Crafting, comparing the proposed approach to several meaningful ablations (no auxiliary objectives, a hierarchical variant, a self-supervised contrastive variant, and the proposed approach). These baselines are incredibly well thought through, and serve as valuable comparison points for the proposed approach.   The final, perhaps salient point of this work is the data regime; the main body of the paper and the appendix both argue that the true value of this approach (language as auxiliary tasks) is when (1) we are operating in a moderate data regime — too few data, and we can’t learn decent behavior, too much data, and we can just learn tasks without additional supervision, and (2) when the supplemental language information is providing something new, over the goal information itself. Experiments on the ALFRED suite show that because intermediate language instructions can be predicted from the goal itself, they don’t have inherent value. ","This work proposes using instruction modelling as an auxiliary objective to improve long-term planning. On BabyAI and Crafter, this method demonstrates significant improvement on longer hop levels (e.g. 3 hop BabyAI).",0.29850746268656714,0.3582089552238806,0.16417910447761194,0.375,0.08333333333333333,0.03133903133903134,0.2777777777777778,0.06837606837606838,0.3333333333333333,0.07692307692307693,0.18181818181818182,0.3333333333333333,0.28776978417266186,0.11483253588516747,0.22000000000000003,0.12765957446808512,0.1142857142857143,0.05729166666666667
255,SP:36cca7d5572ebc5b037efdd210ab58ac4a27999f,"The work proposes a novel method for OOD detection. The method itself uses as scoring function the norm of the gradient of the KL divergence of the softmax vector to a uniform distribution. Importantly, this method does not require ground-truth OOD data and does not require hyperparameter tuning. The main contribution of the work is empirical demonstration that the method achieves state-of-the-art performance on a variety of datasets, as well as ablations to further understanding into the effectiveness of the proposed method.  ","The manuscript proposes using the gradient norm of a trained classifier for out-of-distribution detection. Concretely, the norm of the gradient of the KL divergence between the actual prediction and the uniform prediction with regard to the network parameters is used to distinguish in-distribution and out-of-distribution data. Results on several benchmarks show this method has better OOD detection performance than some other methods. ","This work presents a new method for detecting out-of-distribution inputs that relies on the gradient norm of the KL divergence between the model’s softmax output and a uniform probability distribution.  The method is easy to implement (using the norm of the gradient of a standard cross entropy loss with a uniform label vector), can be used at inference time without access to true labels, and does not require training an additional model.   The authors present empirical evidence that their method outperforms prior work by a significant margin, reducing the false positive rate (FPR95) for out-of-distribution detection by 10.89% and 14.47% on ImageNet and CIFAR-10 respectively. Additionally, the authors perform an ablation study that explores how their method performs as they vary layer parameters, type of target distribution used (uniform versus real), temperature, type of gradient norm, and using only individual terms that form a decomposition of the gradient norm. ","The work presents GradNorm which uses the norm of gradient, back-propagated from the KL divergence between the softmax output and a uniform probability distribution, as the uncertainty score for OOD detection. The experimental results show that the proposed method outperforms the baselines on ImageNet1k vs OOD and CIFAR vs OOD benchmarks. Ablation studies are presented to help better understand the proposed method. The paper is written well and the experiments are comprehensive . ",0.2558139534883721,0.3488372093023256,0.3023255813953488,0.373134328358209,0.2537313432835821,0.1910828025477707,0.3283582089552239,0.1910828025477707,0.3561643835616438,0.1592356687898089,0.2328767123287671,0.410958904109589,0.2875816993464052,0.2469135802469136,0.3270440251572327,0.2232142857142857,0.24285714285714285,0.26086956521739124
256,SP:36f6f10664799e32b87e660953ecd6e34c60a9be,"This paper introduces a set of architectural components and design principles for constructing neural networks for embodied control. The proposed method is biologically inspired, and can be used to build neural networks specifically designed for the considered agent, e.g., the swimmer agent studied in the paper. The paper provides a literature review, describes the method, and evaluates it in an ablation study using the DDPG algorithm.",The paper addresses a really important challenge: To understand innate contributions to neural circuits for motor control. Its goal is to present a set of reusable architectural components and design principles for embodied control.  They show that a resulting model can learn to swim more efficiently and requires fewer parameters while achieving similar accuracy as an MLP. ,This paper proposes a set of bio-inspired Neural Circuit Architectural Priors (NCAP) as ingredients for building trainable networks for continuous control. The authors gave one example of translating the C. elegans locomotion circuits to a network using NCAP to control a swimmer agent in a simulated environment. The proposed network trained by deep reinforcement learning methods (DRL) and evolution strategies (ES) showed comparable performance and more efficient computation with fewer parameters.,"This paper proposes both a set of artificial neural components and interesting principles for producing biologically-inspired neural networks for embodied control. This work aims to be at the intersection between neuroscience and machine learning for improving the design of artificial neural networks as well as improving our understanding of observed biological networks. Various components of biological networks are replicated in their framework, e.g., the balance between excitation and inhibition, intrinsic oscillators, or sparsity. The model is evaluated on C.elegans motion locomotion circuit using different types of training strategies.  ",0.23880597014925373,0.26865671641791045,0.34328358208955223,0.21052631578947367,0.22807017543859648,0.2638888888888889,0.2807017543859649,0.25,0.25274725274725274,0.16666666666666666,0.14285714285714285,0.2087912087912088,0.2580645161290322,0.2589928057553957,0.2911392405063291,0.186046511627907,0.17567567567567569,0.23312883435582826
257,SP:371db603f5151d9d771a89413e0f7a487880b621,"This paper introduces an online MCMC framework for semi-supervised learning with variational autoencoders. The idea of the paper stems from physics problems, where one typically wants to infer certain parameters of a data generating process from empirical data. An example of this is quantitative MRI.   The authors consider the model of data generating process known, and also assume a distribution mismatch between the simulated data, as well as the empirical data, which may include certain confounding factors that may be impossible to model. During the the training of a VAE, the authors progressively improve the psudolabels via a Metropolis-Hastings scheme. The key novelty of the paper is that the proposal distribution for the sampler is an posterior distribution $\hat p(\mathbf{y} | \mathbf{x})$, modeled by the VAE itself. ","Variational inference and distribution sampling are two popular methods for statistical inference. The present paper combines these two approaches in a semi-supervised learning setting, where the distribution of unlabeled data may differ from that of labeled data. Experimental results are provided for an MRI dataset, where the task is to infer the imaging/tissue parameters.",This paper addresses semi-supervised learning problem. The authors propose to adopt Metropolis-Hastings Markov chain Monte Carlo (MH-MCMC) into the sampling process of posterior in conditional variational autoencoder (CVAE) learning. Experimental results on MRI datasets demonstrate that the proposed method performs better than existing methods regarding the number of samplings.,"The authors present a scheme for parameter inference that builds on both simulated/labelled data and real/unlabelled data. It uses MCMC with a CVAE proposal distribution to infer the data labels, and then trains the CVAE on the mixed data, bootstrapping the process. They evaluate their method on an MRI task.",0.11450381679389313,0.11450381679389313,0.0916030534351145,0.19642857142857142,0.16071428571428573,0.15384615384615385,0.26785714285714285,0.28846153846153844,0.23076923076923078,0.21153846153846154,0.17307692307692307,0.15384615384615385,0.160427807486631,0.16393442622950818,0.13114754098360656,0.2037037037037037,0.16666666666666666,0.15384615384615385
258,SP:371eb29fe6481fdd13d2cb9e79d4a026ced42de2,"The paper proposes a novel method for addressing synthesis problems, where they define a synthesis as an object or configuration that meets a set of constraints while optimizing one or more objectives. The authors motivate the problem with manufacturing settings, such as an additive printing path and extruding sequence, as well as soft robotic motion problem that can be described by a partial differential equation (PDE). The authors aims to address a couple of challenges: * Provide a differentiable surrogate for non-differentiable simulations in synthesis problem. The claim and show that this can help with learning a differentiable model of the process that can help learn how to generate viable design via a synthesis process. * Propose and develop a method that can solve many-to-one synthesis problems using a two-stage where one neural network based model learns how to generate designs and another model learns how to evaluate designs. Since both models are differentiable, the authors can train their framework end-to-end, which they claim provides accuracy improvements compared with directly learning a design to evaluation map and compute improvement compared to direct optimization of design to evaluation using method like BFGS. * Provide a series of objective functions that can evaluate various synthesis design for a diverse set of use cases, which the authors then use to inform their two-stage approach.  The authors outline the objective function for an extruder path planning case and a constrained soft robotic motion task. In both cases they optimize the objective and perform the synthesis task using their approach as well as the direct learning (design -> evaluation) and direct optimization (BFGS) baseline. The authors claim, and the experimental supports, that the proposed two-stage method has a performance advantage of direct learning and a compute advantage, in inference, over direct optimization. The authors also discuss how even the training cost for their method is higher compared to direct optimization, the training cost gets amortized in inference as the two-stage does not require re-training/re-running like direct optimization.","This paper considers two concrete problems in fabrication and design: path planning in 3D printing and soft robotic control. In both cases, there is trouble resulting from solutions that are not unique, in the sense that the inverse image from the codomain (solution space) is not injective. The paper proposes using a neural network architecture for feature space learning, which helps to collapse dimensionality in a way that allows the recovery of more robust (i.e. not the mean) solutions.  ","The paper addresses the challenge of synthesis - where the goal is to find a configuration of a physical system that satisfies a set of constraints and optimizes one or more objective functions representing the desiderata. The synthesis challenge has three components - slow and costly realization process for evaluation of each design configuration, the physical realization or its software simulation being usually non-differentiable, and many-to-one mapping from the space of design configurations to the specification (constraints + objective function). Differentiable surrogates are used to address the first two challenges (to the extent possible for a realization process). The third challenge makes supervised learning difficult because the many-to-one mapping from configuration to realization to goal can penalize learned for good configurations/designs just because these were different from the training set. Adding more alternatives in the training set would also not help because the supervised learner would learn to predict average over the configuration space, which would semantically be not meaningful.   The solution proposed in this paper is similar to some other recent work in this space which use autoencoders - a decoder captures many-to-one mapping from design configuration to design goals, and another network that maps goal to a design.  As described in the limitations section, the paper's claim to novelty is not fully justified by their choice of benchmarks and baselines, and limited discussion of related work. Further, the reviewer is not convinced (for reasons described in the limitation section) that the presented approach would be practical even for physical design problems (ignoring the larger synthesis literature). ","The core concern of this paper is the synthesis problem of choosing design parameters in a complex engineering design process mapping a high level goal to a high dim description of how the process achieves it. This is studied using the examples of 3D printing path planning and trajectory generation for a soft robotic arm.   The core contribution is to set up this problem in terms of two stages - mapping goals to a space of designs, and then an evaluation of designs in terms of a cost function. This breaks down one of the difficulties identified - the ill-posedness of the problem due to multi-valued maps. So, the authors set up an autoencoder architecture with an encoder that conjectures designs based on goals, and a surrogate model to evaluate the designs. ",0.06764705882352941,0.1588235294117647,0.10588235294117647,0.275,0.225,0.11787072243346007,0.2875,0.20532319391634982,0.2727272727272727,0.08365019011406843,0.13636363636363635,0.23484848484848486,0.10952380952380952,0.1791044776119403,0.15254237288135591,0.1282798833819242,0.169811320754717,0.15696202531645567
259,SP:372ef33b33b8781d0bcf4531d8a5931ca5194c63,"This paper proposes a method for detecting two types of distributional shifts: covariate shifts in the input space $\mathcal{X}$ (due to input corruption) and semantic shifts (due to test data falling outside the support set of ID classes, $ y_\text{test} \notin \mathcal{Y}$). The idea is based on the decomposition of KL-divergence between softmax prediction and a uniform vector. Furthermore, the authors propose Geometric ODIN to improve OOD detection and calibration, outperforming strong baseline on CIFAR10, CIFAR 100, and SVHN datasets. ","The submission proposes to evaluate covariate shift and ""concept shift"" through two scoring functions, one based on feature norms (for covariate shift) and the other based on angles (for concepts). To help the network decouple norms and angles, (a modified version of) a very recent method is used to decompose the feature norm and the cosine-distances and also calibrate the network. Experimental results appear to demonstrate good performance on both near and far OOD examples, as well as on calibration metrics. ","This paper studies the problem of Out-Of-Distribution detection, which a focus on analyzing the covariate and concept shift. Leveraging these analysis, it further investigate score functions that capture sensitivity to those shifts and then theoretically derive new score functions, to improve OOD detection. This proposed method leads to a calibration function that obtain the state-of-the-art calibration performance on both in-distribution and out-of-distribution data. Experiments are conducted on small image recognition benchmarks, such as CIFAR100 and SVHN. ","The paper starts from the KL divergence between a uniform distribution and the predicted distribution, based on which two score functions are derived for covariate shift and concept shift, respectively. The covariate shift score measures the feature norm while the concept shift score is essentially the difference between the cosine distance of the predicted class and the average cosine distance of all classes.  Furthermore, the paper integrates a variant of the geometric sensitivity decomposition method introduced in a recent work into the proposed score functions, aiming to make them more sensitive to distribution shift and to help calibrate the model as well.  A new dataset called CIFAR 100 Splits is also built, which contains 10 sub-datasets with an increasing level of concept shift compared to CIFAR 10.  The results show that the proposed approach achieves decent performance in OOD detection (in both the scenarios of covariate shift and concept shift) as well as calibration.",0.16666666666666666,0.17857142857142858,0.2261904761904762,0.2073170731707317,0.34146341463414637,0.25,0.17073170731707318,0.17857142857142858,0.12258064516129032,0.20238095238095238,0.18064516129032257,0.13548387096774195,0.16867469879518074,0.17857142857142858,0.1589958158995816,0.2048192771084337,0.23628691983122363,0.17573221757322177
260,SP:3768ecb8687e5228672183206d6f8dba742d1476,"The paper is motivated by choosing the causal mechanism for the so-called ""Gumbel-max"" structural causal models (SCM). The authors study constructing a coupling between two categorical distributions through a Gumbel-max-style sampler such that the coupling is maximal. The authors seek to learn such a sampler through a smooth parametrization and optimization with gradient-based methods. Two such architectures are developed. The performance of the proposal is studied numerically. ","This paper studies the problem of estimating counterfactual distributions from a set of interventional distributions. More specifically, let X represent treatment and Y represent outcome. Our goal is to estimate the counterfactual distribution P(y_{x=0}, y_{x=1}) from the marginal interventional distribution P(y_{x=0}) and P(y_{x=1}). In the most general cases, the counterfactual distribution is not identifiable. The authors make several parametric assumptions. First, the exogenous variables U are drawn from a Gumbel distribution. Once values of U are fixed, values of Y_{x=0} and Y_{x=1} are decided by a Gumbel-max function. Finally, we would like to find a counterfactual distribution P(y_{x=0}, y_{x=1}) that minimizes the Wasserstein distance between Y_{x=0} and Y_{x=1}, called the maximal coupling. The authors describe an algorithm to approximate such maximal coupling. Simulations are performed to validate the findings.","This paper introduces a continuously parameterized family of causal mechanisms where all members of the family are identical when used in a level 2 context but different when used in a level 3 context. The families contain the existing Gumbel-max technique proposed by Oberst & Sontag (2019), but a wide variety of other mechanisms can be learned by using gradient-based optimization. They show that the causal mechanisms can be learnt using a variant of Gumbel-softmax relaxation and that the mechanisms improve over Gumbel-max and other fixed mechanisms. Further, they show that the learnt mechanism is generalizable from a training set of observed outcomes and counterfactual queries to a testing set of observed outcomes and counterfactual queries. ","The authors propose a method to parametrize the functions of a certain class of structural causal models (SCM) called Generalized Gumbel-max SCMs. The aim is to train the model while minimizing the variance of treatment (counterfactual?) effects computed from them.  The paper introduces two different families of functions (gadgets) that can be learned through an optimization process aiming to find a maximal coupling of a pair of logits vectors defining a discrete distributions.  The experimental section compares the two gadgets with the Gumbel-max representation from Oberst and Sontag, and a cumulative distribution functions (CDF) inversion method. The experimental results seem to suggest that the proposed gadgets improve may achieve lower variance.",0.2638888888888889,0.20833333333333334,0.2361111111111111,0.11688311688311688,0.13636363636363635,0.16806722689075632,0.12337662337662338,0.12605042016806722,0.1504424778761062,0.15126050420168066,0.18584070796460178,0.17699115044247787,0.16814159292035397,0.15706806282722513,0.1837837837837838,0.13186813186813187,0.15730337078651685,0.1724137931034483
261,SP:376ea24783eaa6efb56068a8aebf1db2dc7a471a,"This paper proposes iDAD, an amortized learning framework for implicit (likelihood-free) experimental design. The iDAD framework extends the previously proposed deep adaptive design (DAD) method (Foster et al., 2013), which is an amortized learning framework for experiment design in likelihood-based (i.e. non-implicit) models, which learns a policy network offline that can then be deployed online for fast experimental design. Additionally, the proposed iDAD framework aims to alleviate the constraint in DAD that requires conditionally independent experiments.","In this work, the authors extend the DAD method by using implicit models to overcome some weaknesses of DAD. The authors further derive several bounds to design re-parametrization gradients. The authors show that the proposed method outperforms existing methods in several Bayesian experimental design tasks. ","The paper tackles the problem of adaptive Bayesian experimental design. Specifically, the authors try to relax the conditionally independent experiments and the explicit likelihood of the policy-based adaptive design model proposed by Foster. Code and supplemental materials are provided.   There are 3 contributions: - By leveraging the advance in variational mutual information, the authors can establish a suitable likelihood-free objective in the implicit setting.  - The authors theoretically show that they can obtain a unified information objective without conditionally independent experiments.  - Based on those insights, the authors propose the iDAD algorithm, which is much based on the previous DAD work.  ","This paper proposes a method for Bayesian Optimal Experiment Design (BOED) which uses a policy network to decide which experiment to perform next based on the history of past experiments and their outcomes. The goal is to enable experiments to be chosen quickly for real-time applications. The method trains a policy network in simulation, and does not require assumptions necessary for a prior method (DAD), which also trains a policy network in simulation but requires additional assumptions such as the availability of a likelihood model of the history given the parameters and policy. The present approach removes this requirement by using variational approximations together with a reparameterization which enables optimization with gradient descent. Experiments are done in a 2D location finding environment, a pharmacokinetics model and an epidemiology simulation. Results show that the proposed method almost matches the performance of an exact method which has access to exact likelihood on the first task, and does better than baselines on all the tasks while being fast to evaluate. ",0.1375,0.2125,0.2375,0.2608695652173913,0.2826086956521739,0.24,0.2391304347826087,0.17,0.1130952380952381,0.12,0.07738095238095238,0.14285714285714285,0.17460317460317462,0.1888888888888889,0.1532258064516129,0.1643835616438356,0.12149532710280374,0.1791044776119403
262,SP:37dec967d272f81f11a427cc37c9271819b7e9ae,"This paper studies lower bounds for without-replacement fixed-step-size SGD  (both single shuffling and random reshuffling) for the finite-sum optimization problem with an attention to dependence on the condition number of the problem. Their primary result is to prove a lower bound on the optimization error (realized by a convex quadratic construction) that suggests that unless the number of epochs $k$ is significantly larger than the condition number, neither single shuffling nor random reshuffling variants of without-replacement SGD outperform with-replacement SGD (previously, upper bounds for the with-replacement variants of SGD were considered to beat without-replacement SGD since often results did not take the condition number into account in the bounds). Moreover, they show their lower bound is tight with respect to a class of quadratics which includes the lower bound instance, suggesting that the analysis is somewhat tight.   The restriction on $k$ being large in order for without-replacement SGD to be more efficient is problematic for practical justification for using without-replacement SGD, since if $k$ is large enough, at some point one will prefer to use methods like conjugate gradient instead of using SGD, thus making the regime in which without-replacement experiences gains irrelevant to practice (since there is a better algorithm).","The paper shows that there exists classes of quadratic objectives where single shuffling or random reshuffling cannot offer improvements over SGD with-replacement sampling unless the number of passes exceeds the condition number of the problem. The paper supplements these lowerbound constructions with matching upperbounds that apply to the class of functions considered in their lowerbound construction. In addition, the paper offers simulations supplementing their theoretical results.","## I update my score after authors' rebuttal I thank the authors for their detailed rebuttal. I now understand better why and how the results can be useful. I hope that the authors would do a great job of fixing some of the confusing parts in their work, in particular about the exact novelty of their paper, most notably regarding the conditioning. As promised, I thought more about this work and decided to increase my rating from 4 to 5. ####  This paper presents improved upper and lower bounds for shuffling-based algorithms, which are without-replacement variants of SGD. In particular, the authors consider Random Reshuffling, which reshuffles the functions at each epoch, and Single Shuffle that reshuffles them only once at the very start. For both methods, the authors provide bounds that take into account the conditioning number.  The abstract of the paper about the prior work ignoring the problem geometry looks like a big overstatement, while the title seems to claim something already known from prior literature. Thus, I find the results to have limited motivation, even though both upper and lower bounds are of some interest. Another small flaw of the upper bounds is that they are stated for quadratic objectives.","This paper compares the efficiency of using different variants of stochastic gradient descent to find the minimum of a function that is the sum of multiple convex functions. It focuses on the case where the minimum of the function is dramatically flatter in some directions than in others. More specifically, it compares using a random sample at each time step, going through the samples in a fixed cycle, and repeatedly going through the entire list of samples in a random order. The conclusion is that the more complicated versions do not significantly outperform regular stochastic gradient descent unless the number of timesteps is unreasonably large.",0.10377358490566038,0.14150943396226415,0.1179245283018868,0.2537313432835821,0.1791044776119403,0.10344827586206896,0.3283582089552239,0.1477832512315271,0.23809523809523808,0.08374384236453201,0.11428571428571428,0.2,0.15770609318996415,0.14457831325301207,0.1577287066246057,0.1259259259259259,0.13953488372093023,0.13636363636363635
263,SP:382c6c99d620c21fde1e2a6613a6e3b4a6a4057a,"This paper proposes to solve composite visual reasoning problems represented by ungrounded visual Sudoku through augmenting SATNet with several extra learning stages. A differentiable digit classifier is first obtained through distilling unsupervised clustering, which produces quite accurately clustered but potentially classes-permuted classification results. A Symbol Grounding Loss is developed to align the correspondingly permuted prediction from SATNet with ground truth classes, while also supervising the reasoning result. The experiments on ungrounded visual Soduku demonstrate that the additional training steps substantially improved the performance. It is also shown that a proofreader, neural network that finetunes the digit classification before SATNet, further improves the performance.","The paper addresses how the symbol grounding problem arises in Visual Sudoku problem and proposes a self-supervised approach to grounding symbols in the SATNet architecture without relying on inadvertent label leakage. The leakage problem and its impact on SATNet for Visual Sudoku was known for previous work, so the paper’s key contributions revolve around a solution to this problem. In particular, it introduces a perceptual pre-training stage (clustering and distillation), a symbol grounding loss, and the performance boosting “proofreader” technique.","This work focuses on the problem of symbolic grounding in order to solve a sudoku task with perceptual input. The proposed method extends SATNet, a previous work based on the differentiable MAXSAT solver, and introduces several improvements that enable the model to solve the task without label leakage. Experiments are conducted on a visual sudoku dataset, demonstrating a superior performance of the proposed method.","Advancement of deep leaning comes with several limitations such as lack of interpretability, adversarial attacks and difficulties in incorporating logical constraints. To alleviate these limitations, neurosymbolic AI has been studied that integrates neural networks with logical reasoning. One recent breakthrough is SATNet, which proposes a differentiable MaxSAT solver and finds its application in visual reasoning problems, such as learning rules of Sudoku solely from image examples.    This paper tackles the symbol grounding problem in neurosymbolic systems which is the inability to map visual inputs to symbolic variables without supervision (also known as label leakage). In this paper, authors argue that without any label leakage (that is, without any supervision in digit classification task), SATNet’s abilility to solve Sudoku drops to 0%. As a result, this paper proposes a self-supervised pre-training pipeline that enables SATNet to solve ungrounded MaxSAT problems, where label data are available only for output variables of the MaxSAT problem. The contributions of the papers are (i) proposing a self-supervised clustering and a distillation process to train a visual classifier within a SATNet architecture, (ii) introducing a loss function specific to symbol grounding problems that allows to train logical constraints on ungrounded symbol representation. Empirical evaluation shows a similar performance on ungrounded MaxSAT problem compared to grounded MaxSAT problem (which allows label leakage).  ",0.14423076923076922,0.1346153846153846,0.17307692307692307,0.18072289156626506,0.3132530120481928,0.3125,0.18072289156626506,0.21875,0.0821917808219178,0.234375,0.1187214611872146,0.091324200913242,0.16042780748663102,0.16666666666666669,0.11145510835913311,0.2040816326530612,0.17218543046357615,0.1413427561837456
264,SP:38409d7b001e1615ad3833c5ade907eae7b68b4f,"The paper studies the problem of Domain Generalization (DG), where a model trained on single or multiple source distributions / domains is expected to generalize to data from unseen / novel distributions at test-time. The authors propose an adversarial teacher-student representation learning setup consisting of two stages -- (1) synthesizing “plausible” out-of-domain instances as augmented source data (termed Novel Domain Augmentation) and (2) enforcing the representations obtained from augmented and vanilla source domain instances to be similar and relevant to the task at hand (termed Domain Generalized Representation Learning). This is achieved via teacher (fed regular instances) and student (fed augmented instance) encoders and a novel data augmentor unit. In the second stage, the student encoder is incentivized to be good at the downstream task and ensure representations from augmented samples are similar to the vanilla ones (the teacher parameters are kept up to date via an EMA step). To generate novel augmented samples, the augmentor unit is incentivized to maximize the discrepancy between the representations obtained from the teacher and student encoders. Broadly, the proposed approach falls in the category of DG relying on hallucinating reasonable out-of-domain samples (without assuming any access to domain labels) during training. Obtained results indicate that the proposed approach is competitive with existing DG approaches.","The article proposes an approach for domain generalization (DG) based on adversarial augmentation and knowledge distillation. In particular, the model contains three components: a teacher, a student, and an augmenter. The student receives input processed by the augmenter while the teacher the original data, and a discrepancy loss measures the difference between student and teacher features for a given sample. Training is held out in two cyclic stages: in the first, the augmenter is fixed and the student parameters are learned by minimizing the semantic and discrepancy loss, with the teacher being an exponential moving average of the student. In the second stage, student and teacher are fixed and the augmenter is updated by minimizing the semantic classification loss and maximizing the discrepancy one. The teacher is used at test time. Experiments on a variety of benchmarks show the efficacy of the proposed approach.","This paper proposes a new data augmentation method for domain generalization. The main idea is to train a neural network-based generator to generate novel images that can be correctly classified by a student model and in the meantime, make the feature discrepancy larger between the student model and the EMA teacher model. The student model, which is fed with the generated images, is trained to minimize both the classification error and the feature discrepancy with its teacher. Only the EMA teacher is deployed at test time. This method is evaluated on both multi- and single-source domain generalization settings, outperforming previous SOTAs by a clear margin."," This paper proposed adversarial teacher-student knowledge distillation for domain generalization. Motivated by recent progress in self-supervised representation learning, the authors proposed two learning stages including domain generalization representation learning and novel domain augmentation. During domain generalized representation learning, the student is updated while the augmenter is fixed and the loss minimizes the discrepancy between the teacher and student. During the Novel domain augmentation learning, the student is fixed and the augmenter is updated. The objective is to maximize the discrepancy and encourage the augmenter to generate more distinct samples. Experiments on several public benchmark datasets very the effectiveness of the proposed method. ",0.17757009345794392,0.12149532710280374,0.17757009345794392,0.24305555555555555,0.2638888888888889,0.2523364485981308,0.2638888888888889,0.24299065420560748,0.36538461538461536,0.32710280373831774,0.36538461538461536,0.25961538461538464,0.21229050279329612,0.161993769470405,0.2389937106918239,0.2788844621513944,0.30645161290322576,0.2559241706161137
265,SP:385ce336955a3593446ce77a357248e1d2e712e1,"The paper analyzes the setting of shallow and deep linear neural networks under gradient flow and gradient descent and deduces a superpolynomial directional convergence guarantee. The result is proven under one assumption: spherically symmetric data distribution. Additionally, in the deep linear networks case, it requires a balancedness condition on the initialization. Among the existing convergence analysis of binary classification on separable data, the current result diverges by examining the dynamics of minimizing the population logit loss (rather than empirical loss) and tests the case of zero margin separable data. It mitigates some assumptions like prior convergence to zero classification error or overparameterization. The findings are shown empirically by experiments on both benign and real-world datasets and also partially on non-linear fully-connected ReLU networks. ","In this paper the authors present a convergence proof of linear neural networks trained with a logit loss. The analysis is based on previous works of Telgarsky, and is only concerned with binary classification of linearly separable points with 0 margin. In addition the authors make the assumption that the data distribution of the points are isotropic namely, spherically symmetric. With this the authors prove a directional convergence result, that states while the parameters magnitude approaches infinity as seen in previous works with separable data and logit losses,  the direction of the parameters converges to the orthogonal vector to the decision hyperplane. The authors present the result for shallow (1 layer) networks as well as deep linear networks. In addition to convergence the authors observe a trend of decreasing and then increasing parameter magnitudes that they also analyze empirically.  ","The authors study both shallow and deep linear networks under the assumption of spherically symmetrical input data. They show that gradient descent provides a super-polynomial convergence rate in this setting, which does not rely on any assumptions on the margin of the data or over-parameterization of the network. They provide some experiments demonstrating their results when their assumptions exactly hold, as well as with deep non-linear activation networks where their assumptions no longer hold. ","The authors study the directional convergence of linear and deep linear networks under a specific setting:  data is separable with zero margin and assumed to be spherically symmetric, and the model is trained by gradient flow/descent with the logit loss. In this setting, the authors improve upon previous results by showing fast directional convergence (superpolynomial). Moreover, unlike previous works the authors are able to analyze the directional dynamics along the entire training process without relying on over parametrization or zero error assumption.",0.1984126984126984,0.19047619047619047,0.1746031746031746,0.1510791366906475,0.17985611510791366,0.22077922077922077,0.17985611510791366,0.3116883116883117,0.26506024096385544,0.2727272727272727,0.30120481927710846,0.20481927710843373,0.18867924528301888,0.23645320197044334,0.21052631578947367,0.19444444444444442,0.2252252252252252,0.2125
266,SP:38ee872e811bb44369a0d988d3af1934ba9aff43,"This paper studies a practical problem, namely long-tailed recognition. To address this problem, this paper proposes two methods: UniMix and compensated cross-entropy loss. To be specific, UniMix adopts a balanced data sampler to improve the number of head-tail pairs for mixup. Experimental results show the effectiveness of the proposed method.  ","The authors propose two models to improve calibration for long-tailed recognition. 1) a Uniform Mixup is proposed, which adopts an advanced mixing factor and sampler in favor of the minority. 2) the authors propose a Bayias loss to compensate for the prior bias. The proposed method achieves good results on CIFAR-LT, ImageNet-LT, and iNaturalist 2018.","The authors proposed a new method based on two approaches, UniMix and Bayias, to sample input-output pairs from unbalanced datasets and use them in Mixup. The authors report experiments on CIFAR-10-LT, CIFAR-100-LT, ImageNet-LT, and iNaturalist 2018, showing superior performance in terms of accuracy and calibration scores over a variety of baselines.  While I could get the main idea behind the proposed method, which seems appealing, I had problems understanding all the details, mainly because of a substantial lack of clarity. Sentences are often disconnected which makes it difficult to understand what the authors mean.  My score is borderline but I am open to increase it if the authors can provide a solid rebuttal and substantially improve the paper in terms of clarity.",This paper addressed the problem of long tail recognition using a novel mixup technique. The proposed mixup technique (Unimix) considered the class prior and helps to create a more balanced virtual dataset. The author provide theoretical analysis on why Unimix is a better mixup technique. Extensive experiment also demonstrates the performance gain of using Unimix.,0.20754716981132076,0.22641509433962265,0.2641509433962264,0.27586206896551724,0.1724137931034483,0.09375,0.1896551724137931,0.09375,0.2545454545454545,0.125,0.18181818181818182,0.21818181818181817,0.1981981981981982,0.13259668508287295,0.25925925925925924,0.17204301075268816,0.1769911504424779,0.13114754098360656
267,SP:394dab2ce77867e01813e03c016908c4f9331548,"This paper improves the sub-supernet splitting strategy in few-shot NAS with a gradient matching (GM) score. During supernet optimization, the GM explicitly computes the ""agreements"" among different operations on an edge in the supernet, which is then treated as the criterion to split those most mismatched operations into different sub-supernets. Although the basic intuition makes some good points and is easy to grasp, I have several concerns on the methodology and the experimental designs of this paper. ","This paper proposed a method for few-shot NAS tasks. It found some operations may behave similarly and can be grouped into one and others may not. Those patterns should be considered to save the computation as well as improve accuracy. It proposed a graph construction and spitting pipeline to achieve the goal. Specifically, they used gradient matching and cosine distance to construct the graph among layers. Then, splitting the graph by solving a graph clustering problem.   It conducted experiments on three benchmarks (cifar10, cifar100, and ImageNet) over three search spaces (NASBench-201, DARTS, MobileNet Space) and compared with various baselines. The results show the strength of the proposed method. ","This paper aims at improving few-shot NAS by proposing a gradient-guided schema to partition the supernet into sub-supernets during the search. The proposed splitting criterion is based on the cosine similarity between the gradients from different child models. Namely, child models that have similar gradients are more likely to be grouped together without splitting, so as to save computational costs and to allow the splitting of more layers than the vanilla few-shot NAS, achieving better performance. Extensive experiments show its superiority over the few-shot NAS.","This work is a further exploration in the direction of Few-Shot NAS. It proposes a method of partioning supernet based on gradient-matching score. Compared with the method based on exhaustive-spltting, the method proposed in this paper can achieve better results. Its contributions are as follows:  1. Point out the problem based on exhaustive-splitting method; 2. A partioning method based on gradient-matching score is proposed, and this method can achieve good results on multiple search spaces and datasets.",0.2,0.2375,0.2125,0.15454545454545454,0.15454545454545454,0.15555555555555556,0.14545454545454545,0.2111111111111111,0.2073170731707317,0.18888888888888888,0.2073170731707317,0.17073170731707318,0.16842105263157894,0.2235294117647059,0.20987654320987653,0.16999999999999998,0.17708333333333334,0.1627906976744186
268,SP:39740bc182eda2989ce530b7640ab766dd12f20e,"This paper presents a method that can work on a single text style transfer task with multiple datasets. One example scenario as exemplified in the paper is the sentiment transfer task with datasets on multiple product categories. The basic idea of this work is to design a cross alignment between datasets from different categories, so the learning algorithm can focus on the expected stylistic information and eliminate other confounding factors. The proposed methods were evaluated on the sentiment transfer task with two different setups to demonstrate the value. ","This article deals with style transfer. It proposes an approach to generate texts with opposite sentiments in a adversarial context where the style is correlated with other contextual factors such as domain or noisy punctuation. Given the fact that other contextual factors are weakly labeled, the authors introduce Invariant Classifers to extract the style from other attributes. The general framework has been defined by (Arjovsky et al., 2019, IRM) and the basic classifier is (Kim, 2014). Section 3.2 describes the authors' contribution where multiple orthogonal classifiers enables them to transfer style while preserving other attributes. The authors propose relevant baselines corresponding to hypotheses ablations. The text generator is a 1-layer LSTM. The authors demonstrate that their approach is able to transfer sentiment while preserving domain or punctuation depending on the experiment.","Style transfer is a text generation task where a certain attribute of a sentence (like formality) is modified while preserving sentence content. It is generally studied in ""unsupervised"" settings --- no access to parallel data of sentences differing only in the target attribute. Nevertheless, algorithms assume access to corpora of unpaired sentences in each of the attributes, which may significantly differ in content. Prior works [1, 2, 3, 5] have noted this difference in content, which is encouraging models to modify non-target attributes like semantics (See Table 1 in [4]).  This paper is an attempt to fix this issue using invariant risk minimization (IRM) [6]. IRM encourages classifiers to be invariant across ""environments"", or datasets which share a target class but differ in other aspects which are not essential for classification (""spurious correlations""). In this work two spurious correlations are studied in sentiment transfer --- (1) a synthetic punctuation correlation; (2) product category of Amazon reviews. To perform style transfer, the authors first train two orthogonal IRM classifiers --- the first classifies the target attribute (""sentiment"") irrespective of the confounder (""product category""), and the second for the confounder (""product category"") irrespective of sentiment. These two classifiers are used to guide model generation, along with an LM smoothing and backtranslation objective (similar to [4]).  The authors compare their proposed approach against strong baselines and report promising results.  [1] - https://arxiv.org/abs/1910.03747   [2] - https://arxiv.org/abs/2010.05700   [3] - https://dl.acm.org/doi/10.5555/3016100.3016326   [4] - https://arxiv.org/abs/1811.00552   [5] - https://arxiv.org/abs/1905.13464   [6] - https://arxiv.org/abs/1907.02893","This paper considers a text style transfer task where no paired sentences are available. For example, we have positive Clothing reviews and negative Cell Phone reviews, the paper tries to generate negative Clothing reviews. Note that it is likely the model transfers not only the sentiment (from positive to negative) but also product category (from clothing to cell phone). To avoid that, the paper proposes to learn an invariant style classifier (which can classify the sentiment regardless of the product category). Besides, the paper also proposes to learn an orthogonal classifier which can monitor any style-independent changes (e.g. product category). ",0.2159090909090909,0.23863636363636365,0.23863636363636365,0.18045112781954886,0.12781954887218044,0.08921933085501858,0.14285714285714285,0.07806691449814127,0.20588235294117646,0.08921933085501858,0.16666666666666666,0.23529411764705882,0.1719457013574661,0.1176470588235294,0.2210526315789474,0.11940298507462685,0.14468085106382977,0.1293800539083558
269,SP:3989617d84a3522f05d5b69e9400c004b60e05b5,"This paper proposes to leverage prompt tuning on large pre-trained language models to achieve Lifelong Few-shot Language Learning (LFLL). Lifelong learning and few-shot learning have been considered different ML paradigms. The authors argue that with the emergence of modern large-scale pre-trained language models, AI models now have the capability to achieve these 2 at the same time. This work attempts to formally define the LFLL problem and benchmarked a strong pre-trained LM (T5) on 3 tasks (NER, text classification, and summarization) over 9 different datasets (defined as domains in LFLL).","The paper extended the use of prompt tuning to the scenario of life-long language learning. To prevent catastrophic forgetting, the paper proposes to generate pseudo samples as part of the training data when adapting to a new domain, and adding new set of prompt tokens when adapting to a new task. Using the T5 model, the paper demonstrated the effectiveness of the proposed methods using text classification, named entity recognition (NER), and summarization tasks. The method achieved better results for these tasks using the proposed life-long learning mechanisms, compared to fine-tuning, prompt tuning, and their combinations with regularization-based life-long learning methods such as elastic weight consolidation (EWC) and memory-aware synapses (MAS).","The paper defines the problem of lifelong few-shot language learning (LFLL) where the goal is to continuously learn a model with new few-shot tasks without forgetting the previous ones. Towards this problem, the paper introduces a prompt tuning-based framework that augments the pre-trained language model (T5) with continuous prompts (trainable prompt embeddings). Prompts are simultaneously optimized for task solving and data generation. During continual learning of the new domains, pseudo-examples of the previously seen domains are generated for episodic rehearsal and further KL-based consistency regularization is implemented to prevent drifting of the model. Lastly, for new tasks, separate prompts are appended with the existing prompts to enable the transfer of knowledge from previous domains/ tasks. The main contribution of this paper is to showcase the effectiveness of the recently proposed prompt tuning-based method for lifelong language learning. Empirically, this paper reports superior results over EWC and MAS methods when evaluated on text classification, NER, and summarization tasks.","This paper explores lifelong few-shot language learning. Their framework trains the model as a task solver and data generator. They use pseudo data for new domains, and additional prompt embeddings for new tasks.",0.19791666666666666,0.28125,0.11458333333333333,0.26495726495726496,0.09401709401709402,0.11585365853658537,0.1623931623931624,0.16463414634146342,0.3235294117647059,0.18902439024390244,0.3235294117647059,0.5588235294117647,0.1784037558685446,0.20769230769230768,0.16923076923076924,0.22064056939501778,0.14569536423841062,0.19191919191919193
270,SP:399f8f2d1f95882d467ff69aa6c4f9379d5b0a57,"The authors conduct theoretical studies on how activation quantization affects weight tuning, and their conclusion is that involving activation quantization into the reconstruction helps the flatness of model on calibration data and dropping partial quantization contributes to the flatness on test data. They present both empirical and theoretical findings, and propose the QDROP algorithm to exploit this phenomenon. QDROP randomly drops quantization during post-training quantization (PTQ) reconstruction to pursue the flatness from a general perspective. The authors also claim a new state of the art for PTQ on various tasks including image classification, object detection for computer vision, and text classification and question answering for natural language processing.","The authors proposed a random dropping quantization method at the post-training stage to achieve a low bit quantization network. By observing the performance on partial activation quantization,  the authors analyze the influence of incorporating activation quantization into weight tuning. Experimental results show that their QDROP methods have benefits on several scenarios, such as detection and NLP tasks.","This paper aims to analyze how activation quantization affects the PTQ process and provides some theoretical analysis and experimental results on the effects of activation quantization. The authors argue that previous studies only model the weight quantization as perturbation while ignoring activation quantization, causing a sub-optimal solution by missing out on the main factor in the performance degradation of quantized models in low-precision PTQ environments. Based on the empirical and theoretical analyses, this paper proposes QDrop to pursue flatness and demonstrate that partial activation quantization is more beneficial.  Contributions:  1. Observed the benefits of activation quantization in low precision PTQ. 2. Conducted theoretical studies on how activation quantization affects weight tuning. 3. Presented QDrop by showing that both integrating activation quantization into PTQ reconstruction and dropping partial activation quantization may help the flatness of the model, which is vital to the final accuracy. 4. Established a new SOTA for W2/A2 PTQ with QDrop.","This paper proposes the post-training quantization for extremely low-bit neural networks. By considering the activation quantization during reconstruction, the presented QDrop randomly drops the quantization of activations with higher loss flatness that adapts the activations with various activation bitwidths well. Experimental results have demonstrated the superiority of the presented method.",0.13761467889908258,0.26605504587155965,0.12844036697247707,0.3275862068965517,0.27586206896551724,0.10897435897435898,0.25862068965517243,0.1858974358974359,0.2692307692307692,0.12179487179487179,0.3076923076923077,0.3269230769230769,0.17964071856287428,0.21886792452830192,0.1739130434782609,0.17757009345794392,0.29090909090909095,0.16346153846153846
271,SP:39e7781c3129158f7dc4e52abfcac9e9081bd3d9,This paper proposes a new method for deep image retrieval. The main contribution is a differentiable and decomposable loss function based on average precision loss. The method shows superior performance and the theoretical seems correct.,"The paper focuses on the problem of learning to rank. The paper proposes a method, ROADMAP (RObust And DecoMposable Average Precision), consisting of two main contributions: - A new surrogate loss for average precision (AP). This loss is similar to recent losses that try to  provide a smooth approximation of AP / rank functions, but has some favorable design choices that lead to empirical improvements. - A ""calibration"" loss (essentially a classification loss), to be used together with the AP loss, that implicitly provides consistency between batches and facilitates training on small batches. Evaluation shows improvements with respect to recent AP approximations (e.g. [2, 3, 27]) on standard benchmarks.","This work tackles the problem of large-scale fine-grained image retrieval by proposing ROADMAP, short for ROBust And DecoMposable Average Precision which is a loss function that consists of two components 1) A smooth approximation to the non-differentiable average precision (AP) loss which provides an upper bound and better gradient flow compared to existing variants of the loss. 2) A calibration component which tackles the problem of mismatch between in-batch and true AP values commonly seen in previous methods.  The authors provided thorough theoretical proofs to support their claims on the proposed loss. Experimental results show consistent and significant improvements over existing methods in various categories, and gives state-of-the-art (SotA) performance among AP-loss based methods across three large-scale image (object) retrieval benchmarks. ","The paper proposes a new loss for image retrieval based on average precision.   Importantly, this paper proposes to address the problem in using AP using batches, which is not equivalent to the true AP computed in the whole dataset (which they refer to as 'non-decomposability'). It also proposes an improvement over other soft-binning approaches in which the version described in the paper is a proper upper-bound on the AP.  The paper lists as contributions: 1) a new surrogate loss for AP-loss that works by introducing a smooth approximation of the rank function with a different behavior for positive and negative examples, 2) improving the 'non-decomposability' problem associated with other AP-loss approximations, 3) experimental validation on CUB, SOP, and iNaturalist, showing state-of-the-art results.",0.34285714285714286,0.4,0.42857142857142855,0.2523364485981308,0.2803738317757009,0.2076923076923077,0.11214953271028037,0.1076923076923077,0.11450381679389313,0.2076923076923077,0.22900763358778625,0.20610687022900764,0.16901408450704225,0.16969696969696973,0.18072289156626503,0.22784810126582278,0.25210084033613445,0.20689655172413793
272,SP:39f9e3e2ac81070446c918cec1cd16b0d6927fb2,"This paper studies a specific distillation scenario, adversarial distillation, to improve the model robustness. Different from the constant sort supervision in the ordinary distillation, the teacher model will become progressively unreliable along with training, since the adversarial data are dynamically searched by the student model and might not be well identified by the teacher model. Therefore, the authors introduce an introspective adversarial distillation, which considers to bootstrap the learning from both the teacher model and itself. A range of experiments demonstrate its superiority on improving the model robustness.","In this paper, a new method called introspective adversarial distillation (IAD) is proposed for conventional adversarial training. Concretely, it targets the unreliable teacher case, where teacher is good at adversarial / natural data or none of it. Experimental results validate the effectiveness of the proposed method towards enhancing adversarial robustness. ","This paper proposes a new knowledge distillation (KD) method for adversarial training. The authors first observed that the soft-labels provided by the teacher gradually becomes less and less reliable during the adversarial training of student model. Based on this observation, they propose to partially trust the soft labels provided by the adversarily pretrained teacher.  ","Adversarial training has been developed for many years, and it aims to provide reliable classifiers in the era of deep learning. However, in many ML-driven scenarios, users might not want to retrain their big deep networks using adversarial training. The main reason is that training a reliable/adversarial-robust classifier will cost many computational resources. To address this issue, some methods are proposed to distil the robustness from adversarially pre-trained models, which is more practical than the direct adversarial training. This is also the point this paper focuses on. In my humble opinion, this is a very promising research direction and should be paid more attentions in future.  Compared to existed works, this paper argues that the adversarial training data of the student model and that of the teacher model are egocentric (respectively generated by themselves) and becoming more adversarial challenging during training, which causes failure of existing works. To address this issue, this paper proposes an Introspective Adversarial Distillation (IAD) to effectively utilize the knowledge from an adversarially pre-trained teacher model. Extensive experiments are conducted on CIFAR-10/CIFAR-100 and the more challenging Tiny-ImageNet datasets to evaluate the efficiency of our IAD. ",0.14772727272727273,0.19318181818181818,0.3409090909090909,0.2857142857142857,0.30612244897959184,0.32727272727272727,0.2653061224489796,0.3090909090909091,0.15151515151515152,0.2545454545454545,0.07575757575757576,0.09090909090909091,0.1897810218978102,0.23776223776223776,0.20979020979020976,0.26923076923076916,0.12145748987854252,0.14229249011857706
273,SP:3a20f88fd51674b9cda25f8ab3bfb4ab6996f54c,"This work proposes a motion representation learning method based on capsule autoencoders. It decomposes an input 2D keypoint trajectory into snippets and segments, and form a hierarchical representation that is also transformation invariant. The proposed motion representation achieves state-of-the-art motion classification results on a self-proposed synthetic 2D motion dataset as well as skeleton-based human action recognition.","This paper presents an unsupervised motion representation learning method with capsule autoencoders. Its framework is composed of two-level motion modeling: snippet-level for short and local motion capturing, and segment-level for full-length semantic-aware motion capturing. It presents a new dataset called as Trajectory 20 and performs experiments on two datasets.","The paper presents a method for incorporating transformation invariance using capsule networks in the unsupervised learning of motion representations. The proposed method solves the problem in two steps, snippet and segment learning, which correspond to a lower and higher level of motion signals (basic trajectories -> reconstruction/time -> Contrastive learning and reconstruction). The method was evaluated on a synthetic dataset and three skeleton datasets for action recognition achieving convincing results.","This work presents a Motion Capsule Autoencoder (MCAE) which learns to represent motions. The two-staged architecture first learns a set of snippet capsules, which learn motions for short time-spans, and then learns segment capsules that learn the long-term motions from all the snippet capsules. These capsules are learned end-to-end in an unsupervised manner with reconstruction losses and a contrastive loss. The approach is evaluated on a synthetic motion dataset proposed in this work, Trajectory20, as well as on three real-world skeleton-based human action recognition datasets, where it is shown to achieve strong performance when compared to previous state-of-the-art methods.",0.21311475409836064,0.22950819672131148,0.36065573770491804,0.2777777777777778,0.2777777777777778,0.30434782608695654,0.24074074074074073,0.2028985507246377,0.2018348623853211,0.21739130434782608,0.13761467889908258,0.1926605504587156,0.22608695652173913,0.2153846153846154,0.2588235294117647,0.24390243902439024,0.18404907975460125,0.2359550561797753
274,SP:3a4a6ed67c84189a48497493f617db50456f9059,This paper introduces a novel approach which constructs the best fitting kernel function with adaptive tuned parameters. This kernel function is obtained by the linear combination of multiple kernels sampled from a prior Gaussian Process. The experimental results of two different kernel-based algorithms show the superiority of the proposed method.,The paper proposes a zero-order optimization method where the optimized variable is a kernel function in Hyper-RKHS induced by a selected hyper-kernel. The method is an instance of the Bayesian Optimization method LINEBO [Kirschner 2019]. The contribution is in the adaptation of the LINEBO algorithm for efficient optimization w.r.t. positive definite kernel functions. The algorithm is applied to the optimization of kernel functions for C-SVM and GP regression. Experiments show significant improvement over existing methods. ,"This work is interested in kernel selection. In contrast with most of the literature they consider non-parametric class of kernels using hyper-kernels. Nevertheless, some previous works have already consider this setting but the authors argue that they had several limitations which motivates a new method. The authors first recall the background on Bayesian optimization and hyper-kernels then explain their method to mix the two while avoiding some computational complexity issues. They also show how to fix a naturally arising problem : the posterior mean of the hyper-GP may be positive. Finally, some theoretical and experimental results are provided.","This article proposes to use the hyperkernel formalism coupled with Gaussian Processes to derive the ‘best’ kernel for regression. The latter kernel depends mostly on the observations rather than on a pre-selected kernel family. The proposed procedure relies on several stages of Bayesian optimization as well as a clipping of the coefficients to avoid dealing with (indefinite) Krein kernels. The performance is presented on both synthetic and benchmark datasets, to assess visually the fit of the confidence intervals as well as the numerical performance.",0.2549019607843137,0.2549019607843137,0.23529411764705882,0.19753086419753085,0.20987654320987653,0.1485148514851485,0.16049382716049382,0.12871287128712872,0.1411764705882353,0.15841584158415842,0.2,0.17647058823529413,0.19696969696969693,0.17105263157894737,0.17647058823529413,0.17582417582417578,0.20481927710843373,0.16129032258064516
275,SP:3ab8fb0b5f18d65d81a3db9c19918540d1399873,"The paper shows a number of results on non-parametric density estimation using mixtures of product histograms with two proposed methods called \textit{multi-view histograms} and \textit{Tucker histograms}. A simple experiment is also performed to show these methods work better than the usual histogram estimation in some specific cases. There are two major theoretical contributions as follows.  Firstly, the authors give some asymptotic error control for the proposed methods for any true density functions based on sample sizes, number of histograms' bins, and number of mixtures.  Secondly, they prove that when the true density is a mixture of product distributions, the proposed methods are more favorable compared to the traditional histograms method in terms of the convergence rate.","The authors propose and analyse a novel method for nonparametric density estimation. They provide a method for providing a density estimate with low rank structure which is a mixture of densities that factorizes across dimensions, with each factor defined as a one dimensional histogram. They provide conditions under which their estimator is consistent and characterize upper and lower bounds on its error.","This work is concerned with the (high-dimensional) density estimation problem.  While most minimax rates  for  the  problem  are  expressed  in  terms  of  some  measure  of  smoothness  that  the  unknown  distribution is assumed to verify, the current approach adds more structure, by requiring that the joint density be tensor-decomposable (in a PARAFAC or in a Tucker sense), with Lipschitz components.  The authors show(theoretical analysis) that this subverts the smoothness-only minimax rates together with their curse of dimensionality. In order to illustrate their method, the authors further perform experiments in which –perhaps controversially– they relax the estimator search to an L2 problem.","The authors consider the density estimation problem in multiple dimensions. They study the convergence properties of a low-rank estimator: the estimator is the sum of a few tensor products of univariate histograms. They give asymptotic and finite sample bounds for convergence. When the true density is a sum of k tensor products of Lipschitz densities, the authors show an $L_1$ convergence rate of $n^{-⅓}.$   ",0.09166666666666666,0.14166666666666666,0.175,0.16129032258064516,0.22580645161290322,0.16346153846153846,0.1774193548387097,0.16346153846153846,0.3181818181818182,0.09615384615384616,0.21212121212121213,0.25757575757575757,0.12087912087912087,0.15178571428571427,0.22580645161290322,0.12048192771084337,0.21875,0.2
276,SP:3ac58f9bd996aba343b5d32b79c8da05666761d3,"The paper proposes MIME, a general algorithmic framework for federated learning (FL) that can adapt to (and match the performance of) arbitrary centralized algorithms like Adam, Momentum SGD, and SGD. The authors show that via the MIME framework the convergence of any centralized algorithm can be translated into the convergence of the algorithm in the FL setting. The authors also propose a momentum-based variance reduction (MVR) algorithm that the authors claim to be faster than any centralized algorithm. Finally, the authors conduct extensive numerical experiments to show that the algorithms under the MIME framework perform well on real datasets.","This paper studies cross-device federated learning setting where the number of clients is very large and most of the clients participate in only one round of communication. Because of this low client participation per round, it is hard to have any client state/memory. State is usually helpful in traditional federated learning setting (eg: in SCAFFOLD) in estimating client heterogeneity, which can then be used to correct for the bias of the client. To mitigate this, the authors propose two meta-algorithms: MIME and MIMELITE, to adaptive a general class of centralized algorithms to this cross-device setting. MIME uses variance reduction at the client (like SCAFFOLD) and MIMELITE does not.  The paper analyzes the effect of these meta-algorithms for three standard centralized (server-only) deep learning optimization methods: SGD, Adam, MomentumVarianceReduction (MVR). However, reviewer could not find the pseduo-code for the MVR in the paper. For SGD, and Adam authors show that MIME modification performs at least as good as their centralized base algorithms for nonconvex problems. For MVR, authors prove that the server-level momentum of MIME helps beat the performance of centralized baselines under low-heterogeneity.","The paper introduces a new generalized algorithmic optimization framework named MIME for adapting and analyzing the convergence of any optimization algorithm applied in centralized settings into cross-device federated learning environments. The algorithmic reduction is accomplished by decoupling the centralized optimization algorithm into optimizer (server-side) and parameter state (client-side) updates in federated settings. The convergence of the proposed adaptation is shown both theoretically and empirically over a range of challenging real-world datasets for different optimization algorithms. Interestingly, when applying the proposed federated adaptation for a specific momentum-based optimization algorithm, the adaptation is proven to be asymptotically faster than its centralized counterpart.",The paper proposes a framework MIME to convert centralized optimization algorithms to the federated learning setting. The key components are some control variates to reduce the effect of data distribution heterogeneity.  Contribution: 1. A framework to convert centralized algorithms to the federated learning setting. 2. Theoretical analysis to characterize the convergence of converted algorithms 3. Experiments show MIME framework can have better performance than FedAvg.,0.29,0.3,0.21,0.125,0.09895833333333333,0.1619047619047619,0.15104166666666666,0.2857142857142857,0.3230769230769231,0.22857142857142856,0.2923076923076923,0.26153846153846155,0.19863013698630133,0.2926829268292683,0.2545454545454545,0.16161616161616163,0.14785992217898833,0.2
277,SP:3b62213cad07e04d20bf38a70ef785cb6f2e2ced,"This paper proposes to use non-negatively homogeneous DNN to perform feature attribution with better computational efficiency at a minimal performance cost. The so-called non-negatively homogeneous DNN is obtained by removing the bias term from most common neural networks and using a Relu/Leaky_Relu activation function. By doing this, the model would be linearly scaled by the input data (F(ax) = a*F(x)). With this property, the integrated Gradient (IG) method can be easily computed with one call of gradient computing, which improves its original computational efficiency. In the experiment, this model shows comparable performance with the IG method but much less computation effort.  ","The paper addresses the question of attributing the predictions of deep neural networks to the input features. It proposes to combine a particular restriction on the prediction function: non-negative homogeneity (e.g. a deep neural networks without biases) with the fast gradient x input explanation technique. With this combination, it is possible to produce explanations that satisfy the completeness axiom that can be computed quickly (from a single function evaluation). The paper also argues that non-negative homogeneity makes the model invariant to contrast, which is a useful property in image recognition.","The authors propose a class of DNNs, which they call non-negatively homogenous, where the Integrated Gradient (which satisfy certain desirable axioms) for feature attribution can be computed in closed-form using a single gradient evaluation (as opposed to multiple gradients calls for a Riemann sum as in Sundararajan et al 2017). The authors propose instantiating such DNNs in practice by removing the bias term. They conduct experiments on two tasks where they show that removing this bias does not cause a substantial drop in accuracy while still computing IGs efficiently. ","The paper first identifies the intensive computational cost in using Integrated Gradient (IG) as an attribution prior in training explainable deep neural networks. To this end the authors propose to switch the network architecture to negatively homogeneous networks, specifically networks without bias term, a.k.a. $\mathcal{X}$-DNN, in this paper, where they prove that $\texttt{input} \times \texttt{gradient}$ is an equivalent attribution method to IG; and therefore can decrease the number of gradient evaluations in computing IG. The paper then conducts several experiments over the ImageNet dataset and several tabular datasets to validate the performance of the proposed method. ",0.16666666666666666,0.12962962962962962,0.2037037037037037,0.17204301075268819,0.22580645161290322,0.1978021978021978,0.1935483870967742,0.15384615384615385,0.21568627450980393,0.17582417582417584,0.20588235294117646,0.17647058823529413,0.1791044776119403,0.14070351758793967,0.2095238095238095,0.1739130434782609,0.2153846153846154,0.18652849740932642
278,SP:3c27f06a485b98f42b3d1bf5d92e6e4fcf5c8a74,"The paper proposes ODAC, a new RL algorithm for goal-directed tasks. Using a variational inference approach, ODAC automatically infer a dense environment reward from the goal and derives a probabilistic Bellman backup operator. The latter is used to develop an off-policy RL algorithm. The paper presents detailed theoretical derivation and empirical results on a suite of environments. ",This paper derives goal based reinforcement learning as a task of variational inference. They propose a novel bellman operator and learning scheme and provide extensive evaluations across various complex tasks. Empirical evidence backs the theoretical results showing that this method works in practice.,"This paper proposes to formulate goal-conditional reinforcement learning as a variational inference problem, termed as outcome-driven RL, and shows that the variational objective naturally generates shaped reward signals for learning. The variational objective results in an off-policy temporal-difference algorithm. Experimental results in robotic control tasks show the advantages of the novel algorithm.   ",This paper combines the recent work on viewing RL as Probabilistic Inference and the goal-conditioned RL setting. The proposed approach assigns rewards based on the likelihood of reaching the desired state-action pair and propagates this via learning a dynamic discount function based on time elapsed in the episode. The authors propose solving the objective via variational inference and demonstrate this leads to a probabilistic policy evaluation operator. An actor-critic approach is then taken to perform approximate policy iteration.  The authors show the advantage of their methods on high-dimensional continuous control tasks.  ,0.1864406779661017,0.23728813559322035,0.2542372881355932,0.32558139534883723,0.2558139534883721,0.25,0.2558139534883721,0.25,0.15789473684210525,0.25,0.11578947368421053,0.14736842105263157,0.21568627450980393,0.2434782608695652,0.19480519480519481,0.2828282828282828,0.15942028985507245,0.18543046357615892
279,SP:3c4ad5a4c033844cca34545389da1a9e9f007af5,"This paper introduces a large-scale LM trained directly on the raw HTML in a large-scale web crawl (the common crawl corpus). The resulting model is able to utilize the structure in HTML documents for a variety of tasks such as zero shot summarization, fine-tuning, classification and more. It looks like using structured data for pre-training and creating prompts provides a variety of advantages in LMs. The paper has a comprehensive set of experiments and a thorough ablation study.","This paper presents HTML-based large scale language model pretraining. The model architecture and pre-training method are both based on BART. The author presents the resulting model, HTLM, as a strong zero-shot learner while being on par with other large transformer language models on the GLUE fine tune tasks. the authors show that via various benchmarks in their experiment section.","The paper proposes a new large language model, called HTLM, short for Hyper-Text Language Model. The language model has been trained on 25TB of HTML text. The authors argue that HTML tags provide valuable information regarding document-level structure. Besides the scale, the authors also adopt an optional pre-training strategy where it is possible to provide size hints to govern the length of the generated text or each mask (alongside the BART-like training objective). The trained language model achieves state-of-the-art performance on zero-shot summarization by prompting the model to predict the text within the <title> tags. The method also achieves state-of-the-art performance on several other benchmark NLP tasks, performing better than large pre-trained models that were only trained on text data (as opposed to hyper-text data). The authors also discuss manual and automated prompt construction, and their evaluation shows that the model can also perform better than competitive baselines on the multiple classification tasks. A follow-up analysis also shows that the per-prompt-efficacy is higher for HTLM than for text-only language models. ","This paper introduces HTLM which is a language model pretrained on a large-scale web crawl hyper-text data. There are several contributions in the paper: * A preprocessing step to filter out noisy components in the web pages is proposed. The resulting simplified format, Minimal-HTML (MHTML), is likely to be composed of high-quality documents which can be used for pretraining. * A modified BART pretraining objective is proposed to inject noisy size hints to control the length of the span to be generated by the model during training. * A new prompting method in the form of HTML templates is described to accomplish generation (e.g., summarization & table-to-text) and classification (e.g., GLUE) tasks. * The resulting pretrained HTLM model has superior performance on zero-shot summarization and can do better than some existing language models pretrained on plain texts.",0.17073170731707318,0.24390243902439024,0.3048780487804878,0.3387096774193548,0.2903225806451613,0.18716577540106952,0.22580645161290322,0.10695187165775401,0.1773049645390071,0.11229946524064172,0.1276595744680851,0.24822695035460993,0.19444444444444445,0.14869888475836432,0.2242152466367713,0.1686746987951807,0.1773399014778325,0.2134146341463415
280,SP:3ccd13a92d7f8d8f99ffca69c24abd3589297752,"This paper aims to construct a GAN that can be applied to non-i.i.d. federated data. To achieve this aim, the authors propose an extension of Bayesian GAN called expectation propagation prior GAN (EP-GAN), which obtains a partition-invariant prior using expectation propagation. In particular, the authors introduce a closed-form solution for efficiency. The effectiveness of the proposed method is demonstrated using non-i.i.d data, including the toy data, image data, and speech data.","This paper proposes a method for learning Generated Adversarial Networks (GANs) for non-i.i.d data in a federated learning setting. This is accomplished with the use of a partition-aware prior via an Expectation Propagation (EP) algorithm embedded into a Bayesian GAN setting. Additionally, it proposes a closed-form solution for EP updates aiding efficient federated learning. The claims are substantiated with experiments on both synthetic and real data.          ","The goal of this paper is to train a Bayesian GAN on non-i.i.d. federated data. Specifically, the authors propose to adopt the newly-introduced expectation propagation (EP) prior, being partition-invariant, to address the non-i.i.d. federated challenge. Experiments on synthetic and real datasets are conducted.     ","The authors targeted federated generative modelling in an unsupervised setting. Specifically, the work is built on top of Bayesian GANs. In order to aggregate the information from different clients, the authors proposed to use expectation propagation (EP). It makes sense. Despite being a well established Bayesian inference algorithm, EP operating on the neural network parameters can suffer from intractability. The authors presented a low complexity solution. The experiment results showed improved FID and IS over multiple baseline methods. However, the overall performance is quite poor on the rather simple dataset CIFAR10, owing to the scalability issue of Bayesian models.",0.3,0.3125,0.2125,0.2535211267605634,0.19718309859154928,0.23529411764705882,0.3380281690140845,0.49019607843137253,0.1717171717171717,0.35294117647058826,0.1414141414141414,0.12121212121212122,0.3178807947019867,0.381679389312977,0.18994413407821228,0.2950819672131148,0.16470588235294117,0.16
281,SP:3d01992d588be1aaa5d843a4c904d328a8fc3cc7,"An auto-encoding (AE) loss that reconstructs data according to a local approximation of the autoencoding function. In particular, for each $x_i$ in the dataset, the model reconstructs data around a neighborhood of $x_i$ using a low-order Taylor expansion of the encoder-decoder composition around $x_i$. The loss to each point in the neighborhood is assigned a weight according to a simple kernel function. The author demonstrates empirical advantage compared to many other AE losses in extensive evaluations. ","The paper goes into detail about two major obstacles that frequent Autoencoders, namely overfitting to training data, and learning the correct local geometry of the data. They then claim that all existing methods struggle with one or the other, or they are so computationally taxing that they aren't scalable for most problems. The paper's main contribution is that they provide a new training method/loss function that uses a local 2nd order approximation of the decoder function. The paper claims that the decoder is more important than the encoder in correctly learning the local geometry of the data. They then go into the algorithm's details and then explore the related work of regularization of autoencoders. Finally, they run experiments to demonstrate the algorithm's properties as well as show that NRAE is compatible with other methods (VAE, WAE, CAE, and SPAE) and often gets better results combining methods.","Rather than reconstructing a given input $x$, the first- or second- order* Taylor approximations (centered at $x$) of the decoder are optimized to accurately reconstruct some neighborhood $\mathcal{N}(x)$ of $x$. More formally, the loss for a single $x, x'$ pair, where $x' \in \mathcal{N}(x)$ is written as $K(x, x') \cdot \left \Vert x - \tilde{f}(g(x')) \right \Vert^2$ where $g$ is the encoder and $\tilde{f}(x')$ is a Taylor approximation of the decoder $f$, centered at $x$. The authors evaluate the method across a wide array of empirical experiments and show that it produces superior autoencoding performance by many measures.  *There are two variants of the algorithm -- one that uses a first-order Taylor approximation, and one that uses a second-order Taylor approximation.","This paper proposes a new graph-based autoencoder dubbed ‘Neighborhood Reconstructing Autoencoder’ (NRAE) for learning smoother manifolds. The goal is to (1) learn an AE that is robust to data noise (2) learn a manifold with the correct geometry. The authors claim the focus of regularization to induce a smooth representation should, unlike the previous work, be on the decoder and not the encoder. The proposed model minimizes an objective that encourages neighbouring xs (based on a predefined graph that is constructed using some kernel K(x,x’)) to also be close by in latent space by having a small ‘neighbourhood reconstruction loss’. The authors finally perform an extensive set of experiments on a variety of datasets which demonstrates that the proposed model is able to learn a more smooth manifold compared to other AE baselines, as well as having a better generalization error. ",0.23170731707317074,0.23170731707317074,0.2804878048780488,0.1390728476821192,0.16556291390728478,0.1590909090909091,0.12582781456953643,0.14393939393939395,0.1597222222222222,0.1590909090909091,0.1736111111111111,0.14583333333333334,0.16309012875536483,0.17757009345794394,0.20353982300884957,0.14840989399293283,0.1694915254237288,0.15217391304347827
282,SP:3d24075b997327c7f79cf379542a8dd2f67a7095,"This paper first extends value equivalency (VE) to k-step value equivalency, where the equivalency holds for k-step application of the Bellman operator. In the limit of k to infinity they then show this becomes ‘proper value equivalence’ (PVE), for which the policy uniquely defines the VE class. They also show that MuZero is a special case of the PVE class, and derive empirical experiments on PVE, also extending MuZero with a new loss.  ","This paper introduces a k-th order generalization of the Value Equivalence (VE) principle of Grimm et. al. 2016, generalizing that the candidate value functions are equal under multiple (rather than one) iterations of the Bellman operator for a candidate set of policies. Taking k -> infinity leads to the special case of Proper Value Equivalence (PVE), which only depends on a candidate set of policies. Several structural results and derived, and it is shown that models obtained from PVE (with the set of deterministic policies) have the same optimal policies as the true environment.  A loss function is introduced to learn k-VE models, connections with MuZero are drawn, and experimental simulations are carried out. ","The paper proposes an extension to the value equivalence (VE) principle in MBRL called Proper VE (PVE). The original VE states that a model is equivalent to the true model if the induced Bellman operators coincide for a set of policies and value functions. PVE is motivated by the observation that a repeated application of the Bellman operator will eventually converge to a value function of a given policy for a chosen model. PVE defines a model to be equivalent to the true if, for a set of policies, it results in the same value functions as in the original MDP. While PVE assumes an infinite Bellman operator application (called order-∞), the paper studies the relation between non-asymptotic versions of PVE.  For example, the authors prove that order-k PVE models are also order-K PVE models if k divides K and demonstrate the result empirically. The paper shows that PVE models are sufficient for planning, i.e. a policy that is optimal for a PVE model is optimal in the true MDP. Finally, the authors propose a PVE-inspired loss for model learning and show that MuZero optimizes an upper bound on the loss corresponding to a behavior policy. Inspired by the observation, the authors extend to the MuZero loss with additional terms corresponding to previous policies and show an increase in the median score on the Atari 57 benchmark. ","The paper ""Proper Value Equivalence"" addresses the question of how models for reinforcement learning can be obtained which are sufficient for planning without modelling irrelevant details of the environments. The authors build upon previous work by Grimm et al. and expand the notion of value equivalence to only require value functions actually obtainable by policies in the environment. They show several illustrative examples and proofs of why this notion of proper value equivalence alleviates some problems with previous work, such as not collapsing the space of admissible models down to the optimal model. This is very important in many cases where the true model might not be recoverable due to irreducible approximation errors or missing data.",0.32,0.36,0.25333333333333335,0.3217391304347826,0.20869565217391303,0.125,0.20869565217391303,0.11637931034482758,0.16379310344827586,0.15948275862068967,0.20689655172413793,0.25,0.25263157894736843,0.1758957654723127,0.19895287958115182,0.21325648414985593,0.20779220779220778,0.16666666666666666
283,SP:3d30ee25243d3c3d74b92ac791fb8dc4ca1c3375,"The paper presents a solution for imbalanced classification problem – which a well-known, but otherwise difficult machine learning problem, especially in high-dimensional space. The difficulty typically lies in the unknown nature of minority classes. Naïve methods such as resampling would easily overfit. A plausible approach would impose certain assumptions, such as the ability to ‘disentangle’ the feature representation; and utilising recent identifiability result in nonlinear ICA this paper assumed that the feature representation of a data point can be decomposed to several independent source features conditioned by the data point’s class label. Also, these source features can be effectively extracted from data by a general contrastive learning process. The author carefully designed a framework and a learning algorithm to achieve this goal.  In short, the key contribution is a combination of exploiting recent results in CMT (casual transfer mechanism, but limited to regression), generalize contrastive learning (GCL) and strategy to improve training. This is to learn from an input x to its source representation s = f(z) where z is the usual feature encoder. Now, elements of s are conditionally independent given the label. From here, enhancing minority class is fairly standard. ","This paper consider the problem of learning in the presence of a severe class imbalance, specifically when there are more than two classes. It stipulates a causal generating mechanism to ""enable efficient knowledge transfer from the dominant classes to the under-represented counterparts,"" which hinges on the important assumption that all classes share a common set of salient features. In detail, the proposed procedure involves: (1) pre-training an encoder and predictor, (2) NICA estimation using the learned features, (3) source-space augmentation, (4) minority predictor modeling with the augmented source. Experiments on CIFAR100, iNaturalist, TinyImagenet and ArXiv show that the model ","The paper proposes to use an auxiliary feature space (S) via non-linear independent component anaylsis (NICA) on an existing feature space (Z), e.g., of a pre-trained deep neural network, to transfer causal information of majority classes to minority classes in the context of imbalanced (or long-tailed) classification. More specifically, the method trains an invertible neural networks Z → S using only majority samples to perform NICA, and uses this model to augment minority samples by simply permuting their S-features, thanks to the coordinate-wise independence of S. This augmentation has an effect of transferring the causal features of majority samples into the minority samples. Experimental results confirms that such techniques can indeed prevent classifiers from overly relying on non-generalizable features of minority samples.","- This paper tackles the class imbalance problem in the perspective of knowledge transfer of the dominant classes to minority classes via data augmentation on the feature space. The proposed method first learns a feature extractor using the dominant classes. Based on these features, generalized contrastive learning is conducted with the masked auto-regressive flow to map the features into source space that the independency of each dimension is guaranteed with an invertible transformation. Then, a causal data augmentation procedure on the source space is conducted to enlarge the representation of minority classes. The empirical effectiveness of the proposed Energy-based Causal Representation Transfer (ECRT) is demonstrated on widely used imbalanced benchmarks with the comparison of strong baselines.",0.1076923076923077,0.12307692307692308,0.14358974358974358,0.1568627450980392,0.2647058823529412,0.1796875,0.20588235294117646,0.1875,0.23931623931623933,0.125,0.23076923076923078,0.19658119658119658,0.1414141414141414,0.14860681114551083,0.17948717948717952,0.1391304347826087,0.24657534246575344,0.18775510204081633
284,SP:3d73a58da7a727e5c404379587197527b45bf530,"Paper studies dimensionality reduction and coresets for the Wasserstein barycenter problem. The objects are distributions supported on n points in R^d, and the barycenter is a single distribution which minimizes sum of Wasserstein distances (or related quantities). The show that using random projections, they can reduce the dimension of the ambient space from R^d to R^{O(log(n))} while preserving the cost of optimal barycenter. They also give coresets from which one can compute the Wasserstein barycenter.","This paper present a method to reduce the dimension of k discrete measures defined on (at most) n support points in a real space of dimension d, by an O(log(n)) factor, while the optimum Wasserstein barycenter value maintains a bounded error from the optimal cost.  The main results of this work are given in Theorem 4.1 and 4.2. The paper presents the main plan of the technical proofs, but all the details of the proof are deferred to the online supplement of the paper. The two key ingredients of the proof are Johnson-Lindenstrauss (JL) projection results and the Kirszbraun theorem.  The paper contains also basic computational results on two standard datasets, the FACES and the MNIST datasets, which show the impact of the dimensionality reduction on the optimal value of the barycenter problem. ","This paper introduces dimensionality reduction techniques and coresets for Wasserstein barycenters with fixed support size n. More precisely, motivated by the fact that exact algorithms have a complexity that depend exponentially on the dimension d, the authors first show that for p-Wasserstein barycenters, a random projection to a space of dimension $O(p^4/\epsilon^2 \log n / \epsilon)$ that preserves the cost of the barycenter within a $(1\pm \epsilon)$ factor can be obtained with high probability, independently of the original dimension d, the size of the distributions, and their number. Second, the authors show that for a given set of mesures $\mu_1, …, \mu_M$, there exists a subset of $\mathrm{poly}(d, n) /\epsilon^2$ size with corresponding weights such that for any target measure $\nu$, the original cost lies within $(1 \pm \epsilon)$ factor w.r.t. the weighted cost on this coreset. ","This paper justifies compute Wasserstein barycenter with dimension reduction techniques, which can lead to significant cost saving. Specifically, with the help of JL lemma, the original Wasserstein problem can be solved in a space of dimension $\mathcal{O}(\log n)$, where n is the specified barycenter support size. The reduced dimension is **tight**, and is not dependent on the original problem dimension and the number of input distributions. The authors also discuss the hardness of the barycenter approximation, the existence of a coreset with size poly($n$), and the connection between the Wasserstein barycenter problem and constrained low-rank problems. The authors empirically validate their claims using real datasets on dimension reduction and a synthetic dataset on coreset construction. ",0.275,0.3,0.2375,0.1956521739130435,0.2463768115942029,0.22297297297297297,0.15942028985507245,0.16216216216216217,0.15966386554621848,0.18243243243243243,0.2857142857142857,0.2773109243697479,0.20183486238532108,0.2105263157894737,0.19095477386934673,0.18881118881118883,0.26459143968871596,0.24719101123595508
285,SP:3d953f3dde97ef25fde5c52b58dd55cf61cbfb6c,"This paper proposed a novel recurrent network architecture called Scalar Gated Orthogonal Recurrent Neural Networks (SGORNN). The model is a combination of (1) orthogonal transition matrix and (2) fixed residual connection as in FastRNN (Kusupati et al., 2019). The authors theoretically studied the generation bound and gradient exploding condition of the model.  Lastly, experiment results show that the proposed model outperforms standard FastRNN and a version of vanilla orthogonal RNN on three tasks: (1) synthetic adding problem, (2) HAR-2 classification, (3) Penn Treebank word-level language modeling.  ","In this paper, the authors propose an RNN architecture named SGORNN. The high-level idea is to add a residual connection between hidden states, and to parametrize the weight matrix by the product of a series of rotation and permutation matrices so that the weight matrix is orthogonal. The proposed model can be seen as an extension of the FastRNN model, the theoretical analyses of which also apply as a result. The proposed model is evaluated on a few tasks including the addition problem, HAR-2 classification, and PTB.","Authors demonstrate a new RNN-based architecture that avoids exploding gradients via careful selection of primitives and hyperparameters. The authors motivate their selections with gradient magnitude bounds and show proofs for them. Finally, the authors demonstrate their model's superior performance against several RNN-based baselines directly relevant to their architecture.","This work proposes to approach and partially solve (under certain constraints) the exploding gradient problem of RNN. In practice, the authors combined two existing methods (residual connection + unitary RNNs) in a single novel model. Demonstrations are given for the exploding issue and for the motivation of parameter constraints. The new model, called SGORNN, is then tested on two toy tasks and on a largish one (language modelling). SGORNN outperform models that are based either on residual connection or unitary recurrent connections only. ",0.2727272727272727,0.125,0.1590909090909091,0.0898876404494382,0.19101123595505617,0.17647058823529413,0.2696629213483146,0.21568627450980393,0.17073170731707318,0.1568627450980392,0.2073170731707317,0.10975609756097561,0.27118644067796605,0.15827338129496404,0.16470588235294117,0.11428571428571428,0.19883040935672514,0.13533834586466165
286,SP:3d9e99f92d2d018dd989a4ad5c96e28489c8d6d0,"The paper proposes an algorithm LINEX for learning a robust LIME-like explanation for a black box model. The algorithm is based on the Invariant Risk Minimization Games framework proposed by [1],[2]. LIME is notorious for being very sensitive to its hyperparameters, and the paper posits that a robust variant of LIME is one which is invariant to these hyperparameters. The primary contribution of the paper is to formulate the perturbation-neighbourhoods of the point to be explained as ‘environments’ in an IRM setup. The paper also notes that characteristics of such invariant predictors (Theorem 2 of [2]) make them well-suited for explanations.","This paper proposes an extension of Locally interpretable model agnostic explanations (LIME) called Locally invariant explanations (LINEX). LINEX is inspired by invariant risk minimization and aims to provide high-fidelity, stable, black-box invariant, unidirectional local explanations. LINEX is empirically evaluated using tabular, image, and text data and the authors show improvement over LIME and other baselines over several metrics and datasets.","Update: In our back and forth, the authors have addressed or committed to address the concerns I had with this work. As a result, I have updated my score  ---------------------------------------------  This paper introduces a new model agnostic local explanation method based on a game-theoretic formulation to ensure that resulting explanations are high fidelity and unidirectional across nearby examples. The idea is to first form several local environments via random perturbations or using creating realistic neighbors using generative or retrieval methods. Then, using the proposed algorithm, we iteratively learn a constrained least squares predictor for each environment and the final predictor is then the sum of these individual predictors. Experiments on three data modalities (tabular, vision, and text) show that the proposed method leads to better explanations than existing methods such as LIME.","This paper approaches the problem of locally explaining predictions of black box models. They attempt to do so by creating local environments around a data unit. They propose their method, LINEX to overcome the shortcomings of the method, LIME. LINEX learns locally invariant predictors and aggregates their explanation for locally explaining the original model. The authors show some theoretical properties as well as empirical evidence.",0.11428571428571428,0.18095238095238095,0.12380952380952381,0.24193548387096775,0.1935483870967742,0.09848484848484848,0.1935483870967742,0.14393939393939395,0.2,0.11363636363636363,0.18461538461538463,0.2,0.14371257485029942,0.16033755274261602,0.15294117647058825,0.15463917525773194,0.1889763779527559,0.13197969543147212
287,SP:3daaaaad0151cba9a2055559ff579aff7bb59be5,"The problem considered is adding links to a network to decrease the polarization+disagreement ratio. The proposed method is the greedy algorithm: adding links one-by-one to maximize the improvement. Showing approximation ratio based on the approximation result of [5] (in ICML'17, showing that the  polarization+disagreement  is close enough to being submodular.   Evaluating a greedy step exactly required a matrix inversion, hence it is slow. The authors also offer a faster approximate solution using Johnson-Lindenstrauss lemma","In this paper the authors propose the task of adding links to a social network to minimize notions of disagreement and polarization across the network.  In particular, they define disagreement across neighbors in the graph and polarization by the variance in expressed opinions of the nodes.  The paper then offers two approximate algorithms for greedily adding a fixed number of edges to the graph to minimize the sum of the disagreement and polarization.  They show that their algorithms are both theoretically and empirically tractable, outperform naive baselines, and in some cases close to optimal. ","In this paper, the authors study an opinion dynamics problem using the Friedkin-Johnsen model, where one aims to minimize the polarization + disagreement by adding up to k edges. The authors derive a greedy algorithm for this problem and show that it has a constant factor approximation. They then derive a faster version of the greedy algorithm (but without a constant factor approximation) involving random matrix projections, JL, and fast SDDM solvers. Empirically, they show that their methods are close to optimal for small k, and that their fast greedy algorithm scales well to large networks.","The paper studies the problem of recommending k links to a given graph G from a candidate pool of candidate links Ec. The goal is to minimize the index of disagreement-polarization as introduced by Musco et al. The authors prove that the objective is not submodular, but they analyze the performance of greedy, and provide a fast version based on random projections. They apply their methods on real-world data, with synthetic opinions. ",0.25,0.2,0.2,0.26595744680851063,0.20212765957446807,0.21875,0.2127659574468085,0.16666666666666666,0.21621621621621623,0.2604166666666667,0.25675675675675674,0.28378378378378377,0.2298850574712644,0.1818181818181818,0.2077922077922078,0.2631578947368421,0.2261904761904762,0.24705882352941175
288,SP:3e2d29d6c9fe5cfe850de1bab84a60a92fc9f10f,The paper develops a method for repairing neural networks with ReLU activation functions. The method works by augmenting a given network with a patch function that corrects the output of the network on a given buggy input. This enables the method to be the only one among related work that is not only sound and complete but also guarantees (i) minimal distances between  the functions implemented by the original and repaired networks; (ii) unaltered behaviour of the repaired network for inputs different than the buggy one. ,The paper presents a new method for the provable repair of neural networks. Both point-wise and polytope-based repairs are considered. The main idea is to synthesize a patch network and combine it with the buggy network. The patch network consists of two subcomponents: a support network that is only activated for the inputs in the linear region containing buggy inputs and an affine patch network that repairs the network in the buggy region. The experimental evaluation is performed on similar datasets and networks as prior work and shows the effectiveness of the new methodology.,"This paper presents a new methodology for repairing ReLU neural networks. Concretely, for a single affine region, a patch function is synthesized from an affine function that performs the repair and a support network to make sure the repair only applies to the affine region. Multiple affine regions can be fixed iteratively. The techniques are evaluated on the MNIST and HCAS benchmarks.","This paper proposed an approach for repairing neural networks that use ReLU activation functions. Different with existing techniques that retrain the model or change the weights, the proposed approach generates a patch in the original model, i.e., add sub-models. The LP solver is used to solve the patch function for a linear region. The experimental  results demonstrated that the approach could outperform existing methods. ",0.29069767441860467,0.22093023255813954,0.22093023255813954,0.20833333333333334,0.16666666666666666,0.24193548387096775,0.2604166666666667,0.3064516129032258,0.2878787878787879,0.3225806451612903,0.24242424242424243,0.22727272727272727,0.27472527472527475,0.25675675675675674,0.25,0.25316455696202533,0.19753086419753085,0.23437499999999997
289,SP:3e57706a34fbff352cae2e885591c43ed10ab4c1,"In this paper, the authors present an application of equivariant neural networks to reinforcement learning. They demonstrate new DQN and SAC architectures that make use of equivariant representations in order to improve sample efficiency. Results on manipulation tasks demonstrate sample complexity reduction compared to other techniques. ","This paper defines and theoretically characterizes a class of group-equivariant MDPs and studies its invariance and equivariance properties. It introduces two new models, one for discrete action spaces (equivariant DQN) and one for continuous action spaces (equivariant SAC). It compares the performance of the proposed methods against strong competitive baselines for multiple robotic manipulation tasks and establishes their superior performance. ","The paper uses rotation equivariant CNNs for model-free reinforcement learning. More specifically, it is argued that for robotic manipulation tasks the corresponding MDP is invariant under translation and rotation, and therefore one could more efficiently learn equivariant/invariant policy/value functions. This idea is applied to both DQN and SAC for control with finite and continuous action spaces respectively. Experimental results on several tasks demonstrate substantial improvement over the competing methods. ","This paper explores to integrate equivariance deep learning to robotic applications. The authors propose two main contributions: 1) define and theoretically characterize an important class of group-equivariant MDPs, 2) and integrate equivariant variations to DQN, SAC, and LfD. The paper provides many different sets of experiments. The results are promising which shows benefits of the proposed approach. ",0.2391304347826087,0.2391304347826087,0.2608695652173913,0.22950819672131148,0.2786885245901639,0.18055555555555555,0.18032786885245902,0.1527777777777778,0.20689655172413793,0.19444444444444445,0.29310344827586204,0.22413793103448276,0.205607476635514,0.1864406779661017,0.23076923076923075,0.21052631578947367,0.28571428571428575,0.19999999999999998
290,SP:3e702fd203145e55a9e2ca4e76b16c6cee23baa4,This paper tries to adapt the PATE (Private Aggregation of Teacher Ensembles) framework for DP training of machine learning models to the setting of text generation. There are some significant challenges in adapting PATE to this framework.  PATE as originally proposed is designed to work for classification tasks with a small number of labels. Text generation or next word prediction has too many labels and a direct application of PATE doesn't work. This paper solves this issue by aggregating only top-k predictions of each teacher where the top-k is computed using the prediction of the student on public data and so doesn't leak extra privacy. The paper also crucially uses public data and large pretrained models like GPT2 to further improve the performance of DP learning using PATE on private data.  They claim that their approach improves upon DPSGD/Noisy SGD (which is the standard DP algorithm for deep learning) significantly on some standard benchmarks.,"This paper proposes a variant of PATE (from ICLR 2018), a method intended for standard supervised learning tasks. The present work instead focuses on natural language generation, an inherently sequential task, which introduces new challenges. Relative to PATE (and assuming knowledge of it), some differences in the method involve: extending the public data sentences into ""pseudo sentences"" using GPT-2, simultaneously performing training on all prefixes of the pseudo sentences, averaging the teacher model output distributions rather than aggregating their max score (i.e., their prediction), and using the student model to reduce the support size for said output distributions. Experimental evaluation is provided for two datasets, AirDialog and Europarl_v6. Perplexity and BLEU scores improve upon those which do not use the private data, and those which use previous methods with the private data. ","This paper proposes to protect the privacy of text generation models, which may leak sensitive information of the training data. Specifically, the authors propose SeqPATE, which applies the PATE framework to large-scale language models such as GPT. By reducing the output space and aggregating the supervision from teacher models, SeqPATE can help reduce the privacy cost. Empirical evaluation on two private text corpus shows that SeqPATE achieves a good trade-off between utility and privacy. ","This paper proposes SeqPATE that adapts PATE to text generation while satisfying differential privacy. To overcome the challenge of obtaining sequence-level supervision for text generation, it first generates pseudo inputs by completing the prefix using GPT-2 (a pre-trained language model) and thus reduces the problem to the next word prediction. Then it applies the PATE framework to privately train a student model with several algorithmic innovations, including (1) aggregating teachers' models by averaging their output distributions; (2) reducing output space by top-k/top-p selection; (3) efficient knowledge distillation by only querying the teachers' models when the student model performs poorly. (4) using both public labels and teachers' aggregation to supervise the learning. The paper gives privacy analysis and also shows it can offer privacy guarantees on user-level and secret n-grams. The paper also provides extensive experimental results and shows that it outperforms other DP learning algorithms when the \eps is small (<5). The experiments demonstrate the practicality of the algorithm.",0.16352201257861634,0.11949685534591195,0.20754716981132076,0.14814814814814814,0.2,0.35526315789473684,0.1925925925925926,0.25,0.19760479041916168,0.2631578947368421,0.16167664670658682,0.16167664670658682,0.1768707482993197,0.16170212765957448,0.20245398773006137,0.1895734597156398,0.17880794701986755,0.22222222222222227
291,SP:3ece6dcd67f1443022cc1931c8c982ff84c677b3,"The authors propose a method for modeling single-cell multimodal data using Granger causality. They introduce a graph neural network called GrID-Net (Granger Inference on DAGs), where they assume that the network variables (the cells) follow a DAG structure. They focus on applying their method to the task of inferring non-coding genomic loci that causally influence the expression of a specific gene.","This paper developed an exciting method, GrID-net to infer granger causality between multi-modality on directed acyclic graph (DAG) in the framework of deep neural network (GNN), aiming to improve identifying causal interactions across modalities. Primarily, the authors first find the DAG among samples and represent the historical information of modal features via aggregating GNN layers over DAG. They then apply granger causal inference to test the causal interactions of any two features if adding such GNN-based historical information of one feature significantly helps reduce the prediction loss of another feature.   Also, it is nice that the authors applied GrID-net to multiple recent single-cell multi-modal datasets (scRNA-seq and scATAC-seq) and identified casual chromatin regions for gene expression at the cell type level (grander causal gene-region interactions), providing novel gene regulatory mechanistic insights. Besides, the authors validated gene-region interactions by eQTLs, Hi-C, and TFBS enrichment.   ","In this work, the authors propose a model framed as an extension to Granger causality for DAGs, and apply their model to infer “causal” relations between changes in gene expression and chromatin accessibility over time. Rather than relying on linear models and ordered data, the authors use a graph neural network to account for partially ordered data (i.e. branching structure). Similar to Granger inference, they compare a full model using paired data to a reduced model which excludes the lagging predictor. The authors then focus on analyzing temporally ordered scRNASeq and scATACSeq in 3 data sets from human and mouse. They test the method on predicting “causality” between the data modalities by using eQTLs and and chromatin interactions as proxy labels and compare against several baseline methods, noting that the method is robust to sparsity in data. Finally, they find enrichment of TF binding motifs in peak-gene pair regions and conclude that the model detects a lag between distal ATACSeq peaks and gene expression.   ","The paper proposes a modification of Granger causality so that it is applicable in partially ordered observations. Authors achieve that by adapting the architecture of GNNs to enable lagged-passed information in the DAG. Finally they apply it on a real data problem with the goal to infer non-coding genomic loci that causally influence the expression of a specific gene. In comparison with other three methods, the proposed method GrID net outperforms them on an open dataset. ",0.265625,0.375,0.375,0.16233766233766234,0.11038961038961038,0.10843373493975904,0.11038961038961038,0.14457831325301204,0.3076923076923077,0.15060240963855423,0.21794871794871795,0.23076923076923078,0.1559633027522936,0.208695652173913,0.3380281690140845,0.15625000000000003,0.14655172413793105,0.14754098360655737
292,SP:3f270b3f782cd885c3c3b7805a673a505921443d,"This paper proposes a new method to accelerate proximal-mapping based algorithms  where the proximal mapping usually needs computing the eigen/singular value decomposition.   The authors discusses how to use the Jacobi method to get the eigen/singular value decomposition approximately and parallelly,  and the convergence can be guaranteed by easily controlling the norm of the off diagonal elements.   Numerical experiments show the significant improvement over the state-of-art algorithms.","The paper considers proximal algorithms for convex optimization problems over matrices. When the function to be optimized involves a function of the eigenvalues or singular values of a matrix, then prox methods need to evaluate the proximal operator in each iteration which usually boils down to a full eigenvalue decomposition or SVD. This can be very costly. The paper proposes to use the Jacobi eigenvalue algorithm for computing such decompositions. The Jacobi eigenvalue decomposition can be warm started with the decomposition from the last iteration. On average, a good enough approximation to the eigenvalue decomposition can thus be computed by less than one sweep ($n/2$ parallel Givens rotation) over the matrix. Hence, the evaluation of the proximal operator in ADMM or prox algorithms can be speeded up significantly (by a factor of 5 to 20).  Convergence results are provided and experiments validate the approach and its efficacy. ","This work proposes to apply a Jacobi-based approximate eigenvalue decomposition method in proximal slitting algorithms for minimization of convex spectral function with matrix variable. The adopted Jacobi-based method is used to compute proximal operators in each iteration, and by taking the previous iterate as the initial point, the Jacobi-based method appears to reach required accuracy within fewer sweeps. The method can be parallelized, and its GPU implementation shows improvement in efficiency compared with standard eigenvalue decomposition functions in LAPACK and CUSOLVER in numerical experiments.","This paper proposes using a classical but often unused method for computing the eigenvalue decomposition of a symmetric matrix: the Jacobi method. The method simply iterates by zeroing out off-diagonal entries using Givens rotations one at a time, until the matrix converges to a diagonal matrix. It is further noted that it is possible to parallelize the removal of ~n/2 entries at a time if disjoint indices are picked (for a matrix of size n-by-n).  Proximal methods are a popular choice for solving convex optimization problems over matrices. A commonly required oracle for such problems is the computation of the (full) eigenvalue decomposition of the current iterate, e.g., to compute the projection of the iterate onto the positive semidefinite cone. It is observed that as the iterations progress, the distance between iterates decreases and so the eigenvalue decomposition of consecutive iterates is not perturbed too much. Therefore, it is expected that few sweeps of the Jacobi method are required to compute the (approximate) eigenvalue decomposition of the next iterate.  The paper further provides bounds on the error of the prox operator as a function of the Frobenius norm of the matrix of off-diagonal entries of the iterate of the Jacobi method. This allows one to establish termination criteria in order to run the Jacobi method to obtain an approximate eigenvalue decomposition, while guaranteeing convergence of the (approximate) proximal algorithm.  The authors provide some numerical experiments on several matrix optimization problems such as solving sparse inverse covariance selection, SDPs, nuclear norm minimization, for small to moderately sized problems. Their experiments show that using the Jacobi algorithm for a small number of sweeps obtains sufficient accuracy to proceed to the next iteration of the proximal algorithm, and therefore reduces the overall wall-clock time to convergence, compared to using an eigenvalue/singular value decomposition implemented on CPUs or GPUs.",0.323943661971831,0.2676056338028169,0.43661971830985913,0.1554054054054054,0.2635135135135135,0.3218390804597701,0.1554054054054054,0.21839080459770116,0.09904153354632587,0.26436781609195403,0.12460063897763578,0.08945686900958466,0.2100456621004566,0.24050632911392406,0.16145833333333334,0.19574468085106386,0.16919739696312364,0.13999999999999999
293,SP:3f2e80bdc744c98343a99cb71d34ee65d7724874,"This paper focuses on designing efficient deep networks with a proposed novel parameter-free layers. Drawing the spirit of depth-wise convolution, shift operation, the paper studies the ways of inserting parameter-free operations into the building block using NAS technique. The resulted models perform both effectively and efficiently on ImageNet dataset.","In this paper authors propose to use simple parameter-free operations to replace several kinds of trainable layers to achieve speedup on hardware with less performance drop. Extensive experiments have been done to demonstrate the effectiveness of the proposed method, including experiments on replacement in both single and multiple bottlenecks and neural architecture search. In addition, authors also employ such parameter-free operations to redesign CNNs and vision transformers for a good balance between speed and performance.","This paper proposed to use some parameter-free operations to replace the spatial operations (e.g., 3x3 conv, 3x3 depth conv). The author shows that using the parameter-free operations (e.g., max pooling) can also achieve a good performance. The author conducted thorough ablations to show the effectiveness of parameter-free operations, and the method is evaluated on different architectures and datasets.","This paper argues for more extensive use of max-pooling layers in image classification networks, as an inexpensive substitute for convolutions.  Recent works have removed parameters (and often-expensive operations) using bottlenecks, depthwise/channel-wise operations, etc., and this paper pushes further in this direction by studying max-pooling as a cheap and effective spatial combination mechanism in the context of recent resnet-derived and image transformer architectures.  Empirical experiments are performed on CIFAR-10 and three Imagenet variants, demonstrating the effectiveness of incorporating max-pooling layers beyond applications in strided layers and secondary branches. ",0.23076923076923078,0.3076923076923077,0.19230769230769232,0.2597402597402597,0.16883116883116883,0.2222222222222222,0.15584415584415584,0.25396825396825395,0.10526315789473684,0.31746031746031744,0.1368421052631579,0.14736842105263157,0.186046511627907,0.2782608695652174,0.1360544217687075,0.2857142857142857,0.1511627906976744,0.1772151898734177
294,SP:3f328ba9e7e92691c0c670b63ae9f6118babef82,"The paper introduces an estimator of expressivity or trainability of deep networks, that they call ""variability"", that characterizes the variations in a scalar output of the network, when the input change, and seems correlated to trainability of the network, and to its depth (to an extent).  The paper proposes to parametrize weight matrices by using Householder reflectors, parametrized by a single vector each, which has two benefits: - A reduced number of free parameters, which enables more depth (more layers and non-linearities) for the same model size - Orthogonality of the matrices.  Experiments on a synthetic 2D benchmark, two small-scale regression datasets, and small-resolution image datasets show improved results compared to dense nets. ","**Contributions**: - Introduces the notion of variability to explain the better trainability and generalization of certain deep neural architectures. This is backed by a concrete definition of the variability in terms of the third order derivative information of the loss landscape.  - Empirical experiments verify that variability correlates positively to the number of activations and negatively to a phenomenon called Collapse to Constants (C2C).  - Inspired by the notion of variability, authors propose Householder-absolute neural layer aka Han-layer that replaces square weight matrices with Householder reflectors, and use absolute-value function for activations.   -  HanNet achieves a high variability as well as guarantee an immunity to vanishing or exploding gradients, and chance for collapse to constants has been diminished.  - HanNets achieve better generalization with lesser parameters than fully connected networks.  ","This paper introduces some concepts, i.e., variability, activation ratio, and collapse to constants, to help understand deep neural networks, and proposes a new layer called Householder-absolute layers to improve network variability. Experiments are conducted on both synthetic datasets and empirical datasets. The synthetic experiments show some interesting results and visualizations, and for the real datasets, improvements over baseline architectures can be observed.","The paper introduces a new notion of _variability_ of a DNN, that intuitively is the ""richness of landscape variation"" wrt data and parameters. The paper attributes the _variability_ to the generalization performance of fully connected DNNs. Inspired by the increasing _variability_ the work proposes _Han-layer_ that is a linear layer with a Householder matrix and abs(.) activation function. The empirical results show that _Han-layer_ can improve the performance of MLP-Mixer on various datasets.",0.21929824561403508,0.14912280701754385,0.19298245614035087,0.1015625,0.1953125,0.21875,0.1953125,0.265625,0.2894736842105263,0.203125,0.32894736842105265,0.18421052631578946,0.20661157024793386,0.19101123595505615,0.23157894736842105,0.13541666666666666,0.2450980392156863,0.19999999999999998
295,SP:3f3c9cf4b0f3ca22d3ce4b282c8b1791a29a2e78,"(I am not an expert in information theory) The authors extend partial information decomposition to the case of continuous variables with arbitrary distributions.  My review will focus on the last section, where a recurrent neural network was trained on a generalized XOR task. By decomposing the unique, redundant and synergistic components of each neuron’s contribution to the output, the authors identify different strategies used by these networks.  ","This paper extends the popular PID measure of Bertschinger et al. (2014) to continuous random variables, and provides a method to compute and estimate the PID in this case. The authors use a novel method based on copulas to reparameterize the optimization problem used to compute the PID, and estimate it by using an approximation based on a variational upper bound. They also provide several examples to show that their estimates match known PID quantities and that the PID can be useful in various neuroscientifically relevant settings.","Authors present a partial information decomposition (PID) method that can be applied to continuous variables thereby extending a popular approach for analysis of discrete variables. Authors validate their method  by leveraging known analytic results for Gaussian random variables. Additionally, the authors demonstrate that their approach is able to recover the effective connectivity of a neuronal network with chaotic dynamics. Finally, authors use their method to investigate a complex trade-off between redundant, synergetic, and unique information components in recurrent networks that have been trained to solve the XOR problem.","   This paper presents a novel method for computing the Partial    Information Decomposition (BROJA flavour) in the case of three    continuous random variables. The method uses a variational approach    on a lower bound derived from a copula representation of the joint    PDF of the three variables. The paper presents some application    examples, including two where the proposed method is used to    elucidate the properties of recurrent neural networks.",0.16176470588235295,0.23529411764705882,0.27941176470588236,0.1839080459770115,0.1724137931034483,0.19101123595505617,0.12643678160919541,0.1797752808988764,0.2835820895522388,0.1797752808988764,0.22388059701492538,0.2537313432835821,0.14193548387096774,0.2038216560509554,0.28148148148148144,0.18181818181818182,0.19480519480519484,0.21794871794871795
296,SP:3fd663b1836a76ac3290cbd92a7f482cd86498b1,"The paper proposes to learn latent representation and transition models that are equivariant to symmetry transformations of states and actions. The proposed “meta-architecture” consists of an encoder followed by an equivariant encoder and an equivariant transition model. The model is trained using (state, action, state) triples, where contrastive loss is used to prevent the collapse. The paper evaluates the proposed model in four different settings. In each case, a different choice of symmetry group and equivariant architecture is used. The quality of the learned latent representation is evaluated by comparing the relative distance of the predicted next state embedding to the true next state embedding, in comparison to negative samples. ","The goal of this paper is to learn world models - where the network must predict actions from images - in an efficient manner.  This is done by exploiting symmetries in the data.   Results are shown for a number of world model benchmarks, showing improvement over standard methods. ","This paper proposes to learn an equivariant world model without access to group representation on input space $\rho_S$. To this end, it learns a symmetric abstract state mapping from states $s$ to abstract states $z$ in a space $Z$ with an explicit action $\rho_Z$ of the symmetry group G. It then learns a transition model in latent space that can enforce symmetry using an equivariant neural network.   The problem of learning the equivariant model without assuming any group representation on input space is important and motivated well in the paper. The paper evaluates the model and compares it extensively on various benchmarks. ","This article assumes knowledge of an underlying group to learn symmetric latent representations, by utilizing equivariant transition models. The paper claims that the basic setup can be applied to a variety of existing equivariant networks (or non-equivariant ones) to improve performance as well as data efficiency. Experimental results are shown on 3 tasks, which are somewhat hand-crafted, in that for each of these tasks the allowed groups are assumed to be known a priori. Some of these experimental results are intriguing but the results themselves beg the question of why one needs all the complex machinery, at least in comparison with other recent efforts which admit a more direct mechanism (e.g. with regard to SO(3) and the teapot rotation task - see comments below).",0.10810810810810811,0.2072072072072072,0.2072072072072072,0.2826086956521739,0.2826086956521739,0.15384615384615385,0.2608695652173913,0.22115384615384615,0.18110236220472442,0.125,0.10236220472440945,0.12598425196850394,0.15286624203821655,0.21395348837209302,0.19327731092436976,0.17333333333333334,0.15028901734104047,0.13852813852813853
297,SP:40434871570d2919aab2dda7df25e460f992ea68,"In this paper, the authors aim to find an offline solution to the imitation learning problem with corrupted demonstration data. They propose a simple robust behavior cloning (RBC) approach based on the Median-of-Means (MOM) objective. The contributions of this work are as follows:  1.	A novel RBC algorithm is proposed with a MOM objective in policy estimation based on behavior cloning. 2.	The authors provided theoretical justifications on the error scaling and sample complexity of the proposed RBC to show that RBC guarantees robustness to corrupted demonstrations at no cost of statistical error. 3.	Experiments on some low-dimensional tasks show the effectiveness of the proposed RBC. ","This paper considers an offline imitation learning task wherein a constant $\varepsilon$-fraction of demonstrations have been potentially arbitrarily corrupted.  This data corruption is motivated by sub-optimal experts and/or erroneous/adversarial sensors.  In order to address this, the paper proposes minimizing a Median-of-Means (MOM) objective.  Theoretically, it is shown that up to constant factors, the resulting learned policy achieves the same statistical rates as behavior cloning, which was shown to be optimal for the offline imitation learning setting in recent work by Agarwal et al.  This analysis is done under the assumption of a discrete policy class containing the expert policy.  Algorithmically, an approach based on alternating gradient descent/ascent is proposed, and it is shown on two continuous control benchmarks in PyBullet (LunarLanderContinuous-v2 and HalfCheetahBulletEnv0) that the proposed approach compares favorably to a behavior cloning as applied to uncorrupted data, whereas vanilla behavior cloning performs very poorly.","This paper proposes the definition of corrupted demonstrations and a new robust algorithm for offline imitation learning from corrupted demonstrations. The main idea to guarantee the robustness is median-of-means (MOM), which is used for high-dimensional robust regression.  The authors propose an optimization formula based on MOM. The proposed algorithm, named robust behavior cloning (RBC), solves the min-max-median optimization to train two policies $\pi$ and $\pi’$.  Under the assumption that the policy class is discrete, two meaningful theorems are provided. The first theorem shows that the policy obtained from RBC is close to the expert policy with high probability. The second theorem shows the degree of suboptimality measured by the difference between the expected return of the policy obtained from RBC and the expert policy.  Finally, the authors provide empirical results on two continuous control benchmarks. In both domains, the proposed algorithm achieves competitive performance with BC on expert demos and outperforms BC on corrupted demonstrations by a large margin. ","This paper tackles behavioural cloning from expert demonstrations, where a fixed fraction of the demonstrations are corrupted.  This is a somewhat (see comments below) practical scenario where trajectories gathered are limited or biased in response to constraints on collection.  A crucial (again, see comments below) point is that _no_ further environment interactions are allowed -- only the possibly corrupted demonstrations can be used.  The paper provides a definition of a corrupted dataset, an objective for training using a corrupted dataset, and provides some convergence and error bounds.  The method is tested on two standard Gym environments, and some ablation studies are provided.",0.24770642201834864,0.25688073394495414,0.1559633027522936,0.23529411764705882,0.16993464052287582,0.10365853658536585,0.17647058823529413,0.17073170731707318,0.16831683168316833,0.21951219512195122,0.25742574257425743,0.16831683168316833,0.20610687022900767,0.20512820512820512,0.16190476190476194,0.22712933753943218,0.2047244094488189,0.12830188679245283
298,SP:404af2a78c0fb106eb7be0359a0b3cefc3f918a8,"The authors introduce a novel idea to analyze accelerated methods from a continuized point of view, which allows to have a type of ""random learning rates"". In particular, this allows to be oblivious of the iteration in an asynchronous network, as long as you have a common synchronized clock for the nodes. The analysis shows accelerated rates for their deterministic and stochastic methods, one of which can be applied to obtain acceleration in an asynchronous gossip algorithm.","This paper introduces a ""continuized"" randomized variant of the Nesterov acceleration scheme for convex optimization. In particular, ""updates"" are triggered at random i.i.d. exponential intervals of rate 1, leading to Markovian dynamics. The implications of this scheme are investigated in terms of streamlining, simplifying and improving the Nesterov acceleration.","This paper studies accelerated gradient descent algorithm for convex optimization from a continuous-time perspective. The authors introduce the continuized version of Nesterov's accelerated gradient descent (AGD) algorithm, which takes gradient steps at random times with exponentially distributed intervals. The authors show the continuized AGD has accelerated convergence rates in expected function values similar to the standard Nesterov's AGD. The authors derive an exact discretization of the continuized AGD which resembles Nesterov's AGD with random parameters, and show the algorithm has accelerated convergence rates similar to the rates in continuous time. The authors extend their algorithms and analysis to stochastic gradient descent with additive and multiplicative noise. The authors apply their framework to derive accelerated randomized gossip algorithm which can be implemented in a decentralized way.","This paper introduces a new continuous-time framework for understanding Nesterov’s accelerated gradient descent. The authors consider the following dynamics for two variables (x, z): at the jump times of a Poisson process, each of the two variables x and z takes a step in the negative gradient of the objective function. In between these jumps, the variables x and z mix continuously according to an ODE. The authors show that with appropriate choices for the parameters, this process exhibits similar guarantees as Nesterov’s algorithm.  Continuous-time versions of Nesterov’s algorithm have been considered before, but usually from the perspective of letting the step size of the algorithm tend to zero and thus recovering an ODE. Although it is usually much simpler to study the resulting ODE, it is quite non-trivial to start with an ODE and find a suitable discretization that preserves its convergence guarantees; thus, many of these prior works simply reverse-engineer Nesterov’s algorithm without providing an understanding of how to produce discretized accelerated algorithms. The main novelty in the present approach is that there is no need to discretize the continuous-time dynamics; the gradient steps already happen at discrete times (the jumps of a Poisson process) which can be simulated (with the caveat that the algorithm is now randomized). In between the jumps, the pair (x, z) evolves according to an ODE which does not depend on the objective function (besides the smoothness/convexity parameters). For the parameter choices leading to accelerated rates for convex-smooth and strongly convex-smooth classes, the authors show that in fact this ODE can be solved in closed form. Even if this were not the case, it is noteworthy that this ODE can be implemented in the usual oracle model of optimization.  Besides introducing this framework, the authors provide extensions to stochastic gradients with “additive” or “multiplicative” noise, as well as applications to accelerated gossip.",0.12987012987012986,0.2987012987012987,0.3246753246753247,0.29411764705882354,0.3333333333333333,0.3488372093023256,0.19607843137254902,0.17829457364341086,0.0778816199376947,0.11627906976744186,0.0529595015576324,0.14018691588785046,0.15624999999999997,0.22330097087378642,0.12562814070351758,0.16666666666666666,0.09139784946236558,0.19999999999999998
299,SP:405ef214829faa4350c18d6efdac34b5c683ad48,"The authors devise a neural network architecture, SIMONe, for learning unsupervised representations of scenes viewed from multiple camera poses. The latent representation is factored into slots meant to represent time-invariant entities (i.e., physical objects) and time-varying ""frame latents,"" which can capture changes in the camera pose. They show that this method is able to capture the structure of scenes from three synthetic datasets by testing it on two tasks: novel view synthesis and (unsupervised) object instance segmentation. A number of qualitative results suggest that the learned latent representation successfully factors into time-invariant objects and time-varying viewpoint parameters.","This submission presents a video scene model that decomposes a scene into object components, and frame components in an unsupervised way. The architecture improves over previous models that are static (MONET, IODINE, etc) and use videos (GENESIS) in that the model performs better and is conceptually simpler. The main idea is the use of a transformer model that stacks all feature representation generated from CNNs for every individual frame, then aggregate to object and frame latents.   Experimental results are shown on synthetic images only. Comparison are done in two settings: knowing the camera locations (against GQN and Nerf-VAE) and fully unsupervised (compared against MONET and SlotAttention). Empirical results verify the model to be stronger.  ","Authors propose a structured generative model of video clips. This uses a factored latent space, with per-object latents that are global the video, and temporal latents that are not split between objects. The decoder uses a spatial GMM (as in many prior works), albeit with pixels given by per-coordinate MLPs not a CNN. The encoder uses a transformer on CNN features. Results on synthetic datasets show higher performance than one existing method (S-IODINE) on video segmentation, and higher performance than GQN and NeRF-VAE on novel view synthesis.","This paper proposes SIMONe, a variational auto-encoder that decomposes a video into object latents and frame latents without any supervision. The object latents correspond to the time-invariant, object-level contents of the scene. The frame latents correspond to the global time-varying elemnts of the scene such as viewpoint. Each pixel is decoded independently by querying a pixel-wise decoder using sampled latents. To infer latents, SIMONe uses a transformer-based encoder that integrates information jointly across space and time. The experiments demonstrate SIMONe's capabilities in view syntheses and instance segmentation. Furthermore, the object latents are shown to be disentangled and summarize the trajectories of the objects while the frame latents are shown to contain viewpoint information.",0.16666666666666666,0.13725490196078433,0.18627450980392157,0.16521739130434782,0.20869565217391303,0.17582417582417584,0.14782608695652175,0.15384615384615385,0.15833333333333333,0.2087912087912088,0.2,0.13333333333333333,0.1566820276497696,0.14507772020725387,0.17117117117117117,0.18446601941747573,0.2042553191489362,0.15165876777251183
300,SP:414dc889eebb525f5e7f97a7a5e77af8fd08c559,"The paper provides some interesting and tighter spare approximation bounds for two-layer ReLU neural networks. The authors generalize the results for $\mathbb{R}^d$ to a bounded open set $\mathcal{U}\subset\mathbb{R}^d$ by defining the Radon-based $\mathcal{R},\mathcal{U}$-norms of functions. They also show that the representation of infinite width neural networks on $\mathcal{U}$ are not unique.","This paper studies the class of functions that coincide with an infinite width two-layer neural network on a fixed bounded open set. First, they introduce a Radon-based R, U-norms for functions defined on a bounded open set U. Then they prove tighter sparse approximation bounds for two-layer ReLU neural networks. They also prove that the representation of infinite width neural networks on bounded open sets are not unique. ","This paper studies the approximation bounds for the 2-layer  ReLU network.  The authors extend the analysis framework of Ongie et al. (2019) and show the approximation bounds for the infinite-width network on a bounded open set as well as the bounds for sparse (i.e., finite-width) network that refine the similar results in the literature. At last, the authors show that the infinite-width neural network representations may not be unique on bounded open sets and provide a functional view of the mode connectivity. In particular, the authors refine the R-norm introduced by Ongie et al. (2019) to R, U-norm to tickle the bounded open set case.   ","The main result of this paper is a representation theorem for functions on a bounded set U by two-layer neural networks with bounded width and bounded weights, via a semi-norm ||_{R,U} based on the Radon transform. Prior work [OWSS19] used the Radon transform to get a representation theorem for functions on R^d, but the bounded-domain setting is more realistic and allows for stronger results, since the semi-norm ||_{R,R^d} will naturally be larger than ||_{R,U}. Moreover, this paper shows that when U is a Euclidean ball, the semi-norm ||_{R,U} can be upper bounded in terms of the Barron norm, so this paper's representation theorem strengthens Barron's Theorem.",0.4307692307692308,0.35384615384615387,0.3230769230769231,0.3333333333333333,0.25,0.16964285714285715,0.3888888888888889,0.20535714285714285,0.17647058823529413,0.21428571428571427,0.15126050420168066,0.15966386554621848,0.4087591240875913,0.2598870056497175,0.2282608695652174,0.2608695652173913,0.18848167539267013,0.16450216450216448
301,SP:41727d7e60ce76642842203e21150c7293264ed1,"The paper proposed the augmented Intermediate Level Attack (ILA) algorithm to strengthen the transferability of adversarial examples. Also, it claimed that increasing the diversity of input references could improve the generalization of adversarial examples when attacking different models. Specifically, this paper performs augmentation operations, including common image data augmentations and transformations exploiting adversarial perturbation (e.g., reverse adversarial updates and attack interpolation on the reference attack), before maximizing the projection of intermediate feature map discrepancies.","This paper introduces improvements over Transferable Intermediate-level Attacks (ILA), by incorporating data augmentation into the attack tuning stage. The data augmentations introduced include 3 kinds: simple transformations (cropping), reverse-adversarial update, and attack interpolation. An empirical study has been conducted on Imagenet, and 3 well-known architectures have been used as source models, and the attack transferability was evaluated onto 9 models using I-FGSM with $L_{inf}$ distance and 2 different perturbation sizes. Ablation studies have been conducted to demonstrate the effectiveness of hyperparameter choices, and different kinds of data augmentation used, demonstrating the effectiveness of the proposed approach compared to other ILA-based blackbox attacks.  ","The authors proposed / used three techniques to enhance the transferability of ILA on undefended models. The techniques include image augmentation, reverse adversarial update on the clean example and reference attack update through interpolation with an automatic parameter selection scheme. The authors evaluated their method on nine undefended models and showed the proposed method outperforms ILA and ILA++ and other state-of-the-art methods ","This paper presents a transfer-based attack method based on Intermediate Level Attack(ILA). Image augementation and reverse adversarial updateare applied to ILA input, to make diverse adversarial references. Moreover, the interpolation of cumlative attacks can maintain a better transfer direction.The paper is clear and well-written and experimental results and analysis support the claims.",0.25333333333333335,0.21333333333333335,0.18666666666666668,0.16666666666666666,0.12037037037037036,0.171875,0.17592592592592593,0.25,0.25,0.28125,0.23214285714285715,0.19642857142857142,0.20765027322404372,0.23021582733812948,0.2137404580152672,0.20930232558139536,0.15853658536585366,0.18333333333333335
302,SP:41a7e829c0c10954781a832735ef57d6a5ee4609,"This paper works on the deep learning models for automatic drug design. The authors propose a method named GEKO, the geometric editing under knowledge guidance, which incorporates the physicochemical knowledge into the designed 3D model. The authors claimed this is an unsupervised drug design method that the training is conducted in a purely training data free fashion. The training algorithm is based on geometric (3D) editing with the knowledge (Vina, QED, SA core) guidance by self-training and simulated annealing. The representation of the molecule is modeled at atom-level and fragment-level. With the geometric editing process continuing, the model continues updating and learning towards a better result. The method is evaluated on the drug design task on two sets of target datasets, by generating the molecules (drugs), the results are evaluated by multiple metrics. The performances are improved compared with other baseline methods (including both 2D and 3D methods).  ","The authors propose an approach for drug design that generates molecules by simulating adding or deleting parts of the molecules, and using graphnets to capture atom and fragment level information and construct new molecules. They use simulated annealing to ‘edit’ the 3D structures, and docking simulations, drug-likeness and synthesizability to provide information back into training. In this way, the authors claim the model can be trained on self generated data. The authors compare with multiple baselines on a test set of 12 targets, including the current SOTA model, and report improved performance.","The paper proposes a method that designs molecules by combining fragments along ""editable"" (single) bonds. The method uses two graph neural networks (on atoms and on fragments) to learn to pick editable bonds. The method also uses simulated annealing to guide the combination of fragments (and possibly their dihedral angles) to optimize a weighted score that the authors have found helps their training. The model is able to propose reasonable molecules in 3D conformations within a ligand pocket using a docking score for guidance. The authors perform an ablation study that shows that even without the deep neural network for selecting the editable bonds, their sampling methodology performs rather well. ","This paper proposes a method to optimize the multiple properties of a molecule. They first define rigid fragment library as a building block, which is used to generate a new molecule through geometric editing. They represent a molecule using HMPNN and the whole model is trained using the weighted maximum likelihood estimation. The proposed model is compared with some baselines on several multi-objective optimization tasks and shows good performance. ",0.152317880794702,0.12582781456953643,0.13245033112582782,0.21505376344086022,0.16129032258064516,0.13636363636363635,0.24731182795698925,0.17272727272727273,0.2857142857142857,0.18181818181818182,0.21428571428571427,0.21428571428571427,0.18852459016393444,0.14559386973180077,0.18099547511312217,0.19704433497536947,0.18404907975460122,0.16666666666666663
303,SP:41cc644f6667dfc08462c0c2559685b83c25c5a2,"Authors propose a method to perform adversarial attacks on BCI deployed on wearable devices. Using an electrode enclosed in a head mounted device, they show how to propagate a perturbation through the subject’s scalp capable of significantly affecting a EGG based classification model. Authors test the approach on the movements detection domain using a public dataset, showing how the proposed attack can lead to a significant decrease of  the system’s performance.","The paper examines vulnerability of the brain computer interfaces to adversarial attacks. A new method is proposed to generate more realistic, smoother representations of the adversarial attack. A case where an attack happens at a specific location (close to an ear) is studied as a representative case of the perturbation added to the signal acquisition source. ","This work analyzes the adversarial attacks on BCI systems. It induces DoS attack and incorporates domain-specific insights to achieve the adversarial attack. The designed attack is evaluated on a public dataset EEGMMIDB and shows that a popular deep learning model, EEGNet, is vulnerable to adversarial attacks.","Authors study the vulnerability of brain-computer interface (BCI) systems from a security perspective, and present a physiologically plausible adversarial attack approach on BCI systems that can induce denial-of-service after being transmitted by external devices. Empirical assessments are performed on an offline three-class motor imagery (MI) BCI dataset (right hand, left hand, and rest classes), where denial-of-service indicates a ""rest"" classification regardless of the subjects' motor imagery intent. Proposed methods are motivated to be physiologically plausible and effective to an extent that can cause failure modes for DNN-calibrated BCI systems.",0.1780821917808219,0.1643835616438356,0.1780821917808219,0.19642857142857142,0.23214285714285715,0.23404255319148937,0.23214285714285715,0.2553191489361702,0.13541666666666666,0.23404255319148937,0.13541666666666666,0.11458333333333333,0.20155038759689922,0.2,0.15384615384615383,0.21359223300970875,0.17105263157894735,0.15384615384615385
304,SP:421131b3f64c978dda8499250764f4175da8eeea,"The paper analysis differentially private (DP) optimization convergence in neural networks by examining the neural tangent kernel (NTK). In particular, the papers analysis focuses on DP optimization which utilize gradient clipping and the Gaussian mechanism for gradient updates. Through the analysis of the NTK, an alternative clipping operation is proposed which has guarantees for convergence of the loss function. In addition, this alternative clipping operation provides a DP guarantee. Experiments in the paper then verify that the proposed clipping performs well in practice.","The paper analyzes convergence of DP-SGD using the NTK framework. The general idea is that the learning dynamics of SGD converge when the NTK matrix is positive-semi-definite.  They  characterize the effect of clipping and noise and Propose global clipping strategy. The global clipping idea consists of eliminating samples that have gradient norm bigger than some threshold and then applies a common clipping multiplier for each sample gradient. The authors claim that global clipping reduces gradient bias.    Gradient Clipping Summary: DP-SGD consist of the following steps: Over T rounds: 1) the algorithm samples a batch of samples from a dataset I_t. 2) it computes the gradient of each individual samples. 3) Each gradient sample is multiplied by a number such that the final gradient has norm bounded by R. 4) Finally, all clipped gradients are aggregated, noise is added to the average clipped gradient and the result is applied to update the model.  Difference between local and global clipping:  In local clipping, the gradient of each sample is multiplied by a factor specific to that sample such that the gradient  norm  of each sample is bounded by R. In global clipping, there is an additional parameter Z, each sample in a batch with gradient norm smaller than Z are multiplied by a constant factor R/Z and samples with gradient norm bigger than Z are not used. Therefore in global clipping each sample in a batch has gradient norm bounded by R. The intuition behind global clipping is that each gradient in a batch gets multiplied by a constant factor, this means that the direction of the aggregated gradient should be preserve.    Convergence Analysis: The convergence analysis uses the Neural Tangent Kernel (NTK) framework. The premise is that, the convergence of SGD is controlled by the NTK matrix in that the condition for convergence is that the NTK matrix is positive-semi-definite. The paper provides two main theorems: The first one claims that local clipping cannot guarantee convergence because local clipping can break the conditions for convergence under the NTK framework. The second theorem says that when the parameter Z is large then global clipping can guarantee convergence because each gradient is multiplied by a constant factor and does not affect the positive-semi-definitess of the NTK matrix.   Experiments: The paper empirically evaluates global clipping on the MNIST and CIFAR10 dataset. They  ","This paper studies the problem of understanding the convergence of differentially private deep learning models. The most common technique to ensure differential privacy (DP) in deep learning is to adapt the SGD algorithm by clipping the gradient at each learning step before perturbing it with Gaussian noise. The present work proposes to study the impact the clipping step and the noise injection can have on the convergence of the algorithm. In doing so, the authors claim to make several contributions summarized below.  1. Provide a characterization of the training dynamics of differentially private gradient descent methods through the lens of gradient flow theory, using neural tangent matrices (NTK).  The authors claim that this framework enables a fine-grained analysis of the impact of clipping and noise injection on the convergence of deep learning algorithms.  2. Develop a new clipping method called ""global clipping"" that improves convergence and mitigates the lack of calibration of differential private gradient descent methods. Moreover, this new approach does not change the DP guarantees of the gradient descent algorithm, is easy to implement and is as computationally efficient as the previous clipping methods (called ""local clipping"" in the paper).  ","The paper proposes a new clipping method for DP-SGD. It is shown that this new clipping method performs favorably empirically. Furthermore, the paper analyzes the clipping method using NTK.",0.3253012048192771,0.3253012048192771,0.13253012048192772,0.12562814070351758,0.04020100502512563,0.06217616580310881,0.0678391959798995,0.13989637305699482,0.36666666666666664,0.25906735751295334,0.5333333333333333,0.4,0.11226611226611226,0.1956521739130435,0.19469026548672566,0.16920473773265648,0.07476635514018692,0.10762331838565023
305,SP:42cb59ea59eed121bec7240729963ca2f83a7012,"This paper presents a new generic framework of relational neural network defined over hypergraphs, which they call RelNN. RelNNs take an architecture which hierarchically operates on representation functions of node tuples of different sizes and can be stacked into multiple layers. The framework is claimed to have unified graph neural network, neural logical machines and tranformers. The authors conduct analysis of RelNNs and show that its expressiveness increases as the artity or the number of RelNN layers grows sufficiently large. The paper also characterizes the generalizability of RelNN under certain conditions. ","Neural Networks for relational data have been an active topic of research interest in recent years. There is still a lack of full understanding of the expressivity, generalisability of these models. The contributions of the paper are: 1) Development of relational neural networks (RelNNs) for (hyper)graphs that unify graph neural networks (GNNs), neural logical machines (NLMs), and transformers, 2) Analysis of the expressivity of RelNNs in terms of (i) maximum hyperedge size and (ii) depth of RelNN, and 3) A study of the generalisability of RelNNs to unseen test data.","This paper explores the expressiveness of neural network-based relational learning models, such as graph neural networks and Transformer. To this end, a rational neural network is built upon the hypergraph structures to unify different models. Synthetic datasets are used for performance evaluation.","The paper proposes a unifying study of models for relational learning, namely Transformers, Graph Neural Networks (GNNs), and Neural Logic Machines (NLM), under the RelNN framework, and provides a series of results pertaining to their expressiveness relative to the arity of relations they support, as well as the depth, i.e., the number of iterations, they use during computation. In particular, arity and depth hierarchies are studied, and a set of arity and depth requirements are stated for a set of computational problems. Furthermore, the generalization ability of RelNN models is studied in the fixed-precision and infinite-precision settings. In the former setting, it is shown that there exists a finite size N for input graphs at which perfectly fitting all possible input graphs yields perfect generalization onto graphs of size greater than N. In the latter case, the sample complexity required to achieve certain error bounds is studied, and it is shown that more range-restricted aggregation function such as max and mean enable improved sample complexity. Finally, the paper conducts an experimental analysis on several synthetic problems, and achieves results validating the earlier provided arity and depth results for RelNN.",0.2087912087912088,0.10989010989010989,0.23076923076923078,0.10989010989010989,0.27472527472527475,0.3488372093023256,0.2087912087912088,0.23255813953488372,0.10880829015544041,0.23255813953488372,0.12953367875647667,0.07772020725388601,0.20879120879120883,0.14925373134328357,0.14788732394366197,0.14925373134328357,0.176056338028169,0.1271186440677966
306,SP:42d736c1f7bf2c837ebf0373ff988baa0d967197,"The paper proposes a method for learning invariant latent representation of the observations from MDP processes that share some aspects of their dynamics but differ in states.  This is useful for generlising reinforcement learning agents to wider range of variability of test conditions (i.e. test data can be out of distribution of the train data).  The invariant latent representation is derived through minimisation of mutual information between latent encoding of the in-distribution and the out-of-distribution experiences by ""fooling"" a GAN-like discriminator tasked at differentiating the two.  ","In this paper, the transfer of a reinforcement model (RL) setting from an idealized (training) environment to a more realistic environment with distractors in the observations is considered. Instead of augmenting the training environment with more data so as to make the system more resilient to variations and distractors, the system is adapted at test time to be invariant to the specific distractors found in the environment. Experiments in simulation show the benefits of the proposed approach. Crucially, the agent is not able to access any reward data at test time."," **Introduction:**  In this paper the authors aim to tackle the problem of learning a model that generalizes well on a test distribution that samples outside of the training data distribution.  They aim to do this not via data augmentation but by using an inference model over the latent space of the input.  Such an approach needs to go beyond in-distribution generalization. If the target domain is not know a priori things like domain transfer, domain randomization and meta-learning techniques may fail.  Out-of-distribution generalization is very important but more challenging:  * Prior experience will be critical  * Lack a reward function over the test distribution  * In robotics for instance, this type of learning is critical  The authors propose to: *""recast out-of-distribution generalization problem into an unsupervised policy adaptation between two MDPs that share a similar latent dynamics and reward structure, but with distinct observations""*  Statement of purpose for the paper: *""Harness probabilistic inference to produce invariance by taking advantage of latent structure""*  This work attempts to distinguish itself from other approaches that *""bake policy invariance""* in at training time.   **Problem Formulation:**  Given an MDP: M = <S, O, A, R, P, γ>, S contains ground truth info not always accessible to agent.  In particular two distinct MDPs: A train MDP M_train and a test MDP M_test.  The agent has no access to the reward function at test time.  The problem is defined: *""Often, we are most interested in the case where shift between M_train and M_test is induced by differences in the observation spaces O_train and O_test. That is, the state and reward structures between the train and test MDPs are quite similar, but the observation spaces between the two MDPs are significantly different.""* It is noted that recent work in RL has tried to deal with this via data augmentation but this can lead to instability.  **Invariance Through Inference and Algorithm**  The goal here is to learn an encoder mapping semantically similar states in the train and test distribution to latent vectors.  This is to be accomplished given there is no access to rewards at test time and no access to paired trajectories (ie. matched test/train trajectories). The approach is defined by two objectives:  1. a distribution matching objective encourages the latent distribution induced by both MDPs to match.   2. a GAN-style loss to help ensure that the latent code is not using distracting information that would help distinguish the train and test MDPs.  The authors state that *""This adaptation objective is effectively maximizing the mutual information between the prototypical representation kept from during training, and the adapted representations of the new observations""* and further claim that *""our adaptation objective produce invariance by making statistical inference in the latent space.""*  As regards the algorithm they also state that: *""The encoder G takes an observation from target environment, and learns to trick the discriminator, while the discriminator predicts whether the input comes from the encoder or source buffer.""*  **Experiments**  The authors us the Deepmind Control Suite (DMC) as a training environment and the Distracting Control Suite (DistractingCS) for a test environment.  *""DistractingCS adds three types of distractions to the DeepMind Control Suite through deviations in background, camera pose, and color.""*. Results are presented showing that baseline methods augmented with ITI show much slower performance degradation as the distraction intensity is increased.  The authors also compare ITI to Policy Adaptation during Deployment (PAD).  They compare to ITI on DMC for fixed distraction intensity. ITI proves to yield stronger results in this case and it is posited that PAD does not encourage latent train and test distributions to match as ITI does: *We suspect this instability is caused by the large deviations in the latent variable distribution as a result of changes in the target environment. In particular, we posit that the signal from PAD’s inverse dynamics head does not encourage the latent train and test distributions to match, which is a feature specifically baked into Invariance through Inference.* ",This paper proposes to learn the invariances relating to MDPs with different observations (e.g. invariances in the underlying dynamics of the systems) using a generative model of the observations in an unsupervised manner. The inference in this generative model is accomplished using a GAN framework with auxiliary losses for the inverse dynamics of the system.,0.16483516483516483,0.38461538461538464,0.15384615384615385,0.46153846153846156,0.17582417582417584,0.041979010494752625,0.16483516483516483,0.05247376311844078,0.25,0.06296851574212893,0.2857142857142857,0.5,0.16483516483516483,0.09234828496042218,0.1904761904761905,0.11081794195250659,0.21768707482993196,0.07745504840940526
307,SP:42fdf2ef88f29b5c65306fe29874089224d4119e,"The paper describes a method for representing videos using neural networks. This is inspired by recent advancements in implicit neural representations, which encode various types of signals (audio, images, video, shapes, radiance fields) in the weights of a MLP overfit to the signal. The paper accomplishes this by contributing a new network architecture utilizing convolutional layers which maps each frame index t to the **entire image frame**. Since this network memorizes each frame of the video, compression methods can then be applied on the network instead of the frames themselves to yield a comparable compression versus video quality trade-off. The paper demonstrates that this alternative compression method is comparable to existing standard video compression methods, and that it is robust to noise in the ground truth video.","This paper proposes  a novel neural representation for videos. The key idea is to represent a video as a function conditioned on the time index input. Given a video, it uses a MLP to fit such a function. Two applications are conducted: video compression and video denoising. ","Video is encoded as a neural network that takes frame index and outputs the video frame corresponding to the index. Video compression problem is cast as a model compression problem, and the proposed neural representation achieves performance comparable to standard frame-based compression method. The proposed neural representation is demonstrated for temporal interpolation and video denoising tasks. ","The paper proposes to model videos as a function mapping time to image, and uses a neural network to model this function. It trains a neural network on one video or a concatenation of several videos, by overfitting the neural network on the videos. Since the motivation is to represent a video in full, the network is not trained for generalization, rather it is trained to overfit. Then, the paper tries to do video compression by compressing the weights of the neural network. It uses 3 model compression techniques in a cumulative fashion : pruning, weight quantization, entropy encoding. It is found that this gets the compressed representation close to standard video compression techniques in terms of PSNR vs BPP (distortion ratio).",0.1328125,0.1484375,0.2109375,0.2765957446808511,0.40425531914893614,0.2982456140350877,0.3617021276595745,0.3333333333333333,0.2231404958677686,0.22807017543859648,0.15702479338842976,0.14049586776859505,0.19428571428571428,0.20540540540540542,0.21686746987951805,0.25,0.22619047619047622,0.19101123595505617
308,SP:430e49ca10bdeb2e3101f9b11d9a131b65496eec,"This paper analyze the compressibility of over-parameterized neural networks using the heavy-tail property of SGD. The authors show that when the network weights are approximately sampled from a heavy-tailed distribution, the weights are more compressible. The authors also provide a generalization guarantee showing that more compressible networks have smaller generalization gap.","The main contribution of this paper is to link the two properties of compressibility and SGD. (i) as the network size goes to infinity, the system can converge to a mean-field limit, where the network weights behave independently, (ii) for a large step-size/batch-size ratio, the SGD iterates can converge to a heavy-tailed stationary distribution. In the case where these two phenomena occur simultaneously, we prove that the networks are guaranteed to be ‘lp-compressible’. Further experiments on fully connected and convolutional neural networks and show that the results are in strong accordance with the theory.","The paper looks at the compressibility and generalization ability of multi-layer neural networks under the heavy tailed mean-field limit. The mean-field condition is invoked to simplify the analysis of the distribution of weight parameters as the distributions of individual weights are independent, while the heavy-tailed condition is needed to ensure an abundance of small weights (weights at the tail) whose removal would result in bounded relative error. Compression error is defined in weight space as the relative distance between the compressed weight vector (weight vector where small weights are set to zero) and the original weight vector.  The paper shows that arbitrarily low compression errors can be achieved under a range of pruning strategies (global pruning, layer-wise pruning, singular value pruning, and neuron pruning) if the network is sufficiently large. Another condition is for the p-norm used to calculate the error to have p larger than the heavy tail coefficients. ","This paper gives a theoretical characterization of the underlying cause that makes the neural networks amenable to common compression schemes in the over-parameterized regime. It is done by connecting the dynamics of the learning algorithms such as SGD to the heavy tails of the weights of a network, which leads to better compressibility. The main contribution is to provide sufficient conditions for the compressibility from the limiting distribution of SGD, and show how the compressibility of a network can lead to better generalization.  ",0.3148148148148148,0.3888888888888889,0.2962962962962963,0.23,0.18,0.15384615384615385,0.17,0.1346153846153846,0.19047619047619047,0.14743589743589744,0.21428571428571427,0.2857142857142857,0.2207792207792208,0.19999999999999998,0.2318840579710145,0.1796875,0.1956521739130435,0.2
309,SP:433f867669b26ae6ff16c3318303f0f34d33073b,"This work analyzed the fine-tuning phase of unsupervised contrastive learning for adversarial robustness. Furthermore, they proposed AdvCL which is more robust than other unsupervised adversarial contrastive learning methods. They demonstrated that the model trained with this method shows discriminative and interpretable representations, flat loss landscape, and better transferability to other tasks.","The submission investigates the adversarial robustness of self-supervised pretraining using contrastive learning with the NT-Xent loss [29]. The goal is to improve robustness of the pretraining step such that a linear classifier suffices to be fine-tuned for state-of-the-art performance, and to increase robustness across tasks. The proposed design, AdvCL, extends the loss formulation to contain four designed views (Eq. 9), as well as a supervision stimulus based on K-means clustering [50]. Experimental results show that the resulting system significantly increases adversarial robustness in the self-supervised setting. The paper also contributes a visualization of qualitative results as well as an ablation study of some of the key parameters.","This paper proposes a unified adversarial contrastive learning framework. It proposes to improve the previous methods in the following ways: 1. Besides the previous augmentation methods, it proposes to utilize the high-frequency components as data augmentation to enhance the robustness. 2. It introduces the pseudo label for the pre-training task to mitigate the task difference between pre-training and fine-tuning for improving the transferability of the pre-training network. Many experiment result verifies the effectiveness of the proposed method. The author also offers ablation experiments to demonstrate the improvement brought by each component.","The focus of the paper is to make contrastive pre-training (e.g. SimCLR), which has seen great success in recent years, more robust to adversarial attacks. To this end, the authors propose AdvCL, which makes the following modifications to prior contrastive methods: 1) high-frequency components of images are shown to be important for robustness and are leveraged when building the views. 2) augmenting contrastive learning with ""pseudo-supervision"" obtained by clustering the features further improves performance. The authors evaluate test accuracy on CIFAR10, CIFAR100, STL-10 under adversarial perturbations (AutoAttack and standard PGD) using both linear head fine-tuning and full fine-tuning and compare against recent SOTA methods. AdvCL seems to outperform the rest mostly across the board. Ablative studies are conducted.",0.21153846153846154,0.17307692307692307,0.2692307692307692,0.16521739130434782,0.16521739130434782,0.21875,0.09565217391304348,0.09375,0.112,0.19791666666666666,0.152,0.168,0.13173652694610777,0.12162162162162161,0.15819209039548024,0.1800947867298578,0.15833333333333333,0.1900452488687783
310,SP:437605d7a5e84e0ba1140d818599d848835ce34c,"This paper proposes a scheme for partially sharing CNN parameter across layers.  Specifically, convolutional layer parameters are factored into the product of layer-specific filter atoms (which define spatial filter structure) and a set of shared coefficient that mix features across channels.  Training CNNs reparameterized in this manner yields improvements along the accuracy-parameter tradeoff curve, compared to baseline models on image classification tasks.","This paper introduces a joint subspace view between the layers of convolutional neural network. Since the important features across the layers in deep CNN is maintained, the filters can be decomposed into shared coefficients and layer-specific coefficients. Motivated by multi-scale decomposition, they compose the shared coefficient in order to exploit correlations between features maximally. Also, by using group convolution, they reduce the amount of parameters as well as maintain the performance. This method is easily compatible to modern CNN architectures.","This work first shows empirical observations that there exist obvious correlations in features across layers within a CNN after proper linear transformations. Accordingly, the authors propose coefficient sharing strategies across layers or across groups of filters, which aim to decompose convolution filters for reducing model sizes. The experiments are conducted on four image benchmarks using several CNN backbones, and the results show the effectiveness of the proposed ACDC on image classification and few-shot image classification tasks.","This paper proposes a novel method to decrease redundancy of parameters in a CNN through a joint subspace view. The authors use a low-rank decomposition of the convolutional kernel (A * D) into shared coefficients (A) and atoms (D). The authors propose a variety of proposal methods for their method termed Atom Coefficient Decomposed Convolution. Notably by sharing A across all layers, sharing A across groups of layers and by utilising the idea of filter groups. The authors demonstrate that ACDC-x outperforms baselines at moderate to significant reductions in model size. Further, they show that their method can alleviate issues of model degradation as deeper networks are used.",0.203125,0.21875,0.21875,0.14634146341463414,0.24390243902439024,0.24675324675324675,0.15853658536585366,0.18181818181818182,0.12844036697247707,0.15584415584415584,0.1834862385321101,0.1743119266055046,0.1780821917808219,0.19858156028368792,0.16184971098265896,0.1509433962264151,0.2094240837696335,0.2043010752688172
311,SP:43afbc602eadcbe410b5e04ababeae699362f3b8,"This work proposes (1) a new theoretical model for algorithms to explain the predictions of black-box classifiers and (2) an algorithm in this model that produces such explanations with polynomial guarantees on its running time for producing explanations of a given fidelity.  The model is an analogue of learning under the uniform distribution, making use of the ability to query the black-box classifier. Given a point of interest, the algorithm finds a conjunction that is relatively small with high probability (over the draw of a point of interest) and such that points matching the conjunction receive the same label as the point of interest with high probability over subsequent draws of points. All draws are w.r.t. the uniform distribution on Boolean features. Note that the size of the conjunction  therefore also corresponds directly to its coverage, which is 1/2^size.  The algorithm is an adaptation of recent decision tree learning algorithms, that greedily branch on the attribute that reduces the noise sensitivity the  most. The explanation is given by the branch that the point of interest follows in this tree. The guarantees follow from a connection between certificate  complexity and decision tree depth. ","The paper considers a new algorithm for creating  explanations of predictions of arbitrary black box models.  In this paper, an explanation is a subset of the input features that explain the prediction in the sense that changing the input features, not in the explanation subset at random, has only a small probability of changing the model prediction.  The paper considers only binary features and binary classification and assumes uniform distribution over the input.  The main result is an algorithm that with probability 1-delta over the random choice of input provides an explanation (subset of features) that is polynomial in the average size of the smallest explanation when averaged over all inputs (and 1/delta, 1/epsilon where epsilon is error allowed).  The main algorithm assumes an existing decision tree algorithms, and simply builds the path from the root to the leaf where the input data point would end up, the explanation being the features queried in the nodes on the way. This is possible due to the assumption of uniform distribution over the binary features.  The main component of the theorem is based on a result from complexity theory, that bound certificate complexity (an explanation is a certificate) of a function, polynomially in terms of the decision tree complexity (depth) of the function, which leads to the polynomial in the bound given.  The second important part is the decision tree construction algorithm required for the explanation algorithm. This methods is based on recent results on decision trees and that works for only uniform distribution over binary features and binary classes, providing the limiting features of the result shown. The algorithm is sped up with random sampling which is straightforward by the same assumptions.  ","The authors propose a local explanation method based on certificates, i.e. sets of important features that closely (in conjunction) determine the prediction of a given black-box model. The obtained certificates provide guarantees regarding two sensible properties: succinctness and precision (i.e., intuitively, they remain small/sparse and, with high probability, also locally faithful w.r.t. the black-box model). The authors propose an implicitly learned Decision Tree to obtain certificates. This implicit approach reduces the computation time compared to related methods. The authors further demonstrate the relationship between the complexities of such implicit Decision Trees and the corresponding certificates.","The paper discusses the problem of providing explanations when predicting that some instance x belongs to a particular class. In this direction the authors use a decision tree in order to justify the assigned label, but the interesting part is that the decision tree is only implicitly learnt. Basically such an explanation is provided as a list of variables that one should query on the (implicit) decision tree, from the root node down to a leaf node that provides the classification for x. Furthermore, such an explanation can act as a certificate in the sense that indeed the variables mentioned in the explanation, have low error rate with high probability, when the attributes that we find along this path remain the same as in the query instance x, for any point sampled from the underlying distribution. ",0.2777777777777778,0.14646464646464646,0.15656565656565657,0.10915492957746478,0.13732394366197184,0.17647058823529413,0.1936619718309859,0.28431372549019607,0.22794117647058823,0.30392156862745096,0.2867647058823529,0.1323529411764706,0.2282157676348548,0.19333333333333333,0.18562874251497008,0.16062176165803105,0.18571428571428572,0.15126050420168066
312,SP:43cae7c8b85829b9f79b3bc891d5eb9a9b00b0b5,The work focuses on a very practical problem of strict cold start(SCS) recommendations which is a highly prevalent and relevant problem. The author’s main contribution to address the SCS is to use GNN with knowledge distillation – this proposed solution does not have to rely exclusively on the node features. The authors also define an FCR (feature contribution ratio) that can help determine the ideal network architecture – FCR can optimize the model selection which significantly affects the quality of overall system performance. ,"This paper proposes Cold Brew, a new method for learning cold start node embeddings in graphs. Cold Brew leverages teacher-student framework (knowledge distillation) to handle nodes without neighbors by transferring knowledge of teacher network (learned from head nodes) to student network (for tail or isolated nodes). Experiments are conducted to show that the proposed method outperforms some baseline methods. ","Many real-world graphs have power-law distributions of node degrees and learning the representations of nodes with few or even no connections may only depend on their attributes. This paper studies the problem of learning good representations of such nodes using inductive GNNs. It proposes a new method to generalize GNNs better for tail nodes compared to pointwise and graph-based models using a distillation approach. A metric, feature contribution ratio, has been proposed in quantifying the contribution of nodes' features in predicting labels. Experiments on several graph datasets demonstrate the effectiveness of the proposed method especially in learning better representations of the tail and isolated nodes.",The paper proposes Cold Brew to distill the knowledge of a GNN teacher into an MLP student to handle the tail and cold start generalization problem by using the head part of the graph to guide the discovery of the latent neighborhoods of tail and isolation nodes. The paper also proposes a new metric to measure the contribution ratio of node features w.r.t. the adjacency structure. The experiments on several public datasets and a proprietary e-commerce graph show the effectiveness of the proposed method.,0.0963855421686747,0.18072289156626506,0.20481927710843373,0.23333333333333334,0.31666666666666665,0.24074074074074073,0.13333333333333333,0.1388888888888889,0.19540229885057472,0.12962962962962962,0.21839080459770116,0.2988505747126437,0.11188811188811189,0.15706806282722513,0.2,0.16666666666666669,0.25850340136054417,0.26666666666666666
313,SP:43d3f6203ff2a3db342dd45a63f0d5f530a9c7a9,"The authors have considered the problem of imitation learning using only visual observations, and they've explored the paradigms of using methods based on adversarial learning and optimal transport to solve this problem. In particular, the authors propose two new methods--P-SIL and P-DAC--presumably as a way to explore design choices and tradeoffs in this space. Some experiments designed to demonstrate the superiority of P-SIL and P-DAC are presented.","This paper focuses on learning visual policies by imitating video data without observing the expert actions. The authors introduce a method to compute the imitation reward based on the representation from the expert RL encoder. Besides, a data augmentation method is proposed to scale to non-trivial control tasks. They conduct comprehensive experiments on DeepMind control suite tasks, showing the versatility and strong performance of the two approaches (P-DAC and P-SIL).","This paper presents two algorithms for imitation learning P-SIL and P-DAC which are built on top of SIL (Papagiannis & Li, 2020) and DAC (Kostrikov et al, 2019). The first one is an algorithm based on the Sinkhorn distance and the second one uses an adversarial approach, by training a discriminator. These two approaches use state-only trajectories to generate reward sequences to match the agent state-trajectories and learn through that reward. By combining the advances in image-based reinforcement learning by the usage of data augmentation and target encoders, the two new algorithms, P-SIL and P-DAC, are tested and reach good results on visual control tasks. ",This paper introduces two methods for imitation learning from pixel observations based on adversarial learning and optimal transport. Representation from the RL encoder are adopted for calculation imitation rewards. P-SIL (Pixel Sinkhorn Imitation Learning) learns and compares latent representations of the encoded expert and agent behaviors; and P-SAC (Pixel Discriminator Actor Critic) augments the representations of the agents and the experts. The key component of this work is the RL encoder used in both P-SIL and P-SAC. ,0.20270270270270271,0.2702702702702703,0.2972972972972973,0.2602739726027397,0.2328767123287671,0.2072072072072072,0.2054794520547945,0.18018018018018017,0.2716049382716049,0.17117117117117117,0.20987654320987653,0.2839506172839506,0.20408163265306123,0.21621621621621623,0.2838709677419355,0.2065217391304348,0.22077922077922077,0.23958333333333334
314,SP:43dbe1f63901e89ff8e8733a60734074435fdfba,"This paper studies the evolution strategies (ES) based method to estimate the Jacobian matrix used in bilevel optimization. The main idea is to use first-order information to estimate the second-order information in the Jacobian matrix. Some convergence results have been provided, companied with some simulations. ","The paper aims to reduce the computational cost in solving a bilevel problem. In bilevel optimization, the outer gradient can be decomposed into direct and indirect gradients. The indirect gradient requires computing the response Jacobian, which involves computing & inverting the Hessian and computing mixed derivative. To avoid expensive second-order approximations, the authors propose to use the evolution strategies (ES) based method to approximate the response Jacobian. The authors provide the convergence guarantee of the algorithm and empirically demonstrate that the proposed algorithm performs well on large-scale bilevel problems. ",This work mainly focuses on proposing a computational efficient for approximating the response Jacobian matrix in the hypergradient of bilevel optimization with only first-order information of the loss functions. The proposed methods are applicable for developing both deterministic and stochastic bilevel optimization algorithms. The author further show the convergence guarantees and computational complexity of the proposed algorithms. Multiple numerical results showcase the efficiency of the proposed method in applications of machine learning.,"The paper introduce two new algorithms to tackle bi-level optimisation, which has wide-ranging application in machine learning. Crucially, these two algorithms forego second order gradient computation completely, favoring instead an evolution strategies approach.  While this approach has been proposed before in this context, the new algorithms are more efficient because they do not treat the whole problem as a black box, and leverage the structure of the hypergradient. Theoretical convergence analyses are then provided for both the full gradient and the stochastic gradient alternatives, as well as experiments on various bi-level optimisation problems, showcasing improved performance over previous approaches.",0.2978723404255319,0.2978723404255319,0.19148936170212766,0.16666666666666666,0.2,0.1643835616438356,0.15555555555555556,0.1917808219178082,0.08823529411764706,0.2054794520547945,0.17647058823529413,0.11764705882352941,0.20437956204379562,0.23333333333333334,0.12080536912751677,0.18404907975460122,0.18750000000000003,0.13714285714285712
315,SP:43e20d2d0864c2adc4533ea2e8a14d853c8a01a1,"The authors propose a procedure for safe end-to-end learning of parameterized trajectory optimization problems. The entire learning problem is presented as a bi-level optimization. The outer optimization prescribes a loss function and safety constraints, which take as inputs the parameters and the output of an inner trajectory optimization problem using these same parameters. The overall map from parameters to the output of the outer problem is differentiated approximately with the help of: 1) approximating the inner and outer problems with unconstrained versions via log-barrier objective terms, and 2) using the discrete-time constrained PMP to differentiate the inner problem along an optimal trajectory. Safety is enforced by the log-barrier functions in the outer problem approximation.","This paper provides an efficient method based on Pontryagin Differentiable Programming (PDP) for the control of safety-critical systems. The method entails formulating the control problem as a bi-level optimization problem, where * The lower-level problem solves for a trajectory that minimizes some control cost subject to dynamics, terminal constraints, and path constraints associated with the underlying system (all of which are potentially parameterized), and * The upper-level problem aims to pick the parameters of the lower-level problem to minimize some task-specific loss subject to task-specific constraints.  The authors provide an efficient method to solve problems of this form (e.g., both terminal and path constraints) by employing barrier methods to both rewrite the outer problem as an unconstrained optimization problem, and to efficiently solve for solutions of and implicit gradients through the inner problem. They then prove that under certain assumptions, their method satisfies all problem constraints, as needed for the safety-critical setting. They provide experimental validation for the problems of policy optimization, motion planning, and imitation learning in several dynamical systems settings.","In order to satisfy both immediate and long-term safety constraints, this paper proposes Safe Pontryagin Differentiable Programming (Safe PDP). The proposed method is based on interior-point methods, and it enables us to incorporate heterogeneous types of constraints. The advantages of Safe PDP lie both theoretically and empirically. In the experiments, the authors demonstrated that their proposed method is better than unconstrained methods (in one experiment, a baseline is ALTRO, though) in toy problems.  ","This paper proposes a Safe Pontryagin Differentiable Programming (Safe PDP) methodology. The contributions are at both theory and algorithmics for a new safe differentiable framework for various safety-critical learning and control tasks. The proposed approach is inspired by interior-point methods. Both backward and forward pass can be approximated and computed efficient by solving an unconstrained counterpart. The approximation can also be controlled for arbitrary accuracy using a barrier parameter. In addition, the authors can also show that all constraints are respected throughout the course of approximation and optimization, thus guaranteeing safety. Experiment results are shown in many learning and control tasks on different challenging control systems such as 6-DoF maneuvering quadrotor and 6-DoF rocket powered landing. ",0.2916666666666667,0.125,0.13333333333333333,0.12290502793296089,0.15083798882681565,0.30666666666666664,0.19553072625698323,0.2,0.13333333333333333,0.29333333333333333,0.225,0.19166666666666668,0.23411371237458192,0.15384615384615385,0.13333333333333333,0.1732283464566929,0.18060200668896326,0.23589743589743592
316,SP:4427a20ad2caee2f378b124239f4f91dca9254ff,"This paper describes various MLMC debiasing schemes for computing stochastic gradient estimates, and applies them to conditional stochastic optimization problems. Here, unlike other more-standard problems in optimization it is impossible to compute true gradients of the objective function: the objective is an expected value over infinitely many points. However, as this paper observes, approximate minimizers to these problems may nevertheless be computed. The main ideas underlying this result are the use of sampling and a multi-level Monte Carlo (MLMC) to generate low-bias stochastic gradient estimates: these are then combined with stochastic gradient descent. ","This paper provides a systematic study of the bias-variance-cost tradeoff for several MLMC gradient methods and L-SGD, under the assumption that (i) there exist a sequence of obj. approximations; (ii)  there exist stochastic oracles which generate unbiased estimates of the gradient for the obj. approximations, and unbiased estimates of the difference between function approximation gradients. They assume three parameters a, b, c which respectively describe the bias, variance of the stochastic oracle, and cost to query the oracle, and provide insights on which class of algorithms work better under different configurations of a, b, c. They also show that for conditional stochastic optimization problems, MLMC methods can significantly reduce the sample complexity.","The paper studies a family of MLMC methods for gradient estimation and systematically analyses their performances under different stochastic optimization settings (strongly convex, convex, and nonconvex). In each setting, the order of the total cost is given in terms of the decay rates of the bias and variance, and the corresponding increasing rates of the oracle cost. It is shown in the CSO problems that some of the MLMC methods have lower computational costs compared to existing state-of-the-art (e.g., [19]).",This paper consider stochastic optimization when we only have access to a sequence of biased functions that converge to the true function. They apply various Multilevel Monte-Carlo methods (MLMC) to handle the bias-variance tradeoff. Convergence rate and computational complexity are computed for both convex and non-convex objective. They also demonstrate the superiority of MLMC gradient methods when applied to conditional stochastic optimization. ,0.15625,0.17708333333333334,0.16666666666666666,0.20869565217391303,0.1565217391304348,0.16666666666666666,0.13043478260869565,0.20238095238095238,0.24615384615384617,0.2857142857142857,0.27692307692307694,0.2153846153846154,0.14218009478672985,0.1888888888888889,0.1987577639751553,0.24120603015075373,0.20000000000000004,0.18791946308724833
317,SP:442e615bd5f82eb6ea100d3a7b9d3ee52728eac8,"The authors propose a method for knowledge distillation specifically for  Deep Directed Graphical Models. The authors compare their method with marginalization methods, which integrates the latent variables out, and the factorized (local) method, which distills knowledge between teacher and student on each factor. They validate their approach on continual learning, model compression, and discriminative learning.   ",This paper proposes to use the reparameterization trick to convert the latent variables in DGMs to deterministic variables in the context of KD. It then proposes a surrogate distillation loss and latent distillation loss and evaluates the performance of the proposed method in three applications. Experimental results confirm the effectiveness of the proposed model.,"This paper proposes a new knowledge distillation framework for directed graphical models based on the reparameterization trick. The new distillation framework overcomes the intractable marginals in marginalized distillation, and error accumulation in local distillation. Empirically, the new distillation framework surpasses the baselines on deep generative model compression, VAE continual learning, and discriminative DGMs compression.  ",The authors propose an unified Knowledge Distillation technique for general deep directed graphical models. They use the reparameterization trick on the intermediate latent variables of the original DGM network and the student network. This converts the networks to a compact semi-auxiliary form. Then they use a surrogate distillation loss (combined with latent loss) to reduce the error accumulations over the chain of random variables. They discuss the similarity of their technique with others and demonstrate its performance for 3 applications. ,0.14545454545454545,0.2909090909090909,0.36363636363636365,0.25925925925925924,0.35185185185185186,0.2962962962962963,0.14814814814814814,0.2962962962962963,0.24691358024691357,0.25925925925925924,0.2345679012345679,0.19753086419753085,0.14678899082568805,0.2935779816513761,0.2941176470588235,0.25925925925925924,0.2814814814814815,0.23703703703703702
318,SP:44538fca6d60b7685121b9e48e98a0eadfea17f1,"The authors propose a new method for (offline) continual learning using bayesian hypernetworks to obtain a distribution over a task-based parameter generating function. This meta perspective enables replay at the level of level of the hypernet parameters (rather than at the input level), while the bayesian approach provides reliable uncertainty estimates which the authors show can be used to perform task inference at test time.   The hypernet setup, (which is really a 3-level hierarchy) comprises a 1) Task-Conditioned network, which maps a task embedding to hypernet parameters $\theta_t$, 2) a hypernet with params $\theta_t$ remaps a noise sample $z$ to a set of weights $w$, 3) which are used as parameters for the downstream classifier.   The authors provide two toy setups to illustrate the validity of their method, before benchmarking their method on Split-Cifar and Split Mnist. ","This submission proposes a continual learning method based on a shared hypernetwork that parameterises a distribution over the network parameters for each task given a learned task embedding. It presents this as a Bayesian method, which is **not correct** as the objective function does not minimize a divergence from the posterior. Practically, the method is mostly **incremental** over (von Oswald et al., 2020), replacing the weight outputs of the hypernetwork with the parameters of a distribution over these. Finally, the paper omits large parts of the literature on continual Bayesian deep learning, displaying a disappointing **lack of scholarship**. Overall, this is a **clear rejection**.  Reference: J. von Oswald et al. Continual learning with hypernetworks. In ICLR 2020.","The paper proposes to tackle task-agnostic continual learning from a Bayesian deep learning perspective. In contrast to previous work along these lines the paper proposes to parameterize the posterior (over network weights) using a mixture distribution with one mixture component per task (seen so far). Three main options are explored - a point estimate (i.e. a single set of network weights) per task, a parametric distribution (Gaussian w. diagonal covariance) per task, and an “implicit” distribution parameterized by a neural network (using the reparametrization trick essentially). All three options can be unified/generalized by learning a weight-generator which, when conditioned on the task, produces a distribution of weights for the network that solves the task (e.g. the classifier). Importantly, the parameters of the weight-generator are produced by another neural network (the hyper-network) whose input is conditioned on the task. The question is how this task-conditioning is obtained: the paper proposes to automatically infer the task via the (epistemic) uncertainty modeled by the distribution of functions (classifiers) defined by the task-conditioned posterior over parameters (produced by the hyper-network). Experiments on synthetic data and continual learning versions of MNIST and CIFAR-10 are shown - with lots of ablations and control experiments. Results look promising, though they hint at some scalability issues for implicit distributions (an issue commonly observed in the Bayesian Deep Learning literature), as well as issues with reliable task inference via epistemic uncertainty on OOD data (another important open problem whose solution is beyond the scope of this paper). Limitations and conclusions are well discussed and related literature if well summarized.   **Contributions:**  1) Extension of a previously published approach to Bayesian CL via hyper-networks. The main novelty is to allow for the hyper-networks to parameterize distributions over networks via the weight-generator (including the earlier work as a special case - the MAP case with point-estimates for the posterior). The necessity of this extension is well motivated and the benefits are demonstrated nicely on synthetic data and result in significant performance gains on the image classification tasks. Significance: a main drawback of a SOTA method was identified and solved in a well-motivated and satisfactory manner (particularly from a Bayesian perspective). While similar approaches can be found in the literature (as discussed in the paper), the combination with automatic task inference is novel, and crucial relies on this innovation. I think the method will receive a fair amount of attention in the Bayesian CL community.  2) Automatic task inference via the epistemic uncertainty on OOD data given by the Bayesian ensemble (parameterized by the hyper-network). While previous CL methods have proposed to use task inference - the combination with contribution 1) allows for a potentially powerful and principled solution to the problem. Benefits (and some remaining issues) are shown in the results. Significance: the approach is very reasonable from a theoretical/principled point of view. The combination of contributions 1) and 2) is perhaps one of the most promising advantages of Bayesian CL. The particular way this is executed in the paper is novel and original (to the best of my knowledge), and it is well motivated and critically analysed in the paper. The results obtained here will be important for the Bayesian CL community, and allow for a clear separation of the task identification problem such that improvements in reliable modeling of epistemic uncertainty (which is an important topic outside CL) will directly contribute to improvements of Bayesian CL.  3) Solving the problem of catastrophic forgetting and task interference with a regularizer (posterior meta-replay) that acts on the level of the hyper-network (parametrization of the posterior over classifier weights). Significance: solving the problem is crucial to successful CL - the approach taken here is sound and empirical results underline the success of the regularizer. This is an interesting result for the Bayesian CL community.  4) Well-written and comprehensive discussion of the two main innovations, including a range of specific implementations for each. These variants are discussed, related with the current literature and evaluated in a large number of experiments. Results are critically evaluated and limitations are nicely discussed. Significance: given how well the paper is written and executed, and the wealth of experimental results, ablations and control experiments, I think the paper has the potential to become a seminal next step in Bayesian CL. My only slight worry is that the paper certainly pushes against the constraints of a conference-format paper, with many interesting parts of the paper pushed to the appendix, and a quite high information density in the main text. ","The authors propose a Bayesian approach for continual learning that models the weights distributions in a task-conditioned fashion, then relying on task inference. As expected, the main main issue is the task inference phase. The experimental validation is complete, although it is limited to only a small number of tasks in the same sequence.",0.13986013986013987,0.3146853146853147,0.13286713286713286,0.3504273504273504,0.1282051282051282,0.037859007832898174,0.17094017094017094,0.0587467362924282,0.34545454545454546,0.05352480417754569,0.2727272727272727,0.5272727272727272,0.15384615384615388,0.09900990099009901,0.1919191919191919,0.0928652321630804,0.1744186046511628,0.07064555420219244
319,SP:44671581f37fc514e1d45dfa31afd92afbc3c15a,"This paper shows experimentally that training on generated data (from a generative model not trained on external data) can improve adversarial robustness.  Surprisingly, even a class-conditional Gaussian generative model can improve adversarial robustness. The paper additionally provides sufficient conditions for generated data to help adversarial robustness.","The paper proposes to improve robustness of neural networks against adversarial attacks without using any additional data. For it, they make use of samples generated by a generative model as their proxy for additional data. They want the additional samples to be diverse enough, and list additional conditions needed for robustness to improve using generated data. Empiricall they obtain strong results, often performing better than the methods that use additional data. ","This paper examines whether using additional generated data can improve the robustness through adversarial training.  They propose several conditions for generated data to best improve the performance of adversarial training and validate the impact of these conditions empirically.  They evaluate the performance of several generative models (DDPM, VDVAE, BigGAN, Class conditional gaussian) on improving robust performance and demonstrate that using samples from DDPM, they are able to outperform many models using external data in training.","This work shows that data augmentation using good generative models helps improve adversarial robustness. Various experiments are conducted to show the importance of the quality of the generated images. The proposed method achieves state-of-the-art adversarial accuracy on CIFAR-10 and CIFAR-100 datasets, while the clean accuracy is not sacrificed.",0.23404255319148937,0.2978723404255319,0.23404255319148937,0.19718309859154928,0.11267605633802817,0.17333333333333334,0.15492957746478872,0.18666666666666668,0.20754716981132076,0.18666666666666668,0.1509433962264151,0.24528301886792453,0.18644067796610167,0.2295081967213115,0.22,0.19178082191780824,0.12903225806451613,0.203125
320,SP:448a1826253aa189c33818f11b88e5305d1bf066,"This manuscript proposes two algorithms for estimation of transition matrices in single-structured as well as superposition-structured VAR models. Linear convergence to true parameters are verified under certain stability conditions, and additional assumptions on the data distribution, step size and minimum sample size. Some empirical results are reported to illustrate good performance of proposed algorithms in finite sample cases.","This paper presents a recovery guarantee for gradient-based optimization method applied to high-dimensional vector autoregressive (VAR) models. It considers both single-structured and superposition-structured transition matrix, and shows that the distance between the true parameter and the iterates generated by projected gradient descent (PGD) algorithm will decrease linearly to certain statistical error controlled by sample size and Gaussian width of the assumed structure. In the experiments, PGD is shown to outperform the baseline algorithm FNSL empirically.","This paper considers the problem of estimating the transition matrix of a lag-1 vector autoregressive model. The authors propose projected gradient descent as well as alternating projected gradient descent algorithms to learn both single- and supervision-structured transition matrices, respectively. Non-asymptotic analysis is also provided which shows the algorithms convergence linearly tp the statistical error in the high-dimensional settings.","The paper proposes projected gradient descent algorithms for estimating high-dimensional vector auto-regressive models, and conducts convergence analysis following the restricted strong convexity lens.  Specifically, sparse, low-rank and sparse plus low-rank models are of interest.  While many of these topics have been studied ""to death"", the novelty here seems to be (a) there's limited work in applying sparse plus low-rank approaches to time-series settings, where there is an additional constraint that the transition matrix has to lead to a stable dynamical system (b) Typical convergence analysis is done under iid setting, but for VAR models, samples are naturally dependent with strong auto-correlation.",0.21666666666666667,0.18333333333333332,0.16666666666666666,0.21518987341772153,0.17721518987341772,0.1774193548387097,0.16455696202531644,0.1774193548387097,0.09174311926605505,0.27419354838709675,0.12844036697247707,0.10091743119266056,0.18705035971223022,0.18032786885245902,0.1183431952662722,0.24113475177304963,0.14893617021276595,0.12865497076023394
321,SP:44ae47bf73f77ea3f7a8fc65d244a40c2977e796,"This paper studies the following problem: given d-dimensional data living in an (unknown) linear subspace of dimensionality k<d, learn the subspace under the constraint of differential privacy (DP).   The authors Provide the first DP method(s) for learning a low dimensional subspace that the data lives in, where the sample complexity of their algorithms can scale with k instead of d. Their algorithms can serve as an important private pre-processing step that can reduce the sample complexity of further estimation tasks.  The authors consider two different settings. In the first “exact case”, the authors assume the data lies in a subspace s* of dimensionality k < d. For technical reasons, the authors also make the assumption that any strict subspace of s* contains at most l points. Their final sample complexity bounds scale with l instead of d (this is good when l is small, aka ``nice’’) and they present an (eps,delta)-DP algorithm that outputs s* with probability 1. Finally they give a matching lower bound for this setting proving their result is sharp.    The result in the exact setting can be quite restrictive if the dataset is slightly perturbed (e.g. from measurement noise). This motivates the second “approximate case” in which the authors assume the data is generated from some 0 mean d-dimensional Gaussian. They assume a multiplicative ``eigenvalue gap’’ between the top k and bottom d-k eigenvalues and present an algorithm that takes advantage of the celebrated sub-sample and aggregate algorithm and stability based histogram. Their algorithm outputs a projection matrix that (with high probability) is close in operator norm to the “true” projection matrix (corresponding to the subspace spanned by the top k eigenvectors of the covariance of the Gaussian). Here, $\Pi$ is precisely what we want to recover since for further data analysis tasks, one could project to the low dimensional subspace and work in this subspace for future tasks. Finally, the sample complexity of this second algorithm grows polynomially with k instead of d. ","This paper studies the problem of learning low-dimensional structure from the data subject to differential privacy. The paper considers the setting where the data lies in or very close to a linear subspace of dimension (k<<d). For the exact case where all the data lies in a linear subspace, the proposed algorithm DPESE, which applies the GAP-MAX, can recover the subspace exactly when there are enough samples. For the approximate case, they assume that the data comes from a Gaussian distribution where the covariance matrix has a certain eigenvalue gap. This paper proposes the DPASE algorithm that is based on the subsample-and-aggregate algorithm. In both cases, the sample complexity the dimension-independent. ","The paper looks at the problem of privately learning a subspace. They consider two different settings and give algorithms for each setting.  In the first setting, they assume that almost all the points lie in a fixed subspace of dimension $k$. They also make a non-degeneracy assumption which states that no subspace of dimension $k-1$ contains too many of the points. Under these assumptions, they give an algorithm which returns the subspace exactly and with probability 1. Their algorithm has the optimal sample complexity. However, it is not efficient except for constant $k$. One interesting aspect of the result is that it has no dependence on the ambient dimension.  In the second setting, they make a distributional assumption. They assume that the points come from a mean-zero Gaussian with some unknown covariance matrix. The covariance matrix is assumed to have a large gap between the $k$th largest and the $(k+1)$th largest eigenvalue. Under these assumptions, the authors given an algorithm which returns an approximation to the subspace. Here, approximation is measured as the operator norm in the difference between the respective projection matrices.","The paper studies the problem of learning a low-dimensional subspace from high-dimensional data in a differentially private manner.  The emphasis is on getting sample complexity that only depends on the dimension of the subspace but not the dimension of the original space. Such a guarantee is not achieved by previous techniques such as private PCA.  Formally, the paper studies two settings: the exact setting and the approximate setting.  In the exact setting, they assume that they are given $n$ points and almost all of them are contained in some $k$-dimensional subspace.  It is also necessary to make a non-degeneracy assumption that no $k-1$-dimensional subspace contains too many of the points.  In the approximate setting, they assume that all of the points are drawn from a Gaussian that is close to $k$-dimensional (i.e. its covariance matrix has $k$ large eigenvalues and $d-k$ much smaller eigenvalues).  In the first case, they obtain a sample complexity of $O(k + \log (1/\delta)/\epsilon)$  and in the second setting they obtain a sample complexity of  $O(k \log(1/\delta)/\epsilon)$ for achieving $(\epsilon, \delta)$ differential privacy.  Crucially, these are linear in $k$ and independent of $d$ and also their privacy guarantee is in the worst case, where any of the sample points may be altered arbitrarily.  It is also worth noting that for the approximate setting, they do not really use the Gaussian distribution assumption very strongly and only need that the sample covariances approximate the true covariances.",0.1572700296735905,0.19287833827893175,0.19287833827893175,0.36752136752136755,0.36752136752136755,0.3439153439153439,0.452991452991453,0.3439153439153439,0.2559055118110236,0.2275132275132275,0.16929133858267717,0.2559055118110236,0.23348017621145375,0.24714828897338406,0.21996615905245345,0.281045751633987,0.23180592991913748,0.2934537246049661
322,SP:44fa4a1523fa776b1266ffeb1a022992228966df,"The paper studies resource allocation among a group of agents, where   the agents can express diversity constraints on the allocations they   receive. Here, the diversity constraints require the allocation to   satisfy some proportionality constraints on the amount of each   resource received by the agent, or more generally requires the   allocation to lie in an arbitrary convex set.  In this setting, the   authors seek to identify allocation rules that satisfy two natural   requirements. First, it is desired that the allocation rule satisfy   non-negative externality, wherein the utility of any agent should   not decrease due to some other agents' expression of diversity   constraints. Second, to maintain the right incentives, it is desired   that the allocation rule satisfy monotonicity, wherein an agent   cannot increase her utility by expression a diversity   constraint. The authors study the class of welfarist allocation   rules, which maximize the sum of a concave, continuous function of   the agents' utilities. Among the rules in this class, the authors   show that the Nash welfare rule uniquely satisfies (approximate   versions of) both the requirements. The authors also show that the   guarantees of the Nash welfare rule are near optimal, and support   their theoretical results with numerical simulation.","The paper considers the problem of allocating unit divisible items under several welfare objectives (i.e., social welfare, gamma-fairness, nash welfare, and max-min fairness) subject to general diversity constraints (modeling as a polytope for each agent). The paper presents two notions of measures, non-negative externality (NNE) and monotonicity (MON), that quantify the impact of the constraints and define their approximated notions of q-NNE and p-MON, respectively.   In particular, they provide a lower bound of q for a class of continuous functions and upper bounds of q for specific welfare functions that depend on the \delta-scaled. For the MON condition, they provide lower and upper bounds for Nash welfare and \gamma-fairness objectives.   Finally, they conduct experiments on two real-world ad allocation datasets (i.e., UCI Adult and Yahoo A3) and using the budget-capped valuation function (along with other parameter settings) to demonstrate the q and p values of various social welfare functions.","The authors study the problem of allocation in the presence of a diversity constraint. Any participant can impose a diversity constraint - demanding proportional amounts of multiple items. An allocation can be computed using a social objective such as social welfare, Nash   welfare subject to this constraint. There are two things that can go ""wrong"": a participant other than the one imposing the constraint might obtain lower value and the participant imposing the constraint might obtain higher value. The authors ask the question to what extent does the choice of objective guarantee that things won't go ""too wrong"". That is, any participant not imposing constraint gets at least q fraction of its value and the participant imposing the constraint does not gain more than 1/p factor.   The authors show that for almost any objective that transforms individual participants values and sums them, it is not possible to obtain q = 1. In particular, for social welfare, \gamma-fair, max-min fair objectives it is not possible to get q > 0. In contrast, for Nash Social welfare, one can get q = 1/4. Finally, any rule satisfing pareto-optimality, Pigou-Dalton principle cannot get q > 1/2. The authors also extend these to settings where multiple participants impose constraints.   For the monotonicity - ensuring the constraining participant does not gain too much value - the authors show results for Nash-welfare and \gamma-fair welfare functions. For Nash welfare, 3/4 > p >= 1/2 and for \gamma-fairness, p >= (1-\gamma)^1/\gamma. They also provide an upper bound on p for  \gamma-fairness that is given implicitly by an equation. The \gamma-fairness result implies that for social-welfare p cannot be > 0.   The authors also provide an empirical evaluation of q and p values obtained on real datasets.   ","This paper proposes a framework for and and analysis of algorithms that allocate divisible resources to a set of agents when the agents can specify both utilities as well as constraints over the items as well. This is motivated by, e.g., computational advertising settings where we want to allocate users to advertisements so we have a reward (revenue of user) as well as constraints over the distribution (e.g., we do not want to show all job adds to men/women). The framework proposed captures popular allocation rules such as Nash and SW-Max, and the ability to specify constraints can be done through either proportionality or general constraints over the allocation set. The known mechanisms are analyzed in terms of their ability to achieve allocations that come within p,q fractions of allocations that do not impose negative externality and/or are not monotonic.",0.14285714285714285,0.1989795918367347,0.1377551020408163,0.275,0.125,0.08080808080808081,0.175,0.13131313131313133,0.18493150684931506,0.14814814814814814,0.136986301369863,0.1643835616438356,0.15730337078651685,0.15821501014198783,0.15789473684210525,0.1925601750547046,0.13071895424836602,0.10835214446952596
323,SP:44fab1c5969e82f85c30fcebafcc1c5bd41dc3d6,"A benchmark for continual reinforcement learning is provided, where the main task contains 20 subtasks. A certain number of choices are made such as the agent being aware of task change, not carrying forward the replay buffer, and learning policies with separate output heads per task. The evaluation was overall performance, forgetting, and forward transfer. Seven learning methods are compared extensively and results are provided. ","This paper proposes a new benchmark for continual reinforcement learning. It modifies the Meta-World robotics benchmark that’s designed for meta- and multi-task RL evaluation by learning the tasks in sequence rather than together. This work is very timely as there lacks a common benchmark for continual/lifelong RL.  The benchmark consists of 10 tasks from Meta-World. To evaluate catastrophic forgetting, these 10 tasks are repeated for a sequence of length 20. For quicker iteration, there are also different triplets of tasks where the middle task might inhibit transfer to the last task.","Summary. The paper proposes a robotic benchmark for continual reinforcement learning built on top of meta-world benchmark. It analyzes the difficulty in the problem of continual RL (such as catastrophic forgetting) and proposes metrics (average performance, forward transfer, forgetting) to measure performance of continual RL algorithms. Additionally, the paper evaluates existing continual RL algorithms on their benchmark and discusses the limitations of the proposed benchmark in detail as well.","The paper presents a suite of continuous control tasks for studying continual reinforcement learning. The paper includes a comprehensive study evaluating prior continual learning methods, both the ones designed for RL and for other tasks. It also highlights the challenges of applying existing RL methods to continual learning setting.",0.23076923076923078,0.23076923076923078,0.16923076923076924,0.20833333333333334,0.15625,0.2,0.15625,0.21428571428571427,0.22448979591836735,0.2857142857142857,0.30612244897959184,0.2857142857142857,0.1863354037267081,0.22222222222222224,0.1929824561403509,0.24096385542168672,0.20689655172413796,0.23529411764705882
324,SP:4546e61a7debb0db77672a0e760b04db658c1734,"The authors create a language game in which one neural agent describes an item to another.  They study what happens to the language that emerges if an explicit parameter promoting compositionality is altered, considering a range of different environmental factors. The results show an impact of this parameter on the speed at which communication converges and the degree of generalisation of the language","The paper is an incremental study that tries to go beyond some existing baseline in the generalization ability of language learned through message exchanges between two neural agents (sender and listener).  The proposed method is to add to the optimization objective of the agents the effect of compositional pressure using a metric called topological similarity, under a set-up of the game where the primary objective is reconstruction.  Senders are said to sample from some message space a certain representation about an object.  The listener tries to decode the message and reconstruct the object.  Using some synthetic experiments, it is shown that though in some cases the method has an advantage on generalization, the advantage is not consistent but depends substantially on the agent architecture, structure of input data, and parameters of the game environment.","This paper proposes to use a well-studied compositionality metric as an additional auxiliary loss function in a reconstruction task used to study language emergence. In certain cases, this loss function allows for improved generalization. The paper additionally finds that there is no simple correlation in increased weighting of this loss and increased generalization.","The paper introduces a novel loss term, which encourages high topological similarity, in the context of emergent communication. The paper hypothesizes that increasing the topological similarity of utterances from the two agents will increase the generalization ability of the agents to unseen inputs.  The paper presents evidence that mmoderate amounts of compositional pressure do improve the generalization ability, but large amounts of compositional pressure negatively affect the generalization ability.  The paper goes on to show evidence that the correlation between topological similarity of utterances and the generalization ability is complex and not always clear-cut.",0.2698412698412698,0.15873015873015872,0.20634920634920634,0.1037037037037037,0.18518518518518517,0.24074074074074073,0.1259259259259259,0.18518518518518517,0.1368421052631579,0.25925925925925924,0.2631578947368421,0.1368421052631579,0.17171717171717168,0.17094017094017092,0.16455696202531644,0.14814814814814814,0.21739130434782608,0.17449664429530204
325,SP:45755b67828623ce9729e134b6fab08ea0b9371d,"This paper presents a method to build mini-batches of training data using data segments (DS) and periodic sampling. The idea is to define DSs over the training set that correspond to a real split of the data. For example, in the case of mammogram images, the manufacturer of the imaging equipment could be used to split a dataset in 2 DS (all images obtained using equipment from manufacturer#1, same for manuf#2). The author's proposed method for sampling mini-batches using DSs is a form of periodic sampling, where both labels and DS are alternated to form a mini-batch. The paper then investigates the differences in performance when using 3 sampling methods for the binary classification of mammogram images: 1) BL (balanced labels) is a baseline that ignores DSs 2) BS (balanced segments) alternates labels and DS equally 3) PS (periodic sampling) alternate labels and DS based on the proportion of the least represented label in each DS. Results show that PS outperforms BL and BS for 2 experiments with different number of DSs.",This paper looks at the problem of training deep neural models with mammography data from various sources and manufacturers of acquisition device. It shows that naive approaches of training models by sampling from the various subsets perform poorly. The authors propose an algorithm for mini-batch sampling that overcomes this challenge and evaluate it on a large unspecified mammography dataset.,"The authors in this paper mitigate the issue of DNN bias and difficulty in training of heterogeneous datasets by using mini-batches for SGD algorithms. They focus on the use of periodic sampling, as compared to balanced sampling within each batch but independent of data segments (DS) and balanced sampling within a batch and each DS. DSs are homogenous subsets of a dataset that share some attributes. In this paper, the authors primarily aim at binary classification and efficiently training a Deep Neural Network (DNN) for breast cancer prediction from mammography images (Full-Field Digital Mammography (FFDM) dataset that has been created from multiple sources of data and manufacturers . They ensured the ratio of malignant to benign is consistent throughout the validation and training sets. Another contribution of this paper is the benefits of (periodic) sampling. The authors also train a DNN with a mini-batch SGD-like optimization algorithm. The authors use AUC and ROCs for metric evaluations. They have set 3 benchmarks to compare the performances - “Bal. labels” (balance labels only), “Bal. segments” (balance every DS equally) and “P. sampling” (periodic sampling). In experiment 1, they identified that DNNs trained with Bal. labels and Bal. segments performed globally better than the DNNs trained with P. sampling. However, in experiment 2,  DNNs trained with P. sampling showed better performance gain than the other two DNNs. Also, according to this paper, DNNs trained with P. sampling generalize better on new manufacturers. The main motivation behind using periodic sampling was achieving a better performance gain using DNNs. In order to identify exactly when data segmentation should be used, the authors came up with 3 ways - a) when generalization performance is poor, b) validation metrics to identify the goal, c) good knowledge of the learning set. The authors concluded this paper by discussing the performance of periodic sampling on the dataset (with the claim that it improves the performance of the DNN models), the ways to identify when segmentation should be done and the assumptions they made during this.","The authors present priodic sampling, that allows systematic sampling from multiple dataset to construct mini-batches for stochastic gradient descent algorithm, such that balances labels overall and within each batch. The periodic sampling is based upon construction of data segments, sharing similar latent properties. Periodic sampling comes to tackle skewed and heterogenous data that's typically shown in medical imaging applications and is specifically targeted in the paper in the form of breast cancer classification based on FFDM images. The authors compared their proposed sampling to 2 other sampling methods to demonstrate its benefit in improving performance. ",0.09550561797752809,0.24719101123595505,0.12359550561797752,0.4166666666666667,0.16666666666666666,0.08605341246290801,0.2833333333333333,0.13056379821958458,0.2268041237113402,0.07418397626112759,0.10309278350515463,0.29896907216494845,0.14285714285714285,0.17087378640776701,0.16,0.1259445843828715,0.12738853503184713,0.1336405529953917
326,SP:45bb223e2d1af5a4b3ecdbc48299930473d92efb,"This work proposes to use a transformer that attends over all nodes in the graph, given an intermediate representation obtained using a standard GNN module. Contributions 1) usage of a self-attention mechanism that takes into account all nodes in the graph, as opposed to previous methods discussed in the related work. 2) the removal of the positioning encoding to allow permutation invariance. 3) Quantitative results on several datasets, overcoming the baselines and supporting the method with empirical evidence.  ","This paper proposes to combine GNN with a permutation-invariant transformer called GraphTrans for the graph classification tasks. The GNN module is used to learn representations from graph structure,  while the transformer module without a positional encoding is to learn long-range pairwise relationships, with ""CLS""-like readout to obtain a global graph embedding. Experimental results show that the proposed simple architecture leads to state-of-the-art results on several graph classification tasks, better than other GNN models. These results also suggest that modeling all pairwise node-node interactions in the graph is particularly important for large graph classification tasks.","Current GNN methods fail at utilizing long-range dependencies in graphs due to over smoothing. However, in some cases, long-range information might be useful. Inspired by the success of transformers in computer vision tasks, this paper tackles the challenge of learning long-range dependencies in GNNs by simply augmenting a common GNN architecture with a transformer sub-network. They also borrow a “readout” token from NLP which improves the final learned representation of the graph.","The paper proposes that adding transformers on top of Graph Neural Networks layers aids performance, and furthermore that adding a readout position is a better way to get a final output vs. simple aggregation strategies.  Other papers have in various forms attempted to add transformers to neural networks, but this paper uses a position-encoding free, approach that performs better while being simpler.  This also demonstrates the importance of long range information propagation in solving GNN problems.  They demonstrate the superior performance of their method empirically in a variety of interesting benchmarks. ",0.24050632911392406,0.13924050632911392,0.13924050632911392,0.1188118811881188,0.13861386138613863,0.18421052631578946,0.18811881188118812,0.14473684210526316,0.11956521739130435,0.15789473684210525,0.15217391304347827,0.15217391304347827,0.2111111111111111,0.14193548387096774,0.1286549707602339,0.13559322033898305,0.1450777202072539,0.16666666666666666
327,SP:45fa22c38afce620ee7b77f84573aec3366c2d28,"This paper proposes a unified visual tracking method using a single and task-agnostic appearance model, in which this model can be learned via n a supervised or self-supervised strategy. For this purpose, they examine appearance model performance on five tracking tasks and describe it using radar charts in fig1. Experimental results show that  most tracking tasks can be solved within the proposed framework. ","The paper investigates the possibility of using a single appearance model for all tracking-related tasks/subtasks (the paper proposes a precise classification of these tasks). The authors propose an elegant unified framework with 3 levels: basic appearance model (level 1), propagation and association blocks (level 2), and task-specific heads (level 3). The only level that needs a training phase is level 1, while the other levels are training-free. Another contribution is the introduction of the RSM metric, which is used to measure the similarity between features within the proposed association algorithm. Following a rich experimental section, the authors conclude by giving a negative answer to the question contained in the title of the paper itself.","This paper proposes using a unified self-supervised framework for various video-related tracking tasks including single-object bounding box (SOT) or mask propagation (VOS) or multiple object bounding box (MOT), mask (MOTS) or keypoints (PoseTrack) association of detections across video frames. The idea proposed in this paper is inspired by the finding that mid-level representations of networks trained either with full-supervision (e.g., for object classification) or via self-supervised learning for a pre-text task are believed to contain information for establishing dense correspondence between objects and instances. The authors propose to use a single backbone pre-trained network to extract fc3/fc4 features for these various tasks and further employ methods based on dense affinity/ discriminative correlation filters/Hungarian matrix, etc to perform the various propagation or association tasks. The main finding that the authors share is that a common task-agnostic mid-level representation learned using ImageNet is competitive with many supervised and unsupervised task-specific algorithms designed for individual tasks.","This paper categorizes tracking object tasks into five lines and divides tracking algorithms into two classes: propagation and association. It indicates no necessity to fragment the problem of tracking in so many different specifications and separate trained models for each one. A unified tracking solution is proposed to address five different tasks using the same appearance model, which is task-agnostic and can be learned in a supervised or self-supervised way. The proposed model contains multiple heads to address individual tasks without training, and a reconstruction-based similarity metric is designed for association that preserves fine-grained visual features and supports multiple observation formats (box, mask and pose). I feel this work is interesting and has strong contributions to the tracking community, but experimental results are somewhat weak.",0.2153846153846154,0.26153846153846155,0.3384615384615385,0.17796610169491525,0.15254237288135594,0.1317365269461078,0.11864406779661017,0.10179640718562874,0.17054263565891473,0.12574850299401197,0.13953488372093023,0.17054263565891473,0.15300546448087432,0.14655172413793105,0.22680412371134023,0.1473684210526316,0.145748987854251,0.14864864864864866
328,SP:460138066b3d54805de1575bbea9cd8f29719568,"This paper proposes a module called Recurrent Layer Aggregation (RLA) to improve the performance of general CNN backbones. This module is motivated by the analysis of both layer aggregation and the time series method. Experiments on tasks like classification, detection, and segmentation validate the effectiveness of this RLA module.","A CNN architecture with two network paths (one is non-recurrent and another is recurrent) is introduced in this paper. The features from the two paths are aggregated through the RLA module (with residual connection) and redistributed to the individual paths. This brings performance improvements on image classification, object detection, instance segmentation at very small increases of parameter and computation costs.","The paper proposed a recurrent layer aggregation (RLA) structure. The RLA structure proposes a recurrent module to aggregating information from the previous layers. The proposal is inspired by an additive form of describing the information aggregations in deep neural network and its similarity to autoregressive models. The structure is compatible with many popular CNN models including the ResNets, Xception and MobileNet. Extensive sets of experiments have been conducted to demonstrate the effectiveness of the proposed method on different vision tasks.","This paper is inspired by the layer aggregation concept which is also used in DenseNet to better reuse the information from previous layers and extract features at the current layer. Then the authors propose a light-weighted module, called recurrent layer aggregation (RLA), and prove it is compatible with many common structures in ResNets, Xception and MobileNetV2. Finally, they use the controlled experiments to show the effectiveness of the RLA modules on classification and detection tasks. ",0.22448979591836735,0.3469387755102041,0.3469387755102041,0.18032786885245902,0.2459016393442623,0.325,0.18032786885245902,0.2125,0.2236842105263158,0.1375,0.19736842105263158,0.34210526315789475,0.2,0.26356589147286824,0.272,0.15602836879432624,0.21897810218978103,0.33333333333333337
329,SP:46a34ccabd20599b18060d3db4abbb96bb61c5b6,"This paper tackles the few-shot learning problem, which is a hot topic in the representation learning field. Specifically, the authors aim to train accurate classifiers using a small number of samples – Maximum Likelihood Estimators are biased for them. As a result, the authors propose to utilize Firth’s Penalized Maximum Likelihood Estimator to modify the ordinary MLEs, and prevent the bias when training classifiers for few-shot instances. The authors have validated the effectiveness of Firth bias reduction from multiple aspects, and the experimental results show the performance improvement is consistent across various circumstances, e.g., with different backbones, data distributions, and classifiers.",The paper proposes to reduce the bias inherent to the estimation of the classifier used in Few Shot Classification settings. To this purpose Firth bias reduction is proposed to be used. Theoretical computation of this bias reduction is carried out within the context of Few Shot Classification resulting to a practical implementation. It turns out that it could be associated to a regularization term encouraging uniform class assignment probabilities. This results is also generalized to the context of using a cosine classifier on the features.  Extensive experiments are carried out to show the benefits of this bias reduction on SOTA approaches and with various backbone architectures.,This work introduces a method to reduce the bias of MLE for FSL problems. Authors developed the bias reduction formulation based on the Firth bias reduction method and extended it for cosine classifier as well. A through set of experiments demonstrate that the proposed method provides statistically significant improvement over the baselines. ,"The submission examines the impact of Maximum-Likelihood Estimation (MLE)'s small-sample bias in the few-shot classification setting. The authors point out that while asymptotically consistent, MLE is biased in the finite-data regime, and propose to apply Firth bias reduction to few-shot classifiers to counter this bias. In this setting, the technique simplifies to a KL divergence regularizer between the uniform distribution over class labels and the learner's predictions.  Experiments on mini-ImageNet, CIFAR-FS, and tiered-ImageNet using ResNet and WideResNet network architectures show modest but consistent improvements across all settings. Firth bias reduction is shown to outperform other regularization approaches such as L2 regularization and label smoothing alternatives (confidence penalty and unigram label smoothing). ",0.17307692307692307,0.14423076923076922,0.20192307692307693,0.14150943396226415,0.24528301886792453,0.23076923076923078,0.16981132075471697,0.28846153846153844,0.17355371900826447,0.28846153846153844,0.21487603305785125,0.09917355371900827,0.17142857142857143,0.1923076923076923,0.18666666666666665,0.189873417721519,0.2290748898678414,0.13872832369942198
330,SP:46afdd0cc35a5e3c6cac14c7fcffd21d61c1499a,"The authors present a novel pre-training method (Motif-based Graph Self-Supervised Learning, or MGSSL) for graph neural networks specific to molecular property prediction. The pre-training involves constructing synthetic tasks to predict atom information, generation of the molecule, and motif identity. They demonstrate this pre-training improves GNN model performance on several MoleculeNet tasks across 5 different GNN models."," This paper proposes Motif-based Graph Self-supervised Learning, which pre-trains GNNs with a novel motif generation task. The authors first use a retrosynthesis-based algorithm with two additional rules to fragment molecule graphs and get meaningful motifs. Next they propose a motif generative pre-training framework, where in each step the pretrained GNN is required to make topology and motif label predictions.","*Summary  This paper presents a novel motif-level GNN pretraining called ""Motif-based Graph Self-Supervised Learning (MGSSL)"" and multi-level self-supervised pre-training. Most existing self-supervised pre-training frameworks for GNNs are only defined as node-level or graph-level tasks. But molecular graphs would follow a compositionality principle, and the complex structures are defined by their parts (motifs/fragments/scaffolds) and how to combine them.  It leverages the BRICS fragmentation algorithm [4] and further refinements to decompose a molecule into a ""motif tree"" representation that describes parts-level compositions of the given molecule in a tree form. Along with atom-level self-supervised tasks by atom-label masking, topology and label prediction for this 'motif tree' defines a nice self-supervised task at a motif level.   Extensive experiments on multiple downstream tasks demonstrated that the proposed pretraining framework outperforms all other state-of-the-art baselines. ","This paper presents a new self-supervised pre-training framework for Graph Neural Networks (GNNs) called Motif-based Graph Self-supervised Learning (MGSSL) to predict molecular properties. First, the method uses BRICs, a molecule fragmentation method, to create a Graph Motif vocabulary. Next, it performs pre-training using a model with three components -  1) a motif generation module that uses GRUs to recursively generate motifs given the previously generated motifs. 2)  a motif labeling module that uses dense layers followed by a softmax to predict the label of the motif from the vocabulary created in the previous step. 3) a GNN module that generates node and edge embeddings which are finally used to predict node and edge attributes of a molecule graph. MGSSL is trained as a multi-level model (with atom and motif layers) and it minimizes a hybrid loss consisting of Motif, Atomic, and Bond prediction losses, with weights adjusted by the Frank-Wolfe Algorithm. The results report improved performance for molecular property predictions over existing pre-training frameworks 7/8 datasets.  ",0.2459016393442623,0.3442622950819672,0.39344262295081966,0.328125,0.375,0.22,0.234375,0.14,0.13793103448275862,0.14,0.13793103448275862,0.1896551724137931,0.24,0.1990521327014218,0.20425531914893616,0.19626168224299068,0.20168067226890754,0.2037037037037037
331,SP:46d2b3937a37985bcd264b55747f63ce858b72d2,"This paper tries to demystify the power of embedding and embedding space, trying to connect the structure of learned embeddings and knowledge generation process. The authors tested Gaussian model, Preferential Placement (PP) model, and Directional Preferential Placement (DPP) model for node generations. The authors also tried to learn embeddings by graph representation learning algorithms after running BA models with several variances with gravitational movement and centroid node/edge weighting.   (Main Contributions) 1) Propose reasonable processes that can explain the evolution of knowledge embeddings in terms of preferential attachment and attractive-repulsive force.  2) Compare observed and generated embedding spaces via non-parametric statistics, identifying frequency concentration and clustering velocity properties.  3) Evaluate and find the best generative process: incremental insertion process with spatial context-dependent preferential attachment. ","This paper proposes two measures of word/graph embedding to character the evolution of word/graph, i.e., frequency concentration and clustering velocity. Many existing graph generation models are surveyed. The proposed measures are calculated on many real-world datasets."," In this paper,  the authors try to study if we can learn about the human's process of generating new ideas or concepts from embeddings. The authors first define a set of models for generating embeddings. Then the authors observe two properties: (1) frequency concentration, and (2) cluster velocity. The authors finally compare the embeddings from generative models with the embeddings from real-world data driven methods.","This paper tries to explore the natural processes that generate new knowledge or concepts. Specifically, the authors propose two metrics to characterize embeddings trained on different datasets. Then, they compare some synthetic data generated by models and real-world data according to the aforementioned metrics. Finally, they conclude that the real-world data can be well simulated by a certain generative model.",0.11023622047244094,0.15748031496062992,0.12598425196850394,0.325,0.2,0.26865671641791045,0.35,0.29850746268656714,0.25806451612903225,0.19402985074626866,0.12903225806451613,0.2903225806451613,0.16766467065868262,0.20618556701030927,0.1693121693121693,0.24299065420560745,0.1568627450980392,0.27906976744186046
332,SP:472021738996808cb6764d2ba4c1c3636eb717e5,"This paper proposes a new structured bandit model, called congested bandits, where an arm’s reward is a decaying function of how many times it has been played during a recent time window. The proposed bandit model aims to address recommendation problems such as route recommendation, in which recommended routes tend to get congested (hence yield lower rewards). Different from prior work on bandits with changing arm reward distributions, the current paper proposes a model in which the effect of congestion resets over Delta time steps. By viewing the problem as structured MDP, the paper develops a variant of UCRL2 that learns to recommend the optimal arm for each congestion state. It is shown that the proposed algorithm achieves a policy regret of tilde{O}(sqrt{KDeltaT}), which significantly improves upon the exponential dependence on the state space of UCRL2. The authors also propose a variant of their algorithm tailored for the linear stochastic contextual bandit setup and analyze the policy regret for this case as well. ","This paper raises and studies a new problem named congested bandits, an extension of the standard multi-armed bandits (MAB) problems. In this setting, the reward of an arm depends on the number of times this arm has been pulled during the past $\Delta$ time. The authors also extended the congested bandits to graph-based congested bandits and congested linear contextual bandits.   The authors proposed new algorithms for the above settings and derived regret upper bounds similar to those of MAB or linear contextual bandits.   To be specific, for congested bandits, the regret upper bound is $\tilde{O}(\sqrt{K\Delta T})$, where $K$ is the number of arms and $T$ is the time horizon. For graph-based congested bandits, the regret upper bound is $\tilde{O}(\sqrt{VE\Delta T})$, where $V$ is the number of nodes and $E$ is the number of edges. For congested linear contextual bandits, the regret upper bound is $\tilde{O}(\sqrt{dT})$.  The authors also conducted numerical simulations to confirm the above theoretical results. ","This paper studied congested bandits, a special class of multi-armed bandit problems where each arm's reward depends on the number of times it was played in the past few steps. The authors showed the reduction of the congested bandit problem to an MDP problem. A UCRL-type algorithm was then proposed and the regret bound of the algorithm was analyzed. ","This paper proposes a novel multi-armed bandit framework to formulate the scenario of route recommendation based on dynamic traffic situations. The formulated model fits in both stochastic bandits and linear contextual bandits.  The paper first introduces the stochastic bandit setting with time-varying expected reward for each arm, which depends on the pulling history within a fixed-sized sliding window. The authors propose an algorithm CARMAB, with regret analysis indicating that the regret upper bound scales as $ O(\sqrt{K\Delta T\log(\Delta KT)})$ with high probability.  Then the paper discusses the contextual bandit setting with known contexts. The authors updated the model and propose an algorithm CARCB with regret analysis indicating the regret upper bound scales as $\tilde O(\sqrt{dT+\Delta})$ with high probability. Then the authors further relax the assumption on contexts by allowing unknown stochastic contexts samples from a known distribution. The regret analysis indicates an additional term with respect to  $T$ comparing to the known contexts setting.",0.23952095808383234,0.1497005988023952,0.19161676646706588,0.1695906432748538,0.2573099415204678,0.3548387096774194,0.23391812865497075,0.4032258064516129,0.1951219512195122,0.46774193548387094,0.2682926829268293,0.13414634146341464,0.23668639053254437,0.21834061135371177,0.1933534743202417,0.24892703862660942,0.2626865671641791,0.1946902654867257
333,SP:4747f91fbd063230aad1feb072eb27541ed4c924,"This paper poses a new method for projecting vectors onto the k-capped simplex. This method reformulates the quadratic program into a form that can harness Newton method, a second-order iterative optimization algorithm, to solve the projection goal. The key novelty is using a second-order method to iteratively solve for a minimum, as opposed to sorting-based methods with a cubic complexity. Furthermore, first-order methods are shown to be too slow based on a proof that the Lipschitz constant scales with dimensionality. Compared to existing methods for k-capped simplex projection, the proposed method is several times faster, and in synthetic experiments achieves state-of-the-art accuracy. On experiments in high-dimensional bioinformatics, the proposed projection converges faster and equally well as other state-of-the-art methods.","In this paper the authors propose a method for projecting onto the capped simplex. The algorithm lies on a well-known framework, which transforms the optimization problem via Langrange and duality theory, in other equivalent problem. The contribution is on a different approach, using the Newton method, to solve one of the sub-problems. The algorithm is tested on a problem with genomic data. ","This paper addresses the problem of accelerating the projection of a vector onto the k-capped simplex. From the theoretical side, the authors showed that the given problem can be casted as a scalar minimization problem, which is tackled by a Newton’s method, unlike existing methods based on sorting algorithm. In computational experiments, the authors evaluated the proposed method in a sparse regression problem. The baselines are Matlab’s quadprog, reference[15], and Gurobi, a commercial interior point solver. The proposed method worked faster especially when the response vector y is high-dimensional. It is also shown to have a higher support recovery nature than that of Elastic net. In experiments using real dataset consisting up to 1.5 million SNPs, the proposed method worked much faster than Gurobi.","This paper proposes an algorithm based on Newton’s method for projecting a vector with bounded elements onto a hyper-cube cut by a hyperplane (also known as the k-capped simplex). The time complexity of the algorithm is roughly linear (in terms of the vector dimension) and theoretical insights for partial justification of the method are provided. Empirical evaluation demonstrates that the algorithm (1) finds a solution with high precision on large-scale datasets and (2) considerably outperforms state-of-the-art sorting-based methods in terms of run time. Finally, the proposed algorithm is also applied to solving a sparse regression problem from the bioinformatics domain on which it is able to accelerate the Projected Quasi-Newton (PQN) method from 3 to 6 times compared to its alternatives.",0.17424242424242425,0.22727272727272727,0.21212121212121213,0.328125,0.34375,0.23076923076923078,0.359375,0.23076923076923078,0.2153846153846154,0.16153846153846155,0.16923076923076924,0.23076923076923078,0.2346938775510204,0.22900763358778628,0.21374045801526717,0.21649484536082478,0.22680412371134018,0.23076923076923078
334,SP:47708dd75ce46e8aa3b44e524abdfdcaf87a6d26,"This paper studies the multi-label classification problem in a weakly supervised setting, where each instance is assigned with the relative order of label pairs. Authors formalize this task as a new framework called pairwise relevance ordering. An empirical estimator of the classification risk based on a cost-sensitive loss is proposed. Then authors theoretically prove that the proposed estimator is unbiased with the symmetric condition of the loss function. The error bound and consistency of the estimator is also proved. In addition to the theoretical results, the paper also validate the effectiveness of the proposed method empirically on multiple datasets.","This paper presents a new multi-label learning framework called Pairwise Relevance Ordering (PRO). In this setting, the supervised information is given in the form of relevance orders between labels pairs.   To solve the PRO problems, an unbiased estimator of the classification risk is proposed, whose properties of unbiased, consistency and error bound are theoretically proven.  Experiments are performed on benchmark datasets to show the superiority of the proposed method. ",This paper attempts to solve the multi-label learning problem with only pairwise relevance ordering. Authors argue that deciding the relative order of label pairs is obviously less laborious than collecting exact labels. A method is also presented to learn multi-label model with only pairwise relevance ordering and experiments shows its superiority.,"In multi-label learning, the supervised information of pairwise relevance ordering is less informative than exact labels. Hence, it is an important challenge to effectively learn with such weak supervision. In this paper, a multi-label learning with pairwise relevance ordering approach is proposed. Meanwhile, it demonstrates that the unbiased estimator of classification risk can be derived with a cost-sensitive loss only from PRO examples. The theoretical analysis also has been provided. ",0.26732673267326734,0.1782178217821782,0.2079207920792079,0.21428571428571427,0.24285714285714285,0.37735849056603776,0.38571428571428573,0.33962264150943394,0.2876712328767123,0.2830188679245283,0.2328767123287671,0.273972602739726,0.31578947368421056,0.23376623376623376,0.24137931034482757,0.24390243902439024,0.2377622377622378,0.31746031746031744
335,SP:479936413c03696ed474afb64600371cc77571f0,"The paper proposed an iterative finetuning method to adapt pretrained LMs to translation tasks. The framework refines the LM into translation models, which accepts both languages with a predefined format, and translate them into other language bidirectionally. The model is finetuned by backtranslation.  To generate translation counterparts of the given monolingual corpora during finetuning, the paper utilized prompting on LMs: gives several translations X1 Y1 X2 Y2 ... as contexts, then predict the successor Yn given the real input Xn, where Xk and Yk are souce/target sentences.  The context of the prompt X1 Y1 X2 Y2 is also generated by LM using another template, but this part can be replaced as a real examples.  Experiments show that the trained system achieves the better performance than existing similar methods, showing that the effectiveness of the proposed method. The paper provided some additional studies to support the main results.","This paper investigates the way of constructing the UNMT model with a generative pre-trained language model, such as the GPT-X model. This method consists of few-shot amplification, distillation and back-translation, in which the first two steps are leveraged to warm up a UNMT model instead of bilingual dictionaries inferred in an unsupervised way. When using GPT-3 to obtain synthetic parallel data for model training through the zero-shot prompting approach, the proposed method achieves the state-of-the-art unsupervised translation performance on the WMT14 English-French benchmark.  ","This work studies the use of a large English-centric language model (GPT-3) for the purposes of building an unsupervised machine translation system for the language pairs English-French and French-English. Given some monolingual data, the authors use GPT-3 to generate zero-shot translations, which then get used as few-shot examples for a smaller generative model to generate a synthetic translation dataset. This synthetic dataset is used to fine-tune the language model for translation through a back-translation scheme. With this approach, the authors are able to attain a new state-of-the-art result for unsupervised translation on the language pair English-French. ","This paper proposes to solve the machine translation task without any bi-text data and using only a pre-trained generative language model to bootstrap the process. The entire method relies on using powerful pre-trained language models that have been trained on a large amount of data to extract initial boot-strap examples, and then fine-tune the model to solve the machine translation task using specially formatted prompts. The same language model is used to translate in both directions, and back-translation is used to improve results.",0.1292517006802721,0.12244897959183673,0.1292517006802721,0.3118279569892473,0.1935483870967742,0.1834862385321101,0.20430107526881722,0.1651376146788991,0.21348314606741572,0.26605504587155965,0.20224719101123595,0.2247191011235955,0.15833333333333333,0.14062499999999997,0.16101694915254236,0.2871287128712871,0.1978021978021978,0.20202020202020204
336,SP:47afca259f16cb89d9ca0dd84bb992c55f165887,"This paper proposes to measure the quality of a multi-view generative model by the amount of total correlation among observations that is explained by the latent complete representation. While directly maximizing the reduction in total correlation is intractable, the authors derived a variational lower bound as a surrogate objective which encourages the model to learn a minimal sufficient representation and also regularizes the latent representation to be inferable from any view. The proposed formulation works well when some views are missing during training or testing. Hence it's suitable for partial multi-view representation learning. It also naturally calibrates the view-specific encoders to prevent the model heavily relying on only the most informative views. Empirical evaluation shows that the proposed method performs noticeably better than its baselines on most multi-view classification problems and multi-view translation tasks while achieves competitive results on a few others.",This paper proposes a novel Multi-View Representation Learning method to learn a shared representation of multiple observations. The objective is to learn a complete representation through maximizing the reduction of total correlation. The method is based on the VAE framework and is capable of dealing with missing views. ,"The paper proposes the variational-based generative approach to learn the multi-view (multi-modal) representation. The whole framework is motivated and derived from maximizing the total correlation  reduced by the representation, it focuses on learning a shared latent representation that is informative as well as succinct to capture the correlations among the given views (domains). Specifically, the framework is based on VAE framework with product-of-expert posterior encoder, and is trained via extended ELBO with additional regularization KL between joint encoder and view-specific encoder. The authors verify their proposed model on multi-view classification and translation. They also conduct experiments on partial multi-view representation learning where several views are missing in the training, and need to be inferred during testing.  ","The paper motivates a variational objective (eq. (7)) for multi-view representation learning. The proposed objective is an alternative to the standard multi-view VAE objective, and should be better suited for extracting the correct representation when views are missing at random and when a subset of the views is overwhelmingly informative. They then propose optimising a convex combination of their proposed objective and the standard multi-view VAE objective.",0.14189189189189189,0.20945945945945946,0.14864864864864866,0.4489795918367347,0.32653061224489793,0.1693548387096774,0.42857142857142855,0.25,0.3142857142857143,0.1774193548387097,0.22857142857142856,0.3,0.21319796954314718,0.22794117647058826,0.20183486238532108,0.2543352601156069,0.26890756302521,0.21649484536082472
337,SP:47b1263feebdc2d562c517e9fc43876a7da791bc,"This paper proposes a new environment for multi-agent reinforcement learning that addresses the problem of active voltage control on electric power distribution networks. The authors formulate the voltage control problem as a Dec-POMDP, and implement a number of different MARL baselines to assess their performance on the environment. They also assess the effect of different voltage loss functions on this environment.","The main contribution of the article is mention in the abstract ""... establishes an open-source environment. It aims to bridge the gap between the power community and the MARL community"". and In section 1: ""There is no commonly accepted benchmark to provide the basis for fair comparison of different solutions. To facilitate further research on this topic, and to bridge the gap between the power community and MARL community"", The authors seem to forget about L2RPN.",This paper introduces a new open-source environment for training RL agents on controlling the voltage of a power distribution network. The problem is formulated as a multi-agent Dec-POMDP. Agents control PV inverters using only local measurements coming from sensors situated in the region they operate. Their task is to maintain the voltage within safety range. Several MARL algorithms are evaluated in three different power networks. The paper also investigates different reward functions.,"In this work, the authors studied applying multi-agent reinforcement learning (MARL) to active voltage control in the power distribution networks, and targeting for bridging the gap between the power community and MARL community. Main contributions include:  1) develop an open-source environment of active voltage control for MARL and formally define this problem as a 65 Dec-POMDP; 2) conduct large scale experimentation with 8 state-of-the-art MARL algorithms on different scenarios of voltage control problem; 3) found valuable observations, e.g.,  notice the importance of converting voltage constraints to auxiliary reward functions and verify the sensitivity of MARL performances to the auxiliary reward functions, with a new bowl-shape auxiliary reward proposed which stands out in the test results; 4) by analyzing the experimental results, the authors imply the possible difficulties of applying MARL and the further research directions. The authors also conducted the comparison with traditional control methods, and empirically show the rationality and hope to use MARL as a solution for active voltage control.",0.19047619047619047,0.3333333333333333,0.4444444444444444,0.17105263157894737,0.32894736842105265,0.25333333333333335,0.15789473684210525,0.28,0.16470588235294117,0.17333333333333334,0.14705882352941177,0.11176470588235295,0.17266187050359713,0.30434782608695654,0.24034334763948498,0.17218543046357618,0.2032520325203252,0.15510204081632656
338,SP:47ca4fd5f4f946e31c0502151067e2d438499906,"This paper studies the equivalence query learning model where the teacher presents counterexamples if the learner proposes an incorrect hypothesis.  Three types of teachers are considered: (1) the ""worst-case teacher"" that presents adversarial counterexamples, (2) the ""random teacher"" that presents random counterexamples and (3) the ""best-case teacher"" that presents counterexamples designed to guide the learner towards the correct hypothesis in as few learning steps as possible.  Further, the query learner is equipped with a preference function on which the choice of its query depends (if its current hypothesis is incorrect).  ","The paper characterises the sample complexity of teaching in the learning with equivalence queries(LwEQ) model. The teachers can be  1) worst, 2) average and 3) optimal. The number of samples required for the learner to reach the target hypothesis is intuitively the least in the best(3) case. This work draws a connection to the learning from samples model, specifically to the notion of teaching dimension. ","This work studies the sample complexity of machine teaching finite hypothesis classes with equivalence queries (EQ), a model in which the learner queries hypotheses in the concept class directly, and the “teacher” responds “yes” or “no” along with a counter-example with the goal of eventually teaching the learner some ground truth concept.   There are many formalizations of this model in the literature. This work mainly focuses on a natural regime of the problem in which the teacher is “helpful,” always choosing the best possible counter-example to help the learner, and the learner is adversarial, though generally restricted under some fixed “learning rule,” (e.g. they must always pick a hypothesis consistent with previous counter-examples).  The authors study a notion of query complexity they call “Learning with Equivalence Queries Teaching Dimension” (LwEQ-TD) that is closely related to other forms of TD used in machine teaching. They show that for a variety of fundamental concept classes (axis-aligned halfspaces, axis-aligned rectangles, and monotone monomials), a helpful teacher can always teach even the most adversarial learner in O(1) examples. On the other hand, they give lower bounds on the sample complexity when the teacher gives random or adversarial samples that scales with the size of the class.  Finally, the authors give an in depth comparison between LwEQ-TD and previous notions of machine teaching in the membership query model. Indeed, for a very natural class of adversarial learner, they prove that LwEQ-TD is in fact exactly the standard teaching dimension (along with a host of other connections). ",This work studied the problem of learning from equivalence queries (EQ) alone where counterexamples are selected by a helpful teacher. The paper characterizes the teaching complexity for different classes and relates the new notion (LwEQ-TD) to different notions in the literature. LwEQ-TD showed significant savings in the complexity of teaching when compared to other teaching notions in some hypothesis classes. ,0.22826086956521738,0.31521739130434784,0.17391304347826086,0.417910447761194,0.22388059701492538,0.10727969348659004,0.31343283582089554,0.1111111111111111,0.25806451612903225,0.10727969348659004,0.24193548387096775,0.45161290322580644,0.2641509433962264,0.16430594900849857,0.2077922077922078,0.17073170731707316,0.23255813953488375,0.17337461300309598
339,SP:480df63d36588ac70d56703dfdae67bbfa00e6c9,"This paper proposes to use vision Transformer in a coarse-to-fine manner as a new learned multi-view feature fusion scheme in monocular reconstruction task. Additional blocks such as spatial refinement and C2F filtering help the method generate coherent geometry and improve computational efficiency.  Compared to existing approaches, the full TransformerFusion system shows competitive results on 3D reconstruction quality using ScanNet benchmark with detailed ablation study.","This paper presents an RGB 3D reconstruction algorithm with monocular RGB input, using transformer network. The transformer architecture is utilized for temporal feature fusion from multi-view images. Coarse-to-fine hierarchical scheme was designed for efficient surface detection and real-time reconstruction. Also, attention weights from the transformer is used in order to select the most dominant (feature-abundant) frames so that more interactive scene reconstruction can be available. By doing so, the proposed algorithm shows comparable performance on the benchmark experiment, compared to CNN-based previous methods.","The paper presents a fully end-to-end trainable deep 3D reconstruction pipeline taking monocular RGB video sequences as input. The proposed pipeline works in an online fashion and at interactive frame rates. In the first stage, the method extracts 2D features from input RGB images. In a next stage, 3D features are generated by projecting a 3D location into the images, pooling the corresponding 2D features and fusing them through a transformer network. After refining the 3D features with 3D convolutions, the geometry is predicted through a neural implicit decoder and finally extracted through marching cubes. Notably, the method uses a two-level hierarchy of features for improved quality and runtime performance. Furthermore, the self-attention weights are reused to only keep the most important observations for each 3D point at any given time. Finally, the authors compare their pipeline to several state-of-the-art methods and perform an ablation study to validate the individual components. ","This paper proposes a method to reconstruct dense surface geometry of a static scene from a monocular RGB video.   The method combines implicit functions and transformers for achieving the final reconstruction. Specifically, images in the video are encoded by a 2D image encoder. After that, for each voxel in a 3D feature volume, the feature was calculated by: first bilinearly sampling per-view features by projecting the voxel on the 2D feature maps in different views, and then, merge the per-view features from different views together using a multi-head transformer. After calculating the 3D feature volume, 3D convolutions are used to directly refine the features in it. Finally, for each query point in 3D space, an MLP-based occupancy decoder was introduced to decode the occupancy value given the corresponding feature of the query point (which is sampled in the refined 3D feature volume).  To further improve the reconstruction efficiency, the whole reconstruction pipeline was conducted in a coarse-to-fine manner.   The main contribution of the paper lies in the transformer-based feature merging stage. By utilizing the strong self-attention performance in the transformers for aggregating features from different views, the proposed method achieves plausible and more complete surface reconstruction results than previous methods. ",0.208955223880597,0.22388059701492538,0.2537313432835821,0.23595505617977527,0.25842696629213485,0.25949367088607594,0.15730337078651685,0.0949367088607595,0.08173076923076923,0.13291139240506328,0.11057692307692307,0.1971153846153846,0.1794871794871795,0.13333333333333333,0.12363636363636363,0.1700404858299595,0.15488215488215487,0.22404371584699453
340,SP:482ce83628feb9ce7207379d2070c5a03ade2cc1,"This paper proposed a method for UDA object detection. The core idea is to increase the discriminative ability of learned features in the target domain while still globally aligning the feature distributions of the source and target domains. Under this motivation, the paper proposed modules of domain adapative instance normalization, global style alignment and local content alignment. Experiments are conducted on several benchmark adaptation scenarios, showing improvments on two existing methods.","The paper introduces a method to tackle the unsupervised domain adaptation problem, focusing on the object detection problem. Towards the final objective, an architecture building on the style transfer concepts is proposed. The main idea is to build upon the AdaIN ideas to encode the domain-specific and domain-invariant features in the source and target domain. Two additional modules are exploited to regularize the consistency of style and content between multiple domains. Experimental results on 3 benchmark UDA setups are conducted. Similar/better results than existing approaches are obtained.","This paper proposes a novel adaptation framework to balance transferability and discriminability for cross-domain object detection. Concretely, they focus on domain-specific features thus manage to reconstruct the style and content of features by AdaIN and some alignment techniques. Extensive experiments show the performance improvement by proposed modules.   ","This paper presents a new method for cross-domain object detection, which employs the style-aware feature fusion method and two novel modules to build the model. The key idea of this paper is to assign the underlying details to the model by using techniques from style transfer to overcome the domain gap. The proposed method is evaluated on multiple datasets and achieves promising results.",0.3380281690140845,0.19718309859154928,0.23943661971830985,0.16666666666666666,0.2222222222222222,0.32653061224489793,0.26666666666666666,0.2857142857142857,0.26153846153846155,0.30612244897959184,0.3076923076923077,0.24615384615384617,0.2981366459627329,0.23333333333333334,0.25,0.21582733812949642,0.2580645161290323,0.28070175438596495
341,SP:4856fae966245f3bfc92c34a8edf92388d5585b3,"The authors consider the problem of stochastic and adversarial episodic MDPs with unknown transition, is an extension of the work of Jin and Luo (2020), with unknown transitions rather than known ones.   They propose an algorithm which is part of the FTRL framework, where they use some existing tools, such as the self-bounding technique as well as new tools such as the Loss-Shifting Technique.  They outperform the previous results from Jin and Luo (2020) for the problem of stochastic and adversarial episodic MDPs with known transition with a simpler analysis, and they derive new bounds in the setting with unknown transitions.  Both Full information and Bandit feedback are coinsidered. ","This paper presents an algorithm for episodic Markov decision processes with unknown losses and transition kernel. The paper proves that this algorithm achieves order sqrt(T) regret bounds when losses are chosen by an oblivious adversary and order log^2*(T) when losses are stochastic. This is the first algorithm to achieve such best of both works regret guarantees when transitions are unknown.  The key technical innovations are (1) the use of loss-shifting technique in the MDP setting, (2) running FTRL in phases where the empirical transition model is fixed per phase, (3) new biased loss estimators that combine importance weighting using upper occupancy measures with   additional bonus terms to promote exploration and (4) carefully tuned regularizers for FTRL.","This paper studies the problem of sequential learning in episodic MDP with full and bandit feedback and unknown dynamics of the MDP. The paper provides the first algorithm with provable performance guarantees, that simultaneously ensure polylogarithmic regret in the stochastic setting and the \sqrt(T) order regret bound in the adversarial setting. This is the first result of this kind, previous works only answer this question for a setting when the transitions are known. The paper proposes running the FTRL-type algorithm over the set of all valid occupancy measures from the estimated transition matrix. It ensures the exploration by subtracting the exploration bonus from the loss and it gives the importance weighted loss estimator by dividing the observed loss by the upper bound on the occupancy measure. The algorithm is computationally efficient. The secondary contribution of the paper is a more simple analysis of the variation of the same problem where the transitions are known.    ","This paper studies the ""best of both worlds"" algorithm for episodic MDPs with unknown transition with stochastic and adversarial rewards, hence the paper resolves the open question in (Jin and Luo 2020). While the adversarial case alone with unknown transition is closed in (Jin et al. 2020), the paper shows that a simple variation of FTRL with a loss-shifting trick, optimism-based exploration bonus and restarting can also achieve poly-logarithmic regret in stochastic environments assuming the constant minimum-gap $\Delta_{min} = \min_{s, a \neq \pi^*(s)} V^*(s) - Q^*(s,a)$. The proposed algorithm and associated analysis enjoy more simplified analysis compared to the previous work in (Jin and Luo 2020). The simplified analysis is adapted from (Jin and Luo 2020) and (Simchowitz and Jamieson 2019). Main results are the the regret bound of $O(\sqrt{T})$ under adversarial setting, and $O(log^2 (T))$ under stochastic setting.",0.18018018018018017,0.22522522522522523,0.27927927927927926,0.24166666666666667,0.19166666666666668,0.19230769230769232,0.16666666666666666,0.16025641025641027,0.2052980132450331,0.1858974358974359,0.152317880794702,0.1986754966887417,0.17316017316017313,0.18726591760299627,0.2366412213740458,0.21014492753623187,0.1697416974169742,0.19543973941368079
342,SP:48a57dfdda11bb2b35c23054c1a4da822e3f9ce6,"The paper is about semi-supervised domain adaptation.  The inputs are labeled source samples, as well as labeled and unlabeled target domain samples.  The task is to train a model that performs well on the target domain.  The authors propose a framework with two main components to learn a domain-agnostic representation.  - Inter-domain contrastive alignment: This module reduces the discrepancy between classes centroids of the same class from source and target domain, while increasing the distance between class centroids of different classes from both source and target domains. Class centroids are computed as an exponential moving average via equations (2) and (3).  A contrastive loss is used to pull and push the centroids. - Instance contrastive alignment: The goal of this module is to pull the classifier boundary into a low-density region, which is done by regularizing the classifier on the unlabeled target data.  Image augmentation is used to create perturbed images of unlabeled target data and a contrastive loss is employed to pull the representation of the same instance (perturbed and unperturbed) together while pushing it away from other samples.  Experiments on Office-Home, DomainNet and Office31 show promising results.","The authors propose a method for semi-supervised domain adaptation. In their methodology, contrastive prototypical alignment is used for the source and target examples (pseudo labels are computed for unlabeled target examples) and contrastive loss is used for the unlabeled examples from the target domain. The hope is that by using the contrastive loss on the unlabeled examples, the target examples would form clusters and the classification boundaries would lie on the low density data regions. ","This paper proposes a contrastive learning method for semi-supervised domain adaptation (CLDA), where few labelled samples from target domain are available. The CLDA consists in inter-domain contrastive alignment, which decreases distance between same classes from source and target domains, and instance contrastive alignment based on strongly augmented unlabeled target images to reduce intra-domain discrepancy. The experiments are reported on three commonly used domain adaptation datasets.   ","This work presents a contrastive learning framework for Semi-Supervised Domain Adaptation. A basic assumption of few labeled target samples is made according to the Semi-Supervised setting. The key idea is to apply an instance-level and inter-domain contrastive alignment, using labeled source samples, few labeled target samples, and pseudo-labeled target samples. For instance-level contrastive loss, strong augmentations are used as positive pairs. For inter-domain alignment, cluster centroids are aligned across the two domains. The proposed method shows consistent improvement over multiple datasets.",0.16145833333333334,0.171875,0.13541666666666666,0.2631578947368421,0.25,0.36764705882352944,0.40789473684210525,0.4852941176470588,0.29545454545454547,0.29411764705882354,0.2159090909090909,0.2840909090909091,0.23134328358208955,0.25384615384615383,0.1857142857142857,0.27777777777777773,0.23170731707317072,0.3205128205128205
343,SP:48b3fdf354bdef02dcc87132170104795effe081,"The paper introduces methods to integrate causality in the task of controllable text generation, specifically on attribute-conditioned generation and text attribute transfer. The goal is to eliminate biases from spurious correlations with confounders (e.g., jobs when controlling the gender of the text). They do this by introducing a latent confounder variable and an observed proxy variable. They trained their models using a VAE objective, as well as three counterfactual objectives to help mitigate model collapse.","The authors propose a unified causal framework for controllable text generation tasks (attribute-conditional text generation and attribute text transfer). The authors propose a structural causal model (SCM) to formulate the two tasks as causal intervention and counterfactual reasoning in the context of causal latter, which thus provides a unified framework for studying controllable text generation.  The learning stage aims to integrate variational learning (VAE) and confounder disentanglement (and balancing).   For the evaluation, the authors reused Yelp and Bios datasets, and then added synthetic spurious correlations, creating challenging data scenarios.  They evaluate the proposed model with a few simple baseline methods in terms of control accuracy, bias, fluency, and diversity. To support the automatic evaluation of these metrics, they use pre-trained attribute classifiers. They also conduct human evaluations to show the effectiveness of their proposed methods.    Overall, the main contribution of the paper is a casual perspective for two main controllable text generation tasks via casual interventional inference and counterfactual reasoning. The method can be used to reduce toxic content (e.g., gender bias) in downstream applications that require automatic text generation.  ","The goal of the paper is to control an attribute of the text without changing another associated attribute. Let's use the biased YELP dataset the authors created as an exampe. In the dataset, 90% of positive reviews come from restaurants and 90% of negative reviews come from the other categories (e.g., shopping). Their goal is to control the sentiment (attribute a) without changing its category (confounding factor c). This is difficult because the conditional generator might just generate a restaurant review when the task is to generate a positive review. Similarly, if most of the nurses are female, the text generator might just generate nurses to satisfy the requirement of generating a female character.  In conditional generation such as a seq2seq model, even the controlled attribute is given as the condition, the text generator might still generate the text that might imply confounding factors or some other attributes because the confounding factors and controlled attribute are associated or the prior probabilities of those factors/attributes are high. The paper solves the problem using backdoor adjustment (removing the dependency between the attribute a and the hidden representation of the sentence z). My understanding of their method is like assuming confounding factors and controlled attributes are independent or ignore the prior probability of observing other attributes. This would make the generated text less fluent but can generate the text that would have the attribute in a high probability.  The experiments show that the proposed method is significantly better than conditional LM or other text attribute transferring approaches when the training data is biased (e.g., the review category is highly correlated with the sentiment of the review in YELP). When the dataset is unbias, one experiment seems to show that the proposed method is also better than other approaches. ","This paper investigates the problem of the attribute (sentiment, gender) controlled text generation or text style transfer. The authors formulate the problem from a principled causal perspective, which models the two tasks with a unified framework. A framework of structural causal model is introduced to ground the idea. Experiments on text generation and style transfer are conducted and the experimental results show that the proposed framework can indeed improve the control accuracy.",0.2987012987012987,0.2727272727272727,0.18181818181818182,0.1912568306010929,0.1366120218579235,0.0903010033444816,0.12568306010928962,0.07023411371237458,0.19444444444444445,0.11705685618729098,0.3472222222222222,0.375,0.1769230769230769,0.11170212765957445,0.1879194630872483,0.14522821576763484,0.19607843137254902,0.14555256064690025
344,SP:48f7e1d927a18dbbc3d67e97b7d66b04cffbf8fe,"This paper presents a speech synthesising-based attack (SSA) framework that generates audio examples entirely from scratch to attack automatic speech recognition (ASR) systems. A conditional variational auto-encoder (CVAE) based speech synthesiser is trained based on a combined connectionist temporal classification (CTC) and regularization loss. As compared to existing methods, the proposed approach serves as the first work that generates adversarial samples without audio inputs. The results confirm that the proposed SSA framework can attack the ASR system successfully. A comprehensive analysis of different hyper-parameters to the achievable performance is provided. Moreover, demo audio samples are provided by a website link.  ","The paper presents a new adversarial attack mode to ASR systems based on speech synthesis. The main idea is to synthesize speech utterances that naturally sound like ground-truth transcription y_o, but can fool an ASR system. It is a targeted attack model, where each attack signal has its own target label y_t. The paper also presents a CVAE-based speech synthesis system and a learning rate decay method for training. ","This paper describes a white-box attack on ASR systems using TTS.  The main technique is (somewhat) standard adversarial training with the constraint that the CVAE component of a TTS model is the only component that is updated.  Since this component is responsible for prosodic and other non-segmental qualities of speech synthesis, the expectation is that the naturalness of the synthesis will not be impacted, but the ASR recognition will be modified.","In this work the authors explore a new vein of research in the area of audio adversarial attacks wherein instead of focusing on perturbing existing audio, they synthesize audio. To their best knowledge, and this reviewer, this is  the first such attempt. To this end, the authors use a conditional variational auto-encoder as a speech synthesis (TTS) model and develop  a adaptive sign gradient decent algorithm in order to solve their SSA optimization problem. Experiments are provided showing the effectiveness of  their approach, along with a detailed analysis of results. ",0.17475728155339806,0.14563106796116504,0.1650485436893204,0.1917808219178082,0.1780821917808219,0.1643835616438356,0.2465753424657534,0.2054794520547945,0.18681318681318682,0.1917808219178082,0.14285714285714285,0.13186813186813187,0.20454545454545453,0.17045454545454544,0.17525773195876287,0.1917808219178082,0.15853658536585366,0.14634146341463417
345,SP:4940a7a1de0c39ebdb12301b677205189727b3d8,"This paper considers the online facility location problem with predictions.  In this problem, there is a sequence of points which arrive online and the algorithm must either open a facility to serve each point, paying the facility opening cost plus the distance to the opened facility, or it can assign the point to an already open facility, paying the distance only.  Additionally, each arriving point is given a prediction of a facility which serves it in an optimal solution. The goal is to be competitive with the offline optimal solution, and the competitiveness is parameterized by the error in the predictions.  Here, the prediction error is the maximum distance between a predicted facility and the facility in the optimal solution which serves a point.  The main result is an algorithm which is $O(\min (\log n, \log \frac{n\eta_{\infty}}{OPT}))$-competitive, where $\eta_\infty$ is the max error and $OPT$ is the optimal cost.  This improves over worst case lower bounds when $\log \frac{n\eta_{\infty}}{OPT} = o(\frac{\log n}{\log\log n})$, and nearly matches worst case bounds otherwise.  At a high level the algorithm first follows Meyerson's algorithm, then opens some additional facilities by taking into account the predictions.  In order to get the stated bound, the predictions are ""calibrated"" so that $\eta_\infty = O(OPT)$ so that $\log \frac{n\eta_{\infty}}{OPT} = O(\log n)$, thus it suffices to only prove the bound of $O(\log \frac{n\eta_{\infty}}{OPT})$.  A lower bound is also given, showing that the result is nearly tight.  Additionally, an experimental validation is carried out, comparing the proposed algorithm with Meyerson's algorithm and a naive algorithm which follows the predictions.  The experiments are performed on datasets where the underlying metric is either Euclidean or a graph metric.  The predictions are simulated to have a given level of error.  While the proposed method underperforms against blindly following the predictions when the error is small, it is more robust to large prediction errors and often comes close to Meyerson's algorithm when the prediction error is large.","This paper studies the classical online facility location problem in a metric space. Given a new demand in this space, the algorithm will either open a new facility by paying an opening cost $w(f)$ and assign the demand node to this facility, or it will assign the demand node to an existing open facility. The cost incurred by the algorithm is the sum of all the opening costs and assignment costs.  In this paper, the authors consider the setting where, along with every demand node, we are also given a prediction on which facility it gets assigned to in the optimal. Such predictions can potentially be acquired through historical data.   Their main contribution is an online algorithm for facility location with predictions that has a near-optimal competitive ratio. The competitive ratio relies on a parameter $\eta_\infty$ which is simply the largest prediction error, where prediction error for any demand point is the distance between the predicted facility and the facility that is assigned to the demand in the optimal solution. Intuitively, the authors show that if the predictions are accurate (i.e., $\eta_\infty$ is small), then their algorithm is $O(1)$-competitive. On the other hand, if the predictions are highly inaccurate, the algorithm will become $O(\log n)$ competitive.   The algorithm relies on an $O(\log n)$ competitive worst-case optimal (randomized) online algorithm by Meyerson. The idea becomes very simple when facilities have uniform opening costs: Suppose Meyerson’s algorithm opens a facility, one can simply also open a facility at the predicted location. By doing so, the cost incurred will not be more than twice that of Meyerson’s algorithm which is $O(\log n)$-competitive. However, when the predictions are accurate, opening the facilities at predicted locations lead to better future assignments leading to $O(1)$-competitive algorithm.  They extend this approach in a non-trivial way to the case with non-uniform opening costs. ","This paper presents an algorithm for online facility location with predictions. The prediction model is that on the arrival of each client, the prediction provides a facility to connect this client to. The quality of the prediction is quantified in the result by the maximum prediction error, i.e., the maximum distance between the locations of the predicted facility and a facility serving the same client in a fixed optimal solution. The main result is that the competitive ratio is constant (which is the best offline ratio) when the maximum prediction error is small (e.g., when it is a 1/n-fraction of the optimal cost, which of course if true when the predictions are precisely correct), and O(log n) (which is the best online ratio, ignoring lower order terms), when the prediction error is large. The algorithm is an augmentation of an online algorithm for the problem by Meyerson that obtains a competitive ratio of O(log n), where in each step in addition to running Meyerson's algorithm, the algorithm uses a second copy of the incremental cost in Meyerson's step to preemptively open more facilities ""close"" to the predicted facility for the current client. In addition to the theoretical analysis, the paper includes an experimental evaluation of the algorithm comparing it to baselines produced by an algorithm that blindly trusts the prediction and Meyerson's algorithm.","The paper considers the online facility location problem with predictions. In this problem, there is a set of facilities given offline on a metric and a sequence of demand points appearing online. When a demand point arrives, we must either route it to a facility we have previously opened, or open new facilities and route the demand to one of the new facilities. The total cost of a solution is the total cost of facilities opened, plus the total distance between each demand point and the facility it was routed to. In the learning-augmented setting, when each demand point arrives we are also given a prediction, in the form of a facility we believe it is routed to in an optimal solution. Let $\eta_\infty$ be the maximum distance between any the facility we predict a demand point is routed to in the optimal solution, and the facility it is actually routed to in the optimal solution. The authors give a $O(\log \frac{n \eta_\infty}{OPT})$-competitive algorithm, where $n$ is the number of demand points and $OPT$ is the cost of the optimal solution - without loss of generality, we can assume $\eta_\infty = O(OPT)$, so this is at worst an $O(\log n)$-competitive algorithm. They also show that this is the optimal competitive ratio up to $O(\log \log n)$ factors, even if $\eta_1$ (the total distance between predictions and OPT's facilities rather than the maximum) is constant. The authors also give experiments showing that on real-world data sets with synthetic predictions that have a bounded $\eta_\infty$ value, their algorithm outperforms Meyerson's when the prediction error is low, outperforms following the predictions when the prediction error is high, and is not too much worse than Meyerson's when the prediction error is high.  The algorithm the authors use first uses the algorithm of Meyerson for the setting without predictions. In this algorithm, for each power of 2 $2^k$, we consider opening the closest facility to the demand point whose cost is at most $2^k$, or that is already in our set of open facilities. Let $\delta_k$ be the distance from the demand point to this facility. We open this facility with, roughly speaking, probability proportional to $\frac{\delta_{k-1} - \delta_k}{2^k}$. Intuitively, we will never open a facility that is further from the demand point from a previously opened facility, and the probability we open a facility closer than the best previously opened facility is roughly proportional to the decrease in connection cost from opening this facility versus the best facility costing half as much, divided by this facility's opening cost. Following a round of Meyerson's algorithm, we use a ""prediction step"" to possibly open more facilities, whose expected cost is at most the increase in facility and connection cost in the round of Meyerson's algorithm. To do so, we consider facilities in a ball around the predicted facility whose radius is proportional to the distance from the predicted facility to the nearest facility opened in a previous prediction step, and consider opening the cheapest one. We will definitely open it if doing so would not cause the cost of the prediction step to exceed the cost of the round of Meyerson's algorithm, otherwise we open it with some probability, such that the prediction step's expected cost is exactly the cost of the round of Meyerson's algorithm. ",0.27635327635327633,0.2222222222222222,0.36752136752136755,0.21052631578947367,0.33126934984520123,0.3922413793103448,0.30030959752321984,0.33620689655172414,0.22241379310344828,0.29310344827586204,0.18448275862068966,0.15689655172413794,0.2878338278931751,0.26758147512864494,0.2771213748657358,0.24504504504504498,0.2369878183831672,0.2241379310344828
346,SP:4942529db6292cc770f13e67fbe5d420d3ca429f," This paper studied an NN-based method to approximate the Pareto front (PF) for multi-objective optimization (MOO) problems. As a practical example of the MOO problem, the authors focused on the MO traveling salesman problem (MOTSP) and developed a new approximation method.  A critical problem discussed in the paper is how to efficiently and effectively approximate the PF. In contrast to an existing work (Li et al. 2020) that separately training networks for different preferences, meaning that the training process is time-consuming, the proposed approach using PA-Net uses a single network (i.e., an augmented version of the TSP-Net).  The targeting MOO problem is converted to constrained optimization problems (key: divide problems using constraints and preference vectors). The Lagrangians of constraints are used to train a policy in an actor-critic manner using solutions and preference vectors.  From the experimental results, the advantage of the proposed PA-Net is shown in terms of (1) training time, (2) HV, and (3) computational time (i.e., inference time). Some graphical illustrations are helpful to compare the PFs estimated by different methods. Roughly speaking, the performance of the PA-Net outperformed existing methods.",The paper introduces a method to approximate a Pareto front for multi-objective TSP from neural combinatorial optimization using deep reinforcement learning. The PA-Net method uses a constrained optimization problem formalization with a preference vector to control the solution generation. An experimental evaluation is performed and PA-Net is compared to multi-objective genetic algorithms and one DRL-based method from the literature. PA-Net performs competitive and approximates the Pareto front well.,The authors have presented a Pareto frontier approximation network (PA-Net) for multi-objective travelling salesman problem. The authors showcased that the PA-Net was able to generate good quality Pareto fronts with fast inference times as opposed to other learning methods such as The authors leveraged the generalization by using deep neural networks to solve the large values of number of preference vectors that are computationally hard and showcased that the formulation can find concave Pareto frontier too.   Contributions:   a. Computation time in training the models and generalizing for other instances   b. The proposed approximation methodology performs well in hyper volume and on the computation time. ,"This work proposes a Pareto frontier approximation network (PA-Net) to learn the Pareto frontier for multi-objective traveling salesman problem (MOTSP). The proposed method first converts the MOTSP into a set of constrained single-objective optimization problems with different preference-based constraints. Then it builds a modified TSP-Net with preference augmentation to solve all the constrained problems. For a new MOTSP instance, the trained network can generate different approximate Pareto solutions by adjusting the preference. The experimental results show that the proposed PA-Net method has a good performance on MOTSP instances with different numbers of cities and objectives.  ",0.14948453608247422,0.13402061855670103,0.17010309278350516,0.24324324324324326,0.28378378378378377,0.2523364485981308,0.3918918918918919,0.24299065420560748,0.32673267326732675,0.16822429906542055,0.2079207920792079,0.26732673267326734,0.21641791044776118,0.17275747508305647,0.22372881355932206,0.1988950276243094,0.24,0.25961538461538464
347,SP:497b05e97c60f7338fd082d6af802d1e4aaef14d,"Graph Neural Networks have recently become the state of the art for tasks on graphs due to their flexibility, scalability. In this work, the authors propose a framework to build GNN's that operate on local node neighborhoods in a permutation equivariant way - and argue that since LPEGN operates on lower dimensional spaces in comparison to regular GNN's the proposed technique offers significant improvements  in terms of GPU memory usage. The authors make use of category theory basics - and employ restricted representations of finite symmetric groups (i.e. fix some nodes while permuting other elements) based on the number of nodes in the neighborhood of the node - and ensure there is weight sharing between nodes with the same degree to achieve their objective.","This paper introduces local permutation equivariant graph network. The main motivation for introducing these graph neural networks is to improve in term of scalability with respect to global permutation equivariant models. This paper uses the very abstract language of category theory. In Section 6, the authors provide some experimental results on some real-world graph classification problems.","This paper introduces the local permutation equivariant network (LPEGN). Specifically, this work proposes to apply permutation equivariant update functions locally --- i.e., operating in a local neighborhood. The benefit of doing so is that the large graph is handled through sub-graph, which saves a lot of GPU memory. Moreover, this work handled different sizes of neighborhoods by proposing a heterogeneous weight-sharing mechanism. Weight is shared w.r.t neighborhood size -- local neighborhoods with the same size share weights. While being flexible to input sizes, the local update function is also expressive. The paper demonstrates its superiority in several graph classification benchmarks.","The paper introduces the framework of locally permutation equivariant graph neural networks.  This framework applies permutation equivariant layers [Maron et. al. 2018] to local node neighborhoods by treating them as separate subgraphs and using a weight sharing scheme for subgraphs of the same size.  The authors build their framework by discussing the different choices made -- local neighborhoods, weight sharing, and representation space.  The authors also provide a category theory point of view of their framework. ",0.11290322580645161,0.16129032258064516,0.14516129032258066,0.2982456140350877,0.22807017543859648,0.1941747572815534,0.24561403508771928,0.1941747572815534,0.24,0.1650485436893204,0.17333333333333334,0.26666666666666666,0.15469613259668508,0.17621145374449337,0.18090452261306536,0.2125,0.19696969696969696,0.2247191011235955
348,SP:49b1479c3686d0f03263b6c950ab835e80828a28,"This paper addresses the topic of changepoint detection in the event of distribution shift. A Bayesian model meant to detect such changes online, and able to discount historical information after a distributional shift, is formulated to express the problem. Algorithmic implementation is provided and evaluated across a number of data sets.","The paper focuses on distribution shift, i.e. the distribution that generates the data changes with time, something that can cause problems to static models since now the train and test distributions are not the same. The authors take an online Bayesian perspective and try to detect a changes in the distribution based on the data points that are received in each iteration. They also perform extensive experiments in different domains. ","In this paper, the authors propose a method to jointly model the data and also change points in the model parameters. This is accomplished following a Bayesian approach, where the full vector of change point detections (as binary indicators) is part of the model, and it informs the marginal posterior. The method is illustrated in an interesting mix of real and synthetic data.  UPDATE: The authors' responses were succinct and useful. However, I still think that the paper relies on too many approximations/heuristics. The approach is sensible, and the paper could be impactful, but they don't constitute a Neurips-level contribution.","This submission relates a method for adapting to distribution shifts online in a Bayesian fashion. A distribution shift is represented as a binary latent variable indicating that a shift has, or has not occurred. If a shift is inferred to have occurred, then the variance of the posterior is increased before the next step of the Bayesian recursion, weakening the influence of previous observations. Computational constraints associated with maintaining many possible hypotheses of the incidence of distribution shifts are remedied by the use of beam search.   This paper is timely and well-written. The topic of online adaptation to distribution shift is very relevant to the NeurIPS community, and the submission has developed through several rounds of peer review into a mature contribution to the literature. There are some substantial limitations that would be important to address in future work, but I am happy to recommend acceptance and willing to defend my evaluation.  UPDATE 08/17/2021 The author response has addressed all my concerns. In particular it is very good to know that the full changepoint history need not be maintained, and that the method will automatically adapt, in a sense, to a poor choice of \beta. I think this is a very good paper, and have increased my score to 8.",0.19607843137254902,0.27450980392156865,0.3333333333333333,0.19718309859154928,0.23943661971830985,0.24271844660194175,0.14084507042253522,0.13592233009708737,0.07981220657276995,0.13592233009708737,0.07981220657276995,0.11737089201877934,0.1639344262295082,0.1818181818181818,0.12878787878787878,0.16091954022988506,0.11971830985915491,0.1582278481012658
349,SP:49b9333e6d239a67e169c11208043ae45b0d12a7," The paper addresses the task of GZSL – more specifically, they provide a way to improve the quality of the generative samples in generative GZSL. A closed-form probe model is introduced to provide an efficient and differentiable solution in compute graph. In this manner, the generator receives feedback directly based on the value of its samples for model training purposes. It shows the results on two different settings, with fine-tuning features and without fine-tuning the features.  ","The paper outlines a method for improving generative zero-shot learning (ZSL) approaches. Rather than simply training the generative model with the goal of reproducing some real data distribution, the work proposes to train it with an additional goal of synthesizing samples that directly improve the performance of the downstream classification model.  The authors propose to do so through a novel sample-probing loss in which generated samples are used to train a closed-form ZSL model with a differentiable solution. The ZSL model is then evaluated directly on the classification task - and gradients are back-propagated to the generative model's parameters. By applying this approach to an existing generative ZSL model, the authors demonstrate improved sample quality for their synthetic data and increased classification performance across multiple datasets.  In summary, the paper's contributions are: 1) A major contribution: An approach to improving existing generative ZSL methods with a loss that maximizes their performance on the downstream task. 2) A minor contribution: A more detailed and rigorous reporting of the methodologies used to fine-tune model hyperparameters, aimed at increasing reproducibility.","This paper aims to address the problem of generalized zero-shot classification based on generative models. The main contribution is that it considers training and evaluating a generative model in synthesizing training examples that are helpful to improve classification performance. To this end, it leverages the zero-shot learning model of ESZSL that can be efficiently fit using a closed-form solution. This then serves as a sample probing mechanism, so that the generator receives training signal directly based on the value of its samples for classifier training and thus enables end-to-end training. The corresponding sample probing loss function is added into the standard generative model training loss. The final training procedure is performed in a way that is similar to meta-training. The approach is tested on standard generalized zero-shot classification setups, including CUB, SUN, AWA2, and FLO, and compared with state-of-the-art results.","This paper proposes sample probing, a meta-learning scheme for zero-shot learning (ZSL), to measure the quality of the synthetic samples provided by certain generative models. Specifically, an existing closed-form ZSL solver is plugged into an existing generative ZSL framework. Owing to the differentiability of the solver, the whole pipeline is end-to-end trainable. Experiments were conducted on four standard benchmarks, where we can observe the state-of-the-art performance achieved by the proposed sample probing based approach.",0.32051282051282054,0.3974358974358974,0.24358974358974358,0.18032786885245902,0.14754098360655737,0.15333333333333332,0.1366120218579235,0.20666666666666667,0.23170731707317074,0.22,0.32926829268292684,0.2804878048780488,0.19157088122605367,0.27192982456140347,0.23750000000000002,0.1981981981981982,0.2037735849056604,0.1982758620689655
350,SP:4a085214125d2d91b755b50e6f4ca82a66c52d41,"This paper studies a binary hypothesis testing problem that considers whether a given hypergraph has one or more than one community. Under a general class of degree-corrected mixed-membership model for hypergraphs, it identifies conditions under which it is impossible for any test to distinguish between the null and alternative hypothesis. The above results, which are first derived for uniform hypergraphs, are then extended to non-uniform hyper-graphs by viewing a non-uniform hypergraph as a collection of uniform hypergraphs. The paper also proposes a statistical test that successfully solves the above-mentioned hypothesis testing problem under conditions that can get logarithmically close to the impossibility regime. Experiments on simulated data validate some of the theoretical findings.","This paper considers the community detection problem in the hypergraph setting: determine whether there is a single community (null hypothesis) or K > 1 communities (alternate hypothesis). The main contribution is the development of lower bounds via bounding the \Chi^2-divergence, which is a standard method, although quite technically demanding in this setting due to the many parameters involved. Overall, this is a clear contribution to the theory literature on stochastic block models and community detection. It is a bit harder to judge its appropriateness for NeurIPS.","This paper introduces and studies a hypergraph community detection model called RMM-DCMM. In this model, each vertex may have a different degree (degree-corrected) and may also belong to many different communities (mixed-membership).  A main takeaway from the paper is that possibility/impossibility of testing whether this complicated model has K = 1 or K > 1 communities boils down to the quantity ||\theta||^2 ||\theta||_1 \mu_2^2.   The main results are: 1) Region of impossibility result for RMM-DCBM (a subset of RMM-DCMM). This shows that given any: 	(a) degree-correction factors \theta for a 1-community model 	(b) tensor P representing connection strengths between subsets of the K communities, 	(c) distribution F over mixed membership vectors, supported on {e_1,...,e_K}, 	if ||\theta||^2 ||\theta||_1 \mu_2^2 --> 0 (where \mu_2 is the second eigenvalue of the matricization of P), then there is a choice of degree-correction factors \theta^* such that the K-community model given  by (P, \theta^*, F) is information-theoretically indistinguishable from the 1-community model given by \theta  2) Region of possibility result for RMM-DCBM. For any K-community model (P,\theta,F) with K > 2, such that ||\theta||^2 ||\theta||_1 \mu_2^2 / log(n)^{1.1} --> \infty, there is a test (based on estimating the degree-correction vector \theta assuming that the model is null) to distinguish it from any 1-community model.  These results extend to RMM-DCMM under some additional conditions.",The current paper explore information-theoretic results for testing the existence of community underlying an observed hypergraph. The nodes of hypergraph are degree hetergeneous extending homogeneous results in literature. Both uniform and non-uniform hypergraphs are considered.  ,0.13445378151260504,0.23529411764705882,0.08403361344537816,0.2988505747126437,0.10344827586206896,0.044,0.1839080459770115,0.112,0.2702702702702703,0.104,0.24324324324324326,0.2972972972972973,0.15533980582524273,0.15176151761517617,0.12820512820512822,0.1543026706231454,0.14516129032258066,0.07665505226480836
351,SP:4a1716ec360ff99a4d872b691f7cfee5e74e71e3,"I thank the authors for their responses. They main point of remaining concern seems to be the necessity of analysis/evaluation under model misspecification. I feel like this concern is quite significant, but would defer to better experts in the field.  =====  The paper presents robust versions of the Q-learning and TDC algorithms and analyzes their rate of convergence in tabular and linear-approximation representations.",The authors consider the robust reinforcement learning with uncertainty in the transition. A robust Q-learning algorithm is proposed for tabular MDPs and a robust TDC algorithm is proposed for compatibility of (linear) function approximation. Convergence rates are proved for both of the proposed algorithms. Empirical studies show that the two algorithms work for perturbed MDPs.,"This paper considers the model-free robust reinforcement learning, where the uncertainty set is assumed to be unknown. The authors propose a robust Q-learning algorithm for tabular settings and a robust TDC algorithm for function approximation settings. The authors theoretically analyze the converge properties for these two algorithms. Numerical experiments demonstrate the robustness of the proposed methods.",This paper focuses on model free robust Q-learning that can be implemented online. The authors estimate the unknown uncertainty set (R-contamination model) with current samples. Under the perfect duality condition they obtain the bellman operator which optimize their robust objectives. They propose two different algorithms: one for tabular case and another one with function approximation for problems with large state and action spaces. The robust algorithms presented in the paper converges as fast as their non-robust counterparts. They also provide empirical analysis to show the usefulness of their proposed methods.,0.2,0.18461538461538463,0.16923076923076924,0.44642857142857145,0.30357142857142855,0.3793103448275862,0.23214285714285715,0.20689655172413793,0.11827956989247312,0.43103448275862066,0.1827956989247312,0.23655913978494625,0.21487603305785125,0.19512195121951223,0.13924050632911392,0.43859649122807015,0.22818791946308725,0.2913907284768212
352,SP:4a531659ac8a2b7e93823b53c4c5e22c6884cf80,"This paper incorporates user-item similarity score to item-based CF recommender systems.  The user-item similarity score is calculated by Pearson Correlation Coefficient or Cosine Similarity between user features and item features, where item features are tags, and user features are a weighted sum of (user's rated) items' features (tags). The calculated score are used as part of the baseline estimate of the item-based CF method.","In this paper, the authors study the recommendation systems. They first propose three new methods to characterize user preferences utilizing both rating data and item content information. The first method takes the weighted average of the feature vectors of items, the second one incorporates the effect of biases in the first one, and the third method puts weight on the biased feature vector of users using the item feature vector.  Next, they revise the baseline estimate by integrating the user-item similarity. They further integrate these methods into two baseline models, namely kNNBaseline and kNNContent. Experimental results suggest that these improvements lead to an increase in accuracy (2%-6%) with the expense of the additional complexity. ","This paper proposed several ideas to improve neighborhood-based recommendation systems by characterizing user preferences using both rating data and item content information. The 2 major proposed ideas are 1) weight item feature by feature score, 2) incorporate user-item similarity score in item score prediction. Experiments over the MovieLens dataset show that the proposed approach can outperform the baselines.",This paper studies hybrid recommendation methods that specifically combines neighborhood-based techniques with content-based user/item feature techniques. It proposes several ways to estimate a user-item correlation matrix and integrates it into neighborhood-based methods as an additional fine-tuning factor. The paper compares proposed methods with several baselines and evaluates RMSE and MAE on MovieLens20M.,0.2318840579710145,0.2028985507246377,0.15942028985507245,0.20689655172413793,0.11206896551724138,0.2,0.13793103448275862,0.23333333333333334,0.1896551724137931,0.4,0.22413793103448276,0.20689655172413793,0.17297297297297298,0.21705426356589147,0.1732283464566929,0.2727272727272727,0.14942528735632185,0.20338983050847456
353,SP:4a8b882d780be43d6af2452a554a33e56199fbf3,"In this paper, the authors investigate how heterophily affects the performance of GNNs. Specifically, they first show that not all cases of heterophily are harmful. Then, they propose some new metrics to better characterize the graph properties and their relations to GNNs’ performance. Finally, the authors propose a framework named Adaptive Channel Mixing to address some kinds of harmful heterophily. This paper provides some new perspectives on understanding and addressing heterophily.  ","This paper investigates the influence of heterophily on the node classification task of GNNs. They found that not all heterophily cases are baneful for GNNs and propose a similarity matrix to provide a more accurate description of the heterophily degree of graphs. Compared with existing matrices, the proposed matrix can provide more reasonable insights. After, they propose an Adaptive Channel Mixing (ACM) framework to unleash the power of vanilla GNNs on heterophily graphs. ","Graph homophily and heterophily recently gain a lot of attention from the community. First, this paper proposes an interesting metric, which considers the general structure of graph convolutional networks, to measure the potential performance of GCNs on a certain type of graph. The metric shows a strong correlation with the accuracy of graph convolutional networks. They further analyze the characteristics of nodes where the GCN may not work properly that also match the synthetic results. Based on these observations, the adaptive channel mixing framework is proposed. The proposed framework utilizes the high/low-frequency filterbanks and shows strong performance on real-world datasets. ",The paper makes two specific points: 1. First the current reliance on homophily scores to distinguish which graphs are likely to perform well for a particular task is not precise. The paper proposes an alternative metric which seems to correlate better with performance. 2. The paper takes inspiration from their proposed metric to define filter banks that separate high pass and low pass information separately and mixes them for the final desired task.,0.352112676056338,0.19718309859154928,0.14084507042253522,0.2465753424657534,0.1643835616438356,0.1553398058252427,0.3424657534246575,0.13592233009708737,0.136986301369863,0.17475728155339806,0.1643835616438356,0.2191780821917808,0.34722222222222215,0.16091954022988506,0.13888888888888887,0.20454545454545453,0.1643835616438356,0.1818181818181818
354,SP:4a9ca988c7b0b2620adde88bcdea3c1376df9bd5,This work extends the popular RL as inference framework to accommodate for more general divergence measures such as the f-divergence and Wasserstein distance. The author proposed a particle-based optimization framework for learning stochastic policies where the policy is learned using samples generated via Langevin dynamics. The framework can be applied to both online and offline settings.,"This paper proposes a particle-based sampling method for stochastic policy optimization in reinforcement learning. The main observation is that many policy optimization algorithms use reverse KL as the objective to optimize policies, which is mode-seeking and could be problematic in practice. To mitigate this problem, the authors propose to use particle-based sampling such that one can directly use KL for policy optimization. ",The paper introduces a particle-based approach for model-free RL. Multiple experiments compare the performance of the approach to SQL and SAC in online and offline settings. Results suggest that the proposed particle based approach improves on the prior work and shows different divergence measures yield different performance.  ,"This paper proposes a new method for policy optimization in reinforcement learning. Building off of prior connections to probabilistic inference, the authors propose a framework for optimizing policies using general divergences/distances by way of particle-based methods for sampling. Some theory is presented. Experiments show that the method greatly out-performs state-of-the-art algorithms in both online and offline settings. ",0.1896551724137931,0.20689655172413793,0.25862068965517243,0.2,0.2923076923076923,0.2857142857142857,0.16923076923076924,0.24489795918367346,0.23809523809523808,0.2653061224489796,0.30158730158730157,0.2222222222222222,0.17886178861788618,0.22429906542056074,0.24793388429752067,0.2280701754385965,0.296875,0.25
355,SP:4b03551d7f85f4f95b6836e06715cc578b750b7f,"The paper proposes Local Feature Integration Transformer (LIT), a new architecture to extract mid-level local features from images that are used for image retrieval. The resulting features are called SuperFeatures and are shown to yield state-of-the-art performance on two standard image retrieval benchmarks. The paper further proposes a learning framework, called Feature Integration-based Retrieval (FIRe), to train LIT using a contrastive loss with only image-level supervision. In contrast to other deep learning based image features that apply the loss on the image embedding or aggregated local features, FIRe applies the loss on the local features directly. An interesting property of SuperFeatures is that they are ordered, meaning a SuperFeature with the same index i in two matching images will be localized on similar structures.","This paper introduces the concept of super features into image retrieval. The whole framework is named FIRe. The idea is to aggregate a set of features on the feature maps at different scales according to a set of query vectors representing different latent concepts. Hence, the proposed model somehow implements a transformer, which is called LIT in this paper.  To train this model, the authors define two losses. The first one is a triplet-like one executed on the super-feature granular. Some empirical criteria are established to select positive and negative super features. The second one decorrelates the super features from a single image.  ASMK becomes the measurement to compare two images when testing, which is a conventional option in comparing two groups of local features.  We see experiments on ROxford and RParis. The results are quite promising.","The paper proposes a novel model, denoted as super-features, for image retrieval, with the following specific contributions: 1) features are extracted using an iterative attention module (the Local Feature Integracion (LIT) Transformer). Although similar modules have been proposed before in the literature [Locatello 2020], the authors reference them properly and their proposal have several modifications (decorrelation loss, learning the initial templates, residual connections). 2) The model is trained using contrastive losses on the local features, opposed to the more spread training procedure using constraints on the global embeddings. The paper presents solid experimental results in public datasets and compare them against reasonably chosen baselines. ","Authors describe an architecture for deep image retrieval. For this, they use mid-level features that they call super-features (SF). They construct thes SF by an iterative attention module. They build them as an ordered set in which each element focuses on a localized yet discriminant image pattern. Auhtors show that for training, only image labels suffice. Authors present a set of experiments on common landmark retrieval benchmarks to validate that their SF substantially outperform state-of-the-art methods when using the same number of features. They also show that this requires a significantly smaller memory footprint to match their performance.",0.2,0.2,0.13846153846153847,0.16546762589928057,0.1079136690647482,0.1523809523809524,0.18705035971223022,0.24761904761904763,0.17475728155339806,0.21904761904761905,0.14563106796116504,0.1553398058252427,0.19330855018587362,0.22127659574468084,0.15450643776824036,0.18852459016393444,0.12396694214876033,0.15384615384615385
356,SP:4c45e9a5e52a7e717737579ec117dd07846440f6,"The paper proposes a categorical version of ARMS, namely CARMS. By utilizing REINFORCE together with copula sampling, CARMS remains an unbiased estimator and non-duplicated samples reduce the variance of gradients. The authors verify the proposed model through toy example, VAE, and the structured output prediction task. ","The paper proposed a novel gradient estimator for categorical stochastic hidden variables, named Categorical-Antithetic REINFORCE Multi-Sample (CARMS), which is a categorical extension of ARMS (Dimitriev and Zhou, 2021). CARMS reduces the variance of LOORF (Kool et at., 2019) by the means of importance weighted sampling. Using copula, where Dirichlet copula was employed in the experiments, CARMS generates the antithetic categorical samples to reduce variance. The paper compares two versions of the estimator, using inverse CDF sampling and Gumbel max sampling, against UNORD, LOORF and ARSM for categorical VAE and structured prediction tasks on Dynamic MNIST, Fashion MNIST and Omniglot. They find slightly better results compared to other estimators on both tasks.","**Summary** - The paper introduces a multi-sample gradient estimator for categorical variables.  - The estimator is based on the score-function and uses correlated samples to achieve a variance reduction.  - The estimator extends several recently proposed estimators naturally.  - The estimator yields marginal improvements over baselines in the experimental evaluations.   **Edit after Author Response** - Thank you for replying to the points I raised. I think the changes you do improve the draft. However, because I find the experimental improvements fairly marginal, I have kept my score unchanged. I believe the paper is above the acceptance threshold.","This paper tackles the problem of differentiating expectations of discrete random variables. Specifically, this paper proposes a generalization of the recent ARMS approach to more than two categories. The CARMS gradient estimator uses multiple samples from a categorical distribution. Unlike e.g. LOORF, CARMS uses samples that are dependent (typically negatively correlated), hoping to further reduce estimator variance. The authors prove how to construct an unbiased gradient from these dependent samples using importance sampling. The authors propose several joint distributions (copulas) to actually generate such dependent samples. Experimental results are provided on several problems such as a categorical latent VAE. ",0.40425531914893614,0.23404255319148937,0.3191489361702128,0.1592920353982301,0.1592920353982301,0.14893617021276595,0.168141592920354,0.11702127659574468,0.15,0.19148936170212766,0.18,0.14,0.23750000000000004,0.15602836879432624,0.2040816326530612,0.1739130434782609,0.16901408450704228,0.1443298969072165
357,SP:4c9367852eaf4779bcda43000661c52ce0ea8de9,"This paper introduces ICE, an instance-confidence embedding approach to learn with a challenging noisy label setting: instance-dependent noise (IDN). Considering the sparse property of IDN, the authors propose to use instance embedding by equipping a trainable parameter to each data instance. Experiments results on various image datasets and text data show the effectiveness of the proposed method with the presence of label noise.","This paper studies instance-dependent noise (IDN) problems. It proposes a single-parameter (confidence) model for the noise generating process. Moreover, *instance-confidence embedding* (ICE) method is employed in the training process. The experimental results on various image and text classification tasks confirm the effectiveness of the proposed method.","This paper presented an instance-confidence embedding model (ICE), which is a variational approximation of the instance-dependent noise (IDN). Given the observation that most columns of the IDN transition matrix have only limited influence on the class-posterior estimation, this paper uses a single-scalar confidence parameter to model the noise and uses a simpler transformation to transfer clean class posterior to noisy class posterior. The suggested method captures instance-specific noise information and thereby improves classification performance when compared to previous methods based on the class-conditional noise (CCN) assumption.",This paper proposes ICE (Instance-confident embedding) method for learning with IDN (instance-dependent label noise). ICE approximates IDN by using a single-scalar confident parameter for each sample based on the assumption that $P(Y|x)$ is close to a one-hot vector. ICE is incorporated into the loss function for efficient learning process under label noise. Authors conduct experiments on both image and text classification to verify the effectiveness  of ICE and perform gradient analysis for further understanding the method. ,0.3230769230769231,0.35384615384615387,0.36923076923076925,0.32653061224489793,0.5102040816326531,0.21739130434782608,0.42857142857142855,0.25,0.2926829268292683,0.17391304347826086,0.3048780487804878,0.24390243902439024,0.368421052631579,0.2929936305732484,0.32653061224489793,0.22695035460992904,0.3816793893129771,0.22988505747126436
358,SP:4ce1d4c3ecb7e79b1dfdf4ba8fb603aca08c4a01,"This paper extends previous work, Emb2Emb, by replacing the LSTM autoencoder with a Transformer autoencoder. The authors propose some techniques to handle the difficulty of this replacement. They also conduct some experiments to support the claim. ","This paper proposes a text autoencoder with a bag-of-vectors embedding, which is more capable of encoding longer text than a single-vector embedding. To train such a model, the authors first encode the input sequence into a vector sequence of equal length, and then add a gate to each vector and encourage the model to keep a small number of open gates.  The proposed BoV-AE can be used in (non-parallel) seq2seq tasks by learning a mapping within the embedding space, similar to the Emb2Emb method proposed by Mai et al. The difference is that the mapping is now between two bags of vectors instead of between two vectors. The authors use a Transformer with a copy mechanism and an offset vector to produce a bag of vectors. To align two bags of vectors, the authors adjust the average Hausdorff distance to a soft version to make training smoother.  Experiments are conducted on a sentiment transfer task and a sentence summarization task. BoW-AE shows better performance compared to AE with a single-vector embedding.  ","This paper presents an autoencoding model which is trained for conditional text generation tasks in an unsupervised manner. Rather than using a fixed-sized bottleneck layer in the autoencoder, the authors present a method for learning a variable-sized set of representations (referred to as a bag of vectors). This bag of vectors can then be transformed (in embedding space) in order to generate a modified sequence of text based on the original input sequence.","The paper extends the unsupervised conditional text generation framework of Emb2Emb (Mai et al., 2020) from a single encoding to variable-length encodings. The new model is called a bag-of-vectors autoencoders (BoV-AE). To train BoV-AEs, the paper develops a regularization technique called L0Drop, an extension of OffsetNet in Emb2Emb for learning the mapping Phi, and an alignment loss based on the Hausdorff distance. Experiments show that BoV-AEs outperforms Emb2Emb on sentiment transfer and summarization in the reconstruction loss, Rouge/Bleu, and accuracy.",0.3333333333333333,0.3055555555555556,0.2777777777777778,0.12921348314606743,0.16292134831460675,0.21333333333333335,0.06741573033707865,0.14666666666666667,0.11494252873563218,0.30666666666666664,0.3333333333333333,0.1839080459770115,0.11214953271028037,0.19819819819819823,0.1626016260162602,0.18181818181818185,0.2188679245283019,0.19753086419753083
359,SP:4cf04494e8cdb15fa4dfdf525deb9435302a3439,This paper provides task-aware privacy preservation to improve the task performance for multi-dimensional user data for deploying a trained model. The paper’s spirit is to use an encoder-decoder framework with a Laplace noise added to learn a task-relevant but LDP privacy preserved latent representation of user data. The author also provides an analytical near-optimal solution for a linear setting with mean-squared error task loss. They show the effectiveness of their method via experiments on three real-world datasets.,"This paper proposes a task-aware local DP method to improve the privacy andutility trade-off for multi-dimensional user data. Also, a analytical near-optimal solution for a general linear encoder-decoder model and MSE loss is provided. For neural network cases (i.e., nonlinear encoder-decoder), the authors present a heuristic learning algorithm to get the model parameters. The experiments results demonstrate the effectiveness of the proposed methods. ","The paper addresses the problem of large loss in utility due to the addition of noise in a local-DP setup, especially when working with high dimensional data. To mitigate this problem, the paper proposes an encoder-decoder setup in which noise is applied through the encoder, in a lower-dimension to decrease its effect on utility. Then  Laplace random noise is applied to each point's encoding in the latent space. The training  of the autoencoder is done over an offline phase on public data and with respect to the main task that the data is to be used for.  The approach is then evaluated empirically.","This paper proposes a task aware local-DP approach to improve the performance on multi-dimensional data with same level of privacy. This approach is based on encoder-decoder framework that perturbs only the task relevant encoding instead of the raw user data that generally causes less noise addition and improves the task accuracy. Besides, the paper provide a heuristic learning algorithm for more general settings. The proposed approach is compared with task agnostic and privacy agnostic approaches on different datasets and the results show the proposed method outperformes them on overall task loss under different privacy budgets.",0.2823529411764706,0.24705882352941178,0.32941176470588235,0.22535211267605634,0.38028169014084506,0.205607476635514,0.3380281690140845,0.19626168224299065,0.2857142857142857,0.14953271028037382,0.2755102040816326,0.22448979591836735,0.30769230769230765,0.21875,0.3060109289617486,0.1797752808988764,0.31952662721893493,0.2146341463414634
360,SP:4d158922203b8244d5243488a76f242e5dc6f740,"To take the words of the authors directly, this paper answers the question ""What graph functions are representable by GNNs when we can only observe random neighborhoods?"" In answering this question, the authors consider a notion of an estimable function in a random neighborhood model, which they then show is equivalent to uniform continuity in a metric topology, which they dub the randomized Benjamini-Schramm topology. With this perspective in hand, they show that sufficiently powerful graph neural networks coupled with universal approximators are capable of estimating uniformly continuous functions in this topology. They then discuss implications of this in terms of robustness, sampling complexity, and generalization.","The paper uses sublinear algorithms to estimate certain graph functions.  The paper is essentially a property testing (sublinear algorithms) paper posing as a GNN paper. This is not necessarily a bad thing if the connection is strong enough; however, this connection does not become very clear to me.  The main relation to GNN is very vague as core concepts such as universality are not well-defined. I presume the universal approximation theorem is meant here, but I cannot be sure. ","This paper studies a theoretical framework to address learning problems over graphs under partial observation constraints. In this context, a Random Balls Sampling GNN architecture based on random neighborhood model is introduced and its expressibility and learnability are characterized theoretically. Applications of the proposed architecture to settings with perturbations and size-generalizability are also discussed.   ","The paper proposes to use randomly sampled subgraphs from a large graph to estimate the function value computed from the entire graph. The paper has done extensive theoretical analysis using the ""randomized Benjamini–Schramm topology"". The model also has analyzed learnability issues when estimating these functions. ",0.11214953271028037,0.11214953271028037,0.11214953271028037,0.1,0.1125,0.09090909090909091,0.15,0.21818181818181817,0.2608695652173913,0.14545454545454545,0.1956521739130435,0.10869565217391304,0.12834224598930483,0.14814814814814817,0.1568627450980392,0.11851851851851852,0.14285714285714285,0.099009900990099
361,SP:4d829b7d9da8b05051e1143c2254d5a630784ab6,"The authors consider multiple disentangled factors in a variational autoencoder. The proposed method extends the structure of Sha&Lukasiewcz(2021) by modifying the sampling layer. In the new sampling layer, multiple factor representation is generated as well as a probabilistic weight for each factor. Then, the final representation is a weighted sum of all the factors.  The proposed method is evaluated on toy data, i.e., modified dSprites and Base Face Model 2019. The results show that the method is able to encode multiple factors.","This paper proposes a method for projecting datasets with different categorical labels into a unified latent space. It is based on an architecture proposed by Sha and Lukasiewicz (2021) which is a VAE with a structured latent space. The latent space is split into a content and style part, where the style part models the classification of labels, and the content the rest of the features to reconstruct the input. The goal of the proposed modification is to have similar label values close together in latent space while disentangling across labels. In experiments on synthetic datasets, the model is shown to provide the specified properties.","The paper is a follow-up to Sha and Lukasiewicz 2021 ""Multi-type Disentanglement without Adversarial Training"" (SL21 from now on) which uses a slight modification of the technique from SL21 to the new task of training a single model on multiple labeled datasets with different label sets that encode the same information at different levels of granularity (so in a way it's a variant of multi-task learning). The model effectively learns a single continuous embedding for examples from which all label sets are easily predictable, effectively unifying the different label sets. The experiments are performed on modified versions of the dsprites and base face datasets. On dsprits it shows a more smooth representation than SL21.",This work proposes a continuous disentanglement variational autoencoder. The approach is an extension of the disentangled variational autoencoder proposed by Sha & Lukasiewicz in which they modify sampling layer to use weighted sum of multiple distributions instead of single style vector distribution. This ensure continuity in the latent space.,0.21176470588235294,0.23529411764705882,0.15294117647058825,0.18095238095238095,0.14285714285714285,0.09322033898305085,0.17142857142857143,0.1694915254237288,0.2708333333333333,0.16101694915254236,0.3125,0.22916666666666666,0.18947368421052632,0.19704433497536947,0.19548872180451127,0.17040358744394618,0.196078431372549,0.13253012048192772
362,SP:4d86ca826c23ea04806f93e5c9e840e73f072b4e,The paper proposes a new component for transformer architectures to model trajectory attention by modeling temporal correspondences in dyanmic scenes. They also present approximation techniques to address computation and memory challenges for high-resolution videos. The proposed contributions are demonstrated on video action recognition tasks with SOTA results on multiple datasets. ,"The paper introduces a novel transformer architecture in which trajectories in video are considered. The model is able to leverage its own self-attention module to build trajectories around relevant objects in the video. Furthermore, authors introduce an approximation through a set of intermediate variables which enables them to reduce the computational cost of the transformer. Authors validate their model against state-of-the-art models in three main video benchmarks. ","The paper focused on model related motion information along the temporal domain. In particular, they proposed new drop-in block for video transformers that aggregates information along implicitly determined motion paths. The network is based on attentive mechanisms with Transformer. The new model doesn't rely on quadratic dependence of computation and memory on the input size, and is efficient to be applied to high resolution or long videos. The model was evaluated on multiple standard benchmarks, and it achieved convincing results.","This work proposed a novel drop-in block for video transformers. Different from transformers that work on image or language modality, this block aggregates information along motion paths. The key idea is to design an efficient algorithm to approximation the path (trajectory attention), which requires quadratic dependence of computation and memory on the input size. As a solution, the proposed Motionformer model uses the (1) ViT image transformer model as the base architecture, (2) the separate space and time positional encodings of TimeSformer, (3) and the cubic image tokenization strategy as in ViViT. Experimental results on four video action recognition benchmarks show Motionformer can achieve state-of-the-art performance. ",0.21568627450980393,0.29411764705882354,0.27450980392156865,0.18309859154929578,0.2676056338028169,0.36585365853658536,0.15492957746478872,0.18292682926829268,0.12727272727272726,0.15853658536585366,0.17272727272727273,0.2727272727272727,0.180327868852459,0.22556390977443608,0.17391304347826086,0.16993464052287582,0.2099447513812155,0.31249999999999994
363,SP:4d92c9383930ab9114cc9a48f34e7dd84c1c5eb5,"This paper establishes some very interesting connections between the global optimizer of the two-layer ReLU neural networks with the solution of a convex optimization program with cone constraints. More concretely, the authors construct explicit mappings between a smaller class of neural networks called minimal neural networks (that contains all the global optima) and the optimal solutions of convex programming problem. They also show that first-order methods such as SGD can find networks that can be merged to a minimal representation. Lastly, they provide an explicit path of non-increasing loss between any point on the loss landscape to the global minimizer. With this, they prove that has no spurious local minima, provided that the number of neurons is sufficiently large.","Along the line of Pilanci & Ergen (2020), this submission deals learning two-layer ReLU neural networks through convex optimization. It introduces a number of new notions such as (nearly) minimal neural networks, developing a set of interesting tools, and draws connections between the minimal neural networks and the convex optimization landscape. This paper provides a rich framework along with new analyses and solutions for learning two-layer ReLU networks through convex cone optimization. ","In this paper, the authors study the training of two-layer ReLU networks with weight decay.  A previous paper (Pilanci and Ergen 2020) introduced a convex optimization problem that corresponds to this non-convex case.  In the present paper, the authors prove that all optimal solutions of the nonconvex formulation can be found via optimal solutions of this convex formulation.  (Whereas the Pilanci and Ergen paper only constructed a single solution).  The authors additionally show that a Clarke stationary point of the nonconvex objective corresponds to a global minimum of a subsampled version of the convex problem, and they provide a polynomial time algorithm to test if a neural network is globally optimal.  Finally, the authors prove that the nonconvex loss landscape has no spurious local minima provided the number of neurons is large enough. ","The paper explores the landscape of the objective function in training a single-hidden layer neural network with ReLU activation and L2 regularization. Impressively, the paper has the following contributions:  1. It advances the results of Pilanci and Ergen from 2020 by showing that all *global* optimum points can be found via the convex program introduced by Pilanci and Ergen.  2. It shows that for a large enough width of the hidden layer (at most 2*(n+1), where n is the number of training examples) there are no spurious valleys (i.e . all local minima are global), and GD won't get ""stuck"". 3. It defines a subclass of single-hidden layer neural networks which it terms ""nearly minimal"". It characterizes  stationary points of the optimization problem by showing that every such point must be a nearly minimal neural network. 4. It gives a polynomial time algorithm for checking whether a stationary point is a global optimum. ",0.1721311475409836,0.3524590163934426,0.21311475409836064,0.2191780821917808,0.2465753424657534,0.26666666666666666,0.2876712328767123,0.31851851851851853,0.16455696202531644,0.11851851851851852,0.11392405063291139,0.22784810126582278,0.21538461538461537,0.33463035019455256,0.1857142857142857,0.15384615384615385,0.15584415584415584,0.24573378839590443
364,SP:4db61ee19769bbe7f228894c81bbddbe94fc6e47,"The current paper proposes a new algorithm that combines deep learning with the heuristic solver for solving various vehicle routing problems, TSP, CVRP, PDP, CVRPTW. The method proposes a Sparse Graph Network (SGN) that produces edge scores and node penalties, which are then used to the edge candidate set and transformed edge distance. Once the edge candidate set and the transformed edge distances are computed, a conventional /lambda-opt algorithm is repeatedly used to produce the final best routing solution. The proposed sore functions are trained with small-sized problems sampled uniformly using supervised learning and unsupervised learning. Finally, the trained model is used to solve the test problems whose node distribution is different and whose node number is large. The proposed algorithm consistently outperforms its baseline, LKH, and VSR-LKH. ","This paper proposes a machine learning approach to enhance the performance of a traditional heuristic LKH to solve routing problems. Specifically, a Sparse Graph Network is trained to select candidate edges for LKH to refine solutions. The experiments confirm that the proposed method can improve the performance of the original LKH and the VSR-LKH (based on RL) in general. ",The paper proposes a framework for solving the traveling salesman problem by combining deep learning with the heuristic Lin-Kernighan-Helsgaun (LKH) algorithm. The basic idea is to use a sparse graph network model to transform edge distance and create edge candidate sets for LKH searching trials. The experiment results show that the proposed framework improves the performance of the LKH algorithm for TSP and its variations.,"This paper proposes a neural version of the classic LKH algorithm, where the subgradient optimization of LKH is replaced by a sparse GNN that predicts edge candidates and node penalties that can directly be used by the LKH search algorithm. The edge candidate predictions are learned supervised from optimal solutions whereas the penalties are learned 'unsupervised' to push the node degrees of the 1-tree induced by the penalties towards 2 (I consider this is more like RL where the deviation from node degree 2 determines the reward). After training, the NeuroLKH algorithm gives better results than the standard LKH algorithm given equal time, for the travelling salesman problem (TSP) and different vehicle routing problems (VRPs).  As the node penalties are only used for TSP, the major contribution seems to show the empirical benefit of the edge candidate predictions. While this is not a new idea in itself, the combination with LKH is and as the empirical results are very promising, I think this paper makes valuable progress and therefore should be accepted.",0.183206106870229,0.1984732824427481,0.24427480916030533,0.43333333333333335,0.31666666666666665,0.34328358208955223,0.4,0.3880597014925373,0.18497109826589594,0.3880597014925373,0.10982658959537572,0.1329479768786127,0.2513089005235602,0.2626262626262626,0.21052631578947367,0.4094488188976378,0.1630901287553648,0.19166666666666665
365,SP:4dceeea8e4b885ce2ce4a7822c4bc5fc672b4368,"This paper focuses on comprehensive knowledge distillation to transfer the class representation. The proposed method aims to improve the performance of the student model by considering the transferring of class representation. It is a novel view, especially with a causal intervention approach. The experiments are conducted to demonstrate the advantages of the proposed method.","Authors have proposed a novel framework (CID) for knowledge distillation that is able to capture sample and class representations and reduce knowledge bias induced by the context prior through causal intervention. There are two major differences between the proposed approach and previous work: first, CID is able to transfer class representations. Second, it uses soften logits as sample context information. Authors have provided extensive experimental results to show the effectiveness of their approach.","The paper addresses an interesting concern : how can we transfer knowledge from a large model (master) to a tiny one (student) taking into account possible bias in the learning process of the teacher The framework considered is that of  image classification, where the context contained is the image can yield some bias in the classification task (for e.g. garden when distinguishing a dog from a cat) The paper considered two kinds of knowledge transfer * one may want to transfer representation of each class * one may also want to transfer the classifier in itself  The approach proposed in this paper to circumvent the bias problem is based on causal  intervention.  The claim is that there is an underlying graph where the output of the classifier may have as parent in the graph, both the true representation of the object to identify and the context The idea is to perform causal intervention to remove the influence of the context in the classification Numerical experiments are performed in the image context. The task is object clasiffication and several classical datasets are considered","This paper proposes an improved knowledge distillation process named Comprehensive, Interventional Distillation (CID). It adds a class representation term to the loss function of a knowledge distillation model to enforce the student and the teacher to learn not only similar sample representations but also class representations, where a class is represented by the similarity matrix among all of its samples. It further modifies the loss function term of the student to remove the bias learned by the teacher from the context information in training data, based on casual intervention. Experimental results on four real datasets confirm the effectiveness of the proposed model in improving the classification accuracy of the student model. ",0.2222222222222222,0.3888888888888889,0.3888888888888889,0.2328767123287671,0.2465753424657534,0.16666666666666666,0.1643835616438356,0.11666666666666667,0.1891891891891892,0.09444444444444444,0.16216216216216217,0.2702702702702703,0.1889763779527559,0.17948717948717952,0.2545454545454546,0.13438735177865613,0.1956521739130435,0.20618556701030927
366,SP:4e5c91af1c6f40ab3a103ff939bc043fb48db02c,Building on the work by Nyugen et al. 2020  the authors examine the block structure phenomenon where subsequent layers of a neural network have very similar internal representations. They show that a small number of datapoints load very highly to the dominant principle components of the block structure.  They also examine the dynamics of the block structure and the fact that it changes through training. Finally the authors show that some regularizations such as Shake-shake improves the phenomenon. ,"The study of (Nguyen et al., 2020) reported that wide and deep neural networks tend to have similar feature representations across layers (called ""block structure""). (Nguyen et al., 2020) also revealed that the block structure is essentially induced by the first principal component (PC) of the representation in each layer.  This paper provides more detailed empirical analysis of the block structure. To measure the similarity, the authors used KCA (with linear kernel) as the similarity measure, which was also used in (Nguyen et al., 2020). This paper reported that * The dominant datapoints, which lie in the direction of PC, share some common image patterns such as background colors. * The dominant datapoints are different across different model initialization. * Although the block structure emerges in the very first stage of the training, the dominant data points differ between the beginning and at the end of the training. * The block structure can be suppressed by modifying training, e.g., by regularizing PCs, by using Shake-Shake regularization, and by using transfer learning.",This paper studies the block structure phenomenon in training large capacity neural networks. The paper provides an explanation that the phenomenon is caused by dominant data points. The paper also investigates methods to mitigate this issue. ,"Authors study phenomenon in large-capacity neural networks which contain blocks of contiguous hidden layers with highly similar representations. The block structure has two seemingly contradictory properties: on the one hand, its constituent layers have highly similar dominant first principal components (PCs), but on the other hand, their representations, and their common first PC, are highly dissimilar across different random seeds.   The authors investigated the origin of this block structure with the data and training methods. They found that the block structure arises from dominant datapoints - a small group of examples that share similar image statistics (e.g. background color). ",0.34177215189873417,0.12658227848101267,0.22784810126582278,0.07692307692307693,0.16568047337278108,0.3055555555555556,0.15976331360946747,0.2777777777777778,0.18,0.3611111111111111,0.28,0.11,0.217741935483871,0.1739130434782609,0.2011173184357542,0.12682926829268293,0.20817843866171007,0.16176470588235295
367,SP:4e74c00e9f2d06425a26c000c0b9078c2a32d2ca,The authors proposed an effective meta-regularization algorithm (MetaProx) for meta-learning. A learnable proximal regularizer with deep kernel is introduced to learn the base learner. The computation complexity of meta-gradient is linear in the number of meta-parameters. Experiments on synthetic dataset and mini-ImageNet show that the proposed method outperforms the popular few-shot learning methods.,"This paper proposes a new meta-learning algorithm. Following the meta-regularization method, this paper proposes a new proximal regularizer instead of the traditional regularizer to reduce the computational complexity significantly. The algorithm is reasonable and interesting which also shows high performance in experiments.","This paper introduces a new functional regularisation for meta learning. Often times meta learning algorithms can overfit to the training datasets and hence an introduction of a regularisation in terms of the outer loop parameters is a sensible choice. In particular, they build upon the Common mean algorithm and consider the dual form in which they can derive closed form solutions to the optimisation problem. Their main contribution is that the method remains computationally cheap, competitive in terms of performance and provide some theoretical insights into their proposed method MetaProx.","The authors proposed a kernelized proximal regularization method (MetaProx) for meta learning. MetaProx combines deep kernel and proximal meta regularization, allowing learnable regularization by reformulating the proximal problem in dual space. Thorough theoretical analysis are provided as well as empirical studies on regression and classification task.",0.2033898305084746,0.23728813559322035,0.23728813559322035,0.3409090909090909,0.1590909090909091,0.1,0.2727272727272727,0.15555555555555556,0.30434782608695654,0.16666666666666666,0.15217391304347827,0.1956521739130435,0.23300970873786409,0.1879194630872483,0.26666666666666666,0.22388059701492535,0.15555555555555559,0.1323529411764706
368,SP:4e7c541b13fa37663a0a5d82fd1b375b86f46210,"The paper proposes *DeDLOC* framework to enable distributed Deep Learning in open collaboration by leveraging the computing resources in a volunteering user setup. The framework adapts to the available hardware and communication network to maximize training throughput and accordingly behaves as PS  [33], AR-SGD [34], decentralized SGD [35], BytePS [36], or a combination of them in hybrid mode. The experiments are done with real training collaboration across 49 volunteers for Bengali language modeling using ALBERT-large. This is a first-of-its-kind volunteering effort for collaborating training. Additionally, the proposed framework is also applied to self-supervised tasks under 3 compute setups (WORKSTATION: homogeneous low bandwidth, SERVER: homogeneous high bandwidth, and HYBRID: heterogeneous) ","This paper attempts to deploy distributed training of deep neural networks over an open collaborative distributed environment, where participators can dynamically join the computational task.  The approach is mainly based on modification of data parallel training to overcome the technique challenges introduced by the collaborative environment.  Both standard and real-world pre-train benchmarks are included to evaluate the proposed system. ","The authors studied an important topic for many potential future applications in this paper.  Collaborative training means different users in different physical locations share their computing resources to train a deep learning model together.  Otherwise, a single user does not have enough computing resources to finish the training.  Overall, this paper is well written. This could be an important solution for the unbalanced geographic computing resources distribution.",The paper discusses a distributed training setup where multiple small institutes/groups pool computational resources together for training ML models collaboratively. The paper focuses on two key problems: (1) maintaining consistent training outcomes under dynamic composition of participants. (2) determining the communication strategy adaptively based on dynamic participants & network conditions.,0.11304347826086956,0.12173913043478261,0.0782608695652174,0.13114754098360656,0.14754098360655737,0.11940298507462686,0.21311475409836064,0.208955223880597,0.18,0.11940298507462686,0.18,0.16,0.14772727272727273,0.15384615384615383,0.1090909090909091,0.125,0.16216216216216217,0.13675213675213677
369,SP:4e9ae183a0ba3e56268f0a1fb1e52a2dfdeed4e4,"This paper proposes a new computation block that unifies 3D convolution and transformer block for video action recognition. Starting from the transformer block, the core of the proposed block is its attention module which is equivalent to 3D CNN or transformer block depending on the design of the learnable parameters. On the standard action recognition benchmark, the proposed method outperforms prior art in both accuracy and computation efficiency. The comprehensive ablation studies provide insights on the design choices.",This paper proposes a new transformer model for video understanding task. The proposal algorithm has the benefit of both 3Dconv to efficient capture local context and transformer for global reasoning. The other innovation is the dynamic position encoding. The proposed method is validated on Kinetics and Something something where it set new accuracy record.,"The paper proposes a UniFormer architecture, which combines 3D convolutions and spatiotemporal self-attention for efficient and effective video classification performance. Specifically, the authors show that using local 3D convolution-like layers in the early stages of the network and global self-attention layers in the later stages of the network provides a good tradeoff between efficiency and accuracy. The proposed method reports state-of-the-art results on several major action recognition benchmarks.","In this paper, a new architecture, Uniformer, is proposed to learn spatial-temporal pattern in videos.  The new architecture is claimed to effectively aggregate both local information and global information. In contrast, the previous two main approaches, 3D CNN and vision transformer, can only take care of one aspect. The main building block for Uniformer is a Multi-Head Relation Aggregator, which behaves differently at shallow layers and deep layers, to aggregate local and global information respectively. Extensive empirical studies demonstrate the strength of the proposed method.  ",0.24358974358974358,0.2564102564102564,0.23076923076923078,0.2962962962962963,0.2777777777777778,0.21621621621621623,0.35185185185185186,0.2702702702702703,0.20689655172413793,0.21621621621621623,0.1724137931034483,0.1839080459770115,0.2878787878787879,0.2631578947368421,0.21818181818181817,0.25,0.21276595744680854,0.19875776397515527
370,SP:4eb627158a138aea44106d61a6ed97b9eb020686,"In this paper, the authors propose an efficient regularizer for the trace of the Hessian. To circumvent the extra computation needed to do the naive Hutchinson estimator (which pretty much amounts to taking *three* derivatives of the loss), the authors subsample the weights matrices, and then a second time internal to each weights matrix). Experimental results on CIFAR-10 and CIFAR-100 are presented: the proposed regularizer helps, but unfortunately not in a statistically significant way.","The authors propose a method for estimating the trace of the Hessian of the cross-entropy loss with respect to the weights of a neural network classifier. They suggest adding the trace estimator as a regularizing penalty term for training neural networks with improved generalization.  The authors give two theoretical motivations for regularizing the Hessian trace: - For linear models the trace of the Hessian of the cross-entropy loss with respect to the logits appears as a factor in a term for bounding generalization error. The authors draw a connection between the Hessian with respect to the weights and the Hessian with respect to the logits and argue that penalizing the trace of the Hessian with respect to the weights also leads to a tighter bound on the generalization error. - For the continuous gradient dynamics around the optimum the Hessian eigenspectrum is the negative eigenspectrum of the Jacobian of the gradient dynamics which determine the stability of the dynamic system. The authors argue that decreasing Hessian trace increases Jacobian trace and hence reduces data dependend stability which will avoid overfitting.  The authors' suggested method for estimating the trace uses the stochastic Hutchinson trace estimator which samples a random vector sigma with E(sigma sigma^T) = I to get E(sigma^T H sigma) = tr(H). The authors appear to use back-propagation through a back-propagation gradient approach to compute sigma^T H sigma by first computing the gradient g = dl/domega of the loss l with respect to the weights omega, and then computing the gradient of the inner product g^T sigma which is a scalar, with respect to the weights again. This gradient-over-gradient can be inner product multiplied again with sigma.  In order to save cost the authors suggest a drop out method setting a fraction of the terms of sigma to 0 which saves a fraction of the derivative computation and correspondingly drops terms of the trace sum.  The authors then present experimental results evaluating their suggested regularization method in combination with and compared to other regularization methods on - CIFAR10 / Resnet18 - CIFAR100 / Wide Residual Network - WikiText-2 / 2-layer LSTM","The authors propose a regularizer for training DNNs - specifically adding the trace of the Hessian w.r.t. the parameters of the model. They present two efficient stochastic estimators of the trace, based on the Hutchinson method and their own extension to it, specific to NNs.  The authors also provide a motivation of the regularizer through a known generalization bound and through the perspective of dynamical systems stability.  In three experiments - on CIFAR10, CIFAR100 and Wiki-Text they present results suggesting that the proposed method is performing better on average than other techniques.  ","The paper develops a new regularization method for deep neural networks.  The proposed methods penalizes the trace of the Hessian.  The paper adapts Hutchinson method for estimating trace of a positive semi-definite matrix for the purposes of estimating trace of the Hessian.  This adaptation uses a dropout mechanism to efficiently compute trace of the Hessian.   The paper also studies the effects of minimizing the trace of the Hessian using linear dynamical systems theory, and shows that lowering the trace of Hessian diminishes the stability of the equilibrium points in the parameter (i.e., weight) space.  Thereby, reducing overfitting and improving generalizability of the network.  The paper concludes with a set of experiments on standard deep learning benchmarks---CIFAR-10, CIFAR-100, and WIKI-TEXT2---and in almost every case the proposed SEHT-D scheme achieves best results. ",0.39473684210526316,0.2894736842105263,0.3157894736842105,0.10112359550561797,0.13764044943820225,0.3225806451612903,0.08426966292134831,0.23655913978494625,0.17391304347826086,0.3870967741935484,0.35507246376811596,0.21739130434782608,0.1388888888888889,0.2603550295857988,0.22429906542056074,0.16035634743875277,0.19838056680161945,0.2597402597402597
371,SP:4eb8b0e106c1cad039dd7be693bacbe15b60c001,"This paper samples from distributions with complex energy landscapes by the powerful contour stochastic gradient Langevin dynamics (CSGLD) methods. In order to reduce the variance of random-field function, this paper proposes a simple but effective method, that combines  random-field functions from multiple chains. Since the parameter $\theta$ typically has small size, the communication overhead is marginal. This paper show that compared to CSGLD which only has one chain, the variance ICSGLD with P chains and same computation budget is asymptotically more efficient. The authors evaluate ICSGLD with multiple experiments.","The paper proposes an MCMC algorithm containing two key ideas: the approximation of the target density with a simpler function (proposed by [Deng, 2020]) and the parallel simulation of many chains that allow for better capturing the global properties of the target density. To be more precise, the main contribution of the paper is the extension of the algorithm by [Deng, 2020] to the case of multiple chains.   The proposed extension goes as follows. The original algorithm by [Deng, 2020] approximates the target density with piecewise continuous functions using the level sets (slicing the values of the energy function). These functions are parameterized by vector $\theta$, which needs to be defined. In practice, $\theta$ is estimated using the samples from the chain. The current paper then proposes to run many chains in parallel and use all of the available samples for the estimation.  The efficiency of the proposed technique is analyzed both theoretically and empirically. For theoretical analysis, the authors provide convergence speed and asymptotic distribution of the residuals $\theta - \theta^*$. Overall, the analysis favors the proposed method over the original algorithm. The empirical comparison is made on several tasks. Except for the toy task, the authors consider learning policy for contextual bandit problem and sampling from the posterior distribution of a neural network.  [Deng, 2020]: Deng, Wei, Guang Lin, and Faming Liang. ""A Contour Stochastic Gradient Langevin Dynamics Algorithm for Simulations of Multi-modal Distributions."" arXiv preprint arXiv:2010.09800 (2020).",The paper proposed an interacting contour stochastic gradient Langevin dynamics that extends the contour stochastic gradient Langevin dynamics with efficient interactions in Deng et al. (2020). They obtain asymptotic normality as well as show that this proposed algorithm can be more efficient than  the contour stochastic gradient Langevin dynamics with efficient interactions. The theoretical results are complemented by the numerical experiments.,"This paper addresses the problem of sampling from distributions with complex energy landscapes. The authors propose an extension of the contour Stochastic Gradient Langevin dynamics (CSGLD) sampler to efficiently simulate from big-data distributions. The extension is called “interacting” since $P$ different CSGLD are updated simultaneously and the random-field function is obtained as a Monte-Carlo average over the random field functions of these multiple chains run in parallel. This procedure allows to reduce the variance of the self-adapting parameter $\theta$ improving the marginal energy likelihood estimate, and is well-suited for distributed computing. The different chains only share the low-dimensional vector $\theta$ and do not share the model parameters, that are typically high dimensional. The authors prove that the proposed algorithm is asymptotically more efficient than its single-chain counterpart with the same computational budget. This theorem holds for a step size that decreases polynomially in time. Finally, they compare their algorithm to alternative methods on different tasks (e.g., mode exploration on MNIST dataset, uncertainty estimation) achieving competitive performances. ",0.21978021978021978,0.16483516483516483,0.42857142857142855,0.07024793388429752,0.1487603305785124,0.29508196721311475,0.08264462809917356,0.2459016393442623,0.22413793103448276,0.2786885245901639,0.20689655172413793,0.10344827586206896,0.12012012012012013,0.19736842105263155,0.2943396226415094,0.11221122112211222,0.17307692307692307,0.15319148936170213
372,SP:4f0923c7474877fc1571e0cfa1c4ff9c718793a8,"This paper presents that Visual Transformers (ViTs) are not sensitive to patch transformations (e.g. shift, rotation). It thus presents an idea that patch transformations can be used as negative augmentations to train the Visual Transformers (ViTs) to improve robustness similar to human perception. Three losses are thus applied to regularize the model training from being too confident to patch transformed images. A number of experiments on ImageNet-series datasets are presented to prove the finding of the authors.","This paper draws motivations from the observations that ViTs are insensitive towards patch-based transformations. Through detailed analysis by evaluating the models on such transformed inputs, the authors find a relatively stable accuracy, and term such features as **useful** but **non-robust** feature.  In order to push ViTs from these undesired features, patch-based negative augmentation and losses are proposed, which consistently improve the robustness of ViTs. ","This paper empirically investigates robustness properties of vision transformers (ViT) towards image augmentation strategies that destroy the semantic meaning of the image. The authors experimentally demonstrate that ViTs achieve high test set accuracies on ImageNet where the test data was transformed with patch-based transformations (I refer to these collectively as P-corrupted) which render the images unrecognizable to humans. They show that by emphasizing this property of ViT, it leads to detrimental effects to generalization to out-of-distribution (OoD) datasets.  As a remedy, the authors introduce several data augmentation techniques, termed negative augmentation, to alleviate ViTs tendencies to learn features that are robust to patch-based transformations Extensive experimental results on ImageNet-(1k, A, C, R) ablate the proposed approach to extract the most effect hyperparameters and demonstrate the effectiveness of the proposed method","This paper investigates the robustness of vision transformers. They first found that ViTs heavily use features that survived patch-based transformations but are generally not indicative of the semantic class to humans. However, these features are non-robust and the authors propose to use the images transformed with our patch-based operations as negatively augmented views and offer losses to regularize the training away from using non-robust features. Experiments on ImageNet show that patch-based negative augmentation consistently improves the robustness of ViTs.",0.189873417721519,0.26582278481012656,0.26582278481012656,0.2835820895522388,0.3582089552238806,0.19117647058823528,0.22388059701492538,0.15441176470588236,0.25,0.13970588235294118,0.2857142857142857,0.30952380952380953,0.20547945205479454,0.19534883720930232,0.25766871165644173,0.18719211822660098,0.3178807947019867,0.23636363636363636
373,SP:4f9f748b4753af91cb8cd9b9ce3bfa85be632bb4,"The paper presents a theoretical analysis of a 3 layer neural network with ReLU activations. The main result is introduction of adaptive generalization bounds that are defined for a modified SGD algorithm. These bounds are robust to noise and data dependent.   The idea is to define a product kernel for the second layer of the form $K^{\infty}\odot G$, where $$K^{\infty}$ is fixed and $G$ is adaptive and can be modified so that $g_k$ has minimum possible norms. This leads to a definition of a complexity measure that together with a simple modification to the SGD algorithm defines a bound on the population risk.","This paper makes contributions into two main category:  1. Algorithmically, it propose a projected SGD algorithm that can, in polynomial number of iteration, reaches a solution that generalizes. 2. In terms of generalization, it proposes a new generalization bound with data-dependent complexity measure that goes beyond NTK regime. Another generalization bound, based on a new function norm (minimum RKHS norm w.r.t. a family of kernels) is proposed. The proposed bound shows *adaptivity* nature of the generalization results.   ","The paper introduces novel results on the optimization and generalization of three-layer networks, optimized with a variant of gradient-descent. The goal of the analysis introduced in the paper is to obtain bounds that improve over the bounds obtained using the well-studied NTK framework. Namely, the aim is to go beyond the ""lazy training"" regime to allow for better bounds on learning more complex functions/distributions.","This paper studies the behavior of a three-layer neural network trained by a projected SGD algorithm. The authors theoretically show that, in this setting the neural network has more complicated behavior than in the NTK regime. Instead of sticking at one kernel, now the neural network explores different kernels decided by the features learned by the first layer. Therefore, the generalization error is bounded by a minimum of many RKHS norms, and is proven to be smaller than the single NTK norm. The proposed generalization error bound outperforms the NTK bound. For target functions in the NTK, the proposed bound is no bigger than the NTK bound. The model and the algorithm are close to commonly used ones, while there are components specially designed for the theoretical analysis. ",0.14953271028037382,0.14953271028037382,0.19626168224299065,0.1,0.2625,0.29411764705882354,0.2,0.23529411764705882,0.16279069767441862,0.11764705882352941,0.16279069767441862,0.15503875968992248,0.1711229946524064,0.18285714285714286,0.17796610169491528,0.1081081081081081,0.20095693779904306,0.20304568527918782
374,SP:5037990691def01efd9914016c964437fa21af3d,"The paper presents a new benchmark based on a referential communication game (where a speaker has to communicate the stimuli they observe to the listener and the listener has to find the right stimuli among distractors) to test for compositional learning behaviors. The stimuli vary over differently structured semantic spaces (every dimension can have different numbers of semantic types). Models are trained on a series of meta-referential games, where each game has a different semantic distribution structure (different number of semantic types) and are evaluated on zero-shot performance on a referential game with a new semantic structure. To represent the stimuli, the authors propose a continuous vector representation for different referents instead of using a one-hot embedding. Finally, the authors show how commonly used deep learning architectures fail on their benchmark, posing a challenge to the community.",This paper studies the problem of learning compositional behaviors through referential games. The authors first introduce the problem of fixed shape representations that are not suited for continual learning with infinitely many tasks. They propose a continuous representation of discrete stimulus by discretizing a small interval and randomly creating gaussian distributions at each interval. They then define the compositional learning problem using meta-reinforcement learning where a set of new tasks are introduced over time and two agents communicate to develop a compositional language. The authors present some empirical results where DNC and LSTM based agents are not well tailored to solve the problem and exhibits very low accuracy while the LSTM based agent is able to reconstruct an input stimulus successfully. ,"In this paper, the authors propose a new benchmark to investigate state-of-the-art artificial agents' abilities to exhibit compositional learning behaviors. The authors propose a Symbolic Continuous Stimulus (SCS) representation and cast the problem of learning compositional behaviors as a meta-reinforcement learning problem. The authors compare state-of-the-art RL methods on solving the single-agent task of learning compositional learning behaviors.  ","This paper proposes to investigate meta-learning to generalize compositionally. This is an important challenge. The paper proposes a meta-learning setting consisting of reference games, which are a classic paradigm for investigating communication. The paper proposes a hybrid symbolic-continuous stimulus representation, and explores how different agents use this representation scheme within their paradigm. ",0.14285714285714285,0.15714285714285714,0.10714285714285714,0.1885245901639344,0.10655737704918032,0.19696969696969696,0.16393442622950818,0.3333333333333333,0.2727272727272727,0.3484848484848485,0.23636363636363636,0.23636363636363636,0.15267175572519082,0.21359223300970873,0.15384615384615383,0.24468085106382978,0.14689265536723164,0.21487603305785122
375,SP:50b9923168f34484f8b21da475d704ef2fa6a2f3,"The paper provides a method for link prediction based on random walk based pooling method (WalkPool). It aggregates the probabilities of the adjacent structure of a node or link to design the node embeddings. Finally, it shows superiority of the method against baselines via AUC and AP.","This paper proposes a neural architecture for predicting missing links in a graph by designing a so-called WalkPool scheme. WalkPool exploits node features (from a GNN) to compute the transition probability between the nodes, and then manually design a number of links that are extracted from the oder-k encolosing sub-graphs covering a pair of nodes for link prediction. The authors also defined a delta-graph feature that is the sub-graph level feature difference before and after removing the node pairs with which the link is to be predicted. Experimental results are quite optimistic on about 10 benchmark datasets.","The paper proposes a new link prediction method called WP, based on treating link prediction as a subgraph classification problem, where the link of interest is surrounded by the subgraph. Existing GNN architectures could be used for learning node embeddings. With node embeddings, a latent subgraph could be built using self-attention, where attention scores are used for computing the subgraph edge weights. With the latent subgraph, a set of features could be computed to be used as the features for the link of interest. The features will be used for subsequent link prediction. Essentially, the proposed WP method is a feature extractor for links (or its surrounding subgraph).","The paper introduces WalkPool, a model for link prediction that relies on GNNs. The main idea is to jointly encode node representations and graph topology information into node features which could be learned in an end-to-end-manner. The proposed problem formulation is interesting: the transition probabilities to capture proximity among nodes are computed on top of a latent graph which is the outcome of an attention mechanism on node features. The proposed methodology is presented clearly. Besides, the authors have performed experiments on various graphs, and the performance of WalkPool is compared against state-of-the-art baseline models.",0.2553191489361702,0.3404255319148936,0.3404255319148936,0.18627450980392157,0.19607843137254902,0.2018348623853211,0.11764705882352941,0.14678899082568808,0.15841584158415842,0.1743119266055046,0.19801980198019803,0.21782178217821782,0.1610738255033557,0.20512820512820515,0.21621621621621623,0.18009478672985785,0.19704433497536944,0.20952380952380953
376,SP:50ea967a02e2fb51d908d37d7e584e31efbcca73,"This paper presents three results concerning the universal approximation of functions on compact sets under a width regime for which universal approximation in general is not possible. The authors make a convincing case that this represents an important frontier both for applications and for a better understanding of what neural networks can represent. While I appreciate the first result, I am confused with the second and I do not think the third is relevant.  These results are the following:  1) The maximum principle  This is an interesting observation about the maximum (and minimum) value achieved by neural network with continuous monotonic activation and width bounded by the input layer on a compact domain is always at the boundary of such domain. Trying to break their result made me understand and agree with their claim that this could be understood as a ""root cause why universal approximation with functions from NNσ(n0) on arbitrary compact sets is impossible"".  2) The result on disjoint domains  In Section 4, the authors claim through Theorem 2 a result applicable to a ""finite collection of disjoint n0-dimensional compact sets"". Notably, most of their work there focuses on exact representation, which reflects the fact that they are considering functions that are constant on each disjoint set. We can reasonably associate that to classification tasks, but then we cannot assume any probabilistic meaning with something like a softmax layer.  3) The result on harmonic activations  The authors shows that universal approximation can be achieved on a finite and discrete domain with the composition of cosine functions. This result lies entirely on the domain being discrete and finite, which actually breaks any dependence to the width of the input layer. For that reason, I do not think that it is that relevant because it has nothing to do with the main theme of the paper.","The paper focuses on approximation of NNs in the narrow regime, i.e, when the width of the network less than the input dimension ($n_0$) and asks the question: What compact subsets M of $R^{n_0}$ still allow universal approximation or exact interpolation for all functions that map from M to R.?  It first shows the existence of compact subsets that do not allow for such universal approximation guarantee.  Then the paper presents sufficient conditions on certain compact subsets so that some universal approximation/exact interpolation guarantee can be attained and these results are targeted for clustering/classification applications. The functions here are piecewise-constant on disjoint compact sets.  The assumptions on the activation functions are general which includes most of the commonly used ones in practice as well.","In the mathematical theory of neural networks, universal approximation theorems typically establish the density of a class of neural networks within a certain function space. In recent years, it has been shown that a width larger than the input dimension is needed to allow universal approximation theorems for continuous function spaces defined on arbitrary compact sets.  In this paper, the authors investigate what kind of subsets of R^n allow for a universal approximation theory with neural networks that have a width <= input dimension.   ","The paper studies the question of expressivity of neural networks that have restricted width. Specifically, the main question is ""what happens when the maximum (among all layers) width is less or equal to the input dimension?""   From prior works, it is known that such width-restricted networks cannot possibly express all continuous functions on arbitrary compact sets. So the next natural question is what kind of functions/subsets M of R^n are amenable to approximation via such networks?  The main contribution is the derivation of certain topological conditions on compact sets M in R^{input dimension} that allow or exclude universal approximation by such networks.   First, the authors derive a maximum principle stated as Theorem 1: If M is a compact subset of R^{input dimension}, then a neural network of width at most the input dimension, equipped with any continuous monotonic activation function  will necessarily attain its maximum value on the boundary of M. As a consequence, the authors show how previous lower bounds on width (w\ge n_0 +1) are tight by providing several tight examples.  Second, the authors investigate the case where the domain consists of two disjoint compact sets and the goal is to approximate functions that take constant values on each of these sets. They give a sufficient condition under which shallow ReLU nets of width $w=n_0$ (the input dimension) and depth 4 can expreess them.  Third, they also consider some expressivity properties of simple networks with cosine activations that have width only 1 and depth 3.  ",0.12012987012987013,0.10064935064935066,0.1590909090909091,0.16030534351145037,0.3053435114503817,0.40476190476190477,0.2824427480916031,0.36904761904761907,0.19140625,0.25,0.15625,0.1328125,0.16856492027334852,0.15816326530612246,0.17375886524822695,0.19534883720930232,0.20671834625323,0.2
377,SP:51cbdb4594ed73c264dda1380931e343047a7f42,"This work considers learning NLDS. Previous works can only learn mixing systems with sub-gaussian noise in the offline setting, with sample complexity depending on the mixing time. This work improves existing results in several ways, assuming the link function is expansive: 1, it's shown that the offline Quasi Newton Method achieves near-optimal convergence rate for non-mixing systems. 2, the offline Quasi Newton Method still achieves near-optimal convergence rate with heavy-tail noise, assuming mixing systems. 3, a new algorithm SGD-RER is introduced, which achieves near-optimal convergence rate in the online setting. 4, an exponential lower bound is proven for the ReLU link function, showing the expansivity assumption is necessary.","The authors study a very special case of ""non-linear dynamical systems"" where there is no hidden state and there is some expansive, component-wise ""link function"" \phi in the evolution X(t+1) = \phi(AX(t)) + noise. For this very special case, they show both off-line and on-line algorithms. They also show lower bounds on the sample complexity of learning systems with with non-expansive link functions (such as ReLU).  ","This work has four major contributions: a) authors provide the first offline algorithm that can learn non-linear dynamical systems without the mixing assumption, b) authors significantly improve upon the sample complexity of existing results for mixing systems, c) in the much harder one-pass, streaming setting authors study a SGD with Reverse Experience Replay (SGD − RER) method, and demonstrate that for mixing systems, it achieves the same sample complexity as our offline algorithm, d) authors justify the expansivity assumption by showing that for the popular ReLU link function — a non-expansive but easy to learn link function with i.i.d. samples — any method would require exponentially many samples (with respect to dimension of Xt) from the dynamical system.   This paper is comprehensive with both solid theoretical analysis and simulations.  ","The authors consider non-linear dynamical systems of the form $X_{t+1} = \phi(A^* X_t) + \eta_t$, where $\eta_t$ is a noise term, $A$ is a $d \times d$ matrix and $\phi$ acts component-wise, and provide some results relating to the estimation of $A^* $ under certain assumptions. A key assumption is that $\phi$ is ""expansive"", which I understand to be a sort of converse Lipschitz condition. (Since 1-Lipschitzness is also assumed, this means that $\zeta |x - y| \le |\phi(x) - \phi(y)| \le |x - y|$ for some $\zeta \in (0, 1)$. I wasn't sure why this was called ""expansive"", because it seems to me more to be a limit on contraction than expansion per se, but this seemed understandable.) Under these conditions, they provide:  1. An algorithm that, given sub-Gaussian noise, can recover $A^*$ with squared-norm error scaling in $O(d^2/T)$ (though I have questions about this, see main review). 2. An algorithm that, given system stability and finite-fourth-moment noise, can recover $A^*$ with squared-norm error scaling in $O(d^2/T)$ (though I have questions about this too). 3. A _streaming_ algorithm that, given sub-Gaussian noise, system stability and twice-differentiable $\phi$, can recover $A^*$ with squared-norm error scaling in $O(d^2/T)$. 4. A negative result with the non-expansive ReLU function, showing a lower bound on squared-norm error in this case of $\Theta(e^{cd}/T)$, implying a need for exponentially many samples to recover $A^*$ satisfactorily. ",0.07758620689655173,0.23275862068965517,0.1724137931034483,0.2054794520547945,0.3287671232876712,0.17557251908396945,0.1232876712328767,0.20610687022900764,0.07782101167315175,0.11450381679389313,0.0933852140077821,0.08949416342412451,0.09523809523809525,0.21862348178137653,0.10723860589812333,0.14705882352941174,0.14545454545454545,0.11855670103092783
378,SP:51d77e272404c908996a8896a61811679a571de0,"With online learning for multiclass logistic regression, its regret has a trade-off against the computational cost. For example, ONS can be computed in $O(n)$ time ($n$: number of instances) while the regret is $O(e^B \log n)$: exponentially increased against the radius of regret computation $B$. [Foster 2018] can provide the regret $O(\log(Bn))$ but requires $O(n^{37})$ time. The proposed method can provide another trade-off between them, and achieved the regret $O(B^2 + B\log n)$ with the computational cost of $O(n^4)$. This is much faster than [Foster 2018] with acceptable computational cost.","This paper considers the online convex optimization setting with mixable losses. The exponential weights algorithm is known to have constant regret with mixable losses, but it is also known to have prohibitively high running time for certain mixable losses. A mixable loss function of interest is the logistic loss used in logistic regression, for which faster algorithms such as Online Newton Step (ONS) suffer a double exponentially larger regret bound compared to exponential weights.  As a solution the authors propose a mix between ONS and exponential weights called GAF. To update, the algorithm computes a quadratic surrogate loss akin to the surrogate loss used in ONS, but uses the actual hessian rather than an approximation to the hessian to compute the surrogate loss. Another interesting trick that the authors use is that the point around which the quadratic surrogate loss is defined: it is the minimizer of the $l_2$ regularization plus the loss of the previous round and the surrogate losses of all rounds preceding the previous round.  The predictions of the algorithms are the same predictions that would be used for exponential weights. While for arbitrary priors this would pose a computational problem, the algorithm uses a gaussian prior. As is observed in prior work, with a gaussian prior and quadratic losses the posterior of exponential weights is also a gaussian. This allows the authors to devise more efficient predictions.   ","In this paper, the authors consider some online prediction/convex optimization problems, where it is assumed that the losses satisfy a mixability condition, as well as additional assumptions inspired by logistic regression. While these problems can be addressed by Vovk's aggregating algorithm, the resulting procedure would be computationally expensive and involved. Efficient and more direct procedures have been proposed for least-squares regression by Vovk and Azoury-Warmuth (VAW), and more recently for (binary) logistic regression by Mourtada and Gaiffas (MG) and Jezequel, Gaillard and Rudi (JGR). The present paper sets out to extend these constructions; the considered losses are assumed to be mixable, and satisfy conditions such as smoothness and a (weakened) self-concordance satisfied by logistic regression (lines 215-223). A procedure for this setting, called Gaussian aggregation forecaster (GAF), is proposed.  The main outcomes are the following: - In Sec 4.1, for linear regression the authors recover the VAW procedure and its guarantee. [However, there seems to be a possible issue with this derivation.] - In Sec 4.2, the authors consider multi-class logistic regression, and obtain a procedure and guarantee for this problem. These extend those of MG and JGR for binary logistic regression to the multi-class setting. They also go beyond what can be achieved by procedures such as online gradient descent, online Newton step or follow-the-regularized-leader. This is the main result of this work.",The authors propose a new computationally efficient algorithm for multi-class logistic regression. The propose method is based on a tight quadratic approximation of the loss function (extended from Lemma 5 of Jezequel et al.) and utilizing the mixability of the logistic loss (and other properties) to compute the prediction like AA of vovk. They show that their regret bound does not have the infamous exponential constant while being computationally efficient (n^4). ,0.21359223300970873,0.17475728155339806,0.14563106796116504,0.15948275862068967,0.10344827586206896,0.08085106382978724,0.09482758620689655,0.07659574468085106,0.2054794520547945,0.1574468085106383,0.3287671232876712,0.2602739726027397,0.13134328358208955,0.10650887573964496,0.17045454545454544,0.15845824411134904,0.15737704918032785,0.12337662337662339
379,SP:5228f4c3795aee827c4c59209b760b5b302d32b0,"This work presents a self-supervised method for the tabular domain, where structural information is not typically available (in contrast to images/text). The proposed method suggests partitioning input rows into subsets of features, where these subsets can have a certain overlap percentage as controlled by a hyperparameter and mapped to the latent space via an encoder where they define their reconstruction loss (authors provide two options 1—reconstructing the original subset from the subset latent representation 2—reconstructing the complete row from the subset representation), an optional contrastive loss, as well as a distance loss term in the projection embedding space where the negatives are chosen from subset latent representations from other rows. Orthogonal to their work, they leverage various noise strategies commonly used in the tabular domain, such as swap noise (imputing a feature from the same column from a different row), gaussian noise, and zero imputation. The authors compare with the previous SOTA method (VIME) on various tabular datasets.","The paper proposes SubTab, a self-supervised (pre-training) method for learning good representations of tabular data. The method generates multiple views for each (unlabeled) example by selecting different (possibly overlapping) subsets of features. The model is trained with a three term loss, each averaged over all pairwise comparisons between the views -- a reconstruction term that teaches the model to reconstruct the full example from the cropped example, a contrastive term that encourages different crops of the same example to be closer together than crops of different examples (within the minibatch, infoNCE loss), and a distance term that encourages crops of the same example to be close together in L2 distance. Optionally, the input example can be corrupted by noise. After pre-training a linear classifier on the latent representation is trained using supervision. In this step, the latent embedding used is actually an average of those arising from feeding in several different random crops of the example. The authors test the method against VIME (Neurips 2020), context encoders, and denoising autoencoders on 5 datasets, which were used in the VIME paper.","This paper proposes a self-supervised framework for tabular data. The idea is to split tabular data into multiple subsets of features, embed them into a latent space, and compute a joint representation by averaging the subsets’ latent variables. The approach outperforms existing self-supervised tabular baselines and reaches performances that are on par with CNN models on MNIST.",The authors propose a novel self-supervised learning for tabular data. The authors utilize the reconstruction of the entire data using the subsets of the features as the pretext task. The authors show some consistent performance improvements in comparison to SOTA SSL in tabular representation learning.,0.24074074074074073,0.1419753086419753,0.10493827160493827,0.12087912087912088,0.1043956043956044,0.23728813559322035,0.21428571428571427,0.3898305084745763,0.3695652173913043,0.3728813559322034,0.41304347826086957,0.30434782608695654,0.22674418604651161,0.20814479638009048,0.16346153846153846,0.1825726141078838,0.16666666666666669,0.26666666666666666
380,SP:523f621ae093c652461b2322eea7a03b767779dc,"This work proposes a pipeline to learn factored world models to predict the robot actions’ effects. This work proposes to leverage a graph neural network to extract state information and adopt the contrastive loss to train the encoder and latent transition model. This paper applies the proposed approach to pick-and-place tasks with simple shapes, and extensive experiments indicate its effectiveness.","The paper proposes Object-Factored world models, a world model learned using contrastive loss, that tends to generalize over the number of objects in the scene. It also alleviates the assumption that previous world models hold about the object-action association and the author(s) propose an ""action-attention module"" that estimates the probability of an action affecting a particular object (using self-attention). Their experiments mainly focus on robotic manipulation tasks and they show generalization on unseen-test tasks with little drop in performance as compared to the training set. ","In this paper, the authors expand on previous works on contrastive learning with factored world models (specifically the C-SWM model by Kipf et al. - 2020) and apply their model for the task of predicting the dynamics of object manipulation with a continuous action space. Such previous works have shown that by leveraging residual graph neural networks (GNNs) to contextualize the learned latent states with possible actions, the model learns representations that are equivariant to permutation of objects in the scene, thus allowing scaling to a high number of objects regardless of the combinatorial explosion of the state space. Unlike previous works, where new objects are discovered by the model from raw pixels, the authors here use the simulation environment to segment objects appearing in the scene and factor them as part of the state representation. Another difference is that instead of factorizing the action space, they use two action primitives for picking and placing objects from given continuous coordinates and propose an action attention module that predicts how each action will affect each object in the next state. They claim that by departing from the standard action-object dependence assumption made by previous works, the current model can effectively model the effect of future actions for structured tasks unseen during training. They finally propose to stack multiple GNN layers instead of a single layer as mostly practiced by earlier works. The authors evaluate their methodology on two pick-and-place tasks regarding building structures of abstract object shapes or cubes. The objective is to correctly predict the effect of certain action sequences in other blocks in order to build a certain structure (e.g. pile). They sample noise from the correct action sequence in order to generate negative examples for contrastive learning. They train in some sequence configurations and test for zero-shot performance in unseen tasks (new structures of objects). They report results for both mean square error of the block position as well action ranking (how well the final state of the action sequence matches the final latent state). ","The presented work addresses the task of robotic manipulation tailored to environments containing many objects. Task planning in such environments entails large number of possible combinations of actions given the number of objects. To address this challenge, the authors propose to use an attention module in combination with a graph neural network to predict the effects of the manipulation action, limiting the objects that need to be included into the execution planning. Experiments are conducted in simulated environment using a robotic arm interacting with two sets of shapes. The model performs well when for both test scenarios, one where the model operates on a task that is was trained on, and two on a task that was not presented during training showing the generalization capabilities of the presented approach.",0.22580645161290322,0.3709677419354839,0.27419354838709675,0.4065934065934066,0.2087912087912088,0.1111111111111111,0.15384615384615385,0.06725146198830409,0.13178294573643412,0.10818713450292397,0.14728682170542637,0.29457364341085274,0.1830065359477124,0.11386138613861385,0.17801047120418848,0.17090069284064666,0.17272727272727276,0.1613588110403397
381,SP:5245bd13ccefe78b6585bcf566d68f6390d8e520,"This paper presents an architecture where the weights $P$ of a neural network’s linear layer can be determined by the input. The weight matrix $P$ of the linear layer is constructed by using its singular value decomposition $P=SEV$, with each of the columns of $S$ and $V$ determined by attending over a set of ``singular programs’’. Authors also use recurrent multi-headed attention to select multiple singular programs, using both content-based attention but also usage-based attention to improve the rank of $P$.   The authors run a range of experiments to test the architecture in a variety of settings. Starting with instance-based learning with MNIST/CIFAR, the author justifies the use of recurrent attention, and visualizes the attention pattern. The authors further justifies the use of recurrent attention with a piecewise polynomial regression sequential learning experiment, and demonstrates the attention shift at the polynomial boundaries. In the reinforcement setting, authors shows that their approach enables A3C to learn in an environment with sparse rewards. Finally, the authors demonstrate that the approach generalizes well in a multi-task learning setting, and that the approach mitigates catastrophic forgetting issues in a continual learning setting. ","This paper proposes a general-purpose neural architecture with an explicit goal of modularity, called Neurocoder. Neurocoder dynamically produces the weights for a layer in a neural network, based on the input, by combining ""programs"" from memory. These programs consist of left and right singular vectors ($U$, $V$) and singular values ($S$). For a given input, a recurrent multi-head process is used to retrieve the program from memory. In each step of the recurrent process, we obtain a query $q$ and gate $g$ for each of the $H$ heads and for each of the $U$, $V$ and $S$. The query $q$ is used for content-based attention inside $U$, $V$, and $S$ respectively. The gate $g$ is used to combine the content-attention result with a least-recently-used attention. By putting all of this together we obtain a $u_n$, $v_n$, and $\sigma_n$ for each head in each step of the recurrent process. We then sum $\sum_n \sigma_n u_n v_n^T$ to obtain the weights for the layer.  The authors evaluate this architecture on MNIST, CIFAR, RL on Atari games, and polynomial regression.","This paper proposes Neurocoder, a new general purpose neural network that “codes” itself in a data responsive way by composing relevant programs from a set of shareable, modular programs stored in external memory. Unlike previous work, Neurocoder treats modular programs as sharable datum in memory and composes relevant programs from stored “programs”. Experimental results show performance improvement in solving tasks such as object recognition, playing video games, and continual learning.   ","This paper approaches the problem of multi-purpose neural networks that can learn several tasks or several methods for computing a task with a unique approach. This approach involves representing the last portion of a neural network as a task-configurable set of weights. The way this is accomplished is by storing several neural networks in memory in a compressed format, utilizing SVD, and then using the input to produce attention weights on each of the S V and U components of the SVD, thus reconstructing the weights, adding in a non-task-specific residue representing the remainder of the weights, and then using those weights for the remainder of the network.  The paper then uses several standard benchmarks both in standard learning tasks, RL tasks, and continual learning tasks to demonstrate that their network is able to outperform similarly powerful networks that do not contain the specific properties that they have built into their network.",0.20918367346938777,0.08673469387755102,0.1836734693877551,0.09947643979057591,0.18324607329842932,0.2571428571428571,0.21465968586387435,0.24285714285714285,0.23076923076923078,0.2714285714285714,0.22435897435897437,0.11538461538461539,0.21188630490956073,0.12781954887218044,0.20454545454545456,0.14559386973180075,0.2017291066282421,0.1592920353982301
382,SP:52b857949a783a90770363c6bad0fbd69a7e0838,"This paper proposes a study on enforcing semantic consistency on VAEs. The authors show that the current VAEs are sensitive to semantical-preserving transformations and it will lower the quality of representations and generalization. To solve this issue, the authors propose a new algorithm, which first transforms a training sample into multiple semantically similar samples and then minimize the KL-divergence between the encoded distributions of original and new samples. Experiments on both 2D and 3D datasets show that the new algorithm is effective in increasing representation quality of VAEs.","The paper improves VAEs by ensuring that the learned encoder maps augmented inputs $\tilde{x}$ and unaugmented inputs $x$ to the same representations. To do so, they minimize the KL divergence between the encoder conditioned on $\tilde{x}$ and $x$. They empirically show that such consistency regularised VAE (CR-VAE) achieves very strong density modeling results.","The paper proposes a novel method of using data augmentation when training Variational Autoencoders. Given a family of semantics-preserving transformations, the idea is to enforce the invariance of latent code w.r.t. these transformations by adding a KL penalty term. This penalty improves the quality of learned latent representation and of the generative model significantly.  ","The paper  investigates a problematic behavior of current VAE models that lies in the inconsistency of current amortized encoder networks in the sense that the latent representations of semantically similar transformations (translation, rotation etc) of image data are dissimilar. Moreover, it proposes a tractable variant of the ELBO objective that is based on data augmentation and adds a KL-regularization term that penalizes the discrepancy in the posterior of the original and the transformed image data. The VAEs trained with this objective exhibit significant benefits (in terms of the utilization of the latent space and  marginal likelihood ) when compared to naive data augmentation demonstrating sota results in image generation and classification tasks.",0.2,0.15555555555555556,0.2,0.14285714285714285,0.21428571428571427,0.2982456140350877,0.32142857142857145,0.24561403508771928,0.16071428571428573,0.14035087719298245,0.10714285714285714,0.15178571428571427,0.24657534246575344,0.19047619047619047,0.1782178217821782,0.1415929203539823,0.14285714285714285,0.20118343195266272
383,SP:539ed17b77aee51c8bdc464ec29a10596af330ce,"The paper studies the phenomenon of double descent (DD), and how different parameters of the problem affect this phenomenon. First, the authors show several theoretical results on linear predictors in the over-parameterized regime, with and without label noise. These results indicate that a certain eigenvalue of the data matrix affects the generalization error. Next, several experiments are shown on the MNIST and FashionMNIST datasets, that there is a correlation between this eigenvalue and the DD phenomenon.","This paper studies the double descent (DD) phenomena for least squares and proves upper bounds for excess risk of gradient descent. For both cases with and without label noise, the upper bound involves a term $E[(1-\alpha \hat{\lambda}\_{\min}^+)^{2T}]$, where $T$ is the training time, $\alpha$ is the learning rate, $\hat{\lambda}\_{\min}^+$ is the smallest positive eigenvalue of the covariance matrix of training inputs.  The most important observation in this paper is that the peak of the double descent curve around $d \approx n$ can be related to $\hat{\lambda}\_{\min}^+$. By a non-asymptotic version of Bai-Yin limit, $\hat{\lambda}\_{\min}^+$ could be very small when  $d \approx n$, and thus $E[(1-\alpha \hat{\lambda}\_{\min}^+)^{2T}]$ can be very close to $1$ when $T$ is finite.  Experiments show that $\hat{\lambda}\_{\min}^+$ for intermediate features correlates with the double descent curve of test error.","This paper studies the double descent phenomenon of linear least squares problems from an optimization point of view. Specifically,  the contributions of the paper lie in two parts: (1) for a linear least squares problem, an upper bound for the excess risk after T gradient descent iterations is derived by a contraction property of GD. The upper bound depends on the smallest non-zero singular value of the feature matrix, which is also related to the smallest nonzero eigenvalue of the Gram matrix. (2) The concentration of the smallest nonzero eigenvalue around its population counterpart is studied, and the discrepancy of the two is controlled. The control is provided separately for under-parameterized and over-parameterized cases, using existing random matrix theories.   Combining the two components above, an upper bound of the excess risk is provided that shows a peak at the interpolation threshold. The bound depends on the number of parameters d, the number of data n, as well as the number of iterations T, hence it is optimization related. This bound only characterizes the peak at d=n, but does not characterize the first descent in the under parameterized regime.   Finally, numerical experiments on nonlinear models such as neural networks are provided to show the similar double descent phenomenon and the behavior of the smallest singular value of the feature matrix. Here, the feature is taken to be the output of the second last layer. ","This paper deals with the double descent phenomenon in the setting of least-squares regression by gradient descent. Similarly to a number of recent works, it shows a double descent behavior for the excess risk: a U shaped curve until the number of features coincides with the number of datapoints. The main contribution of the paper is to relate this peak around interpolation to the learning dynamics of the problem at hand. More precisely, the main result (Theorem 1) is an upper bound on the excess risk where the dynamics of learning are explicitly present. This result is specialized to the case of least-squares with random design in the noiseless setting (Theorem 2) and with Gaussian noise independent from the inputs (Theorem 3). In both these cases, the dominating term is controlled by the smallest non-zero singular value of the sample covariance matrix. Therefore, under some assumption on the data, one can get precise asymptotics by using the Bai-Yin result. This is done in Section 2.3 in a non-asymptotic fashion. The paper concludes by an empirical study on neural networks with one and 3 hidden layers, showing a link between the smallest eigenvalue of the covariance of the intermediary features at initialization and the test error.  ",0.2987012987012987,0.3246753246753247,0.35064935064935066,0.29605263157894735,0.25,0.23628691983122363,0.1513157894736842,0.10548523206751055,0.12796208530805686,0.189873417721519,0.18009478672985782,0.26540284360189575,0.20087336244541484,0.1592356687898089,0.18749999999999997,0.2313624678663239,0.209366391184573,0.25
384,SP:53db887bf542a1308933b1951fdfad625fb869a1,"The paper tackles the problem of restricted class unavailability after a deep learning model has already been trained on such restricted classes and the aim is to remove any information pertaining to the restricted classes from the model parameters so that the model will not be able to correctly classify the restricted classes in the future. The approach presented includes identifying the model parameters that are most relevant to the restricted classes and removing the restricted class information from these parameters (gradient ascent) while ensuring that these parameters can still be used for accurately classifying other non-restricted classes. With the need to correctly assess the utility of the proposed approach, several baseline methods have been proposed. Empirical results on the CIFAR-100 and ImageNet-1K datasets illustrate how the proposed approach can be used. ","This paper proposes a novel and practical problem called RCRMR-LD, aiming to removel restricted categories from model representations with limited data. They first give some direct solutions and analyze their weaknesses. Then, they propose their own solution to discard the restricted class information from the restricted class relevant parameters. Experiments verify that this approach not only performs similar to FDR but also is faster than it.","In this paper, the authors present a new method to remove information about specific classes from a trained model without reducing the performance of the remaining classes. After the information is removed, the model should not be able to identify the class anymore. Instead of retraining the complete model from scratch without the restricted classes, the presented method only needs a few examples of the restricted classes and the remaining classes. In terms of speed, the presented method is ~200 times faster on ImageNet than a new model training without the restricted classes. Furthermore, they present a method for identifying model parameters that are mainly relevant to the restricted classes. The evaluation of the model is performed on the CIFAR-100, ImageNet-1k, and the CUB-200 dataset. For a detailed comparison, eight baseline methods were designed and evaluated. An ablation study is performed on the class relevant parameters and the number of classes that are excluded. The presented method achieves an accuracy close to the original model on the remaining classes in terms of accuracy. Also, the forgetting prototype accuracy is close to the model trained only on the remaining classes. ",This paper proposes a new learning setting of fine-tuning a pretrained model to forget some specific categories which is motivated by class-level privacy. The solution to this challenge is firstly detecting the most related model parameters that significantly affect model performance on restricted classes and then tuning on a small number of examples with the losses of desired classification capability. The proposed method is experimentally demonstrated effective than possible baselines.,0.11851851851851852,0.31851851851851853,0.14074074074074075,0.2835820895522388,0.19402985074626866,0.125,0.23880597014925373,0.22395833333333334,0.2638888888888889,0.09895833333333333,0.18055555555555555,0.3333333333333333,0.15841584158415845,0.26299694189602446,0.18357487922705315,0.14671814671814673,0.1870503597122302,0.18181818181818182
385,SP:53eee54976fa37debbac6acdbaf7b8fe41db3a98,"This paper evaluates prompt-based approaches with pretrained language models (LMs) for few-shot learning, where limited training examples are given and no validation set is provided. This setup is named “true few-shot learning” in the paper and is distinguished from multi-distribution and tuned few-shot learning, where data from other similar tasks and a large validation set is available, respectively. Under the true few-shot learning setup, model selection can only reply on the few training examples.   Experimental results show that, when only relying on the few training examples, model selection using methods like cross validation (CV) and minimal description length (MDL) often fail to select the optimal prompts and hyperparameters. As a result, performance acquired by using CV and MDL is only slightly better than random model selection and is significantly worse than selection based on a held-out validation set.   Overall, this work shows that the few-shot ability of pretrained LMs is overestimated for true few shot learning, where model selection is fundamentally difficult and needs to be addressed in future work. Moreover, this work provides suggestions for fair comparison on few-shot learning.  ","This paper studies few-shot learning within pre-trained language models. It notes that previous work has shown few-shot learning ability within these models but used many held-out examples to tune parts of learning, including determining what ""prompts"" to use to maximize few-shot performance. This paper considers ""true few-shot learning"", which involves only using the actual few-shot examples to tune these aspects of learning. The authors propose two ways to do this: (1) Cross-validation (CV) - randomly partitioning the training set into equally-size K folds and training on the k-1 sets and using the average loss on the kth set; (2) Minimum description length (MDL) - which is similar in spirit to (1) but defines different way of splitting the train and validation sets. The authors first conduct experiments on LAMA benchmark, which measure the ability of language models to retrieve facts. On this benchmark, it is shown that prompt selection via CV and MDL obtain marginal improvements over randomly picking the prompt. Another set of experiments is conducted on SuperGLUE benchmark where this time hyperparameter selection is done via CV and MDL. The pattern is similar to the first experiment and additionally it is noted that hyperparameter selection done this way drops the SuperGLUE performance below that of other work that used additional validation examples.","The paper proposes to re-evaluate some recent LM-based few-shot learners like GPT-3 and PET without using any additional validation examples from the target task during evaluation. They term this as “true few-shot learning”. The paper proposes to use two approaches to evaluate without any validation examples: (1) leave-one-out CV (2) minimum description length. These methods are evaluated for few-shot prompt tuning as well as model selection and authors find that previous work often over-claimed their performance. For example, GPT-3 prompt tuning leads to much worse accuracy in LAMA than selecting the prompt based on full validation data (the approach used by GPT-3). Similarly, a recent variation over PET, called ADAPET, used full validation without which there is actually no improvement over the PET method which it claimed to improve over.","- The paper studies few-shot learning on language models. - The paper first highlights the fact that many existing works on few-shot learning rely on a large validation set, which is unrealistic for a few-shot setting, and the authors propose a “true few-shot setting,” in which there are no held-out examples and there are a few points for training and validation combined. - They compare the two settings, specifically comparing two types of approaches for prompt selection and hyperparameter tuning: using a held-out validation set vs. using K-fold cross-validation (or maximum description length) for the true few-shot setting. - They show that K-fold CV doesn’t perform much better than random selection of prompts and hyperparameters, and underperforms the existing approach with a held-out validation set.  - They argue that existing literature substantially overestimates the few-shot capabilities of language models. ",0.22631578947368422,0.15789473684210525,0.23157894736842105,0.13513513513513514,0.15765765765765766,0.18439716312056736,0.19369369369369369,0.2127659574468085,0.29931972789115646,0.2127659574468085,0.23809523809523808,0.17687074829931973,0.20873786407766992,0.18126888217522658,0.26112759643916916,0.1652892561983471,0.1897018970189702,0.18055555555555555
386,SP:5423d18eccc60f7b5543e4ebf60b6ae56f782d8f,"This work proposes a novel algorithm for estimating the posterior distribution in simulation-based inference (SBI) settings, where the likelihood is not known but it is implicitly defined via a stochastic simulator. The work proposes truncated marginal neural ratio estimation (TMNRE), which is able to approximate the posterior distribution only for a small number of parameters of interest and making the sampling more efficient by sequentially truncating the prior for a given observed data. In addition, the posterior distribution is amortized and can be used to check the goodness of fit of the posterior distribution, albeit only in the areas which have not been truncated during training. The paper offers extensive experiments for different parameter dimensionality, as well as a comparison with the state of the art in SBI and theoretical analysis of how much error is introduced with the prior truncating process.","The authors suggest the use of neural ratio estimation to target the marginal posteriors directly, thereby circumventing the more difficult problem of obtaining the full Bayesian posterior. The method comes with other benefits such as local amortization as well as a sequential truncation technique for targeted inference.  The method is validated on several benchmark examples.  ","This paper presents a method for performing likelihood-free inference in parametric simulators.  The method presented is thematically similar to, and builds upon, a recent tranche of work using neural function approximators to ameliorate the inherent difficulties in likelihood-free settings.  The method takes the interesting and unique approach of modelling an arbitrary set of many smaller marginals of the full posterior, instead of directly targeting the full posterior.  These marginals are estimated by using neural ratio estimation between the target marginal posterior and the full marginal.  The prior is then truncated (to a hyperrectangle smaller than the support of the original prior in each dimension) to concentrate simulations into regions of significant posterior mass.  These estimators (the ratio estimators and the truncated prior) can then be used to establish an estimate of the posterior distribution.  The method is evaluated on reasonably standard tasks, and an interesting cosmology example (although this was inexplicably demoted to the supplementary materials).  ","This study contributes with a new simulation-based inference method, Truncated Marginal Neural Ratio Estimation (TMNRE). It builds on a previous method, Neural Ratio Estimation (NRE), by combining NRE (for directly inferring marginals of the posterior distribution) and the sequential constraining of the prior distribution by truncation (targeted to a particular observation). On standard tasks, TMNRE is shown to perform on par with state-of-the-art methods. Furthermore, the sequential truncation of the prior allows for local amortisation and thus makes it easy to perform (local) consistency tests.",0.13286713286713286,0.22377622377622378,0.21678321678321677,0.32727272727272727,0.23636363636363636,0.16455696202531644,0.34545454545454546,0.20253164556962025,0.34831460674157305,0.11392405063291139,0.14606741573033707,0.29213483146067415,0.1919191919191919,0.21262458471760798,0.2672413793103448,0.16901408450704225,0.18055555555555555,0.21052631578947367
387,SP:54926c450ff7e24ec173147921413ffee3624bb2,"Summary: The paper focus on building a Point-of-interest (POI) predictor based on temporal, semantic, social and geographical contexts fitted into a deep learning model with aim of overcoming sparsity issue in mobility data. Datasets used are publicly available and results, ablation studies supports the method's efficacy. This is an application paper and has shared code in zip.  Claimed contributions: 1. Auxiliary trajectory forecasting is incorporated in a POI prediction model as a first. 2. A novel consistency loss function to connect the predicted POI (from the main task) and the predicted location (from the auxiliary task). ","This paper proposes a Transformer-based context-aware network for human mobility prediction. Specifically, the authors explored the influence of four types of context in mobility prediction: temporal, semantic, social, and geographical contexts. A base mobility feature extractor is designed using the Transformer architecture, which takes both the history POI sequence and the semantic information as input. A self-attention module to model the influence of the social context. Moreover, the authors proposed to use an auxiliary task to model the geographical context and predict the next location. In the experimental results, the proposed method outperforms other state-of-the-art next POI prediction methods and illustrates the value of including different types of context in the next POI prediction.","In the submitted manuscript, the authors proposed a Transformer-based method to predict the next POI (place-of-interests) of a sequence. The model takes into account not only the temporal sequence of previously visited POIs but also semantic, geographical, and extracted social information. Moreover, an auxiliary trajectory prediction task is introduced to increase the performance of the main task. To prove the efficacy of the proposed strategy, the model is tested against three publicly available datasets: Gowalla, FS-NYC, and FS-TKY. Thanks to the modeling of the additional context information, MobTCast is able to outperform state-of-the-art results by a 7.22% margin with respect to previously proposed architectures.","This paper proposed a novel deep learning-based architecture for the next POI prediction. The proposed approach models the temporal and semantic contexts through Transformer, extracts the social context through self-attention module, and model the geographical context via an auxiliary task. Experiments show that it can achieve better results than the SOTA next POI prediction.",0.23232323232323232,0.1919191919191919,0.1414141414141414,0.24166666666666667,0.225,0.13274336283185842,0.19166666666666668,0.168141592920354,0.25,0.25663716814159293,0.48214285714285715,0.26785714285714285,0.2100456621004566,0.17924528301886794,0.18064516129032257,0.24892703862660948,0.3068181818181818,0.17751479289940827
388,SP:549b14f046da84dc9ab692b59b2ce16fec983aae,"The paper casts attention as a solver for an inference task that consists in estimating the mean of an unknown distribution given two information: an unreliable estimate and a prior distribution. The authors start by explaining this inference task and showing how this setup encompasses several real-life examples (translation, image captioning, filling the blanks). Then, they derive a convex optimization problem from this task and solve it by looking  at its Fenchel dual. Finally, the authors show that the solution obtained from this problem yields a generalized attention that is implicitly used in T5 Transformer.","This work constructs an optimization problem whose approximate solution results in a general form of attention. The optimization objective is derived from three assumptions: 1. the prior of the attention distribution has a support of the memory bank; 2. the likelihood of the query vector in the attention mechanism is roughly conforming to an isotropic Gaussian centered at the difference between the mean of the attention distribution and the mean of a prior distribution; 3. the standard deviation of the isotropic Gaussian is large. Then the mean of the posterior of the attention distribution is a general form of attention, with the dot attention scores reweighted by the prior probabilities. Under this framework, when the prior is a uniform distribution, dot attention is recovered; when the prior is defined by positions and word masks, the attention used in T5 is recovered.","This paper provides a new perspective on the attention mechanism. More precisely, the paper claims that an attention module can be seen as estimating the distribution solving a specific optimization problem. This problem follows the assumption that many deep learning models use a module updating a global guess with local information. The problem consists in minimizing both the discrepancy between an adjusted value of the mean and the mean of the estimated distribution, and between a prior distribution and the estimated distribution. Using some approximation, the solution corresponds to a generalized form of the dot-product attention.","The paper introduces a novel interpretation of the attention algorithm and corresponding theoretical discussions. The paper suggests that the ""(weighted) attention algorithm"" can be interpreted as an approximate solution of a density estimation problem, where we have a noisy observation of a mean and a reference distribution (or ""preference distribution"" in the paper).   In the proposed interpretation, the density estimation solves a proximal-type optimization problem. The penalty function is a reverse KL to a reference distribution, and the objective function is an L2-norm between the model's mean and the observed mean. Here, the authors emphasize treating the observed mean as a sum of the reference's mean and a noisy observation of a mean-shift. Since the optimization problem satisfies the strong duality, the optimization can be solved by its dual; however, it is still challenging to obtain its analytical solutions. Thus, in the dual problem, the authors introduce to approximate a term, a cumulant generating function of a reference distribution, by its second-order Taylor polynomial. The authors show that the closed-form solution of the resulting approximate problem is the dot-product attention, weighted by a reference distribution.  Furthermore, the authors show that the proposed interpretation includes a well-known T5 transformer as a special case. Assume we aim at solving the proposed optimization problem to estimate a density of a categorical distribution, where the distribution is on a set of M vectors (i.e., M keys). Given some context information, a set of networks predicts a preference distribution and a query, which is the mean-shift. The weight on the L2-norm in the optimization problem is the inverse of the temperature parameter. Then the T5 transformer's attention can be obtained by the expected value of keys under the analytical solution of the approximate dual problem. Thus, the authors demonstrate that the closed-form solution of the approximate dual problem is a generalized attention algorithm. Under the proposed framework, the design of preference distributions is flexible; for example, one can choose different types of context information depending on tasks.  In experiments, the authors discuss how much the deviation of the approximate dual's solution from the ground truth depends on \alpha.",0.2604166666666667,0.23958333333333334,0.34375,0.2127659574468085,0.3617021276595745,0.4020618556701031,0.1773049645390071,0.23711340206185566,0.08967391304347826,0.30927835051546393,0.13858695652173914,0.10597826086956522,0.21097046413502113,0.2383419689119171,0.14224137931034483,0.25210084033613445,0.2003929273084479,0.16774193548387095
389,SP:54c6d74f575b5d06b6072f6c0a031ae00a7e693a,"This paper considers online episodic RL with linear function approximation under adaptivity constraints. In particular, two types of limited adaptivity models are considered: - Batch learning model, where the agent is allowed to update the policy at $B$ pre-specifed batch grids. Using a batched version of LSVI-UCB we can achieve $\tilde{O}(\sqrt{d^3 H^3 T} + dHT/B)$ regret with $B$ uniform batch grids, so $\sqrt{T/dH}$ batches are sufficient. A lower bound is provided which shows the tight dependency on $B$. - Rare policy switch model, where the agent is allowed to switch policy ($i.e., \pi^k \ne \pi^{k-1}$) for at most $B$ times. Using a modified version of the rare switching OFUL we can achieve $\tilde{O}(\sqrt{d^3 H^3 T[1+ T/(dH)]^{dH/B}} )$ regret with $B$ switches, so $dH \log T$ switches are sufficient. No lower bound is provided in this setting.  The authors test out their algorithm with the hard-to-learn linear MDP (Zhou et al. 2021b) and the numerical results match theoretical gurantees.   ","This paper examines two variants of the LSVI-UCB algorithm in limited adaptiveness, which is the case where policies cannot be updated continuously. To deal with the limited adaptivity, the authors consider two models and proposes two algorithms, respectively. The LSVI-UCB-Batch algorithm corresponds to the batch learning model where we update the policy every $K/B$ epochs. The authors prove a $\tilde{O}(\sqrt{d^3 H^3 T} + d H T/B)$ regret and a $\Omega(dH \sqrt{T} + d H T/B)$ regret lower bound under this scenario. The LSVI-UCB-RareSwitch algorithm corresponds to the rare policy switch model where we update the policy when $det(\Lambda_h^k) > \eta det (\Lambda_h)$. The authors prove a $\tilde{O}(\sqrt{d^3 H^3 T[1 + T/(dH)]^{dH/B}})$ regret for this case. Experimental results in this paper show that the performance of both algorithms is comparable to that of the LSVI-UCB algorithm, with a substantially reduced number of policy switches.  ","This paper considers sample-efficient RL in linear MDPs under adaptivity constraints, that is, the number of policy switches during the algorithm execution is constrained to be low. The paper considers two types of adaptivity constraint: the batch learning model (in which the switching schedule is predetermined), and the rare policy switch model (in which the switching schedule can be adaptive). The main contributions of this paper are low-regret algorithms for linear MDPs in both versions of low switching models.","The paper studies learning episodic finite-horizon MDPs featured by the policy adaptivity, in the linear function approximation setting. The authors consider two models of adapting policies: batch learning case and rare policy switch, propose two LSVI-UCB algorithms with policy adaptivity by generalizing the well-known LSVI-UCB algorithm and prove their regret bounds. The proposed algorithms and regret bounds generalize the well-known LSVI-UCB algorithm to have policy adaptivity. The authors also verify the effectiveness via computational experiments.   The main contribution relies on the proposed two LSVI-UCB algorithms with added policy adaptivity and their regret guarantees. ",0.329608938547486,0.15083798882681565,0.11731843575418995,0.16071428571428573,0.20238095238095238,0.2222222222222222,0.35119047619047616,0.3333333333333333,0.21,0.3333333333333333,0.34,0.18,0.3400576368876081,0.2076923076923077,0.15053763440860216,0.21686746987951808,0.2537313432835821,0.19889502762430938
390,SP:55310f2182084de5763fd76de41a43fff2bb3a99,This paper studies the implicit bias of gradient descent on the depth-N diagonal linear network for the problem of sparse recovery. It shows the convergence results for gradient descent and provides error bounds to the ground truth sparse signal. The result highlights the importance of early stopping in preventing overfitting and shows the role of network depth in extending the stopping window.,"In this paper the authors study implicit sparse regularization for diagonal linear networks. Specifically, the authors extend the previous result of Vaskevicius et al. [9] to the case of depth>2, showing that for noisy regression under a relaxed assumption on the inputs, increasing depth enlarges the scale of working initialization and the early-stopping window. ","The paper under review studies the implicit sparse regularization induced by gradient descent on a diagonal linear network-like of arbitrary depth. Doing so, they extend a past work on depth 2 showing the possible benefit of larger depth. More precisely the authors show the role of early stopping to recover a sparse signal.","In this work, the authors study the implicit bias of gradient descent for sparse linear regression when parameterizing the ground truth signal as a depth-$N$ diagonal linear network. Previous works have shown that when utilizing a Hadamard parameterization $w = u \odot u - v \odot v$, gradient descent exhibits an implicit bias towards sparse solutions, without any explicit regularization encouraging sparsity. Follow-up work has also considered the depth-$N$ case in simplified settings (e.g., without noise, positive signals, and idealized measurement setups). The authors extend this analysis to the general depth-$N$ case $w = u^N - v^N$ in the presence of noise. They prove that when the measurement matrix is $\mu$-incoherent with sufficiently small $\mu$ and noise is present, there is an early stopping window depending on the problem parameters where any iterate during this time is close in $\ell_{\infty}$ norm to the underlying ground truth sparse signal. These results hold for any depth $N \geqslant 2$ and showcase that depth allows for larger initialization sizes and that the early stopping window grows with deeper parameterizations. Experimental evidence on toy examples is also provided corroborating several elements of the proposed theory.",0.2698412698412698,0.2698412698412698,0.4444444444444444,0.30357142857142855,0.4642857142857143,0.4074074074074074,0.30357142857142855,0.3148148148148148,0.14285714285714285,0.3148148148148148,0.1326530612244898,0.11224489795918367,0.2857142857142857,0.2905982905982906,0.21621621621621623,0.3090909090909091,0.20634920634920634,0.176
391,SP:554ac7c4b9166d7ba195b879ecedf0ff995393b1,"This paper proposes a new method for learning with noisy labels. The proposed method belongs to the research direction of data-selection, and aims to better select the clean data out of noisy samples. The key component of the proposed method is a novel operation, named _filtering noisy labels via their eigenvectors_ (FINE). FINE first computes the covariance matrix of the feature representation for the noisy samples. Then the principal eigenvector is extracted from the covariance matrix via eigen decomposition. Afterwards, the similarity score between the feature vector of each sample and the eigenvector is computed. Finally, the computed similarly scores are used to fit a Gaussian Mixture Model, which predicts the probability that a sample is clean or noisy. The proposed FINE approach is simple and computationally feasible. Theoretically it is shown that FINE is able to approximate the eigenvector of clean data with guaranteed accuracy under a few reasonable assumptions. In practice, FINE is demonstrated to be effective by integrating it into three different kinds of label noise learning approaches. The experimental results show that FINE brings (much) performance gain over previous methods across several synthetic and real-world noisy datasets.  To summarize, the contributions of this paper include:  1. A new method, named FINE, is developed to better select the clean data out of noisy dataset. Theoretically it is shown that FINE is able to approximate the true eigenvector of clean data with guaranteed upperbound. Also, it is shown that the data selection accuracy of FINE has guaranteed lower bound in terms of precision and recall.  2. This paper also includes a few illustrative experiments to explain the characteristics of the proposed method. These experiments are helpful.  3. In practice, FINE is integrated into three different kinds of label noise learning approaches, and (significant) performance gain is observed over different datasets, involving the synthetic and real-world noisy data.","This paper proposes FINE detector to detect samples with noisy labels. Specifically, authors first construct data covariance matrix by using the latent representation of each training sample, then perform eigen-decomposition on the data covariance matrix to generate the first eigenvector for each class. The authors state that the representation of clean samples have larger alignment score with eigenvector than noisy ones. The authors also provide theoretical results to analyze the robustness of FINE detector and shows this detector can be applied to multiple approaches including sample-selection, semi-supervised learning and collaboration with noise-robust loss functions. Experiments are conducted on CIFAR-10, CIFAR-100 and Clothing1M.","This paper is about learning with noisy labels. The authors propose a method which selects clean and noisy examples by first computing a covariance matrix (for each class separately) based on the learned representations. They then compute the first eigenvector corresponding to the largest eigenvalue from the covariance matrix. The ""cleanness"" measurement then becomes the inner product between the original representation and the eigenvector. The authors show multiple applications of their approach, such as sample selection, semi-supervised learning etc., and show that their method consistently improves the classification accuracy.","Deep neural networks become inefficient when the datasets contain noisy class labels. There are two types of robust techniques that are applied in the presence of the noisy labels: a) noise-robust loss functions; b) removing noisy data by detecting them. Most existing detection methods are dependent on the loss values, while such losses may be biased by corrupted classifier. This paper proposes a method that attempts to alleviate this issue by extracting key information from representations, but without using explicit knowledge of the noise rates.  The paper proposes a new method for filtering noisy data. It focuses on each data’s latent representation dynamics and measure the alignment between the latent distribution and each representation using the eigen decomposition of the data covariance matrix. This framework is coined as filtering noisy instances via their eigenvectors (FINE). It provides a detector with derivative-free simple methods with some theoretical guarantees. It provides a provable evidence that FINE allows meaningful decision boundary made by eigenvectors in latent space. It supports the theoretical analysis with various experimental results regarding the characteristics of the principal components extracted by the FINE detector.  It proposes three applications of the FINE: sample-selection approach, semi-supervised learning approach, and collaboration with noise-robust loss functions. It empirically validates that a sample-selection learning with FINE provides consistently more superior detection quality and higher test accuracy than other existing alternative methods such as Co-teaching families, TopoFilter, and CRUST. Further, experimental results show that the proposed methods consistently outperform corresponding baselines for all three applications on various benchmark datasets.",0.11538461538461539,0.11538461538461539,0.15384615384615385,0.25925925925925924,0.3425925925925926,0.3333333333333333,0.3333333333333333,0.4,0.183206106870229,0.3111111111111111,0.14122137404580154,0.11450381679389313,0.17142857142857146,0.1791044776119403,0.1672473867595819,0.2828282828282828,0.2,0.17045454545454541
392,SP:555fab26f5af336c1cdb21c0ce3a5376040ff1ce,"In this paper, the authors propose a scheme to select and update only part of the weights of a neural network during training. The selection of the weights to update is done using an approximation of the Fisher information, and the mask is computed by keeping only the weights associated with the larger fisher information values. The authors then propose an extensive empirical evaluation of their method for transfer learning, distributed training and efficient checkpointing. Finally, the authors perform an ablation study to show the effect of their design choices.","This work introduces FISH - a method of training (or fine-tining) a neural network by updating only a small fraction of its parameters. FISH uses an approximation of the Fisher Information of each of the network’s parameters to determine the top $k$ ones to update. The method is evaluated via transfer learning and distributed learning tasks using GLUE and CIFAR-10 datasets. The experimental results show that FISH has competitive or better performance in comparison to existing methods by updating just a small fraction of a networks’ parameters at a time. Further, FISH can reduce the memory usage when saving checkpoints.","The paper describes a method based on Fisher-information to estimate the importance of different network parameters. Once the top-k parameters in terms of importance are identified, the authors only update these parameters during training. The authors demonstrated the usefulness of their approach in transfer learning experiments (fine-tune only the task-important parameters), distributed/federated learning experiments (to reduce the communication volume through communicating only the sparse updates), and model checkpointing (by updating only a subset of parameters each epoch, and storing the deltas instead of the full parameter tensors at each epoch). ",This paper attempts to discover a fixed sparse subnetwork before the standard training. Only training the masked subnetwork can sufficiently match or even exceed the performance of the dense counterpart. The performance achieved by the proposed method is promising.,0.26666666666666666,0.23333333333333334,0.14444444444444443,0.22549019607843138,0.10784313725490197,0.12631578947368421,0.23529411764705882,0.22105263157894736,0.3333333333333333,0.24210526315789474,0.28205128205128205,0.3076923076923077,0.25,0.22702702702702704,0.20155038759689922,0.233502538071066,0.15602836879432624,0.1791044776119403
393,SP:55e1107528610211689367caa4ab2d89de4b8f3b,"This paper proposes a one-shot pruning strategy, in which the pruned subnetwork achieves good performance on several benchmarks without post procedures or iterations. They make use of the ZIG and propose HSPG to enhance the group sparsity exploration. Good performance across several benchmarks supports  the validity of OTO.  ","This paper proposes a structured pruning method called Only-Train-Once (OTO). OTO first partitions the trainable parameters into zero-invariant groups called ZIGs. This way allows one to prune without affecting the output and therefore it requires no additional training steps after pruning. Once they define ZIGs, they perform Half-space Stochastic Projected Gradient (HSPG), starting from a pre-trained estimate using SGD, it performs a form of projected gradient descent while gradually increasing group sparsity. The authors demonstrate the effectiveness of OTO on CIFAR, ImageNet, SQuAD by showing an impressive gain in terms of FLOPs while maintaining the performance of resulting slim network to the original dense counterparts. ","This paper proposes a group sparsity optimisation approach for training pruned models. This is done by splitting the optimisation into an initial phase where the non-differentiable regularisation objective is optimised using subgradients, followed by a second stage where channels/units (plus their corresponding bias/BN/residual path) are mapped to exact zero by augmenting the sub-gradients with an indicator function.  For each parameter group, this function compares the direction of subgradient with the direction you'd get as a result of pruning that group, and if the angle between the two is larger than 90 it prunes them away.","This paper first proposes the zero-invariant group for neural network. Zero-invariant group is the disjoint group of trainable parameters which results in corresponding output to be zero. Then, this paper formulates a optimization problem with group sparsity regularizer on the zero-invariant groups. Then, authors alternate SGD and Half-space projection to optimize the problem. This optimization method is called Half-Space Stochastic Projected Gradient (HSPG) which benefits from the larger projection region. This method does not require fine-tuning  and pruned model is directly used on the inference. On various dataset (Cifar-10 and ImageNet), this paper achieves state-of-the-art performance.  ",0.30612244897959184,0.22448979591836735,0.24489795918367346,0.13636363636363635,0.23636363636363636,0.16831683168316833,0.13636363636363635,0.10891089108910891,0.11320754716981132,0.1485148514851485,0.24528301886792453,0.16037735849056603,0.18867924528301885,0.14666666666666664,0.15483870967741936,0.14218009478672985,0.24074074074074073,0.1642512077294686
394,SP:561fd267b41832affdb8a918694c2979e5379d3f,"In this paper, the authors propose a controllable decoding mechanism for language generation models (e.g., GPT2) using constrained optimization. The proposed method explicitly formulate the sentence decoding as a constraint optimization problem where the free parameters are the distribution over the vocabulary at each step and two types of constraints are defined one over the generate text itself (f(Y)) and one over the similarity between the input and the outputs (g(x,y)). Please refer to Eq. 1 where the problem is clearly formalized. Since directly solving the problem in Eq 1. is not computationally feasible the author reformulates the problem using the Lagrangian formulation for container optimization problems (Eq. 3). However, this min-max formulation introduces several problems (e.g., the tradeoff between different objectives, hard to find opt for non-convex functions). To cope with these issues, the authors proposed a gradient descent/ascent solution (Section 2.1), a soft relaxation for each token using the probability simplex (exponential gradient descent + normalization) (Section 2.2), and an annealing threshold (Section 2.3).   The authors benchmarked the proposed decoding schema in both style transfer and style-controlled machine translation. In the style transfer, they follow the methodology in Krishna et al. 2020 and they study two contains such as formality and the semantic similarity between the input and the output. Similarly in the style-controlled machine translation, they study both the formality style and the cross-lingual similarity between input-output. In both settings, the author compared the proposed method with other decoding techniques (greedy and beam-search) and other controllable language generation techniques (e.g., FUDGE) using both automatic and human evaluation.  Finally, the authors show some preliminary results in controlling multiple attributes (Age, style pos/neg), and discuss the speed and memory requirements of the model. ","This work tries to solve a general type of text generation tasks, generating text with constrains. The author proposed a a new algorithm MUCOCO to formulate the decoding process as a constrained optimization problem. The author then use continuous relaxation, Lagrangian multipliers and exponential gradient descend techniques to solve the problem. The author validated the algorithm with style transfer and style-controlled machine translation.","This paper presents a decoding algorithm for controlled text generation. The problem is formulated as finding the most likely output $y$ under given constraints. The authors adopt the method of Lagrange multipliers, and introduce dampening in gradient descent to reduce oscillations. To deal with discrete $y$, the authors choose to optimize over probability simplex $\tilde y$ using exponentiated gradient descent. The authors also report that converting $\tilde y$ to a one-hot vector via a straight through estimator helps to converge faster.  Experiments include increasing formality in style transfer and machine translation. Compared with greedy decoding/beam search, the proposed method with formality and semantic similarity constraints achieves better content preservation/BLEU score and better transfer accuracy. It also compares favorably to the recent FUDGE method for controlled generation.","This paper proposes a flexible controllable decoding algorithm called MUCOCO, which incorporates multiple control attributes as differentiable constraints to the optimization. The authors relax the discrete optimization problem to a continuous one, and use continuous optimization techniques such as Lagrangian multipliers and gradient-descent based techniques to generate texts towards the desired attribute. Experiments on three conditional text generation tasks including text style transfer, machine translation and paraphrasing show the superior performance of the proposed model.",0.09302325581395349,0.132890365448505,0.09302325581395349,0.28125,0.328125,0.17829457364341086,0.4375,0.31007751937984496,0.3684210526315789,0.13953488372093023,0.27631578947368424,0.3026315789473684,0.1534246575342466,0.186046511627907,0.14854111405835543,0.18652849740932642,0.3,0.22439024390243903
395,SP:56280e820000b7c3c444b34f287a22b5133812a5,"This is a well-written analysis of Hamiltonian Neural Networks (HNNs), a class of physics-inspired deep neural networks. The work is motivated by a desire to apply HNNs to non-toy datasets as well as to gain an understanding of the key inductive biases that explain the majority of their performance. Through controlled experiments on synthetic trajectory data, they explore energy conservation, a symplectic bias, complexity of state representation, and second-order structure bias. The key finding is that the second-order structure in HNNs is the main explainer of its performance. A simpler model that combines a Neural ODE with second-order structure is introduced, which can be seen as a distilled HNN. The simpler model achieves stronger performance on Mujoco rollout prediction compared to the HNN.","Incorporating physics-informed inductive bias, especially Hamiltonian and Lagrangian dynamics, into deep neural networks has been the focus of a fast-growing body of work over the last few years. This paper has carried out a formal analysis of these methods to identify which specific aspect(s) of these energy-conserving networks contribute(s) the most to their superior performance in prediction and generalization. Through theoretical and empirical analyses, this paper concludes that incorporating the second-order structure (i.e., using a neural network to model the acceleration) and reducing functional complexity through a change of coordinates, not energy conservation or the symplectic structure, play the most critical role in improving the performance. The experiments are thorough and emphasize the message precisely.","This paper presents an in-depth study of the inductive biases in physics-inspired neural networks, especially Hamiltonian Neural Networks (HNNs). The authors break down the biases in HNNs into multiple categories, such as energy conservation of the network, second-order structure of the output, symplectic vector fields produced by the networks, and the role of the complexity of the coordinate system. The authors show through experiments on a set of pendulum-based tasks, that HNNs are not in fact better at conserving energy than competing approaches that dont explictly model this such as Neural ODEs. They also show that Regularizing NODEs with a symplectic field bias does not help in generalization. Through experimentation, they show that a second order output structure similar to that of HNNs helps NODEs quite a bit and outperforms SymODEN. Finally, the authors use these findings to obtain good results in modeling trajectory dynamics with NODEs and a second order output to model dynamics in MuJoCo tasks. ","## Summary and contributions. Motivated by the success of physics-inspired neural networks, authors investigate the different inductive biases (implicitly or explicitly) encoded in Hamiltonian neural networks (HNNs). Authors identify four biases: the ODE, second order, energy conservation and symplectic biases. They claim and empirically show that, as opposed to what was previously argued, HNNs' performance is mostly due to the implicit second-order bias. Indeed, the authors show that explicitly splitting the Hamiltonian into a momentum and potential term, leads to the system's position $q$ being parametrised as the solution of a second-order differential equation (which involves the mass matrix and the potential function).",0.20155038759689922,0.24031007751937986,0.20930232558139536,0.20491803278688525,0.16393442622950818,0.1728395061728395,0.21311475409836064,0.19135802469135801,0.25471698113207547,0.15432098765432098,0.18867924528301888,0.2641509433962264,0.20717131474103584,0.21305841924398627,0.22978723404255322,0.176056338028169,0.17543859649122806,0.208955223880597
396,SP:56472586d7219edc19a7971fb238f2148d3ba988,"This paper considers random finite-width, finite-depth fully-connected neural networks and derives exact formulas for the probability distribution for neural network preactivations, conditioned on a single input, at initialization. The cases considered include deep linear networks and deep Relu networks without biases. These prior distributions are expressable in terms of Meijer G-functions and are rotationally invariant (dependent only on the magnitude of preactivations). The exact priors are visualized for different widths and depths and compared to exact results for infinite-width networks (where the result is a Gaussian distribution) and perturbative results for large but finite-width networks (where the result can be expressed using an Edgeworth series). Notably, the exact results -- which are non-perturbative and have no constraints on depth and width -- reveal a long and heavy tail in the distribution which is not evident in the large width limit.","This paper derives closed from expressions for the marginal distribution of full-connected ReLU neural networks, with Gaussian distributions over weights, and no biases. (Can be viewed as Bayesian NN priors or NNs at initialisation.) Importantly, this is done for finite widths. This contrasts with much previous work which assumes infinite width (-> GP). It unifies/confirms some results on finite width networks.","While infinitely wide Bayesian neural networks (BNNs) hidden units distributions are studied well enough due to the Gaussian process limit, hidden units at finite widths are now under the telescope. The main result of the paper is a derivation of the exact characterization of priors in function space through Meijer G-functions. The results are obtained for Gaussian priors, ReLU and linear activation functions.   **Advantages**:  * It is the first accurate description of hidden units distributions.   * The results are in line with other works on the heavy-tailed nature of hidden units, non-Gaussian corrections, and bounded moments.    **Limitations**:  * Only linear and ReLU functions,  * Only Gaussian priors,  * Without the bias term. ","The paper calculates the function-space prior (marginally for single outputs) induced by an independent Gaussian prior on the weights for deep linear and ReLU finite NNs without biases. This prior is expressed using the ​​Meijer G-function for deep linear networks, and weighted sums of these functions for deep ReLU networks. This makes clear that at finite widths, the prior is increasingly heavy-tailed with increasing depths. Moreover, finite-width corrections are unable to capture this heavy-tailedness as it is not a perturbative effect.",0.1310344827586207,0.15862068965517243,0.1793103448275862,0.14516129032258066,0.22580645161290322,0.13636363636363635,0.3064516129032258,0.20909090909090908,0.3023255813953488,0.08181818181818182,0.16279069767441862,0.1744186046511628,0.18357487922705318,0.1803921568627451,0.2251082251082251,0.10465116279069767,0.1891891891891892,0.15306122448979592
397,SP:5680281cb263aa6afbed464b181a38d3ec22458b,"This paper aims to address the catastrophic forgetting issue with continual learning on streaming graphs. To comprehensively incorporate the feature and topological information of new nodes, this paper proposes atomic feature extractors to project raw node features and neighbor nodes into distinct base embeddings, which are then used to refine or compose atomic-, node-, and class-level prototype embeddings in a hierarchical manner. Experiments demonstrate that by doing so, catastrophic forgetting can be avoided and promising accuracy can be achieved on continual graph learning tasks.","This paper introduces continual learning in graph settings, where new graph-based tasks come in over time. To learn effective node classifiers for new tasks without forgetting the old tasks, a three-level prototype network is outlined. Atomic, node, and class prototypes are jointly used on top of Atomic Feature Extractors, where only the relevant features and prototypes are selected and updated in each new task, without disturbing the others. Experiments on eight benchmarks show the effectiveness and lack of forgetting for continual node classification.","The paper addresses the continuous representation learning problem for graph data. The main goal is to learn emerging novel node categories, and simultaneously maintain the learned knowledge over existing graphs. In the proposed solution called Hierarchical Prototype Networks (HPNs), several levels of general knowledge are extracted, in the form of atomic, node and class-level prototypes. A theoretical analysis for the memory consumption and continual learning capability is presented. Empirical study is conducted  on eight different public datasets, including large-scale datasets with millions-scale nodes.","This paper addresses graph representation learning in a particular online/continual/streaming setting in which subgraphs are sequentially recieved which may contain nodes of categories/classes that haven't been seen before, while performance on the previously seen categories/classes must be maintained.  The approach involves extracting ""atomic"" features based on node features and local network structure, then constructing and maintaining prototype embeddings at various levels of abstraction, in order to produce a final embedding to be used by a node classifier. In each iteration the method decides whether to introduce new prototypes based on matching to previous prototypes. The authors prove a bound on the number of prototypes (hence, memory requirements) and show that the method will avoid catastrophic forgetting. Experimental results versus various baselines are positive.",0.2235294117647059,0.2,0.21176470588235294,0.2,0.2235294117647059,0.19767441860465115,0.2235294117647059,0.19767441860465115,0.140625,0.19767441860465115,0.1484375,0.1328125,0.2235294117647059,0.19883040935672516,0.16901408450704222,0.19883040935672516,0.17840375586854462,0.1588785046728972
398,SP:56abff19dfeeca0e255874d4bb6eae3455289870,"This study considers the label differential private training of the classification model with DNN.  The basic idea is the randomized response, while the labels with top-k prior probabilities are returned when labels are randomized. Furthermore, given a prior label distribution, the proposed scheme can choose k adaptively so that the probability that the input label is returned by the mechanism is maximized. The proposed mechanism is proved to be optimal in the sense that the probability that the input label is returned by the mechanism is equal or greater than any label-DP mechanism.   In application to model training, the authors proposed a multi-stage strategy. Splitting the training data into several subsets and starting from the uniform label prior distribution, it gradually updates the label prior used for the mechanism. The performance of the presented mechanism is demonstrated with CIFAR10 and movie lens dataset with comparison to DP-SGD and its variants. The results show that the classification performance of their label-DP mechanism is significantly better than the existing DNN model with differential privacy.        ","This paper studies a Randomized Response algorithm with prior (RRwithPrior), which takes as input a label y, a prior distribution p on the possible labels [K], and outputs a randomized label aiming to maximize the probability that the output is y, while preserving epsilon-DP. The first claim of the paper is that the proposed algorithm is optimal among all epsilon-DP algorithms, given that the label y is actually distributed according to the prior distribution p. This is proven by arguing that RRwithPrior gives the optimal solution to a Linear Program that models this problem.   Then, the authors use the RRwithPrior algorithm as a subroutine to LP-MST, a multi-stage algorithm for training NNs with label differential privacy (LabelDP). LabelDP is a weaker notion of privacy where neighboring data sets differ on the label of a single example. The algorithm proceeds in stages, in each of which a model is trained on disjoint partitions of the dataset and the private labels are protected using RRwithPrior. An important thing to note is that LP-MST does not require a prior and is essentially boosting the prior that RRwtihPrior uses: it starts with the uniform distribution over all [K] classes, then labels the first batch of examples according to RRwithPrior with this uniform prior and moves to the next stage using the new model as a prior. LP-MST with one stage (called LP-1ST) is equivalent to Randomized Response (without prior). The performance of the epsilon-LabelDP model output by variations of LP-MST is empirically evaluated against other methods, demonstrating increased accuracy. The authors also experiment with new advances in self-supervised learning to retrieve useful priors.   The above are the main contributions of the paper, but this work also includes a study of SGD with LabelDP. The authors give a version of SGD with Randomized Response which satisfies epsilon-LabelDP and achieves a excess true risk in the order of K/(epsilon*sqrt(n)), hiding the dependence on the diameter of the parameter space and the Lipschitz constant. They also give a (not very tightly matching) lower bound of 1/\sqrt(epsilon*n). They extend the study to (epsilon, delta)-LabelDP, saving a sqrt(K) factor from the upper bound. The lower bound above works for (epsilon, delta)-LabelDP as well so in this case the gap between the two is smaller. In the process of deriving the lower bound, the authors give lower bounds on the empirical loss and give an extension of a known reduction from private SCO to private ERM from Bassily et al. to the case of LabelDP. They finally consider a slightly different algorithm and give bounds that depend on the quality of the priors. ","The paper introduces a Randomized response (RR) concept to provide more accurate results at the same level of privacy protection of LabelDP. The method can be incorporated into LP-MST and SSL frameworks to improve model performance. To achieve that, the authors propose RRWithPrior, which maximizes the probability that the output label is correct, and a novel LP-MST for training deep neural networks with LabelDP and RRWithPrior. They show formal proof of optimality of RRWithPrior and some open research questions. ","This paper proposes the Randomized Response with Prior algorithm and then uses it as a preprocessing step to learn neural networks with label differential privacy. It shows the RRWithPrior algorithm is optimal in privatizing a label. It also provides extensive experimental results to show that labelDP is easier than protecting both the inputs and labels. Finally, it gives theoretical analysis to understand this phenomenon to the stochastic conveys optimization setting. ",0.3163841807909605,0.11299435028248588,0.11864406779661017,0.06622516556291391,0.06622516556291391,0.13580246913580246,0.12362030905077263,0.24691358024691357,0.3,0.37037037037037035,0.42857142857142855,0.15714285714285714,0.17777777777777778,0.15503875968992248,0.1700404858299595,0.11235955056179775,0.11472275334608031,0.1456953642384106
399,SP:56dcf880fb78dd1a7bcc2174984e9a15657c06b2,"This paper focuses on the unsupervised training of bi-encoder and cross-encoder for sentences. A self-distillation framework is proposed, in which the bi-encoder and cross-encoder may iteratively distill knowledge from each other. The experiments are performed with typical sentence pair datasets like STS, QQP, QNLI, MRPC, where improvements are achieved over SimCSE and mirror-BERT. ","# Summary  - This paper highlights the fact that bi-encoders (sentence encoders) and cross-encoders (sentence-pair encoders) have been considered somewhat separately in the literature of sentence-pair modeling, and that cyclic knowledge distillation from one to another (and vice versa) can be effective for improving the performance of both the encoder stereotypes. - The authors combine the two sentence modeling paradigms into an iterative joint framework, which is called TRANS-ENCODER, to simultaneously learn enhanced bi- and cross-encoders. - Specifically, given the original SimCSE or Mirror-BERT checkpoints provided by prior work, the proposed framework first (pseudo-)labels sentence pairs sampled from the task of interest by utilizing the off-the-shelf sentence encoders. And then, the labeled sentence pairs are utilized to improve the quality of cross-encoders. Further, the fine-tuned cross-encoders can also be exploited to label other sentence pairs which are again able to be used as a dataset for tuning the bi-encoders (sentence encoders). The framework repeats this process until both the bi- and cross-encoders become satisfactory in terms of their performance. - The proposed method is evaluated on semantic textual similarity (STS) tasks and binary classification datasets such as QQP, QNLI, and MRPC. The results demonstrate that the method improves both the bi- and cross-encoders to be more suitable for sentence-pair modeling tasks.","This paper deals with tasks where we need to do pairwise comparison between two sequences. Specifically, the paper experimented on STS, QQP, QNLI, MRPC. The paper proposes integrating two models together: bi-encoders and cross-encoders.   A bi-encoder encodes each of the two sequences separately and maps to a common embedding space. A cross-encoder first concatenates two sequences, and the concatenation is sent to the model. There’s more computation for the cross-encoder case, but the cross-encoder usually outperforms the bi-encoder.   Figure 1 illustrates the authors’ proposal well. First, start with a pretrained language model. Then, the bi-encoder generates pseudo-labels for the cross-encoder to learn. Next, the cross-encoder generates (more accurate) pseudo-labels for the bi-encdoer to learn. We reiterate this process. This approach is named self-distillation.   The author proposes an extension called mutual distillation where the authors do self-distillation on multiple pretrained language models in parallel (but the paper only experiments with two--BERT and RoBERTa).   On STS, QQP, QNLI, MRPC, the mutual distillation approach archives good performances (much better than the bi-encoder baseline SimCSE).  ","This paper proposes TRANS-ENCODER, an unsupervised approach to training bi-encoders and cross-encoders for sentence similarity tasks such as information retrieval, natural language inference, semantic textual similarity and clustering. The core idea of TRANS-ENCODER is self-distillation that alternatively trains a bi-encoder and a cross-encoder with pseudo-labels created from the other. The authors also propose a mutual-distillation extension to mutually bootstrap two self-distillation models trained in parallel. The effectiveness of TRANS-ENCODER is verified by empirical evidence.",0.3559322033898305,0.4067796610169492,0.3050847457627119,0.18834080717488788,0.11659192825112108,0.15343915343915343,0.09417040358744394,0.12698412698412698,0.21176470588235294,0.2222222222222222,0.3058823529411765,0.3411764705882353,0.14893617021276595,0.1935483870967742,0.25,0.20388349514563106,0.16883116883116883,0.2116788321167883
400,SP:572c6d04ab24899685994f73fa7f91c1ca5fcce6,"This work presents a method for modeling dynamic systems that combines mechanistically motivated differential equations with data driven models learned by RL.  The method is applied to AD progression, measured by MMSE (a cognitive function test) scores.  The key idea is that a mechanistic understanding of the system under study may be incomplete -- e.g., in a system of coupled differential equations, a quantity and its time evolution may be unspecified -- but that it may be possible to fill this gap by learning a reasonable model for the missing function.  This method is applied to AD progression in a real world dataset with mixed results - although adding the learned dynamics model improves accuracy (measured by MSE) a great deal relative to the DE baseline, it does not do as well as a pure ML based baseline (SVR).  "," This paper addresses the problem of predicting long-term Alzheimer’s disease (AD) progression. It proposes a model that combines domain knowledge on the evolving relationships between factors affecting the brain together with a data-driven approach to learning the missing relationships. The learning objective used to learn the data-driven components satisfy a general criteria about the working of the brain, which maximizing cognition while minimizing the cost of supporting cognition. The authors propose using differential equations (DEs) to capture domain knowledge and reinforcement learning (RL) to learn the data-driven component. The model was validated on the ADNI data set and was found to outperform SOTA baselines.","The paper proposes a disease progression model for Alzheimer's disease (AD) that combines data-driven approaches capture trends in data over the course of the disease, which have proved successful over recent times, with more mechanistic biophysics-based models of how pathology spreads over the brain.  The authors introduce the use of reinforcement learning to learn update rules for regional brain quantities as a function of time thereby informing interactions between processes and measurements of neurodegeneration. Experiments with simulations confirm basic efficacy of the estimation procedure; experiments with ADNI data show some promising predictive power and demonstrate potential insights into disease biology.","The authors proposed a method that combines differential equation modeling with reinforcement learning to predict the progression of Alzheimer’s disease.   In terms of contributions, the authors claimed that this study is the first to use reinforcement learning for disease progression. During my non-exhaustive literature review there does not seem to be clear objections against this statement. The authors also claimed that their model showed a 11% lower prediction error compared to a state-of-the-art. Moreover, the authors mentioned the so-called “recovery/compensatory processes” that is present in Alzheimer’s disease could be observed in their proposed model, showcasing the interpretability of the model.",0.16058394160583941,0.12408759124087591,0.15328467153284672,0.22935779816513763,0.1834862385321101,0.1941747572815534,0.2018348623853211,0.1650485436893204,0.19444444444444445,0.24271844660194175,0.18518518518518517,0.18518518518518517,0.17886178861788618,0.14166666666666666,0.17142857142857143,0.2358490566037736,0.18433179723502305,0.1895734597156398
401,SP:5751b2abad772e44e69e125a769f25892c2a2e30,"This paper applied domain adaption ideas into the adversarial settings. Based on it, they proposed Adversarial Feature Desensitization (AFD) by leveraging a discriminator network to minimize the  distance between adversarial feature and natural feature. They show that their method could improve the robustness  compared to TRADE and AT and could also generalize well to other attacks (with different epsilon).  ","The paper proposes Adversarial Feature Desensitization (AFD) to train classifiers robust to adversarial attacks. In particular, AFD trains jointly a feature extractor, a classifier for the original task and a discriminator which distinguishes between natural and adversarial inputs, in order to find features which are effective for classification but not sensitive to adversarial attacks. In the experimental evaluation on four datasets, AFD outperforms standard methods in most of the cases.","This paper proposes a feature desensitization method for adversarial defense. The authors formulate robust representation learning problems from the domain adaptation perspective, the algorithm to solve which is based on two-player minmax methods (i.e., GAN). The evaluation results on multiple datasets demonstrate the advantage over baseline methods. The learned feature also shows to be spare. ","This paper proposes Adversarial Feature Desensitization (AFD) as a defense against adversarial examples. AFD employs a min-max adversarial learning framework where the classifier learns to encode features of both clean and adversarial images as the same distribution, thereby desensitizing adversarial features. With the aim of fooling a separate discriminator model into categorizing the classifier’s adversarial features as from clean images, the classifier is trained with the standard cross-entropy loss and adversarial loss terms. The authors showed through experiments on MNIST, CIFAR10 and CIFAR100 datasets that AFD mostly outperform previous defenses across different adversarial attacks under white- and black-box conditions.",0.22033898305084745,0.1694915254237288,0.23728813559322035,0.2,0.2857142857142857,0.22807017543859648,0.18571428571428572,0.17543859649122806,0.13592233009708737,0.24561403508771928,0.1941747572815534,0.1262135922330097,0.20155038759689922,0.17241379310344826,0.17283950617283952,0.2204724409448819,0.2312138728323699,0.1625
402,SP:57744555fd1e0613c53d353f0aeaad78872ce916,"This paper focuses on the centralized training with decentralized execution (CTDE) setting within cooperative multi agent reinforcement learning (MARL). The authors observe that in some cases training with a joint Q function can lead to myopic agents that do not effectively coordinate with one another. The authors introduce CDS, a new method that factorizes the Q function into shared and individual components, with an additional loss term maximizing the mutual information between the individual Q-function and the agent ID. They demonstrate the effectiveness of the approach on a toy problem and then outperform a set of strong baselines on two challenging benchmarks. Overall the work seems like a solid contribution to an active area of research.   I vote to weak accept the paper, with a possible increase if my questions below are answered in a satisfactory manner. The only reason why this paper is not an accept is a general impression that the authors prioritized SoTA over transparency. There is no discussion of how hyperparameters were tuned, very limited discussion of limitations and no discussion of how environments were chosen. Overall it leaves the impression that maybe the method doesn't generalize to other environments, and is brittle to hyperparameters.","Incentivizes agents to be diverse from each other in cooperative multi-agent tasks by getting them to maximize the mutual information between their trajectories and their ID. Breaks this down into action-diversity (maximizing the KL divergence between each agent’s action distribution & the group average), as well as observation-diversity (using a variational approximation of I(o_t;ID|tau,a_t)). Also propose a new architecture that allows sharing of information between agents, while retaining some agent-specific parameters. Thorough evaluation with 6 state-of-the-art multi-agent baselines show the method gives consistently much higher performance than prior techniques at both Google Research Football and Starcraft. Great analysis of the learned behaviors and why diversity helps in both environments, cool ablation studies. ",This paper proposes a CDS method that adaptively trades off diversity and sharing in multi-agent reinforcement learning. The authors introduce an information-theoretical objective to encourage diversity and decompose individual Q-functions as the sum of shared and non-shared local Q-functions to boost shared knowledge usage whenever possible. Visualizations show that the balance between identity-aware diversity and homogeneity promotes sophisticated strategies.,"The paper deals with the problem of imposing policy diversity along the different agents of a MARL problem. To do so, the paper proposes to introduce a novel objective to maximize the mutual information between the agents identities and trajectories. Additionally, the paper proposes a decoupled Q function architecture to represent shared and non-shared goals.",0.11940298507462686,0.1044776119402985,0.09950248756218906,0.0873015873015873,0.10317460317460317,0.18461538461538463,0.19047619047619047,0.3230769230769231,0.35714285714285715,0.16923076923076924,0.23214285714285715,0.21428571428571427,0.14678899082568808,0.15789473684210523,0.1556420233463035,0.11518324607329844,0.14285714285714285,0.1983471074380165
403,SP:583a913f0c1b7931ca2bbf4fc91af2c3fc89f12f,"This paper provides learning-augmented algorithms for online conversion problems. Suppose you have $1 and want to convert it to a different currency where the exchange rate varies over time. The ideal would be to do the conversion when the exchange is at its highest, but at any time, we do not know if the current exchange rate is in fact the highest (similar to secretary problems). The paper considers both the integer and fractional versions of this problem, i.e., when the conversion of the entire one unit has to be done all at once or when it can be done in fractions multiple times adding up to one. Algorithms are presented for these problems if a prediction of the maximum value is given in advance, the goal being to achieve the optimal tradeoff between consistency and robustness. Matching lower bounds on this tradeoff are also presented. Finally, an empirical evaluation of the algorithms are carried out for the application of trading dollars for bitcoins.","The paper considers the following conversion problem: You start with a good (e.g. 1 bitcoin) that you can convert into another good (e.g. dollars) at a conversion rate that changes over time. Every day you have the opportunity to convert part of the good at the day's rate. On the last day, you have to convert any remaining amount of the original good that you might still own. The goal is to maximize the final amount owned. It is assumed that upper and lower bounds U and L on the conversion rate are known that hold throughout the time horizon.  Two versions of this problem are considered: 1-max search, where the good is unsplittable, i.e., one can only convert it as a whole. And a smoother version where arbitrary fractions can be converted, called one-way trading. It seems to me that one-way trading is equivalent to the randomized version of 1-max search.  Let $\theta=U/L$ be the ratio between the upper and lower bounds. In the classical online setting, for 1-max search it is known that the best competitive ratio is $\sqrt{\theta}$, and for one-way trading it is $1+W((\theta-1)/e)$, where W is the Lambert function.  This paper considers this problem through the learning-augmented lens: The algorithm receives as additional input at time 0 a prediction of the maximal conversion rate that will be observed. The paper gives algorithms for both problems and proves via upper and lower bounds that they achieve the pareto-optimal trade-off between consistency (i.e., the competitive ratio when the prediction is correct) and robustness (i.e., the competitive ratio in the worst case). For 1-max trading, the result is that if the consistency is a multiplicative factor c better than the classical competitive ratio $\sqrt{\theta}$, then the robustness is a factor c worse than $\sqrt{\theta}$. The result for one-way trading is more difficult to express and I have no clear intuition of the trade-off. The paper also includes a brief experimental evaluation.","The paper considers learning-augmented algorithms for the online conversion problem. In the online conversion problem, a trader starts with a unit of money in the source currency. Given a sequence of conversion rates v_i that arrive online, the online conversion problem is to determine what fraction of their money to convert using the current rate so that the total amount of money in the target currency is maximized. The paper considers this problem in both the integral and fractional settings. The primary focus of the paper is to design online algorithms for the online conversion problem when provided with a prediction for the maximum conversion rate that achieves the optimal tradeoff between robustness (competitive ratio) and consistency (competitive ratio when prediction is correct).","The paper considers learning augmented algorithms for the integral and fractional online conversion problem. One has one unit of some asset that he wishes to convert into another asset. In each step $n = 1,\dots N$  the conversion rate $v_n$ becomes known to the algorithm, and the algorithm decides what amount $x_n$ (possibly $x_n=0$) of the asset should be converted in this step to obtain $x_n v_n$ of the new asset. Summing over all steps, the algorithm obtains in total $\sum_{n\in[N]}x_nv_n$ many units of the new assets, and naturally the objective is to maximise this. In the integer version the whole asset must be converted in only one step, whereas in the fractional version the asset can be converted piece by piece over different steps.  The paper designs a learning augmented online algorithm that is Pareto optimal between robustness and consistency.  Experimental results are presented where the flunctuation of the bitcoin exchange rate over the last five years is used as input data. ",0.29518072289156627,0.23493975903614459,0.22289156626506024,0.14,0.13428571428571429,0.336,0.14,0.312,0.21142857142857144,0.392,0.26857142857142857,0.24,0.18992248062015504,0.2680412371134021,0.21700879765395897,0.20631578947368423,0.17904761904761904,0.27999999999999997
404,SP:58843d1adb6d27a72f47ac8a8bf99429c5632541,The paper provides an interesting approach for OOD detection problem for flow models. The approach is based on random projections on the real line where KD statistical test is applied in order to compare two distributions. Two variants of the approach are considered 1 sample test where the comparison is made with respect to flow prior and 2 sample test where the comparison is made with respect to transformed samples from two datasets. The quality of the approach is compared agains standard reference methods.  ,"The paper proposes evaluates using statistical tests on random 1d-projections in the latent space of a flow model for groupwise out-of-distribution (OOD) detection. Concretely, Komolgorov-Smirnov tests are used to compare latent encodings of a given batch of samples to the expected distribution of the in-distribution latent encodings. For the expected distribution, either the predefined latent prior is used, resulting in a 1-sample Komolgorov-Smirnov test, or the empirical distribution of the latent encodings of the training data  is used, resulting in a 2-sample Komolgorov-Smirnov test. The paper reports similar groupwise OOD detection performance as existing methods, with the 2-sample KS-test method having less failure cases.","This paper applies a goodness of fit test (KS test) to the latent space of normalizing flows for purposes of out-of-distribution detection.  To combat high-dimensionality and model misspecification, extensions such as random projections and two-sample tests are also proposed.  The results report AUROC on RNVP (batch size 5 and 10), comparing against a typicality test and a kl-based test.  The paper also tests using autoencoders vs rand projections, alternative divergences, effects of model misspecification, and detection in latent vs original feature space.  The findings are that the KS-test is indeed a practical choice for OOD detection via GOF testing.  Moreover, it is observed that better models don't necessarily mean better OOD detection, as described earlier by Zhang et al. [ICML 2021].","The manuscript addresses groupwise outlier detection with normalizing flows. The main idea is to project latent representations of the input batch onto several random directions and to evaluate the resulting 1D distributions according to the Kolmogorov-Smirnov test either with respect to the prior (GOD1KS) or with respect to the corresponding distribution of the training data (GOD2KS). Finally, OOD ranking is performed according to the average KS value across all random directions. GOD1KS is used when the underlying normalizing flow is well learned, while GOD2KS is an option when there is some uncertainty regarding the learning outcome. Experimental performance decreases  when the method is used in combination with undertrained normalizing flow or a normalizing flow with a larger capacity.",0.25,0.16666666666666666,0.23809523809523808,0.21739130434782608,0.22608695652173913,0.1171875,0.1826086956521739,0.109375,0.16806722689075632,0.1953125,0.2184873949579832,0.12605042016806722,0.21105527638190955,0.1320754716981132,0.19704433497536947,0.20576131687242802,0.2222222222222222,0.1214574898785425
405,SP:58ba80a777e300f5239371ac9b83f2710aae0342,This paper is the latest in a quest to combine imaging and genetics information for diagnosis and/or biomarker identification. The key idea here is to use the prior knowledge of biological processes as an inductive bias to set up the graph architecture.  The paper also proposes Bayesian approaches to weigh feature importance for biomarker discovery. ,"This paper proposes GUIDE for integrating imaging and genetics data for predicting phenotypes. GUIDE uses prior info from gene ontology to restrict connections between genes and biological processes under a graph convolution network (GCN) framework. The genetic and imaging representations are combined for phenotype prediction. Graph pooling and graph attention are used for finding key pathways, and Bayesian feature selection is used for finding key imaging features. GUIDE is evaluated on a schizophrenia dataset.","This work introduces a framework for fusing genetic and imaging data for schizophrenia classification. In particular, the authors propose to exploit the ontological gene hierarchy to define a graph reflecting genetic risk pathways and to use an attention mechanism for subject-level graph edges identification. They also implemented a Bayesian feature selection strategy using a dropout mechanism in discriminative imaging feature extraction.","This paper proposes a novel end-to-end framework for whole-brain and whole-genome imaging-genetics. The genetics network uses hierarchical graph convolution and pooling operations to embed subject-level data onto a low-dimensional latent space. The imaging network projects multimodal data onto a set of latent embeddings. For interpretability, a Bayesian feature selection strategy is implemented to extract the discriminative imaging biomarkers; these feature weights are optimized alongside the other model parameters. The imaging and genetic embeddings are coupled with a predictor network, to ensure that the learned representations are linked to phenotype. Experiments on a schizophrenia dataset show that the proposed method hasthe better classification performance than state-of-the-art baselines, and the biomarkers identified are reproducible and confirmed to be closely associated with deficits in schizophrenia.",0.25,0.21428571428571427,0.26785714285714285,0.22972972972972974,0.28378378378378377,0.3225806451612903,0.1891891891891892,0.1935483870967742,0.11363636363636363,0.27419354838709675,0.1590909090909091,0.15151515151515152,0.2153846153846154,0.20338983050847456,0.1595744680851064,0.25,0.2038834951456311,0.20618556701030927
406,SP:58c3252d5d8a2ca42166242edb6aa507b89d7388,This paper proposes a data augmentation method (SCARF) used for self-supervised learning for tabular data. SCARF generates different views by corrupting a random subset of features (via sampling from the marginal distribution). The experimetanl results demonstrate that SCARF not only improves accuracy in the fully-supervised setting but also in the presence of label noise and in the semi-supervised setting. The authors also conduct ablation studies and compare with various methods.  I think adapting SSL to tabular data is a very important direction and I thank the authors for their efforts in this direction. ,"This paper proposes SCARF, which is a contrastive pretraining procedure for tabular data. SCARF generates an augmentation of a data point by selecting a random subset of features and replace them with their marginal distributions. Unsupervised pre-training is done by update networks consists of an encoder and pretraining head. Contrastive InfoNCE loss is used to pull the original input and its perturbed version, while pushing it away from other data points in mini-batch. On 69 datasets from OpenML-CC18 benchmarks, SCARF is compared with dozens of baselines and showed its efficacy. SCARF pretraining improves model robustness in label-corrupted settings and also helps classification in partially-labeled settings.","This work proposes a good method to use SSL methods on tabular datasets. They achieve this by randomly corrupting a subset of the features. They show comprehensive results and improved performances on 69 real-world, tabular classification datasets from the OpenML-CC18 benchmark. ","The paper presents a method for self-supervised pre-training on tabular data to improve the performance in supervised transfer. The method adapts the successful contrastive learning framework to tabular data by defining “augmentations” of the data wherein randomly chosen feature columns are replaced by sampling from their corresponding marginal distribution. The method is evaluated on tabular classification tasks of the OpenML-CC18 benchmark, both in the fully supervised setting and in the semi-supervised setting and under 30% label noise.  The method outperforms several baselines. ",0.25,0.15625,0.3020833333333333,0.14545454545454545,0.16363636363636364,0.32558139534883723,0.21818181818181817,0.3488372093023256,0.3372093023255814,0.37209302325581395,0.20930232558139536,0.16279069767441862,0.23300970873786409,0.21582733812949642,0.31868131868131866,0.2091503267973856,0.1836734693877551,0.2170542635658915
407,SP:58ce149a773c39a57243ada561e7685afd092605,"In this paper, the authors propose a Feudal Reinforcement Learning (FRL) algorithm for improving the miss-match between high-level natural language commands (e.g., ""share my recent photo to my parents"") and the actual complex set of operations required by the systems (e.g., open the album --> select recent photo --> etc.). FRL (Dayan & Hinton, 1993) is a hierarchical RL algorithm with two agents: manager and worker. The manager issues a plan which is executed by the worker. To elaborate, the manager learns to generate a sequence of targets given $Q$ the goal description, $O$ the wiki paragraph which describes the game, $A$ the object names in the environment, and $H$ the subgoal history ($h_t$ refers to the target object at time $t$). The worker, instead, uses the plan from the manager (in terms of coordinate to the target ($X_target$)), the observation $E_{obs}$, and the position to the other player ($X_{pos}$) (not sure I understood correctly this) to interact with the environment.   The manager and the worker are trained separately using imitation learning (also here it is a bit confusing), from a 100K trajectory collected by a random walk in the map.   The authors benchmarked the FRL with two text-based interactive games such as RTFM (Zhong et al. 2020) and Messager (Wang & Narasimhan, et al 2021) (the results are only in the appendix, but this paper is very recent). The results show that FRL is better than txt2$\pi$ without curriculum learning (here I am a bit unsure because the results in the original paper are different), in the RTFM. ","The paper presents a hierarchical RL model to solve tasks that require reading and understanding instructions or manuals to solve complex goals (these goals usually require multiple steps of solving subgoals). The proposed model has two parts: a manager that uses the instructions/ manuals to design subgoals, and a worker that solves the subgoals and deals with low-level perceptions. The manager and the workers are trained separately. The Feudal model achieves SOTA scores on Messenger and RTFM without the use of a hand-designed curriculum.","This paper proposes a feudal reinforcement learning model to solve the task of reasoning over textual (language) instructions that incorporate with low level state/action/environment information. This feudal model is composed of two agents, one of which is a ""manager"" that generates a multi-hop plan composed of several subgoals, while the other is a ""worker"" that takes in low-level information and executes actions to solve each sub-goal. They empirically evaluate on two challenging domains and show competitive performance to baselines.","The paper applies a feudal RL algorithm to solve text-adventure type problems where information is available in text form about the environment. A manager agent reads the text and generates a sequence of subgoals, and worker agents execute the subgoals to reach the final goal. The models are described, experiments are described (mostly in the RTFM but a callout to Messenger is there in Appendix). ",0.1018867924528302,0.09811320754716982,0.09433962264150944,0.23255813953488372,0.2441860465116279,0.20238095238095238,0.313953488372093,0.30952380952380953,0.3787878787878788,0.23809523809523808,0.3181818181818182,0.25757575757575757,0.15384615384615385,0.1489971346704871,0.15105740181268884,0.2352941176470588,0.27631578947368424,0.22666666666666666
408,SP:594a813c0d0baa66738b9c8331370f861ad3c416,The authors propose a model for link prediction that takes into account the counterfactuals. The main idea is to learn an embedding of the nodes and then use that embedding to train a classifier model for link prediction. The key part is that the embedding is learned in such a way as to minimize the discrepancy between the factual and counterfactual distributions. Experiments of various commonly-used datasets show that the proposed method results in improved link prediction performance compared to state-of-the-art methods.,"This paper introduces the idea of counterfactual prediction to improve link prediction on graphs and proposes a framework, counterfactual graph learning method for link prediction (CFLP). Specifically, the authors first introduce a method of generating counterfactual samples and then utilize these new data as well as original factual data to train the proposed model. The proposed method outperforms competitive baseline methods on several benchmark datasets in the link prediction task. The authors also provide sensitivity analysis and examine the impact of different treatments (i.e., results from different community detection algorithms) to link prediction results.",The paper consider the task of counterfactually predicting edges in attributed graphs. I think this is a good effort (but incomplete) in a consequential and fast growing area. The paper needs to define the counterfactual task formally and prove their approach really does what the paper claim it does.,"In this paper, the authors consider a link prediction problem utilizing the idea of causal inference. To find the essential factors between nodes, they make counterfactual links using a matching-like algorithm for pairs of nodes. A GNN-based model is trained to learn representations so that the model can predict both the factual and counterfactual links. In training, first they jointly optimize the entire loss function including the counterfactual loss and the balancing loss, and then they fix the encoder and only train the decoder using the factual data. In the link prediction experiment, they use public popular datasets to validate the proposed method and evaluate in terms of some metrics such as AUC and HitRatio. They also analyze the proposal in terms of ATE and discover negative correlation between the performance and ATE. The main contributions of this paper are as follows: 1) This is the first work that introduce the idea of causal inference into link prediction task. 2) They make counterfactual links and trained GNN-based methods to learn the causal relationship. 3) The proposed method outperforms the strong baselines.",0.2441860465116279,0.11627906976744186,0.36046511627906974,0.12631578947368421,0.28421052631578947,0.2653061224489796,0.22105263157894736,0.20408163265306123,0.16847826086956522,0.24489795918367346,0.14673913043478262,0.07065217391304347,0.23204419889502761,0.14814814814814814,0.22962962962962963,0.16666666666666666,0.19354838709677422,0.11158798283261802
409,SP:5950b7f1c3e0a3b7f621f2061fd819ec15add366,This paper proposes a new algorithm for model-based offline reinforcement learning in which the learner attempts to optimally balance maximizing the reward under the learned model and minimizing the uncertainty of the model. The algorithm is motivated by the fact that offline RL methods suffer from distribution shift that often results in policies/predictions that are overly optimistic about highly uncertain areas of the state-action space. The authors’ proposed method attempts to address this problem by identifying a relevant collection of policies lying on the Pareto frontier that trades off these two criteria. The underlying supposition is that a well-performing policy should be among this collection of “non-dominated” policies on the frontier. Specialized subroutines are proposed in order to make sure the collection is diverse. Some theory is presented to validate certain aspects. Extensive experimental results support that the method performs well in relevant benchmarks compared to a number of state-of-the-art methods. ,"In model-based offline reinforcement learning, a dynamics model is trained from the dataset and subsequently used to produce a decision-making policy which attains high reward (according to the model). However, one must also take care not to allow the policy to exploit the model’s inaccuracies, so we also want it to visit states and take actions which have low model uncertainty. Prior work has managed this tradeoff by linearly combining the two objectives.  This work proposes instead to find many diverse policies along the Pareto front of these two objectives. The algorithm, Pareto policy pool (P3), proceeds as follows: 1. Generate reference vectors which quantify tradeoffs in the Pareto front. 2. In parallel, find a Pareto-optimal policies for each reference vector using “Algorithm 2” to solve a constrained bi-objective optimization problem, where the constraint ties the solution to the reference vector. Algorithm 2 begins by finding a feasible solution and then applies MGDA to improve the objectives. 3. “Local extension”: Each reference vector is perturbed in two opposing directions, and then each perturbed vector is further optimized via Algorithm 2, with intermediate policies being added to the policy pool. (This is just an optimization to reduce the computational cost.) 4. At test time, each policy in the pool is evaluated, and the best one is selected.  The paper includes some theoretical analysis of Algorithm 2, showing convergence to an approximate stationary point. Empirical evaluation is performed on the D4RL benchmark, where P3 achieves good results, particularly on the datasets generated by lower-performing policies.","The paper proposes a new model-based offline reinforcement learning, where a pool of Pareto optimal policies is trained on a model of the environment provided by offline RL data. First, a model of the environment is trained using supervised learning from the dataset of logged experiences and subsequently a set of Pareto optimal policies is trained along the dimension of return in the MDP model and uncertainty in the MDP model. The authors argue that providing a Pareto optimal set of policies is superior to prior methods that relied on regularization of the two objectives into a combined metric, as having a Pareto optimal set of policies enables one too explore a range of behavior that is inherent with trade-off between model return vs model uncertainty in offline model based RL.  The authors then describe their method, Pareto Policy Pool (P3), which addresses the bi-objective optimization problem two stage method. Initially, the authors initialize a set of reference vectors along the Pareto frontiers to ensure a diverse set of Pareto policies can be found. Subsequently, the authors apply ""Local Pareto Extension"" where first a given policy is corrected to a desired range along the objective dimensions provided by the reference vectors. Once the policies are within the desired range, the method applies MGDA based optimization to further improve the performance of the policy in a Pareto optimal way (ascension stage). In order to alleviate computational cost of training multiple policies, the authors apply a ""Pareto extension"" method to initialize new policies along the Pareto frontier.   The authors then perform experiments on D4RL based benchmarks and compare their method to other literature methods and claim outperformance, particularly in lower quality dataset settings. The authors also perform an ablation study pertaining to the various components of their method, as well as an analysis related to why it is challenging to obtain optimal trade-offs in model based offline RL.","This paper proposes a model-based approach for offline RL that is inspired by multi-objective RL. The approach, called P3, finds a Pareto front of policies that trade off between obtaining high return (with respect to the model trained on the offline dataset) and minimizing the model's uncertainty. To obtain this Pareto front, the approach has two stages: 1) find a few spread-out solutions on the Pareto front using a gradient-based method, and 2) initialize with these solutions to find optimal solutions for the neighboring regions of the Pareto front. The empirical evaluation shows that P3 outperforms existing approaches when the offline dataset is of low or medium quality.",0.23270440251572327,0.29559748427672955,0.18238993710691823,0.23552123552123552,0.138996138996139,0.121875,0.14285714285714285,0.146875,0.25663716814159293,0.190625,0.3185840707964602,0.34513274336283184,0.1770334928229665,0.1962421711899791,0.21323529411764705,0.21070811744386872,0.1935483870967742,0.18013856812933024
410,SP:59698e1010be85d4773017ce35de26cd8f755474,"The manuscript addresses open-set object detection by modifying the ROI classifier of Faster-RCNN. The probability p(O|x,b) is modeled as sigmoidal activation of a generalized energy score of the classification head logits. The method succeeds if i) RPN ignores the outlier object, or ii) RPN detects the outlier but the candidate gets rejected due to high p(O|x,b).  The generalized energy is formulated as log-sum-weighted-exp-logit in equation (6). The probability p(outlier|x) is modeled as a sigmoid of modulated generalized energy in equations (8) and (9). The classification head is trained to output high energy in inliers and low energy in artificial outliers through equation (8). The artificial outliers are sampled from low-likelihood regions of per-class Gaussians which are fitted to latent features of groundtruth ROIs. These Gaussians are periodically updated during training.   It appears that the authors consider ROI-wide representations of RPN candidates. These representations are likely produced by applying ROI-align and two fully-connected layers to the convolutional features of the RPN. These Gaussian distributions are modelled according to per-class means (\mu_k) i and overall covariance (\Sigma). Virtual outliers are sampled so that their likelihood with respect to \mu_k and \Sigma is less than \epsilon.  ",This paper presents an approach for detecting out of distribution (OOD) samples by synthesizing outlier samples and considering an unknown-aware learning mechanism. The synthetic outlier samples are used to learn a tighter class boundary for in-distribution samples. The model is trained with an uncertainty-aware loss function that encourages a high probability score in distribution samples and a low probability score for OOD samples.  ,"This paper proposes an effective method for OOD detection and model regularization, which does not rely on real OOD datasets. Specifically, the proposed method synthesizes virtual outliers by sampling low-likelihood samples in the feature space of class conditional distribution, and adds a novel regularizer to the original ID training objective, which contrastively shapes the uncertainty space between the ID data and synthesized outlier data. Extensive and comprehensive experiments demonstrate the effectiveness of the proposed method.","This paper proposed a novel unknown-aware learning framework dubbed VOS (Virtual Outlier Synthesis), which optimizes the dual objectives of both ID task and OOD detection performance. The key of VOS is to sample outliers in low-dimension feature space instead of generated images. These outliers are used in the training process to help form the compact decision boundary between ID and OOD data.  The extensive experiments show the effectiveness of their method on both the PASCAL-VOC and the DeepDrive-100K datasets. ",0.06511627906976744,0.06976744186046512,0.09302325581395349,0.19696969696969696,0.18181818181818182,0.3026315789473684,0.21212121212121213,0.19736842105263158,0.24096385542168675,0.17105263157894737,0.14457831325301204,0.27710843373493976,0.099644128113879,0.10309278350515463,0.1342281879194631,0.18309859154929575,0.16107382550335572,0.2893081761006289
411,SP:599406b182f750d022a78452b16d04f200efc8ee,"This paper proposes a method called rshSCP (robust-to-site hSCP) to estimate hierarchical sparse connectivity patterns (hSCPs) from multi-site fMRI data while reducing the impact of site-specific variation due to scanner and experiment details rather than biological effects. Their approach is applied on top of the previous hSCP method as well as an extension that uses adversarial learning to improve reproducibility of estimated patterns (Adv. hSCP). The proposed method includes an additive component to the decomposition of the correlation matrix in hSCP, which models site effects as a matrix factorization with diagonal site-specific and full shared (across-site) matrix components. The method then additionally applies an adversarial classifier (a two hidden-layer feed-forward neural network) to predict the site from the subject-specific components of the hSCP decomposition. The entire optimization is formulated as a minimax problem, where the classifier maximizes its ability to predict site information while the remaining parameters are optimized to reconstruct the correlation matrices while minimizing the classifier's accuracy. The authors also propose an SVD-based initialization which improves reproducibility of the estimated components. Experiments on simulated data as well as fMRI data from multi-center imaging studies show that rshSCP improves accuracy (on simulated data) as well as reproducibility (on simulated and real data) of the estimated components, both when applied with hSCP and Adv. hSCP. The proposed method also leads to a reduction in accuracy when predicting site from the estimated subject-specific components using an SVM, as desired. Predicting age from the estimated subject-specific components with a random forest and visualizing the shared components shows that rshSCP preserves relevant biologically-related variation.","This paper deals with the important problem of how to pool fMRI data from multiple sites for connectivity estimation. The authors propose a simple extension to adversarial hierarchical sparse connectivity pattern (hSCP) estimation to remove site effects. The argument against adopting methods, such as the widely-used COMBAT, is to preserve the manifold structure of connectivity matrices. The method is tested on simulated data with higher accuracy shown. The method is also tested on real data with higher reproducibility demonstrated and age effects retained.","This paper proposes a combined technique based on matrix factorization and adversarial models to reduce the batch effect problem in fMRI data. The new model, called rshSCP, is designed to be robust to the site-effects in estimation of sparse connectivity pattern components. The idea is to decompose correlation matrices into a set of shared hierarchical pattern of subject components as well as site related components. Then an existing adversarial model is applied in order to increase the reproducibility of them. They also applied a simple initialization method for optimization procedure and evaluated their approach on both simulated and real datasets.","This paper introduces the method site-robust hierarchical sparse connectivity patterns (rshscp) and its adversarial extension. It builds on sparse connectivity patterns and its adversarial extension introduced in references 22 and 23. These propose a factorization of correlation matrices extracted for functional connectivity analysis. The factorization consists of a symmetric product of  sparse factors around a central diagonal weighting matrix. The present contribution introduces a site-dependent component into the optimization problem and adds a classifier to predict site, which is then adversarially optimized to make the learned structure lose information about site. In synthetic and real-data experiments it is shown that the method is indeed capable of discarding site information while retaining other relevant information.   ",0.10507246376811594,0.11231884057971014,0.13405797101449277,0.25,0.23809523809523808,0.1782178217821782,0.34523809523809523,0.3069306930693069,0.3162393162393162,0.2079207920792079,0.17094017094017094,0.15384615384615385,0.1611111111111111,0.16445623342175067,0.18829516539440205,0.22702702702702704,0.19900497512437812,0.1651376146788991
412,SP:59ba722355c0c7236a779874a27b2795522baa64,"This submission addresses the semi-supervised object detection task. Compared to previous methods, the authors propose to quantify the uncertainty degree and apply it into the model training. Besides, the authors modify the classification error to allow multi-peak in probability distribution from soft label targets. The experiments on four settings of semi-supervised object detection show the model achieves good performances. ","This paper tackles semi-supervised learning for object detection, where only a small amount of the training data is labeled with bounding boxes, while the rest of the training data is unlabeled. To solve this problem, the authors present a region uncertainty quantification method that quantifies the uncertainty of regions in unlabeled images based on their sensitivity to noisy pseudo labels. In particular, a quantification metric for the uncertainty is proposed for positive region proposals according to the classification error and the assignment error. Experiments are performed on both PASCAL VOC and MS COCO, with a comparison to three competitors: DD, CSD, STAC. ","**Goal:** This paper aims to address semi-supervised object detection, where the object is trained with a set of labeled images (fully-annotated box and object category) and a set of unlabeled images.   **Method:** - The authors propose to use both classification score and IoU to pseudo-box to compute the uncertainty of each proposal boxes, and this is based on their analysis that assignment errors are large when the IoU is closer to 0.5.   - Rather than using the hard label for the classification branch, they construct the soft labels by using uncertainty-based to weigh the ground-truth label.   - The author also proposes to use focal loss and sigmoid to alleviate the overfitting issue of incorrect categories.   **Experiments:** - They present the results on VOC and COCO datasets and show the improvement against CSD and STAC.  ",A method for semi-supervised 2D object detection is proposed. The main contribution is an uncertainty estimate of pseudo-labels that comes from a combination of classification score (s)  and a localization score. The latter is estimated by the IoU score with the set of pseudo-labels and is normalized to values between 0 and 1 only in a certain range. The estimated uncertainty is then utilized in setting the soft target and penalizing the L1 regression loss. ,0.27419354838709675,0.3709677419354839,0.22580645161290322,0.33980582524271846,0.21359223300970873,0.17647058823529413,0.1650485436893204,0.16911764705882354,0.1794871794871795,0.25735294117647056,0.28205128205128205,0.3076923076923077,0.20606060606060606,0.23232323232323238,0.2,0.29288702928870286,0.2430939226519337,0.22429906542056074
413,SP:5a2148f4867a1d51ee41d3c11b4cf050dbafb3af,"The authors consider the problem of Matroid Blocking Semi-Bandits. In this setting, there are $k$ arms, an unknown time horizon $T$ and a matroid dictating which sets of arms can be pulled at each step. Pulling each arm yields a reward, which comes from an unknown probability distribution with mean $\mu_i$. Moreover, each arm $i$ remains blocked after use for $d_i$ steps. Observing only the rewards from the used arms, the goal is to maximise the sum of all rewards collected over the time horizon $T$.   There are 3 main results, which can be better explained by a minor detour in the full information setting (where the $\mu_i$ are known). In this setting, the first approach one would follow is to ignore the $d_i$’s and just chose the independent set maximising the current rewards, among the arms that are not blocked. However, this is not optimal (for general matroids): the catch is that the underlying matroid indirectly affects the rewards of each arm, by restricting which other arms can be used together. To get around this, the authors establish a reduction between this setting and what they call the Recurrent Submodular Welfare (RSW), which entails maximising a monotone submodular function over $T$ steps, but where each element has a similar delay $d_i$ before it can be selected again.   The RSW problem can be solved by a clever randomisation: rather than choosing each element with probability $1/d_i$, the authors interleave the elements by adding a random delay $r_i \in [0,1]$ and selecting the item whenever the interval [t/d_i + r_i, (t+1)/d_i + r_i] contains an integer. This simple step takes care of most of the inefficiency and the unknown time horizon $T$ is also handled: essentially, just the last step might be suboptimal and this effect decreases with $T$. This algorithm is optimal for this setting, with a $1-1/e$ approximation.  The full information setting can be handled by selecting arms a subset of arms that are available in the RSW case, but doing it greedily and stopping once an independent set is found, to satisfy that hard matroid constraints. The reduction is completed by using the weighted rank function, which converts an additive function with matroid constraints into a monotone submodular one. As such, this algorithm is also optimal with an approximation of $1-1/e$ (and some minor terms relating to $\max d_i$).  The final step into the bandit setting can be performed because the learning of the $\mu_i$ and the random selection of elements are decoupled: the RSW interleaving technique is ‘static’ and does not update its schedule as the algorithm runs. Therefore, a modification of UCB can be used to balance learning the $\mu_i$’s with exploiting them, while observing the schedule used by RSW. This also yields an optimal (1-1/e) approximation (with an additional  $O(k\sqrt{T \log T} + k^2  + \max d_i \cdot r)$ additive loss, where $r$ is the matroid rank).","This paper considers the problem of matroid semi-bandits with an additional condition that each arm becomes unavailable for a fixed duration after it is played. The paper first considers an ""offline"" version of the problem where the reward for each arm is already known and the goal is to design an optimal schedule for playing the arms. A randomized greedy algorithm is proposed which is (1-1/e)-competitive to an optimal algorithm. The paper then shows that this algorithm can be extended to the `online' version of the problem when the rewards are unknown. This is because the set of arms played in this greedy schedule only depends on the delays and does not depend on the rewards. This online algorithm suffers an additional regret of O(K\sqrt{T} + K^2) in comparison to the best offline solution. ","The paper studies an extension of the matroid bandit problem where each arm $i$ is blocked for a known $d_i$ rounds after it is played, and this is referred to as the matroid blocking semi-bandit (MBB) problem. When the arm distributions are known, the authors reduce the optimization problem to a recurrent submodular welfare (RSW) problem. The authors first propose a 1-1/e approximation to the RSW problem, then show that for MBB with unknown reward distributions, using the upper confidence bounds (UCB) instead of the known mean values would give a sublinear approximation regret with approximation ratio of 1-1/e. The main contribution of the paper is to achieve 1-1/e approximation, while an easy adaptation from some other work only gives 1/2 approximation.","This paper introduces matroid blocking semi-bandits (MBB), which generalizes blocking bandits and matroid bandits. To solve the full-information variant of MBB, the authors reduce it to Recurrent Submodular Welfare (RSW) and propose a novel technique of interleaved scheduling to achieve a (1-1/e)-approximation. For the bandit setting of MBB where the mean arm rewards are unknown, based on the result from the full-information setting, they propose an UCB-type algorithm with regret guarantees.",0.11350293542074363,0.11154598825831702,0.0684931506849315,0.24822695035460993,0.1773049645390071,0.21374045801526717,0.41134751773049644,0.4351145038167939,0.44871794871794873,0.26717557251908397,0.32051282051282054,0.358974358974359,0.17791411042944782,0.17757009345794392,0.11884550084889643,0.25735294117647056,0.22831050228310507,0.2679425837320574
414,SP:5a245fdbaaaeb734dd2ec7d8f33051c1848b72a7,"Reweighting adversarial data has been recently shown to be effective, with larger weights assigned for data closer to the boundaries. Previous work used the LPS to measure the closeness, but the authors suggested that the LPS might not be reliable since they are discrete and path-dependent. To this end, this paper proposed three types of probabilistic margin, which could overcome the drawbacks of the LPS. The probabilistic margins were used to measure the importance of adversarial data, which induced convincing results in experiments.","DNNs are susceptible to adversarial attacks, and AT is an efficacious method in combating this challenging issue. In general, AT creates adversarial examples on the fly, and the DNN is trained to overcome their impacts.   The paper mainly focuses on the drawbacks of the reweighted AT training. First, the authors state empirically that the LPSs used in previous works are discrete and path-dependent, unreliable for weight estimation. They then propose a novel metric (PM) that overcomes the drawbacks in LPSs and devise three specifications that capture different geometric properties of the data. Next, a weighting strategy uses PMs to develop a new kind of reweighted AT method called margin-aware instance reweighting learning (MAIL). ",This paper propose  three types of probabilistic margin (PM) to improve the model robustness. The proposed method can measure the aforementioned closeness and reweight adversarial data.  And some experiments were conducted to validate the effectiveness of the proposed method.  ,"This paper focuses on reweighting adversarial data in adversarial training problems. Actually, how to determine the sensible weights for each adversarial example matters for the adversarial robustness of the trained model. In this way, this paper proposes a new measurement called probabilistic margin, and employs it into multiple baseline methods. The achieved results seem promising. ",0.23809523809523808,0.16666666666666666,0.17857142857142858,0.09565217391304348,0.12173913043478261,0.20512820512820512,0.17391304347826086,0.358974358974359,0.2727272727272727,0.28205128205128205,0.2545454545454545,0.14545454545454545,0.20100502512562815,0.2276422764227642,0.21582733812949637,0.14285714285714285,0.16470588235294117,0.1702127659574468
415,SP:5a470815fd96d730f428004f9c82a079fd812ce4,"Pure exploration plays an important role in online learning. Existing work mainly focuses on the UCB faces some challenges when looking for the best arm set under some specific combinatorial structures, so authors explore the idea of Thompson Sampling (TS) that uses independent random samples instead of the upper confidence bounds to make decisions, and design the first TS-based algorithm framework TS-Verify. In TS-Verify, the sum of independent random samples within arm set S will not exceed the exact upper confidence bound of S with high probability. As for pure exploration of classic multi-armed bandit, they show that TS-Verify achieves an asymptotically optimal complexity upper bound.  The TS-based verification framework TS- Verify takes a target (super) arm as input, and aims to verify that this target (super) arm is optimal with error constraint . At each time step, TS-Verify first draws independent random samples  for all the (base) arms. Then it tries to find out the M best (super) arms under sample sets.  If in most of these sample sets, the target (super) arm is the best one, then the algorithm will output that the target (super) arm is indeed optimal. Otherwise, the algorithm will choose to pull a (base) arm from the exploration set, which contains the target (super) arm and all the (super) arms that have been optimal in at least one sample set at this time step.  ","This work studies pure exploration for stochastic combinatorial multi-armed bandits, for the fixed confidence setting.  Thus, the goal is to take actions so that the optimal action is identified with high probability (achieving a pre-specified fixed confidence) with as few samples as possible.  While similar problems have been well-studied when action space has (best arm identification, top-k identification), the more general combinatorial setting has not been.  A (common) simplifying assumption of linear reward is used.     This work proposes a explore-then-verify type method.  For some action sets and confidence levels, the explore stage can be done quickly while verification can be challenging.  In the proposed method, base arms are first explored (this work suggests low complexity off-the-shelf method), then a verification stage, for which the method proposes and analyzes a novel verification algorithm using Thompson-sampling.   The authors prove sample complexity bounds for their method, showing it can be competitive against state of the art methods, and is asymptotically optimal (as the confidence level delta goes to 0). ","The paper provides a Thompson Sampling (TS) algorithm for verifying whether a given arm is indeed the best arm (or best super-arm in a combinatorial bandit) with fixed confidence and its theoretical analysis. The proposed algorithm TS-Verify can be used as a subroutine within the explore-then-verify framework of [Karnin 2016, Chen et al 2017] which identifies the best arm (or super-arm) with fixed confidence. The theoretical analysis of the algorithm shows it uses a factor of width fewer samples for verification in combinatorial bandits compared to existing CLUCB algorithm [Chen et al 2014]. The key step in the analysis is defining three events that occur with high probability. $E_{0,m}$ is a concentration event due to independent samples from each arm distribution, $E_{1,m}$ is a single sample Gaussian tail and $E_{2,m}$ is a concentration event for independent Bernoulli samples. These events specify random events for all times, and each time step $t$ defines an independent probability measure. The analysis for the combinatorial bandit follows a similar path, and at a high level, the improved sample complexity is obtained due to the concentration of event $E_{0,c}$ in appendix.","This paper designs the first TS-based algorithm to solve the pure exploration problem for both multi-armed bandit (MAB) and combinatorial MAB (CMAB) problems. It mainly focuses on the verification problem where a target arm is given and the algorithm needs to determine whether it is the optimal one.   Benefiting from independent samples of the TS algorithm, the TS-based verification framework achieves lower complexity than the UCB-based one.  ",0.1228813559322034,0.15677966101694915,0.11864406779661017,0.17714285714285713,0.10285714285714286,0.10050251256281408,0.1657142857142857,0.18592964824120603,0.39436619718309857,0.15577889447236182,0.2535211267605634,0.28169014084507044,0.1411192214111922,0.17011494252873563,0.18241042345276876,0.16577540106951869,0.14634146341463414,0.14814814814814817
416,SP:5a4f3b38049e9b2c894550b8ade8495e3b024f23,"The paper proposes a Bayesian framework for source estimation in M/EEG. The Bayesian framework explicitly estimates spatial and temporal covariance matrices for both signal and noise, before estimating the posterior mean. A majorization-minimization algorithm is proposed to estimate the model, and detailed proofs are provided, including a convergence proof. The model is benchmarked against three source-estimation methods on simulated signals. Finally, the model is used in empirical recordings with a challenging low number of trials.",This manuscript presents an efficient majorization-minimization optimization framework to infer the parameters of the spatio-temporal covariance matrix in the MEEG signal source reconstruction problem. The main contribution lies in simplifying a non-convex optimization problem using a convex surrogate function with a convergence guarantee. The method has been tested in few limited scenarios on simulated and real MEEG data and the results are compared against three alternative approaches.  ,"This paper proposes an efficient method for source reconstruction from M/EEG data that estimates the spatiotemporal correlation structure in both the source space and the measurement error. The spatiotemporal pattern of the source and noise covariances is given by a Kronecker product structure with an independent (diagonal) spatial structure and either a full or Toeplitz temporal structure, where the temporal component is shared across the source and noise components. Since the full optimization problem is non-convex, the authors propose a majorization-minimization (MM) algorithm with provable convergence and derive simple, efficient iterative updates for each component. Two versions of the model are proposed: full Dugh, which uses a full positive definite matrix for the temporal covariance; and thin Dugh, which can be applied in the stationary data setting and uses a Toeplitz temporal covariance structure with an alternative set of efficient updates for the MM algorithm. On simulated data with known sources, thin Dugh achieves comparable or better performance as compared to existing source reconstruction methods (Champagne, eLORETA, and S-FLEX). When applied to simulated data that was generated while varying the true underlying temporal covariance structure, full Dugh is shown to outperform thin Dugh when the true matrix is full and vice versa when it has Toeplitz structure. Finally, experiments on real MEG data from subjects presented with visual or auditory stimuli show that full and thin Dugh can recover plausible and well-localized temporal and spatial patterns for the reconstructed sources, even with a very limited number of trials.","Inverse modeling is at the heart of medical imaging techniques, especially in neuroimaging. Many techniques have been developed in recent years, first based on PDE techniques and, more recently, by machine learning approaches. The addition of a priori knowledge to these techniques allows a better understanding of the studied phenomenon. Therefore, it is necessary to develop robust methods combining modeling, a priori knowledge, while controlling the computational cost. This article proposes to derive an efficient inference algorithm with spatio-temporal dynamics in model parameters and noise. The spatial and temporal duality is realized by using the product of  Kronecker product. They assume sparse spatial covariance matrices, while the temporal covariance is modeled to have either full or Toeplitz structure. They then define a Bayesian model whose parametric estimation is performed by a Maximization-Minimization algorithm. The proposed results are motivated by demonstrations (detailed in appendix) and numerical experiments.",0.16666666666666666,0.41025641025641024,0.21794871794871795,0.34285714285714286,0.2,0.11067193675889328,0.18571428571428572,0.12648221343873517,0.11486486486486487,0.09486166007905138,0.0945945945945946,0.1891891891891892,0.17567567567567569,0.19335347432024164,0.1504424778761062,0.14860681114551083,0.12844036697247707,0.13965087281795513
417,SP:5aa3256114491d4eff667449063513af52d63e26,"The paper is concerned with learning a model in situations when some features have spurious correlations with the label (for example, for classifying sheep vs camel, background can be a spurious feature). The idea is that if a lot of source data is available, these spurious correlation features should be easy to find. To find the unstable features, authors hypothesize that they are related to mistakes that a classifier makes in different environments. Therefore, they learn a model on one environment and run it on a different environment (source data), splitting the data into correctly predicted and not. Then for each partition (correct or not) examples with the same labels are encouraged to be close together, so the embedding learns UNSTABLE features (fz). Then the target data is clustered based on unstable features representation fz, and then DRO (existing method that assumes the existance of correct groupping based on unstable features) is used to train a robust target classifier","This paper aims to transfer the knowledge of spurious correlations in a set of source environments to a target environment. They assume the degree of spurious patterns vary among source environments (e.g. color has different correlations with the number in MNIST), and the spurious pattern are the same in both the source and target tasks. Then they aim to transfer the spurious knowledge from source environments to learn a classifier in the target task that ignores the spurious pattern.   They train the model in 3 stages. First, they train a regular classifier in each environment. E.g. given two source environments E1 and E2, they train the corresponding classifiers f1, f2. Second, they use the error of the f1 on E2 to ""separate"" the E2 examples into two groups, one group for correct predictions and the other group for the wrong predictions under each class. They assume the cause of the error comes from the spurious patterns, so each group will correspond to either high or low spurious patterns learned in f1. Thus they learn a f_Z that outputs an embedding to separate these two groups by triplet loss. Finally, in the target task, they cluster the examples into different groups by the similarity of f_Z that captures examples with similar spurious patterns. Then they apply Distribution Robust Optimization (DRO) on these groups that optimizes the worst performance across all groups. This ignores the spurious patterns based on f_Z in the target task and learns a stable classifier.","The authors introduce a method (TOFU) for learning classifiers that are robust to spurious correlations in a transfer learning setting. They argue that approaches which rely only on (input, label) pairs and use no extra information to identify spurious features could fail to learn a robust model due to insufficient data for the target task. The authors provide scenarios where useful metadata, namely varying environments, could be available for source tasks, ready to be leveraged by the target task classifier for the purpose of identifying a shared bias that might exist across all tasks. To identify unstable/spurious features, the authors follow a 3-step procedure proposed in an existing work. Namely, a classifier is trained on data from environment E1, and is evaluated on data from a second environment E2. The data in E2 is then partitioned according to the correctness of the classifier's predictions on a per-class basis. A metric learning objective using a triplet loss is used to learn a model which embeds samples according to their unstable features with the goal that the unstable features of the correctly predicted samples should be closer together than unstable features between correctly predicted and incorrectly predicted samples. The objective is justified based on prior theoretical work and is illustrated visually. Extensive empirical evaluations are performed, comparing the performance of TOFU against many different baseline methods on both image and text classification tasks.","The paper considers a transfer problem when the spurious correlations of source tasks can be applied to the target task. The authors propose to identify the unstable features on source tasks first and then cluster the target data according to these features. Finally, an invariance-based method are incorporated to eliminated the influence of unstable features.",0.2389937106918239,0.25157232704402516,0.16981132075471697,0.18253968253968253,0.0873015873015873,0.11063829787234042,0.15079365079365079,0.1702127659574468,0.48214285714285715,0.19574468085106383,0.39285714285714285,0.4642857142857143,0.18491484184914844,0.20304568527918784,0.2511627906976744,0.18891170431211501,0.14285714285714285,0.17869415807560135
418,SP:5ab8c2c96b2b14285a9b318d2b457e717b0515c4,"This paper proposes a novel normalization layer called Continual Normalization for continual learning. In the paper, the authors point out the problem of global moment bias of Batch Norm. To address the problem, the authors combine the advantages of Group Norm and Batch Norm together to improve sharing while reduce forgetting. Comprehensive experiments on various benchmarks and continual settings demonstrate the effectiveness of the proposed method.","The paper studies the role of normalization layers in continual learning. The main argument is that vanilla BatchNorm is not very suitable since the statistics of data change across tasks. To alleviate this, the authors propose the CntinualNorm layer which is essentially the combination of GroupNorm followed by BatchNorm. The experiments compare different normalization schemes across various CL benchmarks.","The paper considers the problem of activation normalization in neural networks in the context of training on non-i.i.d. data in continual learning. The authors showcase the issue of the ""cross-task normalization effect"", where the data from a given task is normalized by statistics biased towards another (latest) task. Based on this phenomenon, they show that commonly used normalization schemes, such as Batch Normalization or Group Normalization, suffer from certain problems in this setting (high forgetting, low transfer). They propose the Continual Normalization which combines Group Normalization with Batch Normalization to obtain better results and show the improvements empirically on multiple CL datasets.","This paper investigates the problem of cross-task normalization in a non-stationary online continual learning(CL) setting. It argues that batch normalization (BN) is important to CL; however, BN in the current form introduces a bias towards current task, leading to catastrophic forgetting. The paper presents a continual normalization(CN) layer to reduce this bias. In particular, it combines spatial and batch normalization into one layer that is suitable for CL. The authors conducted experiments to evaluate the performance of the proposed method. ",0.22727272727272727,0.24242424242424243,0.2727272727272727,0.3050847457627119,0.288135593220339,0.2169811320754717,0.2542372881355932,0.1509433962264151,0.21428571428571427,0.16981132075471697,0.20238095238095238,0.27380952380952384,0.23999999999999996,0.18604651162790697,0.23999999999999996,0.21818181818181817,0.2377622377622378,0.24210526315789477
419,SP:5ad990291ef6f7fa38de51a2889af112e3b2aa60,"**Update after authors' response:** After reading the other reviews, the updated manuscript and the authors response I think the authors have made several small improvements based on the reviewers' suggestions. Typically I would now be leaning towards a score of 7. However, this year's conference only allows for either a 6 or an 8. To me personally the paper is still slightly closer to 6 than 8, but to make a clear statement in favor of acceptance and facilitate the reviewers' discussion I have raised my score to an 8. I will clearly state my opinion (and hesitation to strongly endorse acceptance of the paper) during the reviewers' discussion.  **Summary**: The paper proposes a novel method for lifelong compositional RL. The latter is formally introduced in the paper as lifelong RL (learning to solve a continuous stream of tasks without re-visiting previously seen tasks) with families of tasks that have a known compositional structure. The paper proposes to use a modular neural architecture (consisting of layers of modules) to capture the compositional nature of the tasks to learn. Through an appropriate module-selection process the (topologically disjoint) modules become functionally disjoint. This is shown to facilitate forward-transfer (faster learning of novel task variations that follow the compositional structure). The paper investigates two module-selection mechanisms: a fixed mechanism where the correct sequence of modules for each task is known, and a brute-force combinatorial search over all module combination to find the highest scoring combination. Once modules are selected, the selected module-parameters are trained to increase current task-performance. This is then consolidated with previous task experience (in an off-line RL fashion) via batch-RL to avoid catastrophic forgetting. Experiments are shown on a set of 2D grid-world tasks, and a number of simulated robot arms with different sensory setups, and the method performs well against a number of baseline methods.  **Main contributions** 1) Definition of compositional families of RL tasks. As the paper correctly points out, a number of previous work on compositional architectures has used variations of off-the-shelf (lifelong) learning tasks, which are not explicitly compositional and a beneficial composition of modules is often not known intuitively (often not even the number of modules is intuitively clear). The two task-families introduced overcome these issues and are a nice contribution towards more meaningful experimentation. The gridworld task is conceptually easy and does not have very complicated perceptual problem or high-dimensional continuous action-spaces; the robotic-tasks on the other hand is closer to real-world applications (and the corresponding complexities). Significance: I think the tasks are well chosen and might become standard-tasks for lifelong compositional RL.  2) Proposal of a compositional architecture and training procedure. Given that the compositional structure of the tasks is precisely known, it is relatively straightforward to propose an exactly matching architecture. The main innovation is a training procedure to train such an architecture. Unfortunately the paper proposes two fairly straightforward solutions that need strong simplifying assumptions: one, the correct sequence of modules is known and applied accordingly, or two, brute-force search over all module combinations (which becomes exponentially costly with increasing numbers of modules, and requires training-data, and relies on disjoint sets of tasks initially until all modules have been trained at least on one task). Significance: the two module-selection mechanisms serve as important controls and lead to promising results. But they are also fairly straightforward conceptually, and rely on often unrealistically strong assumptions. What I would have liked to see for more impactful results is an attempt to train the architecture from scratch without brute-force search and disjoint tasks initially - one possible attempt would be to use RL to train a module selection policy, similarly to Chang 2019, which is used in the paper as a main inspiration for defining compositional RL tasks.  3) Definition of lifelong compositional RL and using techniques from offline RL to avoid catastrophic forgetting. Significance: these are sensible choices, and thus valid contributions, but both have been conceptually proposed before in slightly different settings (Chang 2019 defines compositional supervised problems and using offline RL to avoid catastrophic forgetting and even have backwards-transfer is also not very far-fetched). ","The paper introduces an algorithm for lifelong reinforcement learning using functional neural composition. The algorithm first maps a new problem onto a composition of previously acquired modules; then, the agent trains/finetunes the selected module combination on the new task; and finally, the agent incorporates this newly acquired information into the existing modules. This algorithm is then evaluated on two domains, one multi-task lifelong gridworld domain and a multi-task lifelong robotics domain. The algorithm produces superior results to several other lifelong learning approaches.","This paper introduces and explores the use of *functional compositionality* in lifelong reinforcement learning (RL). In contrast to hierarchical reinforcement, which explores temporal compositionality (the chaining of options or subpolicies across time), functional compositionality involves assembling a novel, overall function (in the case of RL, the policy function) by composing together neural modules that perform multi-stage processing of the policy's inputs. Each module can be regarded as a small function that takes some abstract input X and maps it to an abstract output Y -- the initial set of modules will take the state space as the input X, while the final set will take the actions as the output Y. Modules can perform their computations in parallel, with their outputs concatenated together afterwards, or they can be chained together, with the output of one set of modules serving as the input of the next set. In this setup, the continual learning problem consists of two phases: first, the correct set of modules for the task must be selected and composed into the policy; then, the parameters of those neural network modules must be updated from the data generated by the agent. Selecting the incorrect set of modules will lead to poor transfer and catastrophic forgetting (as the parameters of the incorrect neural network modules get overwritten). This modularity can help improve robustness by reducing dependencies between modules -- this is achieved both through parallelism and through the need of downstream modules to generalise to novel combinations of upstream modules. The authors validate their approaches on a set of tasks with compositional structure, which a functionally compositional policy would be able to exploit, as it could substitute out modules according to the components of the current task. They compare their framework against alternative continual learning baselines and show that their framework is able to (i) avoid catastrophic forgetting; (ii) demonstrate better zero-shot transfer to new tasks; and (iii) even exhibit better backward transfer on training tasks.","This paper combines insights from modular meta-learning, lifelong learning and RL to arrive at a system that can learn, online, to train modules that can be recombined to solve future tasks without further training. In addition, training that happens offline can retrospectively improve modules that were constructed to solve earlier tasks.   These methods are demonstrated in a pedagogical domain and on some high-dimensional robot-control problems.",0.0453257790368272,0.11048158640226628,0.031161473087818695,0.3058823529411765,0.09411764705882353,0.0672782874617737,0.3764705882352941,0.23853211009174313,0.3235294117647059,0.07951070336391437,0.11764705882352941,0.3235294117647059,0.08091024020227561,0.1510164569215876,0.056847545219638244,0.12621359223300974,0.10457516339869281,0.11139240506329112
420,SP:5b66fe250b4b876c76e57fd4355819b4675b7f68,"This paper makes use of the NeRF idea on RIRs (coined here as NAFs).  The main idea is relatively straightforward, sample a room from multiple emitter/receiver locations and learn to predict the resulting RIR using a neural net that receives as input only the involved locations, and some positional information.  It is shown that this approach learns something, and it is used in a few demonstrative examples such as sound localization and enhancing NeRF estimation.","The manuscript considers the problem of encoding room impulse responses in a room. Specifically, the authors propose a network that takes in some geometrical cues from the room, along with speaker, listener positions, and train a neural network using pre-recorded room impulse responses. They argue that the neural network learns to make use of the geometrical cues, and can generalize beyond the dataset.","This paper introduces the concept of Neural Acoustic Fields, which is an implicit representation to capture how sound propagates in a pysical scene. Using a NeRF-like network architecture, the model is able to predict room impulse response given the listener and source locations as input. The idea is new and interesing, but there are multiple overclaims in the paper. Some interesting tasks are demonstrated like sound source location, cross-modal generation, etc.","This paper proposes an neural acoustic field (NAF), which is an extended version of neural radience field applied on impulse responses defined between listener and emitter positions. It predicts an impulse response given the listener and emitter positions so that one can simulate the impulse response in arbitrary positions. One of the key idea to extend the model to unseen positions is to take local grid features as input. The authors show other applications with NAF such as source localization and multi-modal NERF. Experiment results show that the proposed method shows better results when comparing to baseline methods.",0.15789473684210525,0.21052631578947367,0.21052631578947367,0.203125,0.1875,0.2602739726027397,0.1875,0.2191780821917808,0.16161616161616163,0.1780821917808219,0.12121212121212122,0.1919191919191919,0.17142857142857143,0.21476510067114093,0.18285714285714286,0.1897810218978102,0.147239263803681,0.22093023255813954
421,SP:5b8493c4f8645a289d056c978a0f99dbf4d9a6b0,"This paper focuses on making use of latent invariant representations for unsupervised domain adaptation. The goal is to address two State-Of-The-Art existing methods issues. The first is there is no principled way to ensure that marginal invariance (P^S(Z) = P^T (Z)) preserves conditional invariance (P^S (Z|Y) = P^T (Z|Y)). The second is there are cases (especially in lower-dimensional datasets) where the encoding function Phi needs domain-specific information, in addition to the features X.  This paper introduces an invariant representation learning method that uses the data-generating process to justify and efficiently use domain-specific information in the encoder Phi and constraint it to ensure that Z contains relevant label-specific information. Results on three real datasets and one synthetic dataset show the proposed Domain-Specific Adversarial Network (DSAN) model’s performances (accuracy and Wilcoxon p-value) compared with existing methods and an unregularized DSAN (DSAN-U) version. ",The paper proposes a unsupervised domain adaptation method through the lens of learning invariant representations. The paper validates theoretically shows some limitations of current methods when learning from distinct source and target distributions. The paper validates experimentally on a large scale image dataset and 2 other datasets.,"This paper analyzes the mechanism of domain adaptation and proposes a new strategy to reduce the gap between the two domains. The valuable information to achieve accurate classification can be well preserved. Extensive experiments are conducted on multiple dataset, such as Wi-Fi localization and Amazon Review.","**Summary and contributions:** The paper considers representation learning for unsupervised domain adaptation (UDA). They show that, in some cases, it is necessary to learn encoders that vary across domains, something which is rarely done in practice. They thus propose a new method for incorporating domain-dependent features in the encoding process.  The end model/objective contains many different components: it is essentially a DANN with a decoder that aims to reconstruct the inputs by using domain-dependent and independent features. Furthermore, the decoder is forced to rely as little as possible on the domain-dependent features.  ",0.08917197452229299,0.08917197452229299,0.1337579617834395,0.19148936170212766,0.23404255319148937,0.2127659574468085,0.2978723404255319,0.2978723404255319,0.21875,0.19148936170212766,0.11458333333333333,0.10416666666666667,0.13725490196078433,0.13725490196078433,0.16600790513833993,0.19148936170212763,0.15384615384615385,0.13986013986013987
422,SP:5beb8aac7c0ae78cedc1e415798a4e17c6146661,"This paper proposed a new knowledge distillation framework, feature kernel distillation (FKD), by matching the student network's feature kernel and that of the teacher network's feature kernel. The authors extended the theoretical results in Allen-Zhu & Li (2020) to demonstrate the advantages of their framework over standard NN training. The theoretical results are built upon a synthetic dataset consisting of multi-view and single-view data, where standard training will fail to generalize while FKD will generalize well. Empirically, the authors (loosely) showed the consistency between the empirical behavior of FKD and the theoretical predictions, though the empirical setting is fairly different from the theoretical setting. In addition, on several image datasets, FKD can outperform other KD baselines in terms of test accuracy.","In this paper the authors consider extending the knowledge distillation (KD) framework to cases where the student and the teacher do not share prediction spaces. To this end they propose Feature Kernel Distillation (FKD), where one views the last layer weights of a neural network as a data dependent feature kernel. Instead of regularising the distance between temperature scaled logits corresponding to each data point of the student and the teacher in vanilla Knowledge Distillation, the authors instead regularise based on the distance between feature kernels of a pair of data points (inner product of weights of the pre-final layer of the NN).   Under a simplified setting, viewing data as being multi-view the authors show a setting where classical knowledge distillation would not do well, but a FKD based knowledge distillation using an ensemble of teachers does well (the teacher kernel between two data points is the average teacher kernel across the ensemble). The authors then propose using a correlation kernel instead of raw kernels and feature regularisation to spread out the kernel values.  The authors proceed to empirically demonstrate  that this approach helps in dataset transfer between datasets of different number of classes. They also show that using an ensemble of teachers enables FKD to improve performance (but KD also improves performance).","In this paper the authors consider neural networks as data-dependent kernel machines and propose applying a distillation method directly on the pairwise kernel matrix of the models. Authors extend their setting into ensemble settings, examining both some theoretical aspects of the process, building upon the work of Allen-Zhu & Li (2020), as well experimentally. More specifically, authors provide experiments on ensemble distillation on CIFAR-100, as well as on dataset knowledge transfer on CIFAR-100 to STL-10 and CIFAR-10.",There are several approaches in knowledge distillation for neural networks. This paper studies kernel-based knowledge distillation for multi-view data set. The authors revealed that the assembled feature map obtained from the ensemble teacher network effectively improves students' generalization ability. Some numerical results are presented to show the practical effectiveness of the learning method based on theoretical findings.  ,0.312,0.136,0.128,0.13023255813953488,0.07906976744186046,0.14634146341463414,0.1813953488372093,0.2073170731707317,0.2711864406779661,0.34146341463414637,0.288135593220339,0.2033898305084746,0.22941176470588237,0.1642512077294686,0.17391304347826086,0.18855218855218855,0.12408759124087591,0.1702127659574468
423,SP:5bfd97c5f679b1d5e373dd75fc85bc107d94ada9,"A self-supervised visual representation learning method is proposed in this paper. Besides the prevalent two-branch CNN architecture design, the two-branch transformer structure is introduced to improve CNN backbone features via attention enhancement. The proposed method is evaluated on the several visual recognition benchmarks and shown effective through extensive experiments.","This paper uses transformers to improve visual representation learning. The two view augmentation and the response generation solution from existing approaches are boosted by transformers for visual attention enhancement. The main contribution lies in a structure with two transformer branches to improve CNN attention and supervise the original CNN branches. I can see that the attention mechanism obtains gains due to the parallel design. The proposed method shows superiority, compared to existing work. ","In this paper, a self-supervised visual representation learning approach is developed to improve CNN backbone encoders. The transformers are introduced to improve visual attention abilities of transformers. The evaluation on the experiments has shown the proposed approach achieves favorable performance.","Transformers are utilized for representation learning. Instead of using them as backbones to be optimized, this paper proposes to improve CNN backbone attentions via transformers in the form of network prediction supervision. In the experimental validation, the proposed method CARE is shown to improve the original representation learning methods with only ConvNets introduced at the 100, 200, 400 epoches.",0.28846153846153844,0.3269230769230769,0.2692307692307692,0.1917808219178082,0.1643835616438356,0.2926829268292683,0.2054794520547945,0.4146341463414634,0.23728813559322035,0.34146341463414637,0.2033898305084746,0.2033898305084746,0.24,0.3655913978494624,0.2522522522522523,0.24561403508771934,0.18181818181818182,0.24
424,SP:5cb40b77b9510ecdbefe9e744b66f32773337aae,"The authors present DeepSplit, a novel solver for a popular neural network convex relaxation. Relying on ADMM and on a careful problem re-formulation, the authors achieve both a O(1/T) converge rate and closed-form solutions for the inner problems, differently from previously presented solvers for the same relaxation. Computational results are presented for incomplete neural network verification, showing that DeeSplit scales to a ResNet18 and achieves better bounds in the same time compared to relevant baselines.","This paper proposes an efficient solver for computing the convex relaxation of Neural Networks, which is an important component of verification methods. The method is based on ADMM, with the authors proposing a novel decomposition of the optimization problem, which allows them to get closed form solutions of all the intermediate steps of the algorithm, The proposed method also improves on the guaranteed convergence rate and is shown to scale to larger networks than the ones considered in other works.  Evaluation is performed on robustness verification on Cifar10 and on Reinforcement learning models.","The paper proposes a novel method for the neural network verification problem. The authors use variable splitting and Lagrangian relaxation to relax the verification problem, which is then solved with ADMM. The method is GPU friendly (similar to previous work like Dvijotham et al. 2018 and Bunel et al. 2020a), and claims to solve the relaxation to optimality.","This paper proposes an operator splitting method which solves a convex relaxation of a learge-scale nonconvex optimization problem for analyzing the worst-case performance of deep neural networks against input perturbations. The proposed method is modular, scalable and parallelizable. Experimentally, the authors demonstrate that the proposed method has tighter bounds on the worst-case performance of large CNNs in image classification and RL settings. ",0.20253164556962025,0.20253164556962025,0.13924050632911392,0.1827956989247312,0.22580645161290322,0.20689655172413793,0.17204301075268819,0.27586206896551724,0.16923076923076924,0.29310344827586204,0.3230769230769231,0.18461538461538463,0.18604651162790695,0.23357664233576644,0.15277777777777776,0.22516556291390727,0.26582278481012656,0.19512195121951223
425,SP:5cb9153e060d5fae910eacd5869a7b9602ca50b0,"This paper analyzed the empirical Rademacher complexity of polynomial nets, the Coupled CP-Decomposition (CCP), and Nested Coupled CP-decomposition(NCP) nets. The corresponding Lipschitz constants for both models are derived. The Projected SGD and the Projected SGD + Adversarial Training algorithms are used to optimize the CCP and the NCP with the Lipschitz constants. Experiments on classification and adversarial attack tasks are performed to evaluate the proposed method. Results show that the proposed projection regularization is better than the Jacobian regularization and the L2 regularization methods in adversarial attack tasks. ","The paper introduces Rademacher Complexity bounds and Lipschitz constant bounds (both $\ell_2$ and $\ell_\infty$) for two families of polynomial networks. While prior theoretical results on complexity and Lipschitz constant bounds exist for standard neural networks that consist of linear layers and activation functions, these results do not directly apply to Polynomial networks due to their vastly different structures from standard neural networks.   On the practical side, the authors found that imposing $L_\infty$ norm constraint on the weight matrices of the polynomial networks is effective against adversarial perturbation. In particular, the paper focuses on robust image and audio recognition tasks and found out that imposing $L_\infty$ norm constraint via projected gradient descent is more effective than other commonly deployed regularization techniques (in standard networks) such as Jacobian regularization and weight decay. ","The paper derives empirical Rademacher complexity bounds for two variants of PN, along with their Lipschitz constants. The derived bounds motivate practical regularization schemes that are implemented based on projected gradient descent. Empirical results demonstrate the usefulness of the proposed method.","The paper provides theoretical guarantees for polynomial neural networks, specifically bounds on Empirical Rademacher Complexity (which implies generalization guarantees) and on the Lipschitz constant of the networks (implies robustness). Polynomial nets have shown state-of-the-art results for several machine learning tasks. Specifically CCP and NCP models of Chrysos et al. (2020) are considered, and the bounds assume bounded l-infinity-norm which is meaningful for image inputs. Further an algorithm is proposed which adds regularization to polynomial networks using the terms in the theoretical upper bounds (corresponding to weight norms) which leads to gain in accuracy, especially in adversarial settings (where the regularization is combined with adversarial training).",0.24444444444444444,0.14444444444444443,0.25555555555555554,0.11940298507462686,0.16417910447761194,0.2926829268292683,0.16417910447761194,0.3170731707317073,0.20909090909090908,0.3902439024390244,0.2,0.10909090909090909,0.19642857142857145,0.19847328244274806,0.22999999999999995,0.18285714285714283,0.18032786885245902,0.15894039735099338
426,SP:5d390a04a87a43db6b3ee63f009dde00f331b3af,"This paper proposes a non-iterative method to improve adversarial robustness. Compared to mainstream AT baselines, the proposed method avoids iterative gradient calculations, so as to be time-saving. By involving attribution maps and perturbing the allowed set of pixels, the proposed method can achieve better performance. After sufficient experiments, the proposed method beats all baselines.","The paper proposes an AT based defense method, which only focuses on the salient image regions to perturb and attack. Saliency is computed from a pertained teacher model; and is gradually reduced during the training (hence the name curriculum learning). A two phase approach is proposed: phase-1 computes the saliency map; and phase-2 generates adversaries by altering the salient pixels only (by gradually reducing the salient pixels from 90% upto 50%). Experiments are given on CIFAR-10/100 and TinyImageNet. ",The paper proposes a non-iterative adversarial defense. The method trains networks to have similar saliency maps as a teacher model. The authors show empirically that their method outperforms baseline defenses while requiring less training time.,"The paper proposes a two phase non-iterative approach for efficient training of robust models.   The first phase is a distillation phase which trains a neural network while enforcing the gradients of the loss w.r.t. to the input image of the student network to be indistinguishable to that same gradient for the teacher network with the goal of enforcing the saliency maps of the teacher and student networks to be “similar”. In addition to the binary cross-entropy GAN-type loss, the paper also explicitly promotes the two saliency maps to be similar by adding an $\ell_2$ regularization on their differences.   The second phase, which is called model refinement, is a curriculum learning based adversarial training fine-tuning step. At every step of this phase, the attacker is limited to only perturbing the top X% of pixels from the teachers saliency. As training progresses, the attacker is more limited (i.e., X gets smaller). ",0.19642857142857142,0.21428571428571427,0.30357142857142855,0.14634146341463414,0.25609756097560976,0.4444444444444444,0.13414634146341464,0.3333333333333333,0.10828025477707007,0.3333333333333333,0.1337579617834395,0.10191082802547771,0.15942028985507248,0.2608695652173913,0.1596244131455399,0.2033898305084746,0.17573221757322177,0.16580310880829016
427,SP:5d7e71762ee0a0dfeb34073759eeacd2c5ac10fe,This paper proposed a lightweight convolutional neural network based on the idea of quaternion neural networks. The paper extended the hypercomplex linear layers to convolution neural networks to apply to many multi-dimensional applications. The authors defined the parameterization of hypercomplex convolutional layers so that convolution neural networks can utilize the quaternion algebra to improve the parameter efficiency.,"This work extends the recently proposed hypercomplex parameterization of neural layers to convolutional layers. Then it describe common deep CNN models (ResNet, VGG, SeDnet) based on this novel paradigm in two different tasks (Image Classification and Sound Event Detection). The introduced method offer better performance with a reduced number of neural parameters as well as faster training and inference than equivalent hypercomplex layers. ","The paper proposes Parametrized Hypercomplex convolutional Layers to replace convolutional layers to attain similar performance using fewer parameters. The PHC layer can subsume hypercomplex convolution rules. The main advantage of the PHC Layer is that they enable choosing an arbitrary $n$ to reduce the number of parameters to $1/n$, whereas this was limited to $4$, $8$, and $16$ with quaternions. Hyperparameter $n$ also drives the number of convolutional filters. The proposed network can represent a real-valued convolutional layer by setting $n = 1$.  The PHC layer differs from the convolutional layer in the way that the weight matrix is obtained. $H$ acts as a weight for input $x$. The $A$ and $F$ matrices which are trainable and used to obtain $H$. Kronecker products of every matrix in the tensors $A$ and $F$ are taken and then these are summed to get $H$.    Authors have proposed PHC versions of some common neural networks such as ResNet and VGG and have compared the results with real-valued models, Quaternion models, and PHC models (for $n=2$ and $n=4$). ","Traditionally, networks use real-valued weights. However recent work has shown that in certain domains, weights that obey different algebraic rules can lead to various performance enhancements. The authors introduce a hyper complex convolution block and show that it leads to performance gains and parameter reductions over a variety of baseline models on tasks in various domains. ",0.1724137931034483,0.27586206896551724,0.1206896551724138,0.23809523809523808,0.1111111111111111,0.06179775280898876,0.15873015873015872,0.0898876404494382,0.12280701754385964,0.08426966292134831,0.12280701754385964,0.19298245614035087,0.1652892561983471,0.13559322033898305,0.1217391304347826,0.12448132780082989,0.11666666666666667,0.09361702127659574
428,SP:5dc53eaa7923a68bca4465759657a31c3ad4e0b7,Update after discussion among reviewers: I am still strongly in favor of accepting this paper.   The paper presents a new method to represent arbitrary shaped text in images and shows how this representation can be used in the output layer of some of the recent text detection methods.   The results look qualitatively and quantitative great.   The method uses a simple representation that is intuitively understandable.   ,"The paper deals with the problem of scene text detection, focusing on irregular text. The paper proposes a segmentation approach, in line with recent methods that deal with irregular text detection. The Main contributions of the paper are: - A detection method base on predicting both clipped segmentation masks (as other methods do) referred to as text kernels, and shifts for the areas around the kernel that can assign nearby pixels to the kernels. This allows one to perform quick postprocessing, as opposed to some other families of methods like PAN[35] or TextSnake[21]. - A modified loss to predict the shifts, that uses masking. Empirical results on standard benchmarks show accurate results at better FPS.",This paper proposes a novel representation of text instances and an algorithm that detects text instances in an image. A text instance is represented as a kernel that is a shrunk version of the text instance along with surrounding pixels that belong to the kernel based on centripetal shifts. This representation simplifies the post-processing to obtain text instances from the output of neural network but yet yields accurate detection. The experimental results confirm the effectiveness of the approach on common scene text datasets containing irregular texts.,"This paper presents a method for scene text detection with a neural network predicting both the probability of pixels belonging to text kernels (shrunk version of text bounding polygons) and pixels shifts towards or away from those kernels. The text kernels are predicted from the first head and extended by aggregating surrounding pixels which predicted shifts place them inside the kernel, to form the text regions. Experiments carried on various datasets show a better accuracy with faster predictions than the state of the art methods for text detection and recognition.",0.24615384615384617,0.2153846153846154,0.23076923076923078,0.1565217391304348,0.19130434782608696,0.2413793103448276,0.1391304347826087,0.16091954022988506,0.16666666666666666,0.20689655172413793,0.24444444444444444,0.23333333333333334,0.17777777777777778,0.18421052631578946,0.1935483870967742,0.1782178217821782,0.2146341463414634,0.23728813559322037
429,SP:5e0cadc6d7bf9fe8bb0d73753c9ae8f66881fdd7,"The paper presents a new framework to unify two streams of few-shot learning methods, the episodicc meta-learning and pre-train finetune-based few-shot learning. Besides, a new meta-dropout strategy is proposed to improve the generalization ability. Extensive results on several benchmark like PascalVOC, COCO, CUB and mini-ImageNet validate the proposed approach. ","This paper aims to improve the generalization power of meta-learning. It claims to propose an effective strategy named meta-dropout, which is applied to the transferable knowledge generalized from base categories to novel categories. The proposed strategy aims to prevent neural units from co-adapting excessively in the meta-training stage (where somehow I did not get the idea of what it is).","The paper addresses few-shot learning in a combination of episodic meta-learning-based and pre-train finetune-based approach. It proposes meta-dropout, that consists in using dropout on training the base classes during meta-training, but not when tuning on novel classes. It presents results for both classification and object detection tasks.",The authors propose a unified framework for meta-learning based algorithm and pretrain finetune based algorithm. Drop out is applied for few-shot learning and achieve better generalization ability. Experiments on two tasks validate the effectiveness of the proposed algorithm.,0.21428571428571427,0.30357142857142855,0.30357142857142855,0.203125,0.125,0.2037037037037037,0.1875,0.3148148148148148,0.425,0.24074074074074073,0.2,0.275,0.19999999999999998,0.3090909090909091,0.35416666666666663,0.22033898305084745,0.15384615384615385,0.23404255319148937
430,SP:5e20c39a947e78c951fc5d829e8b70dfec598a6e,"This paper studies auction design for advertising slots when the agents are using auto-bidding optimization algorithms.  The bidders: The focus is on “value-maximizing agents”: this type of agent aims to maximize the total value he gets from his allocation, subject to the constraint that his value is at least his price (as opposed to a utility-maximizing agent, who aims to maximize his value minus his price; the authors also study interpolations between value maximizers and utility maximizers). The authors write that this is the prevalent adopted model for the behavior of auto-bidding agents. There are $m$ position auctions and each agent $i$ has a fixed (i.e., not Bayesian) value $v_{i,j}$ for each auction $j \in [m]$. Agent $i$’s value for the $k^{th}$ slot in auction $j$ is $v_{i,j}\cdot pos_{j,k}$, where $pos_{j,k}$ is a position normalizer that doesn’t depend on $i$ and is common knowledge.  The auctions: The paper studies VCG, GSP, and first-price auctions with reserves and boosts, the latter of which means that each bidder’s bid $b_{i,j}$ is increased by some boost $z_{i,j}$. The reserves and boosts are set to be equal to some noisy signals about the values $v_{i,j}$ which the authors write could be obtained, for example, using a machine learning model. These signals are assumed to be accurate up to some multiplicative error. Also, the bids are assumed to be undominated (which includes the support of Nash equilibrium bids as a subset).  The results: The authors prove approximation guarantees showing that for each of these three auctions, the revenue and welfare are approximately optimal, where the approximation factor depends on the accuracy of the noisy value signals. They also provide experiments on semi-synthetic data that validate these theoretical findings. From my understanding, the buyers’ values are based on real-world ad auction data, but the signals and bids are synthetically generated.","This paper studies the auction design when the buyers are either utility or value maximizer with budget constraint. Under this setting the authors focus on three mechanisms VCG, GSP and First Price Auction. They have two main contributions compared to the previous literature: 1. They show that introducing reserve prices might have a positive impact on social welfare. 2. Also the authors try to model that the seller does not know the exact values of the bidders, meaning has only access to noisy signals.  Then the authors assess their findings by running some simulations and running real experiments that confirm their findings. ","This paper focuses on value maximizing bidders with return on spend constraints—a paradigm that has drawn considerable attention recently as more advertisers adopt auto-bidding algorithms in advertising platforms, and show that the introduction of reserve prices has a novel impact on the market. Choosing reserve prices appropriately can improve not only the total revenue but also the total welfare.  Their results also demonstrate that reserve prices are robust to bidder types, i.e., reserve prices work well for different bidder types, such as value maximizers and utility maximizers, without using bidder type information. Then generalize these results for a variety of auction mechanisms such as VCG, GSP, and first-price auctions. Moreover, they show how to combine these results with additive boosts to improve the welfare of the outcomes of the auction further. Theoretical observations are complemented with an empirical study using data from online advertising auctions. ","This paper studies welfare and revenue outcomes in a position auction with boosting and personalized reserves, where bidders behave in a manner motivated by autobidding.  Specifically, each bidder is assumed to want to maximize a modified utility of the form (value - lambda * payment) where lambda lies in [0,1], subject to an ROI constraint that value is at least payment.  Lambda = 1 corresponds to standard quasi-linear utility while lambda = 0 corresponds to value maximization.  As a solution concept, the paper considers all profiles of undominated bids, which is a relaxation of equilibrium notions.  The underlying auction is either VCG or GSP.  Boosting and personalized reserves aim to improve efficiency and welfare by leveraging information about an agent's true value and incorporating it into the auction rule.  This paper asks what happens when this information is noisy, which they model as having the reserve price and boosting parameters be within some constant factor of the agent's true value.  The authors show that when this holds, the welfare and revenue will be approximately optimal under arbitrary undominated bids.  The paper concludes with an experimental study of the proposed method on synthetic data based on historical bidding traces.  The study indeed shows that natural bidding behavior converges with the addition of approximate reserves and boosts, and that welfare and revenue improve. ",0.1027190332326284,0.11178247734138973,0.1782477341389728,0.19607843137254902,0.24509803921568626,0.16778523489932887,0.3333333333333333,0.2483221476510067,0.2669683257918552,0.1342281879194631,0.11312217194570136,0.11312217194570136,0.15704387990762128,0.15416666666666667,0.21376811594202896,0.1593625498007968,0.15479876160990713,0.13513513513513514
431,SP:5e586b7e19133c943e2e9a64bda0fa0df292ef32,"The paper presents an interesting framework (CAMDPs) for teacher-in-the-loop-based RL, that takes into account rich interactive advice. The authors also show how the advice is interpreted in the Coaching augmented MDP. The framework comprises of 3 stages: grounding, improvement, and evaluation. In the grounding phase, the agent learns to interpret an 'advice', in the improvement phase the agent learns a policy using the advice and finally in the evaluation phase the learner deploys it's advice-independent policy. The claim is that the proposed pipeline helps a learner learn more efficiently, as it is able to account for richer forms of feedback. The method was tested on 3 rich RL environments: Baby AI, Ant Env, and Point Navigation Env, and evaluated based on its advice efficiency. The evaluations shpw that an agent learns quicker when presented with advice than when it is not.","The authors propose a reinforcement learning approach where humans can  provide different forms of advice to complete a task. The proposed approach, called CAMDPs, follows a two-stage process: Grounding and Improvement.   During grounding the system learns how to interpret the advice. The result of this phase is a surrogate policy. Grounding learns a mapping from advice to actions using information from the true reward function. Grounding can be seen as an ordinarily multitask RL, where each task is a piece of advice, and the agent learns using normal RL how to achieve the advice.   Distillation is used to learn a policy using supervised learning and bootstrap distillation uses the previously learned policies for individual advice to learn a policy for a particular task.  During improvement the coach provides advice to solve a new task. A very similar approach is followed as bootstrap distillation, a new task is solved using advice followed by a distillation process.  It is expected for the learned policies to be general enough to be applied in other, although similar, domains.  The proposed system is evaluated in three, relatively similar, domains. As expected, it is experimentally shown that the proposed system learns faster with advice. ","This paper presents a framework for teaching agents using language advice. The framework consists of three stages: (i) grounding language advice to action, (ii) learning an unconditioned policy that mimics the advice-conditioned policy (iii) evaluating the unconditioned policy. Experiments are conducted in maze navigation environments. Results demonstrate the effectiveness of the framework compared with imitation learning and reinforcement learning.  ","The paper proposes a mechanism Coaching Augmented MDP for agents to learn from interactive expert feedback to avoid assumptions on reward functions and agent infrastructure. Here, structured advice is provided by experts to guide the agent learning process. A policy learning algorithm is designed for interpreting and acting on the advice to solve a problem. Experiments are conducted on discrete and continuous control problems to show how such mechanism outperforms traditional RL models.",0.25170068027210885,0.14285714285714285,0.12244897959183673,0.08542713567839195,0.10552763819095477,0.23333333333333334,0.18592964824120603,0.35,0.2465753424657534,0.2833333333333333,0.2876712328767123,0.1917808219178082,0.2138728323699422,0.20289855072463767,0.1636363636363636,0.13127413127413126,0.15441176470588233,0.21052631578947367
432,SP:5f268274e6934e51301f5f48c36099dd4588b961," Prior work has shown that in the online linear optimization problem, if at each round, the learner is provided with a hint on the cost prior to its selection by an adversary, then one can achieve a regret of order $O(\log(T))$. The authors consider the question of how many hints the learner requires in order to achieve such a regret rate. They show that $O(\sqrt{T})$ hints are sufficient by proposing an algorithm that achieves $O(\log(T))$. The proposed algorithm is a combination of Online Gradient Descent (OGD) and Follow The Regularized Leader (FTRL). OGD learns adaptively the probability by which the algorithm decides to query a hint or not, while, FTRL proposes which action to play whenever the algorithm decides not to query a hint. Conversely, the authors also show that with $o(\sqrt{T})$ hints, regret is no less than $\Omega(\sqrt{T})$.","This paper considers the Online Linear Optimization (OLO) problem with additional hints (predictions) in which at every round $t\in[T]$, the algorithm can decide to receive the hint $h_t$ to help choose an action $x_t$. Upon committing to the action, a loss vector $c_t$ is revealed and the algorithm incurs the cost $\langle c_t,x_t \rangle$. In case the algorithm had received the hint, it also incurs a query cost of $\alpha |c_t|^2$ where $\alpha$ is a pre-specified constant. While prior works had shown that having access to good hints at every round leads to improved logarithmic regret bound (instead of the standard $O(\sqrt{T})$ regret bound of OLO), this work shows that achieving such improved regret bounds is possible with only $O(\sqrt{T})$ hints. Moreover, they show that $o(\sqrt{T})$ hints would not be enough, and the $\Omega (\sqrt{T})$ regret bound can not be avoided in that case. Finally, some extensions of the framework (including the one which allows for bad hints) and the corresponding regret bound for each setting are provided.","The authors study online linear optimization (OLO) in a setting where the online algorithm has access to hints. They show that $O(\sqrt T)$ hints is sufficient to guarantee $O(\log T)$ regret, and show an explicit algorithm which achieves this bound, along with a matching lower bound on the number of hints required to get $O(\log T)$ regret. They also show that randomization is required for this kind of result in the constrained setting, but surprisingly is not required in the unconstrained setting. They also show some guarantees to robustness against bad hints and various extensions to other online learning settings.","This paper studies how to obtain a good regret bound using only sublinear number of hints for online linear optimization. They show that to obtain $O(\ln T)$ regret, only $O(\sqrt{T})$ number of hints are needed. Moreover, they establish a lower bound showing that $\Omega(\sqrt{T})$ number of hints are necessary to achieve $O(\ln T)$ regret. They also extend their results to various settings such as the presence of bad hints, unconstrained linear optimization, etc.",0.24161073825503357,0.20134228187919462,0.1342281879194631,0.16666666666666666,0.14516129032258066,0.24271844660194175,0.1935483870967742,0.2912621359223301,0.25316455696202533,0.30097087378640774,0.34177215189873417,0.31645569620253167,0.21492537313432836,0.23809523809523808,0.17543859649122806,0.21453287197231835,0.20377358490566036,0.27472527472527475
433,SP:5f2a0a070fe830bf306080c6698c8644ba11f411,"This paper studies the problem of estimating the pose of an object, given an image of it. The proposed method aims to use as few labeled samples as possible, and generate pseudo-labels for a large collection of unlabelled data during training. The method works in an EM-fashion in which two stages (neural view synthesis and matching) are optimized by alternating.   In the first stage, the method extracts a feature map from the input annotated image and shapes it as a mesh cuboid where each vertex is assigned a feature vector depending on its position. The mesh is then rotated at regular angle intervals and rasterized into the image plane. In the matching part, features from unlabeled images are extracted and matched with rotated view rasterizations using a cosine similarity metric.   The experimental evaluations show that the proposed method is able to learn the pose prediction task from as few as 7 annotated images during training. The method mostly outperforms the baselines in comparisons against them.  ",This paper investigates a new problem of semi-supervied 3D viewpoint estimation using few-shot labeled examples. It further proposes a new method for the task based on learning a 3D cuboid feature for a category with features at its vertices which are viewpoint invariant. The authors propose a method of rotating the cuboid to generate feature representations for novel views. Another key insight of the paper is how to expand the ranges of novel views for which the cuboid can be rotated by introducing a contrastive loss to update the cuboid feature representation to be more rotation invariant by combining with the pseudo--labeled images. The authors compare their method to two baseline SoTA methods and outperform the existing methods by a large margin in the few-shot setting. ,"The paper presents a learning based method for 3D pose estimation from single image that works with very few annotated data. Given some exemplar images with 3D pose annotation, the 3D pose of a test image could be estimated by two steps: 1) neural view synthesis and 2) feature matching. In the neural view synthesis step, a 3d feature is generated by unprojecting a pretrained 2d feature map of the image. The 3d feature is then probably ""projected"" to a set of novel views for feature matching and 3d pose estimation.","This paper presents a method for 3D pose estimation of PACAL3D objects (6 vehicle categories). The method bootstraps from 7-50 annotations per category. The idea is: given the initial annotations, finetune a Resnet50 and a featurized cuboid mesh with contrastive losses that bring the 2D and 3D features close to each other, and make them discriminative for position/viewpoint. Then, compute 2D features for the entire dataset, and rotate the 3D featuremap a few degrees in each direction, and compute match scores for all pairs. The confident matches are then used for subsequent re-optimization of the 2D and 3D features, and this process repeats ~100 times, gradually increasing the number of rotation bins covered by the 3D featuremap. Results are quite good against non-bootstrapping baselines, trained just with the 7-50 annotations.  ",0.17365269461077845,0.1497005988023952,0.15568862275449102,0.16923076923076924,0.18461538461538463,0.25274725274725274,0.2230769230769231,0.27472527472527475,0.1925925925925926,0.24175824175824176,0.17777777777777778,0.17037037037037037,0.1952861952861953,0.19379844961240308,0.17218543046357618,0.1990950226244344,0.1811320754716981,0.20353982300884954
434,SP:5f6179036f88c37f0aa6fb2fe67e6454a961fedd,"This paper presents EfficientZero - a variant of MuZero that targets data efficiency in the low-data regime. The authors identify 3 factors that hinder the data efficiency of MuZero in the low-data regime, namely lack of supervision on learning the model, compounding errors in value estimation, and lack of off-policy correction with stale data. To provide richer supervision for learning the model, EfficientZero employs an auxiliary self-supervised objective similar to SimSiam. To combat compounding errors, EfficientZero directly learns the ""value prefix"" rather than per-step rewards. Finally, EfficientZero utilizes the model to address off-policy correction. EfficientZero achieves a new state of the art on the Atari 100k benchmark, outperforming existing methods by a large margin. Ablation studies demonstrate the importance of all three proposed components - removing any of them results in significant performance drop.","This paper presents an algorithm called EfficientZero, a sample efficient model-based RL algorithm built upon MuZero. The authors demonstrate the efficiency of the proposed method via atari games, and call for the research of MCTS based RL algorithms in a wider community. The core contribution lies in the adaption of MuZero to settings where data is limited. The paper is overall interesting, easy to follow, and technically sound. ","The authors consider the problem of sample-efficient reinforcement learning focused on image-based data, particularly in the Atari domain. Building on the MuZero model, they introduce three key changes to improve the learned model and thereby the sample efficiency: (a) a loss to enforce consistency between rolled-out and direct state abstractions learned by the model; (b) an end-to-end approach for predicting the sum of discounted rewards (""value prefix"") used to generate a MCTS Q-value estimate, and (c) the use of an additional MCTS rollout for bootstrapping the state-value estimate with replay buffer data, to limit the off-policy bias of multi-step estimates from old data. They show that the proposed changes result in a significant performance boost in the low-data (100k) Atari domain, over both the original muZero and a variety of sample-efficient baselines. The also provide an ablation study that demonstrates improvement from the combination of the three interventions over each individually. The authors also release the source code.","The authors propose EfficientZero, a sample efficient model-based visual RL algorithm built on MuZero. By making three changes, namely using self-supervised learning to learn the environment model, predicting the value f, and using the model to correct off-policy value targets, EfficientZero achieves state-of-the-art performance on the Atari 100k benchmark. The authors also provide an efficient implementation for future research.",0.13768115942028986,0.2246376811594203,0.1956521739130435,0.2753623188405797,0.2608695652173913,0.17159763313609466,0.2753623188405797,0.1834319526627219,0.4153846153846154,0.11242603550295859,0.27692307692307694,0.4461538461538462,0.18357487922705312,0.20195439739413681,0.2660098522167488,0.15966386554621848,0.2686567164179105,0.24786324786324787
435,SP:5fe041c24ad0c3528d6f2822a144ab45958fcb57,"The paper presents biologically inspired design of recurrent convolutional neural networks with bottom-up, lateral, and top-down connections. In contrast to a ‘synchronous’ update of all nodes according to connectivity, the biologically motivated unrolling of recurrent connections and information flow in time results in an ‘asynchronous’ update where different nodes get updated at different time steps with bottom-up, lateral, and top-down information flow.  These architectures are termed A-FRCNN (asynchronous fully recurrent convolutional neural networks).  A-FRCNN architectures can equivalently be viewed as feed-forward networks with specific weight tying.  These models are applied to the task of speech separation and state of the art results are obtained on two data sets.","The paper proposes a new separation module for extracting multi-scale features and aggregating them for improving the performance of source separation models. The proposed architecture consists of an asynchronous update mechanism which operates at different time-scales and fuses the features in parallel at the latter stage. The authors claim that the inspiration behind their proposed model is the non-simultaneous stimulus of human neurons when they get triggered from some sensory activity. The authors provide quantitative results on widely used benchmarks datasets and show improvement over strong baselines. Finally, the authors conduct ablations studies to show the importance of each update scheme inside the proposed separation module block.","This paper proposes a new speech separation architecture based on asynchrony fully recurrent CNN (A-FRCNN). The paper first describes FRCNN variants inspired by biological systems and discusses implementing such techniques for speech separation. Based on this discussion, the paper proposes a novel A-FRCNN speech separation technique. The experimental results show the effectiveness of the proposed method in two speech separation benchmarks (WHAM! and Librimix), achieving state-of-the-art performance compared with DPRNN. This new architecture can be applied to other similar problems, e.g., speech enhancement, and the contribution of this paper in terms of the technical novelty, experimental effectiveness, and potential wide applications is significant. My major concern about this paper is the clarity of the presentation. I could not fully understand the definition of the stage, bottom-up, and top-down operations until the actual speech separation discussions in Section 3.2. These concepts are not familiar, especially for speech separation researchers, and I recommend that the authors provide concrete examples of these concepts and elaborate the discussions.  Other comments: 1. Is it possible to use a transformer as a node instead of CNN? Recently, transformer-based processing becomes popular in speech separation. 2. Please discuss the effect of the reverberation. The real condition always has to deal with the reverberation. 3. Section 3.3 is a bit trivial, and it can be removed (or reduced). There are several important discussions in the appendix, and they can be put in the main document instead.",The paper introduce a novel method to separate mixture of  sounds. The paper introduce a new neural architecture called Fully Recurrent Convolutional Neural Network (FRCNN) which use neural network with lateral connections. The main variant of the proposed method is the A-FRCNN which use asynchronous updating. The main novel parts are: (1) novel bottleneck structure (2) removing some connection that seems to be redundant. ,0.1565217391304348,0.21739130434782608,0.1391304347826087,0.2636363636363636,0.12727272727272726,0.08032128514056225,0.16363636363636364,0.10040160642570281,0.24615384615384617,0.11646586345381527,0.2153846153846154,0.3076923076923077,0.16,0.13736263736263735,0.17777777777777778,0.1615598885793872,0.16,0.12738853503184713
436,SP:60be694107c984f1ccf31ca9b1985cf20e34bddf,"The authors propose an approach to solve the Dynamic Pickup and Delivery Problem. The main challenge of this problem is that the orders are not known a priori. To solve this, the authors propose a bi-level approach composed of: (1) an upper-level agent to decide whether to solve the problem for current orders in a cache or to wait for new orders, and (2) a lower-level agent based on a Graph Isomorphism Network to solve the problem. Both agents are trained using classical reinforcement learning techniques. According to the results, this approach improves state of the art by showing superior performance over existing baselines for DPDP.","The current paper proposes a hierarchical approach to solve the dynamic pickup and delivery problem (DPDP). The method uses two RL modules. The upper-level RL policy is designed to segment the time window thus transforming DPDP into a static PDP. For the upper-level RL, DQN is used. The lower-level RL policy is trained to select one solution improving heuristics. The lover level policy is parameterized with the GNN network and trained under the framework of Reinforce.  ","The paper considers dynamic pickup and delivery problems in which a limited number of vehicles need to be scheduled to handle all demands with the goal of minimizing the average traveling distance. The problem involves capacity constraints of the vehicles, LIFO constraint, and soft time window constraint, where violating the time windows result in some penalty.   A new algorithm is proposed which is based on the existing idea: Make a buffer to cache the newly arrived orders, then periodically dispatch all the orders in the buffer. The current approaches add the orders to the buffer up to a given size or based on the prediction of the future, but the new proposed approach uses an upper-level agent which determines whether to wait for more items (and get some delay penalty) or dispatch the current orders. For this purpose, a day is partitioned into 144 time-slots of each 10 minutes and the goal is to decide to continue or wait at each of those steps. Then, the received ordered in the buffer with all the orders that are not picked up will be passed to the lower-level agent to solve the static pick-up and delivery sub-problem. The upper-level agent uses DQN to train the agent in which the same goal as of the problem is used as the reward. The state includes ""the number of orders accumulated in the buffer, the number of available vehicles, the amount of time left before exceeding the time limit of each order"".  The lower level agent learns the assignment of the orders to the vehicles and the sequence to pass over the orders. The state includes the position information of the customers and vehicles. Action is choosing one of four introduced heuristic improvements for routes. They can be applied to the routes and if there are any improvements, the new route will be accepted. A graph neural network with REINFORCE algorithm is used to train the policy.  ","The paper proposed a framwork to solve  Dynamic Pickup and Delivery Problem (DPDP) via Reinforcement Learning Based Bi-level Optimization method. The upper-level agent used DQN to make a decision whether to release cached orders, while the lower-level agent used GNN to solve each sub-problem. The overall objective function is the weighted sum of travelling distances and overtime. This proposed framwork got a superior performance over baseline.",0.22935779816513763,0.3853211009174312,0.25688073394495414,0.3924050632911392,0.31645569620253167,0.09146341463414634,0.31645569620253167,0.12804878048780488,0.4,0.09451219512195122,0.35714285714285715,0.42857142857142855,0.26595744680851063,0.19221967963386727,0.3128491620111732,0.15233415233415235,0.33557046979865773,0.1507537688442211
437,SP:6146a75475006f57f05e73221db648c7abae8c05,"In this work, the authors study the problem of synthesizing speech corresponding to a silent video. The main novelties proposed in this work are: a proposal to incorporate a “global visual context” which is hypothesized to help the model resolve ambiguity in the viseme-to-phoneme mapping; an explicit synchronization technique to ensure that the generated speech is time-aligned with the video; and the use of multiple discriminators to which allow the iterative generation of more fine-grained speech representations. Essentially, the authors propose to model the task as video to image translation, where the video corresponds to a log-mel spectrogram representation of the target speech. ",This paper describes a synthesis approach that uses visual lip movement as input and generates speech as output.  They use a GAN style synthesizer. The most novel contribution is the synchronization technique to align visual and speech signals.,"This paper proposes a lip2speech generative framework based on the proposed Visual Context Attention (VCA) module. The VCA module follows the idea of self-attention and fuses the audio-visual information together to generate finer representations. The proposed pipeline stacks several generators together whose predictions are gradually refined by the VCA module. The global and local clues of input videos are also taken into consideration in this framework. The experiments are conducted on three datasets, while the authors have evaluated their method under different scenarios like constrained-speaker, unseen speaker, etc. The final performance is superior to other baselines.",The paper presents a visual context attentional GAN for synthesising speech from silent videos. It generates the mel-spectrogram by jointly modelling both local and global visual representations; furthermore a synchronisation technique is proposed. The proposed method is evaluated on three commonly used datasets and shows superior performance as compared with state-of-the-art methods in the literature.  ,0.10185185185185185,0.16666666666666666,0.12037037037037036,0.21052631578947367,0.21052631578947367,0.15151515151515152,0.2894736842105263,0.18181818181818182,0.22033898305084745,0.08080808080808081,0.13559322033898305,0.2542372881355932,0.1506849315068493,0.17391304347826086,0.15568862275449102,0.11678832116788322,0.16494845360824742,0.189873417721519
438,SP:621b90efeaec47f457211736938a3dd982d3b16c,"This paper proposes an algorithm for federated learning that leverages recent advances in the NTK framework. Participants share sample-wise jacobian matrices instead of model weights or gradients, based on the intuition that the NTK is able to capture useful statistical information when learning under statistical heterogeneity. The paper presents numerical results showing that the proposed approach maintains efficiency across different heterogeneity levels. Further, the authors introduce a practical implementation that reduces communication, and compare this approach with FedAvg and FedNova. ","This paper proposes a federated learning (FL) paradigm empowered by the neural tangent kernel (NTK) framework. The NTK under the infinite-width regime allows us to analyze a dynamic of the corresponding neural network without a gradient descent algorithm. The authors utilize it for predicting the best parameters aggregated from multiple workers. However, this framework requires transmitting Jacobian matrices between workers and the aggregation server, which results in increasing computational overhead and exposing more private information. To this end, they adopt dimensionality reduction via randomness-sharing projection, zeroing out compression as well as shuffling. They also address that their NTK-based FL scheme has a faster convergence rate compared to that of FedAvg, for a two-layer network under specific assumptions. Finally, empirical results support that the proposed FL method performs better than other FL algorithms and achieves similar test accuracy to the ideal centralized case.","This paper introduces new algorithms for the federated learning (FL) farmwork using neural tangent kernel (NTK) paradigm. Two algorithms are referred to as NTK-FL and CP-NTK-FL, where the latter is a variant of the former for improved communication efficiency and privacy preserving. The proposed algorithms are aimed to address statistical heterogeneity across the workers. An important point is that unlike typical training algorithms for the FL setting, here the workers upload the labels and sample Jacobian matrices (representing NTK) to the server. The server then uses tools from NTK to obtain the trained neural network (instead of using gradient descent). Theoretical and empirical validations (on MNIST, Fashion-MNIST and EMNIST), and comparisons with FedAvg are provided.  ","This paper presents a new federated learning (FL) framework that employs the neural tangent kernel (NTK) method to replace the widely used gradient descent algorithm for optimization. To improve communication efficiency and privacy-preserving features, data sampling and random projection techniques are used in the proposed FL-NTK framework. Experiments are conducted to demonstrate the advantages of the proposed FL-NTK in the robustness to data heterogeneity and communication efficiency, as compared to the baseline FedAvg. ",0.24691358024691357,0.2345679012345679,0.20987654320987653,0.18493150684931506,0.19863013698630136,0.25210084033613445,0.136986301369863,0.15966386554621848,0.2236842105263158,0.226890756302521,0.3815789473684211,0.39473684210526316,0.1762114537444934,0.18999999999999997,0.21656050955414013,0.20377358490566036,0.26126126126126126,0.3076923076923077
439,SP:62257ef8972e69338f38b25fa863997069735f3b,"This paper studies the kernel of the higher-order combinatorial Laplacian, which equivalently amounts to a study of higher-order homology. In particular, given finite samples from a manifold with a ground truth decomposition into a connected sum of smaller (""prime"") manifolds, they study the question of when it is possible to computationally recover a ""nice"" homology basis such that the basis vectors are resolved into independent components contributed from each of the prime manifolds. In this sense, the theoretical aim is quite nice and fundamental. The authors are able to provide a positive answer to their question in the setting where the prime manifolds are connected over a small, sparsely connected region. The proof technique is similar to that for obtaining bounds in spectral graph theory, but is carefully implemented because the problem has lots of block matrices floating around. Finally, the authors propose algorithmic applications of this method to the problem of detecting shortest loops in data, and exemplify their constructions on several point cloud and image datasets. ","In this manuscript, the authors study the decomposition of the homology spaces from Hodge Laplacian matrices, and propose a new algorithm for the detection of the shortest homology generators. In their model, the direct sum of homology embedding or spaces from Hodge Laplacian is approximated by connected sum of manifolds. More specifically, the homology space is decomposed into subspaces with simple topological components. An algorithm is designed to detect shortest homology loops in the structure. The approach to use homology spaces for clustering and detection of shortest loops is very novel and interesting. It expands the area of topological data analysis, and will have a great impact for various data modelling and analysis. ","This paper presents a method for calculating (or rather *estimating*) the null space of the $k$th Laplacian. This enables the analysis of the geometry of $k$th homology embedding, making it possible to factorise such an embedding into different subspaces corresponding to individual prime factors of a given manifold. The proposed method also turns out to be applicable to improve loop homology generators, leading to geometrically more concise ('localised') representations of such loops.  The new method is showcases on a variety of data sets, demonstrating its overall utility for loop localisation and geometric decompositions. ","This paper studies $k$-th homology vector spaces through the lens of $k$-Laplacian, a generalization of the usual graph Laplacian to higher order combinatorial structures.  It studies in particular the (fairly general) context of decomposable manifold as a sum of prime manifolds, showing (losely speaking) that under some assumptions (sparse connections), the homology space of a manifold can be obtained as the sum of the homology spaces of the corresponding prime manifolds.  Building on Independent Component Analysis technique, it then provides a way to identify the subspaces of the homology basis computed on the whole manifold to recover the basis of the corresponding prime manifolds.  This has applications in particular in homologous loop detection (detecting the shortest generating element in H1), as showcased on both synthetic and real datasets. ",0.18235294117647058,0.14705882352941177,0.24705882352941178,0.21238938053097345,0.22123893805309736,0.23157894736842105,0.2743362831858407,0.2631578947368421,0.3230769230769231,0.25263157894736843,0.19230769230769232,0.16923076923076924,0.21908127208480563,0.18867924528301885,0.28,0.23076923076923078,0.205761316872428,0.19555555555555557
440,SP:62cf8390886ab02d908043b72ef765e66a5feee6,This paper aims to propose an algorithm for federated reinforcement learning with fault tolerance against Byzantine failure. The paper mainly considers the policy gradient method and proposes a server aggregation method with a Byzantine filter to reduce the effect of gradients from malicious agents. The paper also provides a theoretical analysis to support the design decision of the filter. ,"This paper describes a federated reinforcement learning architecture that is expected to be tolerant to adversarial attacks or faulty agents. The architecture assumes a percent of the nodes are Byzantine agents that are actively trying to fool the central server, and assesses algorithm convergence and performance. Theoretical guarantees support empirical performance evaluation on some benchmark domains.  I have read the other reviews and the author responses. I agree that its difficult to do anything about the exploration policy without having further information. Although I still think this will be a limitation in the real world, drowning out interesting or unique outputs. My score remains borderline on this one.","The authors propose a federated reinforcement learning approach that combines existing stochastic variance-reduced policy gradient method with a robust aggregation method to achieve fault tolerance. Theoretically, the authors provide convergence and sample complexity results under reasonable assumptions. The proposed approach is tested empirically on various benchmark tasks.","This paper proposes a federated reinforcement learning (FRL) algorithm that uses a variance-reduced federated policy gradient framework. The proposed framework is proven to have convergence and robustness guatantees (can tolerant the failures of up to half of the agents), with some assumptions on the environment and the agents. Both theoretical results and empirical results justify that the proposed federation can boost the sample efficiency of RL agents.",0.23728813559322035,0.22033898305084745,0.2542372881355932,0.1111111111111111,0.19444444444444445,0.3125,0.12962962962962962,0.2708333333333333,0.22058823529411764,0.25,0.3088235294117647,0.22058823529411764,0.16766467065868262,0.24299065420560745,0.23622047244094488,0.15384615384615383,0.23863636363636365,0.25862068965517243
441,SP:62d35d7c3185723a52213fb7e35d45400d3e1425,"-- The authors propose C3-GAN, a method that learns ""clustering-friendly"" feature representation for fine-grained clustering (main goal), by learning features of cluster centroids (latent codes) using contrastive loss on the mutual information of image-latent code pairs.  -- The method should improve the GAN's performance in terms of image diversity.  -- The method is unsupervised and applicable for single-object images only.  -- The method is built upon FineGAN (and InfoGAN idea), after adding significant improvements such as removing the dependency on bounding-box labels, applying the mutual information in the embedding space, and directly learning cluster centroids. ","The authors undertake the more difficult task of data clustering, based upon fine-grained features. They use contrastive learning for this, in conjunction with GAN losses. Their representation can be used in the downstream task of image generation, where they use their representations' strengths to show improved resilience to mode collapse, while displaying better intra-cluster variance.  ","This paper studies the problem of fine-grained image clustering. Similar to recent work such as OneGAN and FineGAN, the paper proposes to use a GAN setup, called C-3GAN, where the fine-grained image synthesis and clustering are performed in the same end-to-end pipeline. The main contribution is to use a contrastive loss in maximizing the mutual information of the discriminator encoded features and class-centric features. There are also some modifications in the foreground and background synthesis mechanisms compared to prior work such as the FineGAN. Experiments are conducted on fine-grained datasets such as CUB-200-2011.","*The fine-grained class clustering is more challenging than coarse-grained due to lower sample representation and large scale- and color- variations between the fine-grained classes. The main goal of the proposed approach is to learn stronger representations in an unsupervised fashion. To this end, the paper proposes C3-GAN which uses the ability of InfoGAN with contrastive learning to learn feature representations that maximize the mutual information between the latent code and its corresponding observation. The proposed approach is able to achieve best performance in comparison to the related works.*   *The results demonstrated quantitatively and qualitatively on 4 different datasets along with ablation study validate the proposed approach. The method is promising since it is unsupervised way of learning cluster centroid for unlabeled data.*",0.12244897959183673,0.20408163265306123,0.25510204081632654,0.19298245614035087,0.21052631578947367,0.23529411764705882,0.21052631578947367,0.19607843137254902,0.1984126984126984,0.10784313725490197,0.09523809523809523,0.19047619047619047,0.15483870967741933,0.2,0.2232142857142857,0.13836477987421386,0.13114754098360656,0.21052631578947367
442,SP:631dd0439a1d80e4bd385e17dfad0733f953c786,"This paper considers the problem of prediction with expert advice, and focuses on quantile regret bounds that depend on the comparator distribution and semi-adversarial paradigm, which is an intermediate setting between stochastic one and adversarial one. In the paper, minimax optimal regret bounds for both paradigms are shown. This bound is achieved by follow-the-regularized-leader (FTRL) algorithm with a newly designed root-logarithmic regularizers.","In this paper, the authors tackle the problem of obtaining low semi-adversarial and quantile regret for the problem of prediction with expert advice with possibly uncountably many experts. To this end they develop a general technique to obtain methods for the problem based on the (measure-theoretical) FTRL meta-algorithm. They show that appropriate choices of regularizers can yield algorithms for the semi-adversarial (with improved dependency on the number of experts) and quantile settings (asymptotically matching state of the art). Finally, they give a lower-bound on the quantile regret, showing that their results are minimax optimal.",The paper studies Follow the Regularised Leader algorithms for the Hedge settings.  The paper starts with an overview of FTRL in adversarial settings. A frighteningly general setup is described and Theorem 1 for the adversarial regret is obtained. It is claimed that the theorem allows one to obtain bounds similar to some known bounds from the literature.  Then the semi-adversarial settings are discussed (the definition of semi-adversarial covers the adversarial settings on one end of the spectrum and purely stochastic on another).  The paper presents FTRL-CARL and obtains a performance bound on it in terms of effective stochastic gaps.  Then the paper discusses bounds in terms of f-divergence and obtains a lower bound matching upper bounds.,"This paper studies the classic expert problem and focuses on developing  parameter-free algorithms with two specific guarantees: 1) better regret  bounds for the semi-adversarial paradigm proposed by Bilodeau et al. 20;  2) a KL-type regret bound similar to that achieved by NormalHedge  (Chaudhuri et al. 09) and other variants, possibly for an uncountable  set of experts. The authors achieve these two goals using the standard  FTRL framework with two new root-logarithmic regularizers, inspired by  their general analysis that requires some specific properties of the  regularizer. A matching lower bound for the KL-type regret is also  provided. ",0.26865671641791045,0.23880597014925373,0.34328358208955223,0.23232323232323232,0.2222222222222222,0.175,0.18181818181818182,0.13333333333333333,0.22772277227722773,0.19166666666666668,0.21782178217821782,0.2079207920792079,0.21686746987951808,0.1711229946524064,0.27380952380952384,0.2100456621004566,0.22,0.19004524886877827
443,SP:631efd6f0eda4c20a58ab8bcc2c9d7dd939ce8e7,"The paper proposes to rethink deep learning for learning functions  (with functions being gaussian mixtures GMs, with  positive and/or negative weights unlike for PDFs) as opposed to tensors traditionally used for embedding data.  Filters are learned as GMs since the cross product (convolution) of two GMMs has a closed form solution. Applying  an activation/transfer function (e.g. ReLu) to a GM does not produce a GM and  the paper proposes to approximate a GM from the output of the activation function. Such approach allows to define layers with input/output in GM forms. ","For the curse of dimensionality problem of CNNs, this paper presents the Gaussian mixture convolution network (GMCN), a method that alleviate the problem of high-dimension data. GMCN is a deep functional network method that represents data as functions. It fits a Gaussian mixture to result of transfer functions, such as RELUs. The effectiveness of this architecture is verified on MNIST and ModelNet10.","The paper proposes a new convolution method to process sparse data in a memory-efficient manner. The authors represent input data, intermediate feature maps and convolution filters as mixtures of Gaussians, which allows computing the result of the convolution analytically. Since the number of mixture components would increase quickly from layer to layer and application of ReLU destroys the property of intermediate layers to be mixtures of Gaussians, they re-fit intermediate feature maps and develop a heuristic to reduce the number of mixture components. The authors evaluate their proposed method on MNIST (2d) and ModelNet10 (3D) and show competitive performance compared to classic PointNet/++ methods.","CNNs have made an undeniably successful/impact on computer vision tasks. Its core operation relies on grids of discrete data samples, where structure representation of data is used (e.g. vector matrices …). This paper tries to disrupt this well established idea used so far, by proposing a Gaussian of mixtures (GMs) for tackling the curse of dimensionality, that is not avoidable with the conventional CNN architectures. The resulting architecture is termed as a deep functional network.",0.10526315789473684,0.2,0.11578947368421053,0.2698412698412698,0.19047619047619047,0.08490566037735849,0.15873015873015872,0.1792452830188679,0.14473684210526316,0.16037735849056603,0.15789473684210525,0.11842105263157894,0.12658227848101267,0.18905472636815918,0.1286549707602339,0.20118343195266272,0.17266187050359713,0.09890109890109888
444,SP:63e275b1d92c1cf8bf601f9689dd0c33df7ebefe,"This paper focuses on an emerging topic in adversarial training, which is about catastrophic overfitting. First, the authors observe that such overfitting is related to the limited types of perturbations. So, the authors introduce an adaptive adversarial training method, called APART, which adjusts the parameters of the perturbation generator. The experiments can verify that the APART can achieve competitive results. ","This paper focuses on stable and effective adversarial training and improving generalization using it. This paper first investigates the phenomenon called catastrophic overfits in adversarial training, and provides a new hypothesis that the phenomenon occurs since the generated perturbations become too weak. Based on the hypothesis, this paper proposes APART, which parametrizes the adversarial example and trains it using gradient accent to prevent perturbation deterioration. Using several standard benchmark datasets, they show that APART prevents catastrophic overfitting and provides comparable performance with several existing methods (including PGD, Free-8, and variants of FGSM) in terms of generalization performance and computational costs.  ","The paper made two contributions: 1. it identified the co-occurrence of abrupt robustness drop and perturbation degrading to (potentially) random noise; 2. it proposed a computationally faster adversarial training algorithm while attaining a good accuracy. Point 1. is interesting: it suggests previously identified ""catastrophic overfitting"" may be due to the perturbation in training time fails to mimic those in test time in distribution, while the adversarial training is being performed. This is different from classical empirical risk optimizations, where the training data do not depend on the training processes. ","**Summary**  The paper address the issue of catastrophic overfitting in robust training. Robust overfitting is often attributed to the fact that robust training tends at some point during the training to overfit to the underlying threat model augmenting the adversaries. This cases the test robust accuracy against the augmented training adversaries to be high while achieving 0% robust accuracy against stronger threat models. The paper provides new insights to potential other reasons to this phenomena. In particular, the paper argues that that at the same epoch when catastrophic overfitting happens, the quality of the augmented adversaries deteriorates fast and is no stronger than a random noise agumentation. The paper then goes about resolving this issue by strengthening the augmented adversaries during training by learning the hyperparameters of FGSM adversaries, namely the step size and the initialization. In addition, the paper augments the adversaries per Residual block, i.e. finding FGSM attacks per layer input. This approach is titled APART and shows competitive performance compared against existing methods in both final robust accuracy and training time efficiency. Experiments are conducted on CIFAR10, CIFAR100 and ImageNet datasets.",0.3,0.21666666666666667,0.3,0.1188118811881188,0.2079207920792079,0.2111111111111111,0.1782178217821782,0.14444444444444443,0.0972972972972973,0.13333333333333333,0.11351351351351352,0.10270270270270271,0.2236024844720497,0.1733333333333333,0.1469387755102041,0.12565445026178013,0.14685314685314685,0.13818181818181818
445,SP:63e4f9a599d1238919f6f10ba979a6f10793c466,"The paper deals with end-to-end learning of heuristics for combinatorial optimization problems. The authors propose an extension of the active search method of [Bello et al 2016], where only part of the model parameters are updated at test time for each instance. They propose three ways of applying this idea that consist in fine-tuning part of the instance embeddings, the parameters of an additional layer or directly the prediction scores of the model. Applied to the POMO method [Kwon et al 2020] for the TSP and CVPR and the L2D method of [Zhang et al 2020] for the JSSP, the proposed efficient active search leads to significant improvements on instances of the same size and larger than the training ones. ","This paper studies deep learning methods for solving combinatorial optimization problems. The authors write that state-of-the-art methods typically use models that consist of encoder and decoder units. The methods first create an embedding of the problem instance using the encoder. Then, starting with an empty solution to the problem, the embedding and the decoder are used to autoregressively construct a solution over a series of time steps. Given an already trained model and a test instance, this paper studies how to quickly update the model parameters in order to improve the quality of the solution returned by this procedure. The authors propose three techniques, which adjust (1) the normally static embeddings of the problem instance that are generated by the encoder model, (2) the weights of additional instance-specific residual layers added to the decoder, and (3) the parameters of a lookup table that directly affect the probability distribution returned by model.","The paper studies machine learning-based methods for combinatorial optimization. The paper builds upon Bello et al. (2016) on using reinforcement learning to generate solutions for combinatorial optimization problems (e.g., TSP). The novelty of the paper is to optimize only a subset of the model parameters. The paper then proposes three different implementations based on this idea.","The paper proposes a new method of updating deep neural networks for combinatorial optimization problems during search using reinforcement learning. In particular, the authors show that by updating only part of the network, better results can be achieved at lower cost. They describe and evaluate their method on different combinatorial optimization problems, comparing to other machine-learning-based approaches as well as ""traditional"" solvers.",0.2601626016260163,0.17073170731707318,0.14634146341463414,0.12903225806451613,0.0967741935483871,0.2413793103448276,0.2064516129032258,0.3620689655172414,0.28125,0.3448275862068966,0.234375,0.21875,0.2302158273381295,0.23204419889502764,0.19251336898395718,0.18779342723004694,0.136986301369863,0.22950819672131145
446,SP:641b518c93960d19a97b19f9ac493b7dbbc3da7b,"This paper introduces a new continual learning framework that aims to boost the research in the field. This framework is based on a taxonomy of all possible assumptions that are common to CL methods. Moreover, this taxonomy helps in putting supervised and reinforcement methods in a unified framework. ","The paper attempts to unify all CL research with a single formalism. Next, they present a software implementation of their framework. Finally, experiments are ran which demonstrate that Sequoia can be used to evaluate CL methods.","In this paper, the authors try to establish a unified framework for different continual learning settings. They also provide a Python library, which includes different related methods. Extensive experimental results are also provided. ","The paper proposes a theoretical framework to organize research problems in the continual learning (CL) domain according to a hierarchy. This theoretical framework is used as the basic foundation for Sequoia, a software library designed to reuse methods (i.e. training algorithms) across different research problems (settings).",0.1875,0.14583333333333334,0.2708333333333333,0.19444444444444445,0.25,0.2727272727272727,0.25,0.21212121212121213,0.2765957446808511,0.21212121212121213,0.19148936170212766,0.19148936170212766,0.21428571428571427,0.1728395061728395,0.2736842105263158,0.20289855072463767,0.21686746987951808,0.22499999999999998
447,SP:642493b32bcd4f88d38145e7078930ec7e37c8ba,"This paper studies within-class variability which reduces along the layers of deep neural networks. They mainly question the effect of sparsity and soft-thresholding introduced by ReLU. They show that these classification improvements by eliminating spatial within-class variabilities rather come from a phase collapse, which eliminates the phase of network coefficients. Eliminating the phase of zero-mean filters improves the separation of class means, hence increase in classification accuracy. They introduced a complex-valued neural network in which spatial filters are defined as complex multiscale wavelets and learning is reduced to $1 \times 1$ complex filters across channels. Their results show that such a network is able to reach ResNet-18 performance on CIFAR10 and ImageNet.     ",This paper proposes that using phase modulus operators instead of thresholding non-linearites is beneficial to classification performance in the case of scattering like networks. The authors show that such a network with some learnable operators can come close to performance of small ResNets when they use the modulo operators and performance degrades when using other non-linearieis. This is demonstrated on ImageNet and CIFAR-10 and is accompanied with relevant analysis and theory. ,"The authors present some theoretical results, followed by some experiments, in support of their argument that the linear separability for image classification in deep convolutional neural networks mostly relies on a phase collapse phenomenon. By eliminating the phase of zero mean filters they improve the separation of class means.They present a Phase Collapse Scattering network and demonstrate Resnet-like accuracy. The authors argue that phase collapse are both necessary and sufficient to discriminate class means on complex datasets.","This is a very interesting paper that is based on scattering networks, for which it shows that the so called ‘phase collapse’ leads to state-of-the-art results on par with modern architectures, like ResNet. To derive this, the paper shows that having neural networks on complex numbers is similar to a structure deep network (like with wavelet filters) on the reals. Phase collapse is then when there is an operation, like the modulus (absolute value function), which eliminates the phase from the complex number and maintains only the amplitude. All in all, the paper shows that maintaining the amplitude while eliminating the phase, is what brings the high accuracies, while the reverse (keeping the phase, eliminating the amplitude) yields terrible accuracies. As a corollary, I would say that the paper explains why non-linearities, like ReLUs, are so successful.",0.1440677966101695,0.2288135593220339,0.1694915254237288,0.16216216216216217,0.22972972972972974,0.21518987341772153,0.22972972972972974,0.34177215189873417,0.14184397163120568,0.1518987341772152,0.12056737588652482,0.12056737588652482,0.17708333333333331,0.27411167512690354,0.15444015444015446,0.1568627450980392,0.15813953488372093,0.15454545454545454
448,SP:647445f1bf95a37d9a93ecc1d3ead7e52d821671,"This paper represents a domain adaptation (DA) problem in a multi-objective optimization scheme, which can be solved by a gradient-based perspective. The main contributions of the paper include:  1. Proposes a Pareto Domain Adaptation (ParetoDA) approach to control the overall optimization direction, aiming to optimize all training objectives. That ParetoDA scheme can be plugged into various DA methods and enhance their performance.  2. To find a desired Pareto optimal solution, the authors design a surrogate objective function that mimics the target classification task.  3. Provides experimental results that show the effectiveness of ParetoDA compared to some DA methods.","While doing domain adaptation, we usually have a source classification loss and a domain alignment loss. However, these losses might compete with each other and we might reach a situation where one loss can’t be reduced without degrading the other one. We call that the Pareto front. For DA, we don’t know which loss is better than the other one. In this paper, they come up with an optimization strategy that helps us navigate to an ideal portion on the Pareto front. Essentially, they stop the optimization if one loss can’t be improved without degrading the other.","The paper considers the common class of domain adaptation algorithms consisting in minimizing the sum of a source risk term and a divergence between the domains. Since both terms are required to be small, the authors examine the tradeoff between them via their associated Pareto front. This latter having non-convex regions non-reachable by the classic weighted sum minimization procedures, a multi-objective optimization procedure is proposed in order to reach all of the regions of the Pareto front and to access tradeoffs that are not accessible by the classic approach. Specifically, the authors use a modified version of a multi-objective optimization procedure, where they define a guidance direction that adjusts the directions of both gradients of the two cost function terms. The guidance direction is the gradient of a surrogate loss assessing performance on the target domain. To show the empirical performance of their idea, the authors present experiments in which their method improves previously proposed DA algorithms.","This paper addresses the issue of previous domain adaptation optimization scheme, which learning objectives conflict with each other. The main contribution is to dynamically control the overall optimization direction so that no objective is harmed resulting out to boost the performance. To this end, it designs desired pareto optimal solution with surrogate objective function that mimics the target classification task called Pareto Domain Adaptation.",0.15,0.26,0.31,0.21,0.14,0.11180124223602485,0.15,0.16149068322981366,0.484375,0.13043478260869565,0.21875,0.28125,0.15,0.1992337164750958,0.3780487804878048,0.16091954022988506,0.17073170731707318,0.16000000000000003
449,SP:6479342bd412b9d8946e2d69a7663b6e9205636a,The paper proposes a method to equip discrete-group convolutional neural networks (GCNNs) with a perfectly equivariant subsampling layer. The method relies on a non-fixed sampling grid which is dynamically chosen as an equivariant function of the current input. This method enables encoder-decoder architectures where each layer is equivariant to the global action of the discrete group on the input of the model. Experimental results confirm the accurate equivariance and the improved generalization of the models.,"The paper proposes and evaluates equivariant subsampling and upsampling operations for group convolutional networks, which include vanilla CNNs as a special case. These operations address the issue that conventional subsampling and upsampling operations are only equivariant under the action of a subgroup of symmetries. The models have the property to disentangle the appearance of objects from their G-pose, which allows to build disentangled group equivariant autoencoders.  As a simple introduction, the authors formulate their method initially for the special case of $G = \mathbb{Z}$ being the 1-dimensional translation group. In contrast to conventional subsampling operations, which sample the values on a grid $c\mathbb{Z}$ with spacing $c$, the equivariant subsampling operation samples features on a shifted grid $c\mathbb{Z} + i$, where $i = 0, \dots, c-1$ is an integer offset that is predicted from the signal itself via some equivariant function. The tuple of subsampled features and the integer offset are shown to transform in an equivariant way under the action of the full symmetry group $\mathbb{Z}$. If multiple subsampling layers are successively applied in a deep network, each of them adds a new offset value to a sequence of offsets. When eventually pooling to the trivial group $G=\{e\}$, the resulting feature is position invariant and all position information is encoded in the offset sequence. Upsampling layers are accordingly taking a low-resolution signal and an offset and embed it with this offset into a higher-resolution grid.  These constructions are in the obvious way generalized to feature maps on arbitrary discrete groups $G$: The lower resolution sampling grid is here given by a coset $pK \in G/K$ of a subgroup $K<G$ in $G$. The group equivariant subsampling operation is therefore producing a tuple which consists of a subsampled signal on the coset and the coset (offset) itself. To upsample such a signal, it is embedded in the coset of the supergroup. Lemma 2.1 and Proposition 2.2 assert the full $G$-equivariance of these operations.  The integer offset, or, more generally, coset, needs to be computed via any equivariant map from the input signal to the space of cosets. A simple choice, made by the authors, is that coset $gK$ that consist to the argmax group element $g$ of the signal's l1-norm.  The equivariant subsampling layers are used to construct various group equivariant autoencoders (GEAs), which are evaluated against baseline autoencoders that are constructed from group convolutions but use conventional subsampling and upsampling layers. GEAs are empirically shown to be exactly equivariant and to generalize perfectly out of distribution. This is not the case for the baseline models, whose conventional subsampling and upsampling layers break the models' overall equivariance. The disentanglement of object appearance and $G$-pose in the autoencoders' latent space is in figure 3 (left) shown to allow for an independent manipulation of these factors. GEAs perform on single and multi-object reconstruction tasks significantly better than the baselines.","This paper introduces a group-theoretic framework for equivariant subsampling. Based on a similar approach to ref [3] in the case of translations, the idea is to subsample according to intrinsic landmarks and keeping track of the shifts involved. The framework introduced can handle this procedure for general symmetry groups by seeing the subsampled space as a subgroup, and the shifts are stored as elements of the quotient of the original group and the subgroup. The method is evaluated in the context of VAEs, where it is shown that an architecture using equivariant subsampling and upsampling is capable of fully disentangling covariant from invariant features, and, depending on the group used, is shown to generalize perfectly to unseen object positions.","The authors propose subsampling and upsampling layers that are equivariant to common transformation groups. These constructions are used to design autoencoders consisting of layers that alternate between group equivariant convolutions and group equivariant subsampling/upsampling layers. The result is that the entire autoencoder is *exactly* group equivariant. Experiments on translations, translations+rotations, translations+rotations+reflections for simple images with single/multiple objects clearly show the improved performance over subsampling methods that are not group-equivariant.",0.47435897435897434,0.2692307692307692,0.16666666666666666,0.09330628803245436,0.06288032454361055,0.125,0.07505070993914807,0.175,0.17333333333333334,0.38333333333333336,0.41333333333333333,0.2,0.12959719789842383,0.2121212121212121,0.16993464052287582,0.15008156606851547,0.10915492957746478,0.15384615384615385
450,SP:6487995799eba395f7af7dd75e1b4921b1682a14,"This article considers the behaviour of some modern learning in games dynamics (gradient ascent/descent and the follow the regularised leader family), applied in games that evolve periodically. The essence of the results are that if the equilibrium is constant throughout the cycle of the game, then the algorithms are periodic, whereas if the the equilibrium is not constant, or the game evolution non-periodic, then the algorithms may not be. Even in the recurrent cases, it is not always the case that the time average of strategies converge. Some simple experiments are given to demonstrate the results.","#  Summary of Paper The authors study the behaviour of two continuos time learning dynamics under two types of periodic zero-sum games, continuous gradient descent ascent (GDA) in unconstrained bilinear games, and continuous follow the regularized leader (FTRL) in polymatrix games. It is shown that under mild conditions, both GDA and FTRL are Point-Care recurrent, all orbits eventually return arbitrarily close to the initial conditions. The techniques used are common tools from dynaimcal systems but the applications to periodic games seem novel. Futhermore, strong insights on assumptions are given by providing counter examples to the Theorems, and the authors address the important issue of average convergence in games with insightful negative and positive results.  ","In this paper, the authors aim to analyze the asymptotic behavior of online learning algorithms in periodic rather than static zero-sum games. In terms of positive results, they show that the time-evolving *periodic* versions of both zero-sum bilinear games (under Gradient Descent Ascent) and zero-sum polymatrix games with time-invariant equilibria (under FTRL) satisfy the notion of Poincare recurrence -- a formal guarantee of cyclic behavior. However, the authors provide negative results too: in the described cases, the time-average of the GDA/FTRL plays does not converge to a time-invariant Nash equilibrium (in the latter case there is utility convergence).","The paper aims to analyze online learning behaviors of the gradient descent-ascent(GDA) and the follow the regularized leader(FTRL) algorithms for periodic zero-sum games. Seminal works [23, 19, 7] show that online no-regret learning dynamics is Poincar\'e recurrent in repeated static zero-sum games, but the techniques used in the seminal works does not work well in periodic one. To resolve this, the paper exploits an important theorem about volume preservation for periodic systems by [1], and shows that GDA dynamics in periodic zero-sum bilinear games and FTRL in periodic strategy polymatrix zero-sum games are Poincar\'e recurrent. The paper also shows a negative result that time-average strategy does not converge to time-invariant equilibrium, as a counter example. The empirical evaluation support the theoretical results. (Reference number follows the paper.)",0.25510204081632654,0.24489795918367346,0.2755102040816326,0.22608695652173913,0.24347826086956523,0.3333333333333333,0.21739130434782608,0.22857142857142856,0.19424460431654678,0.24761904761904763,0.2014388489208633,0.2517985611510791,0.23474178403755866,0.23645320197044334,0.2278481012658228,0.23636363636363633,0.22047244094488191,0.2868852459016394
451,SP:64f363ae7da559ce6234a8be5db27e1725ba980c,"The primary contribution of this paper is two folds. First, the paper presents a new crowd-sourcing dataset that contains annotations for object categories, attributes, and affordances. Second, the paper presents a CNN training and inference pipeline that leverages ""causal interference"" to address potential biases.","This paper proposes a new large-scale benchmark dataset for object concept learning, which consists of recognizing attributes, affordances, and their causal raltions about objects in input images. Detailed annotations of object categories, attributes and affordances on both category and instance levels, and their causal relations (on the instance level) are provided. This new dataset will be helpful for the community to advance the research of causal learning from visual data.   A strong baseline method is also proposed that explicitly considers the causal structure and concept instantiation of object categories, attributes, and affordances. Although it shows better results than other baselines, there is still great room for future improvement.","This paper introduces a large annotated dataset for object concept learning. The proposed dataset contains annotation at two levels of granularity (category and instance level). The dataset also provides causal relations between object attributes and their affordances. The paper also provides a thorough insight into their annotation process, and introduces a baseline method called the Object Concept Reasoning Network (OCRN) based on causal intervention and instantiation. The proposed dataset is large-scale for similar ones in this space, and can be of great help to researchers working in the area of concept learning and compositionality.","In short, the paper proposes a new task, that of object concept learning. Object concept learning is a basically a combination of object classification, attribute classification of said object, and affordance classification of said object, both at a category (affordance of any cup) and an instance (affordance of this cup) level. To this end, the paper proposes a dataset, which starts from existing ones and extends them accordingly. Further, it proposes a baseline method that is somewhat inspired by do-calculus (Pearl). Experiments show positive trends.",0.3333333333333333,0.3333333333333333,0.26666666666666666,0.23853211009174313,0.22018348623853212,0.18947368421052632,0.13761467889908258,0.15789473684210525,0.13953488372093023,0.2736842105263158,0.27906976744186046,0.20930232558139536,0.19480519480519481,0.21428571428571427,0.183206106870229,0.2549019607843137,0.24615384615384617,0.19889502762430938
452,SP:650cd944cd51a5121c3d185a5a45dc5b44c9b2a3,"This paper proposes a method that combines volume rendering with surface rendering techniques for multi-view reconstruction. The method represents the geometry of objects using a neural implicit representation that outputs the SDF at query locations. In order to enable volume rendering, the output SDF is mapped to a density function centered around the surface. Furthermore, they also encode the appearance of the object into the neural implicit representation that is needed for multi-view reconstruction. The main contribution is a novel weight function that is centered (unbiased) around the surface while being occlusion-aware. Therefore, the paper resolves major limitations of volume rendering (to few constraints on surface) and surface rendering (very sparse gradients). The paper extensively discusses different formulations of the weight function (properties, weaknesses, strengths). The method is evaluated on the DTU as well as the BlendedMVS dataset and compared to existing state-of-the-art methods like NeRF (volume rendering) and IDR (surface rendering). Moreover, they evaluate some aspects of the method in an ablation study. ","This paper proposes a neural surface reconstruction method which they call called NeuS. The novelty of this method is derived from their use of signed distance functions to represent geometry, and a proposed volume rendering method for accumulating information along rays. With these changes they are better able to estimate depth in more complicated surface geometry, demonstrate clearly superior shape predictions, and achieve state of the art quantitative results on the DTU dataset. ","This paper presents a method for implicit 3D surface reconstruction from posed 2D images, where the 3D surface is represented with an SDF. The authors advocate a new way of using neural volume rendering methods for surface reconstruction, where the density field is induced by the optimized SDF instead of direct MLP outputs (as in e.g. NeRF). In addition, the authors described two key properties that should be satisfied for volume rendering with SDF representations and proposed a novel solution. Experimental results on the DTU multiview dataset shows its advantage over state-of-the-art baseline methods.","This paper presents a novel multi-view reconstruction model that couples an implicit SDF representation (as in IDR [31]) with an unbiased volumetric rendering function (as in NeRF [20]). The volumetric rendering function is deliberately ""de-biased"" by redefining the opacity values $\alpha_i$ such that the weight is maximized exactly on the surface defined by the zero level set of the SDF.  The resulting model enjoys the benefits of both surface-based representation for high-fidelity surface reconstruction, and dense volumetric rendering which provides effective gradients that are not restricted locally to the surface and facilitate surface details. The reconstructed surfaces are much better compared to IDR and NeRF. This result can potentially also be very useful for further downstream applications that require smooth effective gradient signals.",0.13529411764705881,0.18823529411764706,0.17647058823529413,0.2602739726027397,0.1917808219178082,0.19387755102040816,0.3150684931506849,0.32653061224489793,0.234375,0.19387755102040816,0.109375,0.1484375,0.18930041152263372,0.23880597014925375,0.20134228187919465,0.2222222222222222,0.13930348258706468,0.168141592920354
453,SP:65b527ee03b4f74c82149582c700c6350609fde9,"This paper studies ANNs for classification from an NTK perspective. The authors rely on earlier results about the NTK to derive convergence results about ANNs trained with hinge loss (and other loss functions) with an l^2 regularization, showing that these essentially fall in the NTK regime and thus connecting them with SVM. Most of the results are theoretical, but some numerics are also given.",The paper shows the equivalence between the infinitely wide NN trained by soft margin loss and the standard 1-norm soft margin SVM with NTK. The authors further consider general loss functions with l2 penalization and show the equivalence between NN and regularized kernel machines. They also provide two practical applications of the developed theory.,"This paper theoretically analyzes the connection between Neural Network (NN) and Support Vector Machine (SVM). It shows the equivalence between SVM with Neural tangent Kernel (NTK) and infinite-width NN trained by soft margin loss with subgradient descent.  Besides, they extend their theory to general $\mathit{l}_2$ regularized loss functions and show finite-width NN trained by a $\mathit{l}_2$ regularized loss function is approximately a kernel machine.  It provides robustness certificates for infinite-width NN. ","In this paper, the authors consider the infinite-width limit of neural networks and show the equivalence of NN trained by soft margin loss and SVM with neural tangent kernel. They proceed to show that any finite-width neural networks trained by GD is approximately a kernel machine with a certain data dependent kernel. They further show that one can get a robustness certificate from NTK.",0.2,0.16923076923076924,0.16923076923076924,0.36363636363636365,0.2727272727272727,0.34615384615384615,0.23636363636363636,0.14102564102564102,0.16666666666666666,0.2564102564102564,0.22727272727272727,0.4090909090909091,0.21666666666666665,0.15384615384615385,0.16793893129770993,0.3007518796992481,0.24793388429752064,0.37500000000000006
454,SP:660118749748f78b5fc89537663c12430cb8f6ec,"This paper studies a variant of strategic classification where there is not instantaneous interaction between the learner and the agents, but rather, there is a “slower” and a “faster” player (i.e., the whole interaction is governed by the frequencies according to which the learner and the agents update their decisions). This variant is closer to real-life settings, where for example the learner may have much more computational power compared to the agents. In their model, the agents are assumed to update their decisions at a fixed rate, and the learner gets to decide whether to be proactive (i.e., update his decisions slower than the agents) or reactive (i.e., update his decisions faster). After the learner announces the deployed model, the agents are broadly “rational”; to be more specific, they decide their reports based on no-regret algorithms (rather than the standard instantaneous best-response in prior work). The difference in timescales results in fact in a change of who the “leader” and who the “follower” is in the setting. The question that the paper addresses is what changes in terms of loss for the learner and the agents if there exists this timescale difference and whether there are settings where it is best for both the agents and the learner to always have one of the two.  The paper has two main results. First, it shows that if the learner is proactive (resp. reactive), then the learning dynamics of the system converge to the learner’s equilibrium (resp. agents’ equilibrium). Second, it shows that if the learner runs linear or logistic regression and the agents aim to maximize their predicted score, it is preferred by both the learner and the agents that the agents “lead” (i.e., that the learner is reactive). In terms of techniques, the paper uses variants of standard tools in Bandit Convex Optimization (BCO), assuming that everything is smooth enough (i.e., convex, smooth, Lipschitz).","The paper argues for a more general view on strategic classification of objects modified by adversaries. In the standard view, the designer of the classifier is assumed to be the leader and the classifier is optimized with respect to a best response form the data generator to its parameters. This paper argues that in some situations, it may be possible and desirable to reverse the roles and force the rational attacker to create objects first and optimize them with respect to the expected best response classifier. The paper shows how different update frequencies of individual players using no regret and gradient descent learning can lead to convergence to the equilibria in either assignment of the roles. Furthermore, the paper shows that for linear and logistic regression, each of the role assignments can be desirable for each of the players depending on parameters, such as the magnitude of the allowed perturbation by the attacker. ","The authors study an extension of strategic classification in which they consider how the time it takes the model designer and agents to react to one another effects the dynamics of strategic classification. Differences in the updating frequencies of the two parties can essentially switch which party is the leader (typically the model designer is the leader).  Different equilibria can arise depending on which party has the faster update frequency. Convergence to these equilibria is characterized in the case of agents leading and also the model designer leading. Additionally, cases in which having the agents be the leaders is desirable for both parties are outlined.","Almost all of the literature on strategic classification is situated in a Stackelberg game setting in which the decision-maker leads, committing to a model, after which agents follow, best-responding to the model. The core insight of this work is that in some real life situations, the decision-maker is able to adjust their model more rapidly than agents are able to adjust their strategic adaptation, leading to a reversal of the traditional order of play: the agents become the leader, and the decision-maker becomes the follower. The authors compare the dynamics of these ""proactive"" and ""reactive"" regimes, which differ in the comparative frequency of the two parties' updates, under the weak assumption that agents are no-regret learners. In particular they give conditions under which the learning dynamics converge to Stackelberg equilibria. Finally, they show that in two simple examples (linear regression and logistic regression with bounded actions of linear cost), the equilibrium in which the agents lead is at least as preferable (and sometimes more preferable) for _both_ agents.",0.1393188854489164,0.09907120743034056,0.17027863777089783,0.17647058823529413,0.23529411764705882,0.3333333333333333,0.29411764705882354,0.3047619047619048,0.3179190751445087,0.2571428571428571,0.20809248554913296,0.2023121387283237,0.18907563025210083,0.14953271028037382,0.22177419354838712,0.20930232558139533,0.22085889570552147,0.2517985611510791
455,SP:663e71d65947043448c4af54ac542c7ea68d8a0e,"This paper studies the problem of robustness optimization in multilingual machine translation models. In response to this problem, the author empirically found that in the loss optimization of multilingual machine translation models under unbalanced data scenarios, ""sharpness"" of local curvature in the loss landscape causes interference among languages, which caused instability in training. Inspired by the robust optimization work of Multi-task Learning, the paper proposes the Curvature Aware Task Scaling (CATS) optimization algorithm, which alleviates the phenomenon that different translation directions competitive update the loss landscape during the early stage of training in multi-language translation training, while high resource ones dominating the optimization trajectory during the rest of training.","Paper presents a curvature aware bi-level optimization algorithm applied to multilingual/multi-task machine translation. The proposed algorithm is designed to encourage the loss is reduced along the directions of low-curvature (with the implicit inductive bias of flat minimas being better at generalization). Akin to ""Sharpness-Aware Minimization (Foret et al. 2020)"" but in a multi-task environment. Experiments on three different datasets tests the effectiveness of the proposed method (called Curvature Aware Task Scaling - $CATS$). Results show clear trend that the proposed method is highly effective at improving the quality of low-resource languages. Further analysis provided to understand the tradeoffs, sensitivity and dynamics of the algorithm.   - Making use of curvature information during the optimization of multi-task models for translation is novel and expected to be impactful in the long run (while the paper is not demonstrating convincing empirical results, it is showing the potential of using higher order signals during optimization, which I found to be an important contribution).","This work looks at the optimization of multilingual NMT models. Typically, the data across different language pairs is highly skewed and prior work either upsamples low-resource languages such that it's uniform or uses a temperature (T=5) to upsample lowRes and downsample HiRes. This work proposes a new optimization algorithm called CATS which encourages solutions with low local curvature. The proposed technique has been shown to be effective when compared to 3 temperature-based baselines (T=1,5,infinity) and two dynamic methods multiDDS and GradNorm. Experiments are performed on 3 public datasets: TED, WMT and OPUS-100. Robustness of the technique to overparameterization and to large batches has been studied.","This paper theoretically motivates and then implements a curvature-based, online, per-language learning rate adjustment to improve the performance of low-resource languages in multilingual machine translation (MT) models. Experiments in three scenarios varying in multilinguality and data size show that the technique works consistently, outperforming a widely-adopted temperature-based sampling baseline. Through a number of analysis and ablation experiments, they are able to provide insight into how and why the optimization technique works.",0.2882882882882883,0.14414414414414414,0.15315315315315314,0.10975609756097561,0.11585365853658537,0.1504424778761062,0.1951219512195122,0.1415929203539823,0.2236842105263158,0.1592920353982301,0.25,0.2236842105263158,0.23272727272727273,0.14285714285714288,0.1818181818181818,0.1299638989169675,0.15833333333333335,0.1798941798941799
456,SP:6684875cca26418320fe494268f2372678792975,"The authors introduce HiTS, an HRL algorithm in which the high-level policy communicates not only which goal to achieve to the low-level policy but also when to achieve it. This feature enables the agent to adapt to dynamic environments that require planning over long horizons. The authors also discuss to what extent communicating timed sub-goals results in more stable learning for the high-level policy. The authors show that HiTS successfully learn on a set of 3 benchmarks, that have been designed to be dynamic and require planning, where classical sub-goals HRL method fail to learn stable solutions.","This paper proposes a new method for HRL in which the higher level chooses both a subgoal and a time at which the subgoal should be achieved (""Hierarchical RL with Timed Subgoals, or HITS). The main claim of the work is stated early on: ""Hence, the use of timed subgoals extends the class of tasks that can be solved efficiently by subgoal-based HRL methods"".  This claim is motivated by several elements. First, the paper identifies an issue of non-stationarity in dynamic environments: the higher level's reliance on the lower level's behavior induces a particular kind of non-stationarity; as learning progresses, the lower level behaviors will improve, thereby changing the outcome of the same high level actions. This discrepency motivates the claim that ""all levels in a hierarchy should see transitions that look like they were generated by interacting with a stationary effective environment."" Alongside this, it is noted that dynamic environments with elements not controlled by the agent are particularly challenging as they further introduce non-stationarity. Second, the paper focuses on a decomposition of state into the controlled and uncontrolled parts of state, which allows for subgoals to concentrate on the controlled aspects of the environment. These two points are examined to motivate timed subgoals alongside hindsight relabeling techniques to form the core of HITS. Otherwise, by my reading, HITS is quite simple (which I take to be a virtue). Three new benchmark domains are introduced that feature time-based reasoning, such as a ship that needs to carefully furl its sail in order to quickly pass through a drawbridge. These domains are chosen to highlight the positive aspects of HITS, and to differentiate what HITS does differently from other HRL methods. The results quite clearly demonstrate that HITS is well suited to solve these domains.","The paper proposes to study HRL with timed goals wherein the high level controller can not only specify subgoals but also the time when it should be reached. The overarching goal of the work is to enable HRL methods to adapt better to non-stationarity. The key contributions of this work are 1) propose the idea of timed subgoals to address the adverse impact of non-stationarity and ever changing low level behaviors on the overall policy, 2) distil these insights into an HRL algorithm and introduce HiTS as the proposed algorithm, 3) introduce tasks where such challenges are seen, and 4) the empirical analysis shows sample-efficient learning which results in stable agents as opposed to existing methods which fail. Finally, the authors also show theoretically the utility of the timed-subgoals in eliminating non-stationarity in the data generating process for the high level controller.","This paper presents a new framework for subgoal-based HRL where the high-level chooses not only what subgoal to be reached but also when to reach the subgoal, i.e., a timed subgoal, for the lower-level. By emitting such timed subgoals, the non-stationarity of transition times during learning at the high-level can be removed, which is supported by theoretical analysis, and thus, making it possible for stable learning in dynamic environments. Combining timed subgoals with hindsight action relabeling, a practical algorithm HiTS is proposed and it outperforms baseline methods on a set of dynamic environments. ",0.2549019607843137,0.24509803921568626,0.23529411764705882,0.12582781456953643,0.11920529801324503,0.17006802721088435,0.08609271523178808,0.17006802721088435,0.24242424242424243,0.2585034013605442,0.36363636363636365,0.25252525252525254,0.1287128712871287,0.20080321285140562,0.23880597014925375,0.1692650334075724,0.17955112219451372,0.2032520325203252
457,SP:669a0ded885ebe17eaa8d9e66bc99e51f9d040a1,"The paper presents LOM, a new structured latent variable model for object-centric probabilistic modelling of scenes. It differs from previous models of this type, such as GENESIS, by avoiding the use of an autoregressive prior over slots/objects. In prior work, the use of such priors has meant that either inference needed to introduce an order among slots (e.g., by also occuring autoregressively), or the order of slots had to be implicitly integrated out during training. It is argued that either option is suboptimal. The proposed model instead models slot assignments in the prior deterministically given a scene-level variable. Inference commences largely in an orderless manner; the inferred slots are matched with their counterparts in the prior only after the fact for training. The model is evaluated experimentally on a suite of synthetic multi-object image datasets, and compared to prior work such as GENESIS. ","This work tackles the problem of learning object-centric generative models of scenes, and in particular it addresses the challenge of handling the orderless nature of objects (represented by latent variables) when learning the model's prior. Most recent models in the literature either assume an arbitrary sequential scene generation (which can hinder the model's performance), or attempt to model the orderless set of object latent variables with an autoregressive prior which can cause discontinuities during training when the allocations of object change. The proposed method relies on incorporating (a) a scene-level prior that deterministically generates a per-object distribution ordering and (b) an ordering algorithm to match object posterior and prior distributions. By learning to generate an object order from a scene latent variable, it effectively sidesteps the problem of modelling the large equivalent class of sets. Quantitative and qualitative results show that incorporating these two ideas allows learning better generative models that can sample scenes with plausible composition of objects.","This paper proposes a structured latent variable model, called Latent Object Models (LOM), for compositional scene generation. To generate a scene image, LOM first samples a scene-level latent variable, and then samples multiple slot-level latent variables based on a sequence of deterministic variables modeled by an autoregressive model. To infer the ordered slot-level latent variables, LOM first estimates orderless slot posteriors, and then determines the permutation of orderless slot posteriors based on a greedy matching algorithm. After learning, the model is able to generate scene images with reasonable relationships of objects. Experiments are conducted on three datasets, and the proposed method is compared with several existing methods.","This work proposes a VAE-based hierarchical generative model (named Latent Object Models ) for scenes that contain multiple objects. This is done by modeling the hierarchical relationship between scenes and objects via a set of latent variables for scenes and objects. The object latent variables are a sample from their corresponding object slots. The object slots are inferred deterministically from the scene latent variable.  Having deterministic object slots makes the inference more tractable by removing the need for sequential decomposition of a scene into object slotsIn the experiments, the authors show results for random sampling of the latent variables and some qualitative results for their method and different versions of the proposed method. The authors also show quantitative comparison with some prior works",0.20270270270270271,0.17567567567567569,0.16216216216216217,0.14634146341463414,0.18292682926829268,0.23636363636363636,0.18292682926829268,0.23636363636363636,0.1951219512195122,0.21818181818181817,0.24390243902439024,0.21138211382113822,0.19230769230769235,0.20155038759689925,0.17712177121771217,0.17518248175182483,0.20905923344947733,0.22317596566523606
458,SP:67200d6f6a268fb9a3d78e4410da92b5f46e1f24,"The paper presents a learning from observation approach that uses a temporal goal proximity estimate as reward function for policy learning. The temporal goal proximity function is learned from observations and measures how many steps are left to reach the target goal state. Additionally, an ensemble of proximity functions is used to empirically estimate the uncertainty function that is added to the reward function to discourage exploration of uncertain proximity states. Results on a number of simulated tasks show performance improvement over GAIL, goal-GAIL, GAIfO, BCO and other competing methods.","The paper addresses the problem of learning from observation for directed tasks with a focus on generalization beyond the demonstrations. The main contribution of the paper is a goal proximity function (also referred as goal progress in the paper) which predicts the agent progress towards the goal. The paper has extensive experiments on navigation, locomotion and robotic manipulation which proves that the proposed goal proximity function helps to generalize in imitation learning. The results show that proposed method achieves better performance in case of learning from observation tasks and comparable performance with learning from demonstrations method.","Disclaimer:  I was the reviewer of this paper's older version submitted to ICML 2021. Along the reviewing process, the authors' information was kept anonymous.   Overall I'm glad to see this improved version. As the authors stated, 1) comparison to Goal-GAIl added; 2) clearly specify the generalization is limited, and not referring to out-of-demo goals/states which including a different embodiment (for example unseen objects) of environments. Thanks for the authors' improved clarity.  This paper is well written with sound evaluation results, falling in approaches learning reward function from human demonstrations, though I prefer viewing the generalization problem of adapting human demonstrations to unseen states or actions as more of a representation learning or meta-learning problem since such problem settings will flexibly apply to the manipulation of unseen objects, which is important to learn generalizable manipulation skills.","This paper presents a learning from observation technique for solving goal-conditioned tasks from demonstrations and environment interaction, without access to environment provided rewards. The method takes an adversarial learning approach like prior works, where the agent tries to learn behavior that is indistinguishable from that of the demonstrator. However the key difference in this work is that instead of using a binary discriminative objective, the paper uses a temporal progress prediction objective, where the learning agent’s experience is labeled as having 0 temporal progress and the demonstrators experience is densely labeled based on progress in the episode. The learning RL agent then aims to maximize progress (while also avoiding high uncertainty regions)   The intuition is such an objective will provide (1) a denser reward signal and (2) generalize more effectively to unseen states/goals. The paper demonstrates over a range of domains including navigation, locomotion, and manipulation this seems to hold true, with improved stability and performance over most baselines when generalizing to unseen states.",0.24175824175824176,0.18681318681318682,0.2857142857142857,0.17708333333333334,0.3020833333333333,0.1619718309859155,0.22916666666666666,0.11971830985915492,0.15568862275449102,0.11971830985915492,0.17365269461077845,0.1377245508982036,0.23529411764705882,0.14592274678111586,0.20155038759689922,0.14285714285714288,0.22053231939163498,0.14886731391585764
459,SP:67274b3e2d5f30fae8cf9b2ccda2508eb335d700,"In this paper, the authors propose a new acquisition function for Bayesian optimization based on gradient information. A GP model estimates the Jacobian and its estimation is used for gradient update. This method is implemented in the setting of policy search for reinforcement learning. Empirical evaluations are provided for a synthetic setting, LQR and MuJoCo gym environments.","The paper introduces GIBO, a modification of Bayesian Optimization (BO) that performs local policy search. Compared to classic BO, the strengths of this method are that it does not require storing all the past data points, making it more computational efficient, and that it is more stable when moving to higher dimensional problems. The authors compare their method with standard BO and Augmented Random Search, an approach based on finite difference policy search.",This paper concerns the high sample complexity of policy gradient methods and the computational complexity of Bayesian optimization. The authors propose a novel algorithm called GIBO (and its local variants) to minimize uncertainty in gradient estimation. Experiments are conducted and compared with some naive baselines.,"This paper modifies the standard BayesOpt pipeline in 2 ways:  1. Defines a new acquisition function, the ""Gradient Information"" (GI) which is the reduction in the variance of the Jacobian variance of $J$. 2. Exploration and Exploitation become disjointed: During the BayesOpt process, a batch of $M$ queries are made via argmax of the GI acquisition, purely for exploration (rather than a mix of exploitation and exploration as is standard in BayesOpt), with the goal of producing a final gradient update step $\nabla_{\theta}J$.   This defines a ""GIBO"" algorithm, which is then experimentally evaluated over the following benchmarks:   1. Sampled functions from GP's 2. LQR Controllers 3. Linear controllers for standard Mujoco tasks.  which show that GIBO outperforms previous ES/ARS and standard BayesOpt methods especially in high dimensional settings, most likely due to the use of ""local search""/gradients.",0.19298245614035087,0.19298245614035087,0.3684210526315789,0.1232876712328767,0.1780821917808219,0.28888888888888886,0.1506849315068493,0.24444444444444444,0.14788732394366197,0.2,0.09154929577464789,0.09154929577464789,0.16923076923076924,0.2156862745098039,0.21105527638190955,0.15254237288135591,0.12093023255813953,0.13903743315508021
460,SP:676a21e8260e20944c0246ccab14033222203284,"The authors introduce H-NeRF, a geometry-aware neural radiance field for human performance rendering.  The core ideas are to (1) conditioned the neural radiance field (NeRF) on a pre-trained implicit geometric human representation that captures a wide variety of body poses and shapes, and (2) co-learn a signed distance function (SDF) that describes the surface geometry of the NeRF for rendering. Experimental results show that, compared to other baselines, H-NeRF achieves better surface reconstruction and novel view and pose synthesis quality, and are able to extrapolate to new body shape volumes.  ","This paper presents a new NeRF based method for rendering and reconstruction of humans observed from sparse cameras. The main contribution is combining volumetric radiance fields and an implicit SDF for the tasks of novel view synthesis and geometric reconstruction for humans. To achieve this goal, the method utilizes a human prior, imGHUM, which was pretrained on thousands of 3D scans. ","The authors propose a model that combines a parameteric human shape model with NeRF and a residual signed distance function to enable novel-view synthesis of humans in motion, assuming full observation of the subject (i.e., no prior-based completion).  The core contributions is the combination of NeRF with a parametric human body model, which enables the learning of the NeRF in a canonical coordinate frame, as well as regularization of the learned geometry. Several additional tweaks improve the model further, such as leveraging auxiliarly losses via a instance segmentation mask, or feeding the estimated body pose to the NeRF as additional input, enabling dynamic (due to being pose-specific) fine detail.","This paper proposes a method for dynamic human reconstruction and rendering using sparse view videos.  The high-level strategy is to learn both a neural radiance field and an SDF field with mutual constraints in a canonical frame from temporal observations. Moreover, for generating live rendering and surface results, pose and position conditioned SDF offsets are also learned for better live frame surface reconstruction results.  To guarantee robust surface reconstruction, an implicit generative human model was used as the inner layer which not only incorporates strong human shape constraint, but also provides semantic correspondences for accurate warping between live frames and the canonical frame. Another benefit of using an underlying implicit human model is that shape and pose editing can be achieved through interpolating in the parametric body space, which was not demonstrated in previous works.",0.14736842105263157,0.18947368421052632,0.23157894736842105,0.21311475409836064,0.3442622950819672,0.1592920353982301,0.22950819672131148,0.1592920353982301,0.16176470588235295,0.11504424778761062,0.15441176470588236,0.1323529411764706,0.1794871794871795,0.17307692307692307,0.19047619047619047,0.14942528735632182,0.2131979695431472,0.14457831325301204
461,SP:67742f58fde88c28ca409664ec7e405203529634,"The paper targets the bias problem in visual question answering task where the models may only capture the biases instead of showing real reasoning abilities. The paper introduces a method called D-VQA which tries to alleviate this problem from feature and sample perspective. With two unimodal bias detection modules, D-VQA recognizes and removes the negative biases while authors also construct two types of negative samples to increase sensitiveness of VQA models to both modalities by minimizing the possibility of predicting correct ground-truth answer of the positive sample. The results on VQA-CP and VQA v2 dataset show the effectiveness of the approach.","This paper tackle explores how to partially annealed dataset biases when training VQA models.  To do so, they applied to debiasing strategy:   - they generate free negative samples to tentatively debased the answer distribution  - they use a neural pipeline to tentatively debiased the multimodal features Interestingly, both approaches are model-agnostic (up to $f = F(h,g)$), making the approach generic.  The paper has many merits and solid results. Multiple complementary experiments are insightful. Yet, there are a few remaining ambiguities that prevent me from fully advocating for acceptance.  I would be more than happy to increase my score if they are correctly addressed:","The paper introduces “D-VQA”, a training approach dedicated to remove harmful biases (both language and vision ones) in VQA models. D-VQA performs in two ways, namely: feature-wise and sampling-wise.  On the features side, during its training, D-VQA attempts to de-bias the features learned by a VQA model by subtracting the (deliberately) biased features learned by unimodal language-only and vision-only models. Moreover, in order to get rid of the unimodal branches during the inference, D-VQA training includes an additional contrastive loss which aims to bring the original VQA features closer to the debiased ones (i.e. the ones obtained after feature subtraction with the unimodal branches).  On the sampling side, D-VQA employs an existing approach proposed in [42] (which is properly mentioned and recognized by the authors) introducing negative sampling triplets for each positive triplet (by randomly changing either an image or a question in a positive triplet).  Combining the 2 aforementioned techniques allows D-VQA to obtain the state-of-the-art results on both VQA-CP and VQAv2 comparing to the existing bias-reduction methods (while using the UpDn model as a backbone). In addition to that, the authors demonstrate that their approach is model-agnostic (for example, it is compatible with a more recent and powerful LXMERT backbone).","This work addresses a known bias in visual question answering tasks, in which networks learn to pick an answer based on a single modality without perceiving the entire input.  The work novelty comes from proposing a set of regularization strategies s.t: (1) reducing ""bad"" bias that is not a useful commonsense knowledge;  (2) reducing computational inferences through contrastive loss; (3) considering an image-only bias; (4) employ negative sampling to increase attention sensitiveness on both modalities. In both VQA and VQA-CP, the approach shows state-of-the-art performance.",0.11428571428571428,0.3238095238095238,0.20952380952380953,0.20388349514563106,0.0970873786407767,0.09090909090909091,0.11650485436893204,0.15454545454545454,0.24175824175824176,0.09545454545454546,0.10989010989010989,0.21978021978021978,0.11538461538461539,0.20923076923076922,0.22448979591836735,0.13003095975232198,0.10309278350515462,0.12861736334405144
462,SP:677c379a60e0940b48ff1da7cef000d4acaabf30,"This paper studies the use of DARTS within the RL setting. Specifically, it investigates how DARTS can optimize perception modules for RL environments. RL-DARTS is optimised in a typical RL setting, using the standard RL loss function, in an end-to-end manner. The proposed approach is evaluated using the Procgen benchmark, and compared to the IMPALA-CNN architecture. Several ablation studies are performed to further examine the contributions of the components of the proposed approach. ","This paper proposes to combine differentiable neural architecture search (DARTS) with standard reinforcement learning (RL) frameworks by searching the model structures (convolutional cells) for the policy and value functions. The authors have applied the method to infinitely procedurally generated Procgen benchmark and demonstrated the benefits of DARTS in RL on search efficiency in terms of time and compute. In addition, the proposed method can be easily integrated with existing RL pipelines by simply replacing the image encoder with a DARTS supernet, compatible with both off-policy and on-policy RL algorithms. The authors further show that the supernet gradually learns better cells with more training iterations, leading to alternative architectures that can be highly competitive against manually designed policies and verify previous design choices for RL policies.","The authors aim to automatically design Reinforcement Learning Q function approximator via NAS, specifically by adapting DARTS framework. They main motivation behind this is that as RL state spaces become more complex simple function approximator models, even predesigned DNNs prove to be sub-optimal, since supervised models are not always suitably designed to learn in an online fashion from reward signals rather than labels. Beyond that there are also existing RL bottlenecks such as reward sparsity, sample efficiency etc. The authors use the whole DARTS supernet at the function approximator. To bypass the bilevel optimization target in DARTS, which could prove to be intractable for RL context, authors propose to optimize the loss on cumulative returns of the RL problem by optimizing the loss on architecture and weight parameters from the replay buffer samples. ",This paper considers the problem of differentiable neural architecture search for RL applications. It applies existing DARTS to the RL setting and conducts extensive experimental studies to examine the performance and behaviors of RL-DARTS. Experimental results show that the supernet learns alternative architectures that are highly competitive against manually designed policies.,0.2077922077922078,0.22077922077922077,0.2077922077922078,0.14960629921259844,0.2204724409448819,0.08955223880597014,0.12598425196850394,0.12686567164179105,0.3076923076923077,0.1417910447761194,0.5384615384615384,0.23076923076923078,0.1568627450980392,0.16113744075829384,0.248062015503876,0.14559386973180075,0.3128491620111732,0.12903225806451613
463,SP:67d2f4c742c015e3a34b058d0a7aa6ea0ab6d704,"======================= Summary:  This paper studies the problem of how well do neural networks transfer class-agnostic invariances from head classes to tail classes. They found that even a given transformation is class-agnostic, the DNN models still cannot disentangle it from class-specific features, which could partially explain the worse performances on rare classes. To solve this problem, they introduce a GIT method that uses a generative model to augment the tail categories with more diverse samples under certain transformations. It can empirically improve the long-tailed performances and transfer the knowledge of class-agnostic transformations from head to tail. ","The paper investigates if robustness (or ""invariance"") to nuisance transformations which do not change the class label such as lighting, rotation etc are learned across all classes or if such robustness is sensitive to the class size. The paper demonstrates that such invariances seem not to transfer across classes: ie the classes with fewer examples suffer more. THe paper proposes a generative model (GIT) to ""augment"" the less frequent classes which, to some degree, remedies this problem. ","The paper investigates if invariances learned by the model transfer across classes. Focussing on class-agnostic nuisance parameters, the interplay between per-class size and invariant representations has been explored. It is suggested that while networks can become invariant to these parameters for classes with many examples, it is unclear if this is also the case for classes with fewer examples. The paper shows this is not true i.e. invariances do not transfer well to small classes, and suggest that improving this can help increase performance on imbalanced datasets. And so, the paper proposes a two step solution to this problem. First, an image-conditional generative model is learned which learns to transform the image such that only the nuisance parameter changes. Secondly, this model for nuisance parameters is used for data augmentation. Using this approach, the authors are able to achieve a significant improvement on standard long-tail datasets.","This paper works on long-tailed or class-imbalanced learning. The authors found that the learned classifier in such a setting cannot effectively transfer the class-agnostic (in)variance in a dataset from the head classes to the tail classes, which causes poor classification performance for the tail classes. The authors thus proposed to learn such class-agnostic (in)variance via a generative model, and then use it to augment the training data of minor classes. The experimental results on several small-scale datasets demonstrate the effectiveness of the proposed method in improving long-tailed or class-imbalanced classification.",0.16161616161616163,0.2222222222222222,0.26262626262626265,0.35064935064935066,0.2597402597402597,0.1390728476821192,0.2077922077922078,0.1456953642384106,0.26262626262626265,0.17880794701986755,0.20202020202020202,0.21212121212121213,0.18181818181818185,0.176,0.26262626262626265,0.23684210526315785,0.2272727272727273,0.168
464,SP:67d3229f26ff3d6ca8a7a72c98a56c5a5c3d50d7,"This paper provides a statistical tool to analyze different variational inference methods in the f-divergence family. It showed that for high dimensional and low dimensional problems, the behavior of the variational inference methods may differ, and the analysis based on the GPD statistics can suggestion different strategies for different situations. The predictions from this theory were validated by some commonly used models (e.g., linear regression). In summary, this paper can provide useful guidelines for variational inference practitioners.","In this paper, the authors study the pre-asymptotic behavior of the density ratios between the joint distribution and the variational approximation. The authors use generalized Pareto distribution to model this density ratio in the pre-asymptotic regime and make three major predictions for how BBVI methods should behave. They verify their predictions through extensive experimentation, and finally, provide actionable advice to BBVI practitioners (use flows with exclusive KL and PSIS as a good default.) ","The paper discusses the accuracy of variational inference and the choices that need to be me made with regard to divergence and approximating family, particularly as they relate to the dimensionality of the problem. Based on an analysis of the pre-asymptotic behavior of the density ratios involved, the authors propose a framework based on Pareto smoothed importance sampling (PSIS) to help practitioners analyze the success of their approximate inference procedure. The authors provide experiments demonstrating general findings for variational inference for a range of divergences and approximate posteriors. ","The authors propose a conceptual framework and design guidelines for variational inference based on the pre-asymptotic tail behaviour of the unnormalized importance weight. This framework is used to study the behaviour of commonly variational families and f-divergences in mid-to-high dimensions. The authors observe that mass-covering divergences, while often superior theoretically and for low dimensional problems, are hard to optimize in mid-to-high dimensions compared to the mode-seeking exclusive KL-divergence. These observations are consistent with the prediction made using their framework based on estimating the tail-index of the best-fit generalized pareto distribution.",0.17721518987341772,0.24050632911392406,0.20253164556962025,0.24,0.21333333333333335,0.20224719101123595,0.18666666666666668,0.21348314606741572,0.15841584158415842,0.20224719101123595,0.15841584158415842,0.1782178217821782,0.1818181818181818,0.22619047619047616,0.17777777777777776,0.21951219512195122,0.18181818181818182,0.1894736842105263
465,SP:67dabe890b0022185d655ebab840bd4b1bde7d21,"In this work, the authors propose a new training algorithm MAT that adversarially trains Multi Input Multi Output (MIMO) models. They show that ensemble models based on MIMO when trained adversarially , show ``adversarial diversity'' and therefore are less vulnerable to transfer attacks. They empirically demonstrate that such models are robust to a variety of $\ell_1, \ell_\infty$ and $\ell_2$ attacks and achieve better performance than other methods claiming robustness across different threat models. The authors also show computational benefits of their algorithm over vanilla ensemble training. ","This paper uses ensembles--and more precisely multi-input-multi-output (MIMO) neural networks for computational efficiency---for adversarial training (AT), resulting in a method called MAT, short for MIMO AT. Moreover, the authors investigate how the sub-networks should be trained to increase the overall robustness of the total MIMO model. In particular, the proposed MAT generates the adversarial samples using the gradient with respect to the objective of the ensemble.  The authors demonstrate on MNIST and CIFAR-10 that the proposed method achieves generalization and robustness comparable to existing state-of-the-art methods. ","[Summary] This paper proposes an ensemble method to defend against adversarial attacks.  To be specific, this paper combines MIMO strategies (Havasi et al.) and adversarial training (Madry et al.). ","This paper proposes an ensemble based adversarial training strategy, which could improve the worst-case model robustness against multiple $\ell_p$-norm adversarial perturbations. To improve the training efficiency, the author introduces the Multiple-Input Multiple-Output (MIMO) strategy. The motivations and insights provided by this paper are interesting for me.",0.17045454545454544,0.07954545454545454,0.09090909090909091,0.07291666666666667,0.13541666666666666,0.3448275862068966,0.15625,0.2413793103448276,0.1568627450980392,0.2413793103448276,0.2549019607843137,0.19607843137254902,0.16304347826086957,0.11965811965811965,0.11510791366906475,0.11200000000000002,0.17687074829931973,0.25
466,SP:67de51967d6487a5e4a84c9f40ccd230d23822a5,"This paper presents a combination of analytical and (mostly) empirical results evaluating the task of ""honest hyperparameter selection"" under differential privacy; that is, accounting for the full set of tuning experiments conducted when computing the resulting privacy guarantee. The authors provide some empirical comparison between two existing composition methods for hyperparameter selections -- Moments Accountant (MA) and Liu & Talwar (LT) and give a relation between privacy of individual learners and the final learner in Theorem 1; however, their focus is mostly on showing the competitiveness of DPAdam relative to DPSGD. They conclude with a proposed adaptation to DPAdam which discards the second moments term since they observe this to converge to a constant value in practice.","This paper looks to take a deeper dive into DP hyperparameter tuning, an often overlooked aspect of DP learning. It makes several practical claims about this problem, including comparisons of two well-known DP selection strategies and arguing for the promise of adaptive optimizers. For the latter, they provide evidence for why learning rate and clipping are difficult to tune for DP-SGD and how DPAdam can help mitigate this issue. Finally they also propose a new optimizer which matches DPAdam on performance but reduces the amount of necessary computations. ","This paper discusses the problem of private hyperparameter optimisation in machine learning. Although the discussion and results seem interesting, the privacy model is not very clear.   The response clarified this but it should be in the paper.","This paper investigate and compare private hyperparameter tuning using two different methods. Further, the authors demonstrates optimal learning rate is inver proportional to optimal clipping bound. Lastly, the authors showed advantage of DP adaptive optimizer over DPSGD from the perspective of hyperparameter tuning.",0.16521739130434782,0.10434782608695652,0.11304347826086956,0.1111111111111111,0.14444444444444443,0.21621621621621623,0.2111111111111111,0.32432432432432434,0.3023255813953488,0.2702702702702703,0.3023255813953488,0.18604651162790697,0.18536585365853658,0.15789473684210525,0.16455696202531644,0.15748031496062992,0.19548872180451127,0.2
467,SP:67e24210503a9aa06f3691b7502b2fbcd3c61bf1,"The paper proposes modifications to the highway connection network (HCN) architecture, which exhibits some inherent resistance to catastrophic forgetting.  These modification introduce fairly simple masking rule for selection of portion of weights to update and normalisation that (presumably) further adjust the internal representation to be more selective in which weights to update for a given task.  Empirical evaluation shows that these modification are highly effective in alleviating catastrophic forgetting.","In the paper, the authors present two modifications: Masked Highway Connection and Layer-Wise Normalisation. In Masked Highway Connection, authors add a binary mask to classical HCN (Highway Connection Networks) and slightly change the training procedure. On the other hand, Layer-Wise Normalisation is a new normalization of activation in the neural network.","The paper investigates the backbone networks that are less prone to catastrophic forgetting. Two modifications to existing backbones are found to be useful: a mask attached to the gate function of the Highway Network, and layer-norm (without tuning parameters). Experiments on top of existing popular learning algorithms designed for continual learning (EWC, ER and HAT) show that these modifications work.","This paper proposed two architectural improvements for networks designed for continual learning. Specifically, binary masks and layer normalization are added to highway connection classifier networks in order to prevent forgetting. The value of each of these elements is made clear from thorough experimentation.",0.15942028985507245,0.18840579710144928,0.13043478260869565,0.22641509433962265,0.18867924528301888,0.14754098360655737,0.20754716981132076,0.21311475409836064,0.20930232558139536,0.19672131147540983,0.23255813953488372,0.20930232558139536,0.18032786885245902,0.2,0.16071428571428573,0.21052631578947367,0.20833333333333334,0.17307692307692307
468,SP:68420fc6177131a82c34b34706e80ad3ba755f14,"This paper targets the problem of systematic generalization in sequence to sequence models, particularly in the scenario of modeling segment alignments as discrete structured latent variables. To explore the searching space of alignments, the authors proposed to use a reorder-first align-later framework, and use hierarchical permutation trees to produce separable permutations, which has the exact marginal inference with dynamic programming enabling end-to-end training. Experiments show that the proposed approach outperforms standard models on both synthetic and real NLP tasks with better systematic generalization.","The order of the input and output (including their associated representations) can matter for the performance and interpretability of a sequence to sequence model. Based on monotonic alignment seq2seq model (e.g. SSNT), the paper proposes to learn a separable permutation over the input representations so that the monotonic alignment can lead to better performance and interpretability. The authors test their methods on toy datasets and small scale semantic parsing and medium sized machine translation tasks.","The paper tackles the problem that conventional sequence-to-sequence models (seq2seq models) fail to generalize systematically; i.e., they are suboptimal for handling the compositionality of language. The paper makes the following contributions: (1) a new seq2seq model for NLP tasks that accounts for latent non-monotonic segment-level alignments and (2) an algorithm for exact marginal inference with separable permutations, which makes end-to-end training possible. The proposed approaches are evaluated both on synthetic data and two real-world NLP tasks: semantic parsing and machine translation. The experiments show that the proposed model mostly outperforms conventional seq2seq models in an IID setting, and even more strongly outperforms existing models in settings where test examples are longer than training examples. ","The authors propose a model for sequence transduction tasks that allows for source tokens to be reordered and then monotonically aligned with the target tokens. The reordering and the alignment are latent, and since all monotonic alignments can be marginalized over using the approach introduced in the SSNT paper (Yu et al., 2016), the authors focus on approximating the marginalization over the latent source permutation. The authors consider in particular separable permutations, whose derivations are induced by a PCFG. The authors propose two approximation approaches, one which uses a dynamic program to obtain an expected permutation matrix (which is used instead of the expected likelihood under the permutation-matrix distribution), and one which samples a hard permutation matrix during learning and uses a ST-Gumbel-Softmax gradient estimator. The authors experiment on a number of synthetic and small-scale real-world tasks and generally find that their approach outperforms baselines, and is especially performant in settings that require generalization to longer inputs than those seen in training.",0.1724137931034483,0.3333333333333333,0.2413793103448276,0.27631578947368424,0.2631578947368421,0.2459016393442623,0.19736842105263158,0.23770491803278687,0.12574850299401197,0.1721311475409836,0.11976047904191617,0.17964071856287425,0.18404907975460125,0.27751196172248804,0.1653543307086614,0.2121212121212121,0.1646090534979424,0.20761245674740483
469,SP:685bfac3b09438a4669b0d581a8eafdf73a81cc5,"The paper proposes a new regularization approach to introduce geometric constraint to the latent space of an autoencoder. A hierarchy for geometry-preserving mappings is formulated to clarify how strong this constraint can be defined. In particular, authors focused on scaled isometries, i.e., maps that preserve angles and distances up to some scale factor. This scale is learnt together with the manifold and the latent space representation during the training of the autoencoder. This approach was proposed also in (Chen et al., 2020) for the so called FMVAE, but here a new coordinate-invariant regularization term is introduced that measure how close the decoder is to being a scaled isometry. Finally, a post-processing flattening procedure is introduced to further improve the geometry properties of the latent space.  Several results have been reported on different applicative scenarios showing a clear improvement of the proposed approach in comparison with other methods of the state of the art .       ","The authors propose a new type of regularized autoencoder (actually variational autoencoder) that is designed to preserve the geometry of the data. They demonstrate that preserving angles and relative distances lead to an improved representation of the data. Specifically, they add a regularization and explore the tradeoff between reconstruction and geometry preservation. They further propose a scheme to flatten the latent representation in a postprocessing fashion.","The paper makes two main contributions. The first is the introduction of a new regularizer term for the VAE loss, ensuring a (scaled) isometry between the learned latent space and the (typically unknown) data space. The second is a post-processing ""flattening"" step to improve the isometry constraint by directly operating on the latent space, while leaving the reconstruction error untouched. The resulting pipeline seems effective, as showcased on a selection of experiments over standard datasets.","This paper studies the aspect of preserving geometry on the learned latent space representations. In particular, the paper looks at a hierarchy of geometry-preserving mapping (isometry, conformal mapping of degree k, area preserving mapping, etc.). Existing popular methods such as SimCLR pays limited attention to preserving geometric relationships. The paper shows that the mapping that preserves angles and relative distances is better than the ones that preserve angles and absolute distances. The main contribution of this paper is to propose a representational learning technique that uses a reconstruction loss and an isometric regularization term. This allows us to learn embeddings that satisfy isometric properties while not suffering from any reconstruction loss or just marginal reconstruction loss. The comparison is done with VAE (Kingma & Welling, 2014) and FMVAE (Chen et al. 2020). Experiments are shown on CelebA with 40 annotations.",0.1464968152866242,0.16560509554140126,0.17834394904458598,0.21212121212121213,0.2727272727272727,0.2236842105263158,0.3484848484848485,0.34210526315789475,0.2,0.18421052631578946,0.12857142857142856,0.12142857142857143,0.2062780269058296,0.22317596566523604,0.18855218855218853,0.1971830985915493,0.17475728155339804,0.1574074074074074
470,SP:686ccd9eca7860a6b30547c7df5a4afe7f20c50e,"The paper presents an efficient federated learning method which leverages the empirical observation that the gradients used in model updates are usually in a low-rank subspace. Based on this observation, a ""look-back gradient multiplier"" (LBGM) method is proposed that only updates a scalar look-back coefficient (LBC) as long as the angular difference between the stored gradient and the latest gradient is within a given threshold. A theoretical analysis of this method is given in the paper and the performance is also verified in various experiments.",The paper studies the gradient subspace and finds it low-rank propery. This observation motivates them to propose a new algorithm that reuses similar past gradients to save communication. They provide a theoretical analysis (with some mistakes) and conduct experiments to validate their method.,"This paper hypothesizes that,   **(H1)** the subspace $S$ spanned by the stochastic gradients while training through SGD is low rank, and   **(H2)** $S$ is well approximated by a subset of the actual stochastic gradients.   It provides empirical evidence to support these claims by calculating,    (i) the number of principal components that almost explain the entire variance of the stochastic gradients over time, and   (ii) the alignment between the principal components of $S$ and the stochastic gradients, as well as the consecutive stochastic gradients, across different models and data sets.   With these observations, the paper aims to reduce the complexity of the communicated bits between the devices in data-parallel distributed training. Specifically, it proceeds by first estimating the principal components of $S$ through consecutive stochastic gradients. These vectors are then stored at the server as well as each worker. At each communication round the device then only needs to share the projection coefficients for these principal vectors, until the set of the principal vectors needs to be updated and broadcasted to all the machines.   This technique is empirically shown to reduce the communicated bits while almost retaining the performance on certain data sets. A convergence rate is also provided for this algorithm under assumptions that highlight the trade-off between the frequency of updating the principal vectors and the convergence rate to a first-order stationary point.   ","This paper proposed two hypothesis: (1) the space spanned by gradients generated during training is low-rank; and (2) the principle components can be approximated by the gradients generated during training. The authors first validated these two hypotheses on several datasets. Then, based on these, they proposed a new algorithm called LBGM to reduce the communication costs in federated learning. In the algorithm, the clients local gradients will be treated as the principle components (if there are K clients, then we have K principle components). Then, during training, each clients only needs send a scalar, which represents the coefficient of the corresponding components, to the server. Hence, the communication costs in FL can be significantly reduced.",0.1590909090909091,0.26136363636363635,0.19318181818181818,0.29545454545454547,0.25,0.15418502202643172,0.3181818181818182,0.1013215859030837,0.14655172413793102,0.05726872246696035,0.09482758620689655,0.3017241379310345,0.2121212121212121,0.146031746031746,0.16666666666666669,0.0959409594095941,0.13749999999999998,0.20408163265306126
471,SP:689051566d89ca368b7ae23b6554d7ea0d352c75,"The paper addresses the equitable and optimal transport (EOT) problem. One approach to this problem involves perturbation via the addition of an entropy regularization. For this formulation, researchers have proposed a projected alternating maximization algorithm (PAM). The authors of this paper provide a convergence analysis of the PAM algorithm. They also introduce a novel rounding procedure to construct the primal solution of the EOT problem. The paper also contributes a variant of PAM that leads to numerical performance improvements. Numerical experiments are provided for a synthetic dataset.","The authors focus on the equitable and optimal transport (EOT) problem. In particular, EOT is a linear programming (LP) problem, which is expensive to solve in practice. The existing work suggests adding an entropy term to the objective and solving the resulting problem in the dual domain via an algorithm named Projected Alternating Maximization algorithm (PAM). However, there are two main issues with PAM: that is its convergence is unknown and it only provides a solution in the dual domain. The authors precisely address these issues by providing a convergence rate for PAM and proposing a novel method to compute a feasible primal solution from the dual solution. Furthermore, they propose a variant of PAM, i.e., PAM with Extrapolation (PAME), which appears to perform better in practice. ","This paper considers the problem of approximate optimal transport, where population probability measures are replaced by discrete PMFs at collections of samples, and there is an additional ""equitable"" constraint that seeks to ``fairly"" distribute the probability mass. This admits a formulation as a linear program and may be solved efficiently through its dual. However, recovering the primal solution is nontrivial, and there exists no rate analysis to-date. This work develops a stochastic dual ascent method to solve an entropy-regularized version of this LP, where entropy regularization is introduced to ensure strong concavity, as well as avoid a notion of over-fitting. The crux of this method is an alternating block coordinate ascent algorithm, which is then combined with an extrapolation/Nesterov acceleration scheme to accelerate convergence. Numerical experiments demonstrate the practical utility of the proposed approach. ","This paper develops new algorithms, convergence rate analysis and additional tools for the equitable and optimal transport problem. Following the same history of OT, where we moved from the OT formulation, to the entropic regularization, to primal-dual analysis to explicit convergence analysis, this manuscript extends the idea of EOT and provides useful tools with provable performance for the practitioner. The contributions of this paper are on the complexity analysis, as well as new algorithms for the EOT problem.",0.3793103448275862,0.22988505747126436,0.22988505747126436,0.171875,0.1328125,0.13768115942028986,0.2578125,0.14492753623188406,0.25316455696202533,0.15942028985507245,0.21518987341772153,0.24050632911392406,0.3069767441860465,0.17777777777777778,0.24096385542168675,0.16541353383458646,0.16425120772946858,0.1751152073732719
472,SP:689aa38b406f989ae2f6bf764d349927babc2856,"The paper studies the problem of how to use visual information to help NLP models. It argues that videos contain rich visual grounding information and could potentially benefit pure-text NLP tasks. It proposes to distill such visual information into a language model during pre-training. Experiments on GLUE benchmark suggest the efficacy of the method.  Overall, I applaud the novelty and problem the paper tries the address. However, the experiments could be strengthened by better control of the baselines and more analysis (see cons). I lean towards acceptance but I would suggest adding the suggested experiments later if possible.","The paper proposes a new method to improve language understanding with the help of the vision modality. In the proposed method VidLanKD, a teacher model is firstly pre-trained with video-language contrastive loss on Howto100M data. Then the LM part of the teacher model is used to distill knowledge to a student LM model.","The paper proposes to improve language understanding using a video-language knowledge distillation method. Compared to the previous state of the art approach, “Vokenization”, they use a large multi-modal video dataset for video-language pre-training to accommodate larger vocabulary needed for better language understanding. For their knowledge distillation based approach, the teacher is the pre-trained model on the multi-modal dataset whose weights are distilled to the student language model trained only on a text dataset. This also avoids the approximation error in the vokenization approach. They perform extensive experiments and improve performance on several downstream language understanding benchmarks (GLUE, SQuAD and SWAG) and also show the commonsense knowledge and reasoning learned from videos.  ","The paper proposed to distill the large-scale pre-trained video-language multi-modal knowledge into the language modeling. Different knowledge distillation objectives are applied. In the downstream task, the distilled language model shows performance improvement over the conventional language model. ",0.12,0.18,0.09,0.36363636363636365,0.21818181818181817,0.1452991452991453,0.21818181818181817,0.15384615384615385,0.21951219512195122,0.17094017094017094,0.2926829268292683,0.4146341463414634,0.15483870967741936,0.16589861751152074,0.1276595744680851,0.23255813953488375,0.25,0.21518987341772156
473,SP:68ad28b221ea91f8c52730e803de316685e31788,"The paper studies (generalized) linear contextual bandits in the context of local differential privacy (DP). At every time step a single user’s sensitive data point arrives, which serves as a context, and a reward is sampled as a function of this context and an unknown (generalized) linear reward-generating function. The difficulty with minimizing regret with local DP is that one has to learn the reward-generating function without compromising users’ privacy. The analysis is split by cases: in the setting of linear rewards, an OLS-based solution is proposed, and in the setting of generalized linear rewards, an SGD-based solution is proposed. In addition, two different bandit models are considered depending on whether the rewards from different arms are related.","Authors design LDP algorithms for stochastic generalized linear bandits to achieve the same regret bound as in non-privacy settings. The main idea is to develop a stochastic gradient-based estimator and update mechanism to ensure LDP. They also develop an estimator and update mechanism based on Ordinary Least Square (OLS) for linear bandits. Finally, they conduct experiments with both simulation and real-world datasets to demonstrate the consistently superb performance of algorithms to ensure strong privacy protection. ","This paper proposes to design and analyze algorithms for generalized linear bandits with local differential privacy (LDP). The main idea is to design a stochastic gradient based estimator to ensure LDP. It is show, by a comparison to the lower bound that the worst-case results contained herein are close to optimal. ",This paper studies the local differential privacy (LDP) problem for stochastic generalized linear bandits. They develop the Stochastic Gradient Descent and Ordinary Least Square estimators and update mechanism for generalized linear bandits and linear bandits with privacy guarantee. The regret bound is proved to be the same as in non-privacy settings. ,0.11382113821138211,0.12195121951219512,0.13008130081300814,0.2948717948717949,0.2564102564102564,0.25,0.1794871794871795,0.28846153846153844,0.3076923076923077,0.4423076923076923,0.38461538461538464,0.25,0.13930348258706468,0.1714285714285714,0.18285714285714286,0.3538461538461538,0.3076923076923077,0.25
474,SP:68fb3ab855c13b60695faa6e504b913b8114b595,"The paper introduces algorithms for sampling constrained distributions. Assuming the existence of a mirror map that maps from the primal space to the dual space, the authors introduce an algorithm MSVGD that can perform gradient descent in the dual space, and map back to the primal space. They also propose SVMD, which uses a different algorithm to perform gradient descent in the primal space itself. The authors experimentally validate their algorithms on simulated data and one non-simulated dataset (Fig 4b).  ",The aim of this work is to extend the existing particle evolution method Stein variational gradient descent or SVGD to constrained domains and non-Euclidean geometry. Three algorithms are proposed for this purpose based on mirrored Stein operators. The first is Stein variational mirror descent that runs SVGD in dual space s.t. the updated particles stay in constrained domains. The second one is Stein variational mirror descent defined with some adaptive kernels and it is also applicable to constrained domain problems. The third one is Stein variational natural gradient which is intended for unconstrained problems with informative metric tensors. Empirical and convergence analyses are further provided for all three proposed algorithms.,"the paper introduces a new family of deterministic particle samplers which apply mirror transformations to the Stein's variational gradient descent method. This approach can be used to generalize SVGD to both constrained and non-euclidean settings. The authors introduce two algorithms of this type that differ by the choice of the kernel function. One of these, SVMD is shown to reduce to regular mirror descent when a single particle is used. ",The paper introduces new methods to run SVGD in constrained domains and non-euclidean geometries. The authors develop the theory to combine mirror descent dynamics with the Stein's method via what they call mirrored Stein operators. They also show experimental results on two concrete problems and prove convergence guarantees.,0.19753086419753085,0.19753086419753085,0.16049382716049382,0.21428571428571427,0.16071428571428573,0.2222222222222222,0.14285714285714285,0.2222222222222222,0.26,0.3333333333333333,0.36,0.32,0.16580310880829016,0.20915032679738563,0.1984732824427481,0.2608695652173913,0.22222222222222224,0.26229508196721313
475,SP:69700c0c49a3453077af90763e498cd27b8f216b,"This paper gives a novel characterization of well-posedness for implicit networks based on the one-sided Lipschitz constant (osL). This follows in a line of work of other such characterizations [1,2,3,4], but differs in that well-posedness is guaranteed by a constraint on the osL of the weights matrix, which can be wrt arbitrary norms. The authors focus on the $\ell_\infty$ norm, for which they show accelerated convergence of dampened fixed-point iterations.  They also drive a bound for the Lipschitz constant of the network wrt the $\ell_\infty$ norm on the input space, which is more natural for achieving robustness than the $\ell_2$-based Lipschitz bounds of [3] and [4]. They train networks while enforcing well-posedness and using the Lipschitz bound as a regularizer, and demonstrate the empirical robustness of the models.","This paper studies the implicit nn with non-Euclidean norms. In particular, the authors give the sufficient condition for the well-posedness of the fixed point problem under any norms, and provide the corresponding contraction factor. Consequently, the author derives the Lipschiz constant of their implicit model induced by the infinity norm and use the model for robustness tasks.","The paper proposes a new well-posedness condition for DEQs that is less conservative than the l_\infty norm proposed in [El Ghaoui et al., 2019]. A Lipschitz regularization is also developed to improve the robustness to some classes of adversarial perturbations. Several experiments show the effectiveness of the proposed method. ___________________________________________________ The author responses address most of my concerns. So I raised the score and tended to acceptance. ","This paper presents a thorough analysis of constructing provably convergent equilibrium networks in the non-Euclidean space. Instead of directly requiring Lipschitzness as in prior works (which mostly dealt with $L_2$), the authors show that we can further relax the constraint by considering one-sided Lipschitzness and exploit the contraction theory via semi inner products. The experimental results of the paper seem to demonstrate the validity of the theoretical results and the paper provides empirical evidence on the potential application of the theories in improving implicit networks' adversarial robustness.",0.1357142857142857,0.14285714285714285,0.17142857142857143,0.2033898305084746,0.3220338983050847,0.19117647058823528,0.3220338983050847,0.29411764705882354,0.26666666666666666,0.17647058823529413,0.2111111111111111,0.14444444444444443,0.1909547738693467,0.1923076923076923,0.20869565217391303,0.1889763779527559,0.25503355704697983,0.16455696202531644
476,SP:69be8e601eac34f34de70ce3b964eb426e061b1d,"This paper aims to learn a domain invariant representation of observations in the goal-conditioned block MDP (GBMDP). To this end, the authors propose to minimize the deviation between embedded visitation trajectories with a fixed action sequence across different environments. The intention is to learn the latent mapping between states and observations. The authors also characterize the optimality gap between an arbitrary policy and the optimal invariant policy. Their analysis shows that the key to optimal and invariant policy is (i) the optimality of the policy at each environment, (ii) the deviation of the visitation measures across different environments, and (iii) the difference between the test environment and training environments.  The authors conduct extensive experiments on the multi-world environment and show that the proposed PA-SF method outperforms several baselines.","In the paper under review, state abstraction in Goal-conditioned RL (Block MDPs) is discussed.  Theoretical analyses to motivate the use of perfect alignment encoders are provided.  Then, a practical method (including the trajectory sampling method and encoder loss) for building the perfect alignment encoder is proposed.  Experimental results demonstrate that, in robotic arm manipulation benchmarks, the proposed method has a better generalization ability than previous methods (e.g., bi-simulation metric-based method).  ","Summary -------  This paper proposes a pair of regularizers to the Skew-Fit algorithm to ensure that the encoder maps two states to the same encoding if and only if the states are the same (whether the states are in different environments or not). The problem setting is goal-conditioned RL but the paper introduces the block MDP structure to better analyze generalization. The perfect alignment property is motivated from a derived inequality measuring the performance difference of two policies with ideas based on the $\mathcal{H}\Delta \mathcal{H}$ divergence. The proposed algorithm is found to be most performant on multiworld amongst the baselines: SkewFit, MISA and DBC. The performance benefit is attributed to the enforcement of the perfect alignment property.  . ","**High-Level Summary**  This paper studies a new representation learning approach for goal-conditioned RL for the particular case of Block MDPs (Du et al., 2019). The goal is to learn a goal-conditioned policy that generalizes across a set of Block MDPs given access to a train set of Block MDPs. It is assumed that latent dynamics are nearly deterministic which is critical. Formally, the proposed representation learning approach samples sequence of actions and executes them in different sampled environments. An encoder is trained to map observations at the same time step along these two trajectories to a similar value (line 173) while separating two randomly chosen observations (line 186). It is argued that learning this representation yields invariant representation which helps optimize the generalization error. Experimental evidence supports the claim.",0.12213740458015267,0.20610687022900764,0.16793893129770993,0.21621621621621623,0.24324324324324326,0.18181818181818182,0.21621621621621623,0.2231404958677686,0.16666666666666666,0.1322314049586777,0.13636363636363635,0.16666666666666666,0.15609756097560976,0.21428571428571427,0.16730038022813687,0.1641025641025641,0.17475728155339804,0.17391304347826086
477,SP:6a375ed281460237a0ee94b09301d282bdf18af1,"This paper proposes a new loss that consists of three parts, namely MSE+focal loss+regularization loss. The new loss is supposed to benefit DP deep learning in comparison to the cross-entropy loss. Accuracy improvement has been observed on MNSIT, FashionMNIST and CIFAR10. ","This paper attempts to improve the accuracy of the DP-SGD training from the perspective of loss function design. The authors propose a loss composed of SSE loss, focal loss, and L2 regularization penalty. Experiments are conducted to demonstrate the effectiveness of the  proposed loss.  ","The paper proposes a tailored loss for DP-SGD, which includes summed squared error, the focal loss, and a regularization penalty. The summed squared error is for fast convergence at initial stage. The focal loss is used for identify hard samples. The regularization penalty is used for reducing the gradient/weight norm and avoid explosion. For each component of the tailored loss, the paper has empirical/theoretical evidence to argue the necessity. It is a good try to improve the performance of DP-SGD from amending the loss.","The paper proposes a new loss function to improve the performance of neural network models trained by DP-SGD. The new loss function is a weighted average of the sum of squared error, the focal loss, and a penalty on the squared norm of the pre-activation output of different layers. The new loss achieves state-of-the-art accuracy on the CIFAR-10, FashionMNIST, and MNIST datasets. It is also shown that the new loss can reduce the bias of gradient clipping and encourage learning on hard examples.",0.25,0.3409090909090909,0.36363636363636365,0.3333333333333333,0.4444444444444444,0.3181818181818182,0.24444444444444444,0.17045454545454544,0.1797752808988764,0.17045454545454544,0.2247191011235955,0.3146067415730337,0.24719101123595505,0.22727272727272727,0.2406015037593985,0.22556390977443605,0.29850746268656714,0.3163841807909604
478,SP:6a58637bdb81afad25af71ba4f891db7403db316,"Substituting two general functions for linear risk predictors in the Extended Hazard (EH) model, the DeepEH approximates these two functions via feed-forward neural networks (FNN). The DeepEH may fall into the category of FNN-assisted survival analysis which utilizes FNNs to estimate the hazard/survival function. The shining point of this paper is the established asymptotic properties of estimators from DeepEH.    ","The authors propose to extend the ""Extended Hazards Model"" (EH) with deep learning approaches to model right censored time to event data. EH model is a generalization of both the Accelerated Failure Time and the Cox Model Proportional Hazards Model and thus is more flexible than either of the two approaches. They demonstrate empirically that the extended hazards model improves the prediction of survival across multiple datasets in terms of both discriminative performance (C-Index) and Integrated Brier Score on multiple competitive baselines.   ","This work proposes DeepEH, a survival analysis framework that formulates the hazard function as \lambda(t|X) = \lambda_0(t.exp(h_1(x))).exp(h_2(x)). This formulation has the Cox PH and AFT as special cases. The authors estimate h_1 and h_2 by neural networks and provide theoretical support for the consistency and convergence rate of the resulting survival function estimator.","This paper introduces the Deep Extended Hazard Model as a new approach to learning a survival model, based on learning 2 neural nets, to estimate the 2 terms of the extended hazard function.  It explains how this model extends the EH model, which extends both the Cox proportional hazard model and Accelerated Failure Time.  It then provides first a theoretical analysis, following by an empirical exploration of these models ",0.1774193548387097,0.16129032258064516,0.20967741935483872,0.10843373493975904,0.18072289156626506,0.18181818181818182,0.13253012048192772,0.15151515151515152,0.18840579710144928,0.13636363636363635,0.21739130434782608,0.17391304347826086,0.15172413793103448,0.15625,0.19847328244274812,0.12080536912751676,0.19736842105263158,0.17777777777777776
479,SP:6a6d67a30dd57411ab5bdd1d5f0c662c038e76f4,"The paper discussed a correlation of smoothness on **classification loss** and **generalization ability on target domain** and further develop a method to enhance such smoothness to achieve better performance on DA tasks. To do so, it adopted the losses based on Sharpness Aware Minimization, a minmax game of finding a smoother neighborhood of $\theta$ , by computing first order deviation. Empirical studies are implemented to verify the theory and show the soundness of the proposed method.","This work studies the loss landscape of domain adversarial neural networks for domain adaptation. The authors claim that similarly to the iid setting, smooth minima might yield better generalization on the unsupervised domain adaptation framework via domain adversarial training but in case smoothness is enforced only with respect to the task loss. Experiments on the OfficeHome dataset indicated this hypothesis might be true. The authors then proposed to apply a previously proposed approach to enforce smoothness (Sharpness Aware Minimization), so that only smoothness with respect to the task loss would be enforced. Moreover, a theorem showing that the non-smooth domain discrepancy estimation is better than the smoother counterpart was introduced, along with a generalization bound for the risk on the target domain in terms of the smooth risk on the source domain. Finally, the authors showed that the proposed approach, SDAT, improved the performance on target domains of three domain adaptation datasets in comparison to the considered baselines. Further experiments showed that SDAT performed better than other smoothing techniques, as well as yields improved robustness to label noise in comparison to not enforcing smooth minima. ",The paper analyzes the role of smoothness in domain adversarial training (DAT). The main insight of the paper is that smoother minima of the classification loss improve generalization in the target domain. This explains why SGD is usually preferred over Adam when optimizing the training objective of DAT. To further improve the generalization performance by leveraging this phenomenon the author(s) introduce(s) smooth domain adversarial training (SDAT) for classification and object detection. SDAT applies sharpness aware minimization (SAM) to find smoother minimia of the classification loss used during DAT.,"This paper introduces a smoothness penalty for domain adversarial training. The penalty encourages smoothness on the parameter space of a discriminative model. This smoothness penalty is motivated by results in Sharpness Aware Minimization. Results are included on the popular DA datasets such as Office-home and VisDA, where superior results are shown. Ablation experiments show how different smoothness penalties change the observed improvements. ",0.25333333333333335,0.24,0.2,0.11827956989247312,0.0913978494623656,0.16666666666666666,0.10215053763440861,0.2,0.23809523809523808,0.24444444444444444,0.2698412698412698,0.23809523809523808,0.14559386973180077,0.2181818181818182,0.21739130434782608,0.15942028985507245,0.13654618473895583,0.19607843137254902
480,SP:6b73005991ca8ca51b7315c823210e8fc1a409a3,This paper presents an extension of Bayesian optimization with Gaussian processes and expected improvement for target functions that are the results of composite functions from a DAG model. The paper introduces an extension of the expected improvement for the specific case which does not allow for a closed form solution. Instead the authors approximated the acquisition function with a sample approximation and optimized it using sample average approximation. The paper also includes some theoretical results in terms of consistency (although not convergence) and extensive results.,"This work investigates the problem of Bayesian optimization of the output of a network of functions. The network is organized as a directed acyclic graph, where each function takes as input the output of its parent nodes as well as (part of ) the input. Contrary to the standard setting that only observes the final output, this work assumes that intermediate nodes can also produce an output (which can be stochastic or deterministic). The authors model the nodes of the network using Gaussian processes, and employ an acquisition function that corresponds to the expected improvement with respect to the implied posterior on the final output under a proper statistical model. The authors optimize the acquisition function using sample average approximation. The authors show that their approach satisfies asymptotic convergence. Various experiments on synthetic and real problems demonstrate that the improved model and acquisition function result in visibly improved solution quality, at the expense of the runtime cost.","This paper extends methods for Gaussian process ""Bayesian Optimization"" (BO)  of vectors of hyperparameters to structural networks of function, to include hyperparameters from functions of intermediate stages. Unlike a conventional approach, this method considers the output of intermediate stages.  The hypothesis addressed by the paper that including additional information from intermediate stages is a reasonable one, however it raises a challenge since despite the intermediate functions each being modelled as a GP, the final output g is not necessarily Gaussian.  To accommodate this, they apply sample average approximation to sample from the posterior on the final output.  The authors apply this method in the context of several application areas, having run experiments on 8 problems with results showing substantially faster convergence than competing methods.    This paper offers a convincing empirical study together with theoretical convergence results. This is an interesting paper with useful learning to the general community.","This paper focuses on Bayesian optimization of function networks, where the objective function has a known structure that is a composition of individual (node) functions that take the output of the previous function as input (along with potentially other parameters). Observations at each node function are obtained at each objective function evaluation. Such objective functions arise in a variety of applications including materials design and physical processes. Exploiting knowledge of the network structure can result in a lower effective input dimensionality of the model of each node function, depending on the network structure. By modeling each node function as distinct GP, the authors demonstrate improved sample efficiency using EI compared to optimizing the global objective function with a single GP or using compositional objectives.",0.32941176470588235,0.25882352941176473,0.24705882352941178,0.21153846153846154,0.1794871794871795,0.16216216216216217,0.1794871794871795,0.14864864864864866,0.1693548387096774,0.22297297297297297,0.22580645161290322,0.1935483870967742,0.23236514522821577,0.18884120171673824,0.20095693779904308,0.21710526315789475,0.2,0.1764705882352941
481,SP:6b7e3161c23e0c9303f496a4a5c3df1521e3e9a6,"This paper begins with a clear exposition of continual learning and an analysis from a statistical learning theory perspective.  It then introduces the Model Zoo: A sequence of small models that are each trained on a subset of the available tasks at that point (with a separate linear classifier for each task). The tasks selected for training are those that have high loss under the models trained so far (similar to boosting).  The following section contains an exhaustive set of experiments that compares the Model Zoo with a variety of baselines (with and without replay, with and without limited training, a single model with multiple heads, etc.). Surprisingly, the ""Isolated"" model (i.e., a separate model is trained on each task) does better than most baselines that train on multiple tasks. The Model Zoo outperforms all other baselines and even outperforms the multi-head baseline on ImageNet.  The paper ends with a critical discussion on how the problem of continual learning should be approached.","This paper proposes yet another definition of continual learning (CL) and uses it to motivate a divide-and-rule approach. The algorithm exploits all assumptions of the adopted CL setting, including the relaxation of data access rules, in order to perform better compared to algorithms designed for more restrictive settings. The paper concludes that previous CL settings were holding back progress. Empirical evaluations and ablations are given using the new CL setting proposed in this paper, with the help of datasets favorable to this approach, namely splits of existing i.i.d. datasets.","This paper provides two main contributions on the topic of continual learning: (i) the first is to propose a definition of relatedness between tasks in the framework of statistical learning theory, and use it to perform a theoretical analysis of when it can be fruitful to train multiple tasks with the same model and when it can be detrimental to do so; (ii) the second is to propose an architecture for continual learning called ModelZoo, which maintains an ensemble of models that grows as each new task is introduced. Each time a new task arrives, a feature generator h and k task-specific classifiers g_k are initialised, and training proceeds on the combination of h and each g_k on the current task and data from k-1 previously seen tasks. At any point, the performance on a previously seen task can be evaluated by averaging over all small models previously trained on that task. ModelZoo takes inspiration from AdaBoost in how it selecting which previous tasks to train concurrently with the current one, which is by preferentially sampling tasks that have a bad performance under the current ensemble. An empirical analysis is performed by training on various standard continual learning image classification tasks, which demonstrate that Model Zoo is on par with and sometimes outperforms the single model multi-task approach (both , which is often used as an “upper bound” on continual learning methods. The results also demonstrate that a simple baseline of an ensemble of small isolated learners outperforms a selection of existing continual learning methods (with results reported reported from other papers). ","This paper argues and demonstrates that in a task-incremental continual learning setting, in many cases it is beneficial to split up the capacity of a learning algorithm and to learn separate models for different tasks. Building on this insight, and taking inspiration from the boosting literature, the paper then proposes the method Model Zoo, in which a new model is learned for each new task and whereby each new model is trained on multiple tasks. During evaluation, the predictions of all models trained on the task under consideration are averaged. The paper reports surprisingly large improvements over existing continual learning methods on several different task-incremental learning methods, both with and without utilizing replay.",0.12195121951219512,0.2804878048780488,0.20121951219512196,0.27956989247311825,0.1935483870967742,0.14661654135338345,0.21505376344086022,0.17293233082706766,0.28695652173913044,0.09774436090225563,0.1565217391304348,0.3391304347826087,0.15564202334630348,0.21395348837209302,0.23655913978494625,0.1448467966573816,0.17307692307692307,0.20472440944881887
482,SP:6c277798b2f884daaa8b2d4ac74a30c178454d63,"The ultimate goal of the paper is to train an unsupervised generative model, in a semi-supervised manner, as a synthetic training example generator for training segmentation models. Prior works (eg. DatasetGAN) show that one can label a few synthetic images from a pre-trained generative model, and train a segmentation model (called annotation model) over the intermediate representation produced by the generative model. Then, every time a sample is taken from the generative model, the corresponding labels can be produced by applying the annotation model to the corresponding intermediate representations. This provides a valuable way to produce synthetic labelled training examples. However, (some of) these existing approaches require annotating (a limited set of) generated samples for training the annotation model. This is problematic as one needs to re-label such images every time the generative model changes.    This paper aims to address this problem by learning the annotation model over annotated real images instead of annotated synthetic images. To this end, the paper proposes a meta-learning approach. The main idea is to learn the annotation model such that when a (student) segmentation model is trained over the generative model outputs and their labels given by the annotation model, the resulting student model’s loss shall be low on real annotated images. This meta-learning idea is implemented as an approximation by using the Gradient Matching Loss. ","This work deals with the problem of training a part segmentation network by automatically synthesizing pairs of images and annotations. The authors propose a training method of an annotator given the well-pretrained generator model. The formulation is well-motivated and converged to the gradient matching loss.   This work extends [Zhao et al., ICLR 2021] by combining the idea of DatasetGAN and RepurposeGAN in an interesting way. The proposed approach can favor the unlimited number of annotated data with only limited supervised real data + the pre-trained GAN.  The proposed method shows noticeable improvement over the other baselines and the competing methods in scarcely labeled data regimes (especially semi-supervised learning). ","This paper proposes a method to solve the problem that generally requires large-scale labeled datasets to train deep learning models. The proposed method is designed to solve the nested-loop optimization problem (with an annotator that generates a label and a student network that predicts a label) based on a pre-trained GAN that generates images. In particular, the authors solve this problem through a gradient matching approach, and they claim it is much more efficient than the existing end-to-end gradient-based meta-learning approach. This method was applied to the semi-supervised part segmentation task, and its effectiveness was verified through various experiments. ","In this paper, the authors mainly propose a gradient-matching-based method for part segmentation to reduce the annotation cost. Based on the DatasetGAN, the proposed model also used the Style GAN family to generate high-quality images and remove the human annotations on a handful of synthesized images. Compared with semi-supervised learning methods, their proposed method achieves higher performance on three datasets. ",0.12719298245614036,0.11403508771929824,0.07456140350877193,0.16216216216216217,0.17117117117117117,0.14018691588785046,0.26126126126126126,0.24299065420560748,0.265625,0.16822429906542055,0.296875,0.234375,0.17109144542772864,0.15522388059701492,0.11643835616438354,0.16513761467889906,0.21714285714285714,0.17543859649122806
483,SP:6c44a732607e5d63ce6d749d985ea9ba86a2543b,"This paper proposes a baseline RL approach for pixel-based control with distraction in background, which adds reward and latent state transition predictions to actor-critic agent (or DQN agent for Atari experiments). The authors further consider many components proposed in previous contributions on pixel-based control (such as contrastive learning, data augmentation, image reconstruction, etc) and conduct ablation studies on top of the proposed baseline approach. They also directly analyze existing methods for pixel-based control with distraction in background. ","This work investigates representations for pixel-based reinforcement learning. The authors empirically show how different approaches perform in a variety of domains categorized by their reward structure, presence of distractors and use of data-augmentation techniques. The work further argues in favor of an ""exceedingly simple"" baseline method consisting of SAC-AE modified with transition and reward prediction modules. ","This paper presents an approach for learning representations from pixel data that are amenable for control tasks. The proposed approach is a simple baseline that does not require data augmentation, world models, contrastive losses etc. but only contains two simple sub-tasks that are supposed to contribute heavily towards an effective representation: reward prediction and state transition prediction. Along with evaluating this proposed baseline, the paper also compares it to several prior works on representation learning: i.e., several approaches such as data augmentation, distance metric losses, contrastive losses, relevant reconstruction etc. It is shown that the proposed simple baseline either outperforms several of these methods or at least is very close in performance. Finally, the paper presents an interesting discussion about how evaluating an algorithm is not just about the dataset and the chosen benchmark task, but requires a more nuanced point of view of several factors such as reward sparsity, action continuity/discreteness, relevance and irrelevance of features to the task, and so on. The findings of the paper are not just about the effectiveness of the proposed method, but a more overarching view of which types of representation learning methods work in what conditions. ","The authors claim that the usefulness of the learning components (e.g., contrastive objective) proposed in previous representation learning for RL, is highly dependent on the specifics of the task category, and thus show that they don't provide robustness across different and diverse task categories. They also claim that the reward and transition prediction are the most vital (and minimal) components providing robustness. Based on this observation, they propose to evaluate the representation learning methods by considering the specific properties of task categories instead of evaluating on a whole benchmark level such as DMC and Atari. ",0.16049382716049382,0.30864197530864196,0.16049382716049382,0.2711864406779661,0.1694915254237288,0.1065989847715736,0.22033898305084745,0.12690355329949238,0.13402061855670103,0.08121827411167512,0.10309278350515463,0.21649484536082475,0.1857142857142857,0.17985611510791366,0.14606741573033705,0.125,0.12820512820512822,0.14285714285714288
484,SP:6c6f858589727012965c1cfe0bb0f7cb25f4f132,"This work proposes a new type of hard-label attacking method. Hard-label attacking methods only allow access of the top-1 predicted labels. This method is based on geometric properties, i.e., tangent points to reduce the l2 distortion after attacks. The method is shown to be superior compared with the existing hard-label attacking methods. The authors also studied the performance under four different defense frameworks, all of which prove the superiority of the proposed method.         ","In this paper, the authors proposed a new geometric-based method for query-efficient hard-label black-box attacks. The proposed method relies on the observation that the minimum l2 distortion can be obtained by searching a boundary point along a tangent line of a virtual hemisphere. The authors offered a closed-form solution for computing the optimal tangent point and provided a formal proof of its correctness. Lastly, the authors evaluated the proposed approach through extensive experiments.","The paper proposes Tangent Attack, a decision based adversarial attack which can be seen as an improvement of HopSkipJumpAttack [6]: it progressively reduces the norm of the adversarial perturbations according to a method inspired by the geometry of the decision boundary in the input space. This allows to achieve adversarial points with lower distortion than HSJA with the same amount of queries of the classifiers, i.e. better query efficiency.","This paper proposes a new method for performing hard-label black-box attacks, similar to the HopSkipJump Attack but modified with the observation that the gradient direction is not always the best direction to move in when trying to minimize mean attack distortion. Instead, under some assumptions about the decision boundary, the paper derives an update direction based on an optimal tangent point on the semi-circle around the current attack iterate. The paper then implements the corresponding algorithm and shows that it indeed lowers mean distortion on the CIFAR-10 and ImageNet datasets at a variety of query budgets. ",0.2564102564102564,0.1794871794871795,0.23076923076923078,0.15384615384615385,0.32051282051282054,0.21428571428571427,0.2564102564102564,0.2,0.18,0.17142857142857143,0.25,0.15,0.2564102564102564,0.18918918918918923,0.20224719101123595,0.16216216216216214,0.2808988764044944,0.1764705882352941
485,SP:6c9a2b390380fef2548fb8f88febb7520e4bd112,"This paper presents an algorithm named UOTA for self-supervised representation learning. The method is based on the observation that some low-quality negative samples are generated during the learning procedure and these bad samples can confuse the deep networks. So, a Gaussian distribution is built to estimate the weight of each sample and added to the loss term. Experiments show consistent accuracy gain.","The paper suggests that the different augmentations used as a positive pair during self-supervised training often lack the same semantic structure as the original image, and thus should be treated as an OOD sample. Using such OOD samples, they believe can hurt the downstream task performance. The aim should be to choose augmentations that offer the most data variance but are subject to the least semantic deviation. For this, they propose an importance sampling approach to weigh the loss for different augmentations differently. Empirically they obtain improvement for different downstream tasks across different datasets.","This contribution is based on two main claims: i- using data augmentation to create different views from an image might result in out-of-distribution instances; and ii- employing such out-of-distribution views as positive instances in self-supervised approaches that rely on comparing representations from positive and negative views of an example, might be harmful for downstream performance. To address these issues, the authors proposed to apply an out-of-distribution detection approach to estimate how likely it is for a positive view to be out-of-distribution and use this information to weight the contribution of each positive pair when computing the learning criterion. The proposed strategy, named Unsupervised OuTlier Arbitration (UOTA), is then incorporated to three previously proposed self-supervised learning approaches and empirically validated on ImageNet and MS-COCO. Comparisons with other out-of-distribution detection techniques were also provided. Overall, UOTA showed to marginally improve the downstream performance of self-supervised pretraining both with respect to the compared detection approaches as well as self-supervised models without any mechanism to compensate for potential out-of-distribution positive views. ","The paper is generally concerned with self-supervised contrastive learning, such as SimCLR. The authors posit that existing ways to generate (contrastive) views introduce out-of-distribution examples that actually harm performance. To remedy this, the authors propose a latent variable model they term UOTA that, based on importance sampling, suggest good view candidates. They show UOTA's theoretical properties and conduct extensive experiments, showing gains using UOTA against a large number of recent baselines. ",0.21875,0.265625,0.1875,0.18947368421052632,0.14736842105263157,0.09239130434782608,0.14736842105263157,0.09239130434782608,0.16,0.09782608695652174,0.18666666666666668,0.22666666666666666,0.17610062893081757,0.1370967741935484,0.1726618705035971,0.12903225806451613,0.16470588235294117,0.13127413127413126
486,SP:6cbfe15c455f9242e65bf7c54bf98b97376247c5,"This paper is about improving the interpretability of agents trained on reinforcement learning problems where the input observation is given as a graph. Specifically, the authors propose to split the decision-making process in two: 1) an action pruner, and 2) an action selector. Experiments were conducted on a suite of TextWorld environments that have their state exposed as a knowledge graph. Empirically, it was shown that the proposed two-step approach is better at generalizing on unseen environments and is more robust to noisy perturbations on the input graphs.","This work targets to the problem of limited interpretability in RL. To solve the problem, the authors propose a two-step hybrid model, including action pruner and action selector. The proposed method is evaluated on the text games, with results showing its ability of interpretation, generalization, and robustness.","This paper proposes a pipeline to determine (primitive) actions to be performed in states described by knowledge graphs. The pipeline is divided into two steps. The first step, Action Pruner, maps states into abstract actions (i.e., clusters of primitive actions) and is performed by a trained GNN with data from a teacher. The second step, Action Selector, is a rule-based system that defines the primitive action to be performed among those included in the abstract action given by the previous step. The Action Selector defines the primitive action based on the GNN Abstract Supporting Edge set (ASE), also defined from the training data provided by the teacher. The proposal presented good results in games with text-based environments and also allows the generated decisions to be human-friendly explanations.","This paper introduces a two-step method for RL problems with graph-based inputs. This method trains a teacher policy which it then uses to construct a policy ""pruner"" which predicts the kind of action needed. It then uses a ideas from retrieval to select the edges needed to complete the action.",0.2,0.2222222222222222,0.12222222222222222,0.3541666666666667,0.16666666666666666,0.11450381679389313,0.375,0.15267175572519084,0.21153846153846154,0.1297709923664122,0.15384615384615385,0.28846153846153844,0.26086956521739135,0.18099547511312214,0.15492957746478872,0.18994413407821228,0.16,0.16393442622950818
487,SP:6d12a8cee30de2125dbbf5b390b2f3fcf57f93d4,"This paper presents a model-based algorithm for offline reinforcement learning (RL) that attempts to address the covariate shift issue by re-weighting the model losses for different datapoints. Based on an upper bound of the policy evaluation error, a joint loss function is defined over both the policy and the model parameters, and the two are optimized in an alternating fashion. Several simplifications and tricks are described to make the algorithm computationally tractable.  On a simple pendulum environment, the proposed algorithm predicts the policy value much more accurately than a model based on empirical risk minimization (ERM). It also obtained good results on the MuJoCo tasks from the D4RL benchmark.","This work studies how to estimate a learned model in offline model-based reinforcement learning (MBRL). The authors propose to use ""the artificial weight,"" which is the ratio of offline data and simulated future data, to improve the model estimation. They further propose a new objective function for offline MBRL and an EM-style algorithm to optimize it. The authors evaluate their algorithm in several different tasks.","The paper addresses the problem of covariate shift in offline memory-based reinforcement learning by proposing a novel method for more accurate evaluation of the expected return of a given policy by means of simulation of a learned model of the real system that might not be entirely correct. Two algorithm that uses a weighting factor, called the artificial weight, are proposed for tuning the learned system's model to predict more accurately the parts of the state space visited by the policy being evaluated, and thus improve the estimate of the expected return of this policy. This improved estimate can further be used for policy improvement, as customary for RL algorithms. An empirical evaluation in simulated environments demonstrates the validity of the proposed approach, both for policy evaluation and for policy improvement.   Update: I appreciate the clarification about the evaluation task the authors are solving (pendulum swing-up, not stabilization). I also noticed that the other reviewers also found the advantages of the proposed algorithm not all that overwhelming. This, combined with the generally unclear presentation, makes the paper borderline. I am revising my rating to 6, just above borderline - it would still be good to accept this paper, if possible, and depending on the ratings of other papers.","The paper presents a model-based policy optimization algorithm for offline reinforcement learning. To account for the mismatch between the available dataset and the experience used during training, the proposed method uses estimation of the density ratio of the state distributions induced by the estimated and the real model. This importance weight, called “artificial”, is in contrast with the one, referred to as “natural”, employed by other off-policy methods involving the densities for two different policies under a same model. The resulting model learning objective is inserted into an expectation-maximization algorithm, which is then evaluated in a standard offline RL benchmark.",0.17117117117117117,0.25225225225225223,0.24324324324324326,0.31343283582089554,0.26865671641791045,0.11904761904761904,0.2835820895522388,0.13333333333333333,0.2621359223300971,0.1,0.17475728155339806,0.24271844660194175,0.21348314606741572,0.17445482866043613,0.2523364485981308,0.1516245487364621,0.21176470588235294,0.1597444089456869
488,SP:6d403a83f399d328911b38f9f0756bcc0fd5b7ec,"This paper proposes a new approach to perform table-pretraining, this new approach is hugely different from the previous approaches like TaPas, TaBERT, etc in two ways: 1) the model architecture does not need to be designed to fit the table structure, no new objective functions have been invented, a simple BART model can be used to keep pre-training with its original encoder-decoder loss, 2) the algorithm does not require large amount of mined table-text parallel corpus, which avoids huge amount of data preprocessing and heuristics to do noise canceling.   The main technical part of this paper is ""small amount of tables"" while ""large amount of synthesized SQL"". The basic idea mimics the previous papers by (Yu et al. 2021) to utilize the clean dataset and templates to generate arbitrary amount of SQL queries and simulate various environments. The BART model is essentially used as a universal approximator to estimate the logical operations for SQL queries. The massively pre-trained TAPEX model indicates its superb SQL execution capability as indicated in Table 6, outperforming the BART without such SQL-approximation pre-training by a huge margin of 20%+. This is quite impressive, and demonstrates the strong approximation capability of BART model even without specially-designed logic-units.  The empirical results are also very impressive, the model matches or surpasses the previous SoTA on all the four datasets, which is definitely a plus for the proposed model.","The authors propose a pre-training strategy for Table (structured data) grounded tasks (question answering and fact verification). The main idea is to teach the model to execute SQL queries, by predicting the execution of the query, rather than using a pre-training strategy over the tabular data (e.g., Masked LM pre-training, or masked column pre-training). To elaborate, the authors synthetically generate SQL queries, execute them over tables, and collect the execution output. Then, the model is trained to generate (in an autoregressive way) the execution output given the concatenation of the table and SQL query (Figure 3 for more details). In this way, the model is forced to reason over the table to generate an answer, rather than using a reconstruction loss (MLM) that improve the representation of the input.   Experiments The authors use BART as the base model and use the SQL templates extracted in SQUALL (Shi et al. 2020) executed over 1500 randomly selected tables from WikiTable-Questions (Pasupat & Liang 2015) (from the training set and not overlapping with dev and test). The generated input-output pairs are 5M and the accuracy on a held-out is 89.6%. Then the pre-trained model is fine-tuned to 4 datasets (3 tables QA and 1 TableFact), and the results show a consistent and significant improvement on the existing state-of-the-art. Finally, the authors show an interesting analysis on the attention (Figure 4), the efficiency of the pre-trained task (Figure 5&6) and a manual evaluation on 500 samples from WikiTables showing how TAPEX improved over BART on different kinds of QA types (Table 6).   ",This paper proposes a new table pre-training approach. Different from previous masked language approaches for table pre-training. The authors propose to pre-train (or train) BART on the synthetic SQL corpus. The input for pretraining is the concatenation of SQL query and table while the output is the results by executing the SQL query through the SQL execution engine. Results show that the proposed pretraining approach reduces the gap between pre-training and fine-tuning on table understanding tasks including table-based question answering and table-based fact verification -- it outperforms BART and baselines on four datasets.,"This paper proposed `TAPEX`, a table pretraining method that conducts pretraining via learning a neural SQL executor over a synthetic corpus. Such corpus is obtained by automatically synthesizing executable SQL queries and execution results. By selecting a diverse, large scale and high quality synthetic corpus, the method claimed to address the data scarcity challenge. Then the executor could be further fine-tuned for downstream tasks, and demonstrate significant improvements over previous state-of-the-art work on table question answering and fact verification datasets.",0.1799163179916318,0.1589958158995816,0.09205020920502092,0.1282051282051282,0.08791208791208792,0.23232323232323232,0.1575091575091575,0.3838383838383838,0.2619047619047619,0.35353535353535354,0.2857142857142857,0.27380952380952384,0.16796875000000003,0.22485207100591714,0.13622291021671828,0.18817204301075266,0.13445378151260504,0.25136612021857924
489,SP:6d57b9e96259ab1480e67bccb3386d957606b060,"This paper proposes a critically-damped version of score-based diffusion models, by extending the inference process to an augmented state-space and diffusing the data coupled with an auxiliary velocity variable. The authors further proposed a hybrid score matching loss for training the reverse generative process, which provides an empirical advantage of learning a conditional score function that does not blow up (e.g. to infinity) and therefore stabilizes training. For sampling, the authors derived a new numerical integration method based on first principles of statistical mechanics and MD, which they found has a better convergence behavior in practice if the computing budget is limited.  ","The authors propose critically-damped Langevin dynamics (CLD) for score-based generative modeling. This consists of a higher-order Langevin dynamics scheme with particle velocity and position coupled to each other, as in Hamiltonian dynamics. The Langevin dynamics is critical in the sense that it is neither over- nor under-damped. A corresponding score matching objective is derived as an objective, with proof given that it is simply necessary to approximate the score of the velocity given the position. Empirical evidence is provided that this score is easier to estimate on a synthetic example. As DSM is analytically intractable for the higher-order scheme, Hybrid Score Matching (HSM) is proposed and the integration integral to this objective is addressed with a new numerical integration scheme. This approximation scheme, called Symmetric Splitting CLD Sample (SSCS), decomposes the SDE to be integrated into a tractable expression and a (hopefully small) Euler-Maruyama integration for improved accuracy (although still first order) overall. Synthetic examples are used in both the main text and the supplementary material to motivate the theory. Benchmark image datasets exhibit exceptionally strong performance, with improved sample efficiency after training and robust hyperparameters.","In this paper, the authors introduce a novel approach for training a score-based generative model.  With prior score based networks, generation is performed by solving a stochastic differential equation (SDE) based on Langevin Dynamics, using an estimate of gradient of the log likelihood of the underlying signal distribution.  In the present paper, the authors present a novel forward process, where diffusion is run in a joint data-velocity space.  The noise term is only applied to the velocity component.  This reduces the learning problem to only needing to learn the score of the conditional distribution of velocity given data, which is easier than learning the score of the data distribution directly.  The paper shows that the novel scheme (CLD) yields higher quality for image generation when compared to prior models of similar capacity and number of neural network evaluations.   ","The paper proposes a novel class of continuous-time diffusion-based generative models. Specifically, the paper proposes forward diffusions that are simple forms of stochastic Hamiltonian dynamics, unlike the previous methods, where linear stochastic differential equations (linear SDE) diffuse the data distribution. Thus, the proposed forward diffusion transport a joint distribution of data $x_0$ and auxiliary random variables $v_0$ to a prior joint distribution. Then, the generative models are their reverse-time diffusion processes on the joint space whose initial condition is the prior joint distribution.  In the Hamiltonian dynamics-type forward diffusion, the auxiliary random variable can be interpreted as velocity (as in physics), while data corresponds to position. The forward diffusion updates data by the velocities without any noise, while the velocities are updated via linear SDEs, similarly to deterministic Hamiltonian dynamics. In particular, the paper shows that the data distribution will be transported to a prior distribution and that velocities will map to a prior distribution similar to previous approaches. The prior joint distributions are commonly chosen to simple distributions, such as standard Normal distributions.  The authors emphasize two interesting properties of the proposed method. First, the paper points out that the reverse-time SDEs should only contain $\nabla_{v_t} \log p_t(v_t| x_t)$, but not logarithmic gradient wrt $x_t$, which implies that the model complexity (the size of models) is almost similar to the previous approaches. Second, the authors shows that $p_t(v_t| x_t)$ is Gaussians for all $t \in [0, T]$, including $p_0(v_0|x_0) = p_0(v_0)$. This indicates that $\nabla_{v_t} \log p_t(v_t| x_t)$ is potentially bounded unlike previous diffusion-based generative models, where the scores closer to data are possibly unbounded. As a result, esp. with common practice to learn scores at all time $t$'s by a single neural networks, training the proposed model will be more stable.  In order to learn the $\nabla_{v_t} \log p_t(v_t| x_t)$, the paper first propose to use denoising score matching (DSM) following previous diffusion-based generative models. However, observing that training the proposed model via DSM is unstable, the paper proposes a modified objective called hybrid score matching (HSM). HSM can be obtained by marginalizing out $v_0$ from score matching loss similarly to deriving DSM; the marginalization is possible as the auxiliary random variable defined to follow a known distribution, such as Normal distribution.  In addition, as the reverse-time diffusion is also a Hamiltonian SDE, the paper proposes a new integrator for generations, benefitting from the symplectic structure of the Hamiltonian dynamics. Note that it has been well appreciated that the discretization methods that utilize symplectic structure are more accurate than the Euler-Maruyama method under similar computational costs. Thus, the proposed integrator will have a better quality of samples, esp. when the number of discretization is small.  In order to show the effectiveness of the proposed method, the paper runs three main experiments: First, with toy experiments, the authors demonstrate that learning $\nabla_{v_t} \log p_t(v_t| x_t)$ is less difficult in comparison to learning the scores wrt the data directly. Then, the authors evaluate the generation qualities of the proposed models on two image modeling benchmark datasets; CIFAR-10 and CelebA-256. Finally, the paper demonstrates the proposed integrator's effectiveness compared to the Euler-Maruyama method, including additional analysis ablating the effect of hyperparameters of the proposed integrator.",0.22641509433962265,0.19811320754716982,0.32075471698113206,0.16666666666666666,0.28125,0.3357142857142857,0.125,0.15,0.05792163543441227,0.22857142857142856,0.0919931856899489,0.08006814310051108,0.1610738255033557,0.17073170731707318,0.09812409812409813,0.19277108433734938,0.1386392811296534,0.12929848693259974
490,SP:6db210f5588972d3e750ca66813f1b97b772d1ab,"This paper presents the first theoretical analysis of self-training on one hidden-layer neural network with gaussian input. The paper shows that under certain conditions (e.g., the initialization is neither too far or too close to the optimal), iterative self-training on the aformentioned setting can provably recover the ground truth labeling matrix and offers improvement in sample complexity over supervised learning with only labeled data. Experiments validate the theoretical results.","In this paper, the authors make first forrays into the study of the theoretical properties of the following iterative self-training algorithm in a semi-supervised regression setting where we have N labeled samples and M unlabeled samples (and the ground truth is realisable):  (1)  train the model on the labeled data, obtain pseudo labels for the unlabeled data by feeding them through the trained model  (2)  train an auxiliary loss (see equation (1)) on the augmented dataset the includes the unlabeled data with the corresponding pseudolabels (the loss is a convex combination of the empirical loss on the labelled and unlabeled datasets), and  (3) iteratively apply step (2) until convergence.   It is assumed that the ground truth is realisable (though without overparametrization assumptions), the labels are observed exactly (without noise), the model is a simple two-layer neural network with the second layer weights all fixed to one, and the labeled and unlabeled datapoints come from two isotropic Gaussian distributions of variances $\delta$ and $\tilde{\delta}$.   In theorem 2, the following surprising result is shown: under some reasonable assumptions on the initial point and the number of unlabeled datapoints, for a suitable choice of the convex combination parameter $\lambda$, the model converges towards the ground truth with an assumption on the number of samples which is weaker (by a constant factor at best) than what is required in the case where no unlabeled data is provided (which is a particular case of the case studied in [1])  In the more general Theorem 1, it is shown that under much weaker condition (without a requirement on the number of labelled samples), the model still converges towards a given convex combination of the ground truth and the initial point, and is guaranteed to outperform the initial model.    The main idea of the proofs is to define an auxiliary loss referred to as the population risk function (which estimates the risk taking as ""labels"" the images of the points by a convex combination of the ground truth and the initial point, cf. equation (17)) and to show (1) that the optimization lanscape of this functions is mild  (mainly proved in Lemmas 1 and 11), and (2) that this auxiliary function is close to the empirical risk which is minimized by the model (mainly proved in Lemma 2). The proofs of the techincal content of lemmas 1 and 2 rely both on computational geometry arguments and on existing results from [1].    Experiments are provided which show:  (1) an excellent match between the dependence of the bounds and that of the observed generalisation gaps on various parameters and (2) that the unlabeled data indeed improves performance in data-sparse regimes on some real data.     ======================= Reference: =======================   [1] Kai Zhong, Zhao Song, Prateek Jain, Peter L. Bartlett, Inderjit S. Dhillon. ""Recovery Guarantees for One-hidden-layer Neural Networks"", JMLR 2017 ","The paper theoretically analyzes self-training in neural networks with one hidden layer. Self-training is a popular semisupervised learning algorithm where a model uses large unlabeled data by training on pseudolabels generated by a teacher model trained on some small set of labels. The paper studies the generalization performance of self-training on a two-layer network under a Gaussian assumption on the inputs, and has some synthetic and CIFAR-10 experiments to show that the predicted theoretical rates (1/\sqrt{ size unlabeled data}) roughly match empirical performance. ","This paper theoretically analyzes the iterative self-training algorithm with 1-hidden layer neural network. When the labeled and unlabeled data is generated from zero-mean Gaussian distributions with variances of different scales, it is shown that both the convergence rate and the generalization error decreases at a rate of $1/\sqrt{M}$ where $M$ is the number of unlabeled data. Experiments on synthetic data and augmented CIFAR-10 corroborate theoretical findings.",0.3972602739726027,0.2328767123287671,0.2328767123287671,0.07383966244725738,0.07805907172995781,0.24719101123595505,0.06118143459915612,0.19101123595505617,0.2361111111111111,0.39325842696629215,0.5138888888888888,0.3055555555555556,0.1060329067641682,0.20987654320987653,0.23448275862068965,0.12433392539964477,0.13553113553113552,0.2732919254658385
491,SP:6db392849c04c6628d97d56d8dd4363f05efd19b,"This paper describes a few-shot learning approach that takes into account the semantic relatedness of the ground truth labels rather than just using them as binary labels. In the few-shot learning setting, the paper describes a method to train the semantic feature learner (on labels) during the meta-training phase and then at meta-test time use the semantic feature learner to get the said features for the query images. This way both the support and query examples would have semantic features and empirically it has shown to work on many few-shot benchmarks on computer vision datasets.  ","This paper proposes a task-adaptive semantic feature learning mechanism to incorporate semantic features for both support and query samples. Two modality combination implementations, feature concatenation and feature fusion, are devised based on the semantic feature learner. Experimental results are provided on four benchmark datasets that demonstrate that the proposed mechanism outperforms existing methods.","This paper proposes a few-shot learning method that incorporates semantic features (i.e., class label embeddings) for both support and query samples. Although some existing methods use the semantic features for support samples to improve the performance, no methods use them for query samples.  To this end, the proposed method regresses the semantic features for the query samples from the support samples. Experiments show the effectiveness of the proposed method.","In few-shot learning for image classification, visual features alone may represent multiple objects in an image. The label text provides additional information that could serve as useful inductive bias, indicating which object in an image the model should pay attention to. There are multiple ways to use such text information to help few-shot classification. The authors propose to build a model that predicts both the image classes and the text label (or its Glove embedding). In other words, an additional loss term is introduced to regulate the learning of visual features. The authors show that the additional ""semantic features"" (because Glove embedding is used) help improve few-shot image classification.",0.17,0.21,0.23,0.3148148148148148,0.12962962962962962,0.2112676056338028,0.3148148148148148,0.29577464788732394,0.20535714285714285,0.23943661971830985,0.0625,0.13392857142857142,0.2207792207792208,0.24561403508771928,0.21698113207547168,0.272,0.08433734939759036,0.16393442622950818
492,SP:6dca2dd730a194ca0ff064facdceb6f356a86bda,"Short summary:   The authors propose a zero-shot domain adaptation method with the assumption of each domain having the same set of labels. They obtain finite sample complexity bounds that contain the number of samples(n) per class in the training, the number of seen(training) and unseen(test) tasks under regularity assumptions on the loss and mappings. They provide accuracy results for MNIST and MNIST with rotated digits.   Method:   First, a common linear mapping is used to obtain a common representation for tasks. Then each domain-specific classifer(linear) combines at each index as a rank K tensor. A tensor completion is applied at each index that would give classifier parameters for unseen data as well, that allows them to predict without any data from the target domain. They assume same number of samples and same input dimensions for each domain (the latter can directly be extended to different dimensions). For the experiments, they use LeNet and directly predicted decomposed tensors rather than training and applying tensor completion.  Contributions:   The authors extend Tripuraneni et al. (2020) method and proofs from transfer learning to zero-shot case. They show a uniform convergence bound that has polynomial complexity with respect to the # tasks, # dimensions and rank. They derive excess risk on the order of n^-1/4.  ","The paper addresses the problem of domain generalization in the setting where each domain is indexed on a multidimensional array. It assumes that only a limited number of domains is observed during training and presents a technique for learning a classifier with provable good generalization performance on the unseen domains. The method consists of learning a common latent feature representation for all domains and domain-specific linear functionals, which are obtained by imposing a low-rank constraint on the tensor built by aggregating the functionals of all domains. The authors present a thorough theoretical analysis of the algorithm, including an upper bound for the excess risk on the target domain.","The paper considers the problem of zero-shot domain adaptation. The contributions are two-fold. The authors propose a novel domain adaptation technique and provide bounds on the prediction error, which they provide proofs for. In addition, they provide empirical validation of their proposed approach, on two sets of datasets: one derived from MNIST, the second corresponding to sensors.","This paper proposes a specific domain adaptation framework where a subset of all possible domains is are observed during training. Given sufficient training samples for the observed source domains, a common latent representation as well as a domain specific linear classifier is learned. Theoretical conditions under which the learned representations will be effective for unseen domains are provided. Experiments using two datasets are provided. ",0.1574074074074074,0.0787037037037037,0.08333333333333333,0.13636363636363635,0.20909090909090908,0.1864406779661017,0.3090909090909091,0.288135593220339,0.28125,0.2542372881355932,0.359375,0.171875,0.20858895705521474,0.12363636363636363,0.1285714285714286,0.17751479289940827,0.264367816091954,0.17886178861788615
493,SP:6dd4423fce4090b2b30299f5c49c0674b5bf6665,"This paper introduces the use of Fourier convolutional neural networks to model both image encoding and reconstruction processes. These neural networks learn multiplicative weights in the Fourier domain, enabling efficient global convolutions (i.e., convolutions with kernels the size of the entire image), which are particularly important for imaging applications with a wide PSF. These networks compare favorably to UNets for the task of joint 3D snapshot microscope parameter optimization and image reconstruction. These networks also outperform the state-of-the-art architectures for a lensless imaging reconstruction task.","This paper introduces a convolutional neural network architecture for designing phase patterns to display on a spatial light modulator in the imaging path of a microscope, as well as decoding the resulting image into a 3D volume. The authors propose to use Fourier features in the decoding stage to enable features that are more strongly nonlocal than in regular (primal-domain) convolution than what they claim would be possible to achieve using ""vanilla"" UNet/ResNet-like architectures. They demonstrate their approach for snapshot 3D microscopy as well as the reconstruction of light fields from input captured using lensless system (with randomized optical elements). ","This paper proposes a neural network architecture that applies a series of convolution kernels on the Fourier domain to focus on global information of the captured images. The authors make use of the Fourier-plane phase modulation and apply this idea to single-shot 3D microscopic imaging using the Fourier-plane phase modulation for 3D microscope and lensless imaging. Also, this is further extended to the end-to-end optimization of the phase modulation and the reconstruction network. An extensive synthetic evaluation is conducted to validate the effectiveness of the proposed method over existing non-Fourier neural networks in the context of computational imaging.",This paper introduces an end-to-end optical encoder and deep learning decoder optimization for 3D snapshot microscopy. The main challenge is that the decoder needs to be able to handle the global PSFs used in the optical encoder in 3D snapshot microscope. This is difficult to be achieved in conventional UNet architectures as the global context requires many layers that is computationally expensive. The authors use a Fourier convolutional network that achieves the global context in a single layer with a much lower computational cost. The method is demonstrated on simulated data of one volume with four different fields of view. The authors also applied the technique to lensless imaging. ,0.20224719101123595,0.23595505617977527,0.20224719101123595,0.20388349514563106,0.1941747572815534,0.19230769230769232,0.17475728155339806,0.20192307692307693,0.16216216216216217,0.20192307692307693,0.18018018018018017,0.18018018018018017,0.1875,0.21761658031088082,0.18,0.2028985507246377,0.1869158878504673,0.18604651162790695
494,SP:6df2b1cb21d126ba25eed7641e5138061297d986,"This paper studies unbiased estimators of SGD minibatch via DPP with orthogonal polynomials. The estimator can be constructed by sampling DPP whose kernel consists of the orthogonal projection of some kernel proposition. Samples from the projection DPP are guaranteed fixed sizes (corresponds to the rank) and the variance of minibatch SGD can be achieved by O(p^{-(1 + 1/d)}) for batch size p and feature dimension d. This is smaller than the uniform batch sampling with O(p^{-1}) which leads us that DPP can play a role of variance reduction. Empirical results on both synthetic and real-world datasets show the superior performance of DPP sampling than uniform one in terms of the norm of gradients, distance to the optimal solution as well as the variance of gradient estimations.","The authors introduce the first complete theoretical analysis of minibatch sampling using determinantal point processes in the context of (batched) stochastic gradient descent.  The core of this analysis is built upon continuous DPP results (specifically, on orthogonal polynomial ensembles, or OPEs) in which it has been shown that DPPs achieve faster-than-iid error decay. Assuming knowledge of a KDE $\tilde \gamma$ for the distribution over data points, the authors introduce two batched estimators of the full gradient: 1) A weighted sum of gradients at points sampled by a DPP with kernel $K$, where $K$ is build by restricting the eigenvalues of an OPE kernel reweighted by $\tilde \gamma$. This kernel (and associated DPP) can be computed and sampled from efficiently. 2) An estimator of more theoretical nature, which can sample batches of points that do not arise in the actual data, and is as least as costly in practice as estimating the full gradient.  The authors then prove that the first estimator is unbiased, and obtain a bound on the bias of the second. They then also show that the variance of the resulting estimator decays at a rate $\le p^{-(1 + \frac1d)}$, where $p$ is an integer parameter controlling the number of polynomials used to build the OPE and $d$ is the dimensionality of the input data.  Finally, the authors show on synthetic data that the resulting samplers improve the empirical convergence speed of SGD as a function of the number of required gradient evaluations, and confirm empirically the theoretical variance decay rate obtained earlier.","The authors introduce a sampling minibatches method in SGD based on determinantal point processes (DPPs) to reduce the estimation variance. Specifically, the authors proposed two gradient estimators using multivariate orthogonal polynomial ensembles, and provide theoretical results on the bias and the variance of the estimators. One of the two proposed estimators is tested on a synthetic dataset.  ","This paper presents a framework for variance reduction in stochastic gradient descent (SGD) that involves detrimental point processes (DPPs) based on orthogonal polynomials.  This framework uses a class of continuous DPPs based on orthogonal polynomials, and kernel density estimators built from the data, to perform minibatch sampling in SGD.  Theoretical analysis of this approach proves a bound on the variance for two gradient estimators, leading to variance reduction compared to uniform minibatch sampling without replacement.  This approach is empirically validated using experiments performed on synthetic simulated data. ",0.26717557251908397,0.10687022900763359,0.1450381679389313,0.10505836575875487,0.08949416342412451,0.2982456140350877,0.13618677042801555,0.24561403508771928,0.21839080459770116,0.47368421052631576,0.26436781609195403,0.19540229885057472,0.18041237113402064,0.14893617021276595,0.1743119266055046,0.17197452229299362,0.13372093023255813,0.23611111111111108
495,SP:6e039bfca70f4b6d1afe0fc61501b487c49eb27c,"This paper analyses the phenomenon of resonance in momentum SGD (SGDm) under a time-dependent covariate shift. This setting is useful to depart from iid-sampling-based theory, and could have useful implications for continual learning and reinforcement learning. In the paper, it is shown that such a setting corresponds to a _parametric_ oscillator of the parameters, which explains instability and divergence in a non-iid SGDm setting. The authors then test their hypothesis empirically, first in a simple setting that respects all the assumptions of the theorem, and then progressively getting rid of more and more assumptions, which ends up in finding a similar (albeit heavily dampened) phenomenon in non-linear ReLU-MLPs using Adam. ","This paper studies the convergence of SGD with momentum under linear regression with covariate shift. Data comes from $Y_i = <\theta^*, X_i> + \epsilon_i$ for a fixed $\theta^*$ and iid zero-mean noise $\epsilon_i$, but the distribution of $X_i$ varies over time (""covariate shift""), leading to non-iid samples. In this setting, the paper shows that the expected progress of SGD with momentum is a discretization of a second-order ODE. Then, the paper proceeds to show that if the covariate shift is periodic, the convergence/divergence of the ODE can be determined by looking at the spectral radius of a matrix called the monodromy matrix. The paper moves on to show experiments that test the theoretical characterizations and also investigate if different settings under relaxed assumptions show similar ""resonance"" behavior.  The keyword ""resonance-driven divergence"" in this paper can be understood as follows: divergence of SGD with momentum happens only at some specific frequency of covariate shift. Metaphorically, this is similar to breaking a wine glass with a sound wave tuned to the right frequency. ","The paper discusses conditions under which SGD with momentum (SGDm) would diverge. Specifically, training samples are assumed to be non-iid, connected through oscillations in first or second moments. For the specific Gaussian setting of linear regression with oscillatory covariate shift, a driven (with time-dependent parameters) linear oscillator ODE is formulated for the parameters under SGDm. This allows to derive conditions, related to the learning and momentum rates, under which parameters would con-/diverge. Theoretical predictions are tested empirically, and it is shown that the basic phenomenon (resonance or at least suboptimal convergence) is also present in setups with noise, non-harmonic oscillations, other optimizers (Adam), and nonlinear regression models (NN).","This paper studies the behavior of SGD with momentum when the training data are not iid. The authors consider a linear regression problem with covariance drift. In this case, they show that SGDm is the Euler's method for a second order ODE with a changing coefficient. Under the assumption of periodic covariance of the data, the authors identify a resonance phenomenon between the oscillation of data covariance and the oscillation of SGDm iterator. SGDm is proven to diverge when resonance happens. Though, when there is no resonance the authors are unable to prove the convergence of SGDm.   Beside the theory, numerical experiments are conducted for cases covered and not covered by the theory. Numerical results show a dependence of the optimizer performance and the covariance pattern in much broader settings than linear regression, even including neural networks. ",0.25,0.1810344827586207,0.21551724137931033,0.1340782122905028,0.18994413407821228,0.20535714285714285,0.16201117318435754,0.1875,0.18115942028985507,0.21428571428571427,0.2463768115942029,0.16666666666666666,0.19661016949152543,0.1842105263157895,0.1968503937007874,0.16494845360824742,0.21451104100946372,0.18399999999999997
496,SP:6e2129cafd0e4114b38e16351ed5f6b5fb91521e,The authors propose to scale Gaussian process modeling and sampling to high-dimensional outputs with an intrinsic coregionalization model (ICM). They do it by leveraging both a Kronecker structure on the outputs (all tasks observed everywhere) and the so-called Matheron rule to update prior samples. High-order GP models are considered as well. A variety of application examples are presented to illustrate the benefits of the method and pushing the state of the art in terms of number of outputs when MC acquisition functions are used.,"This paper presents an efficient sampling method for multi-output Gaussian processes and demonstrates the benefits of the proposed sampling method on Bayesian optimization with high dimensional outputs. The proposed sampling method is based on the Matheron’s rule, which was brought into the attention of the ML community by Wilson et al. (2020), and extends the formulation into the multi-output GP scenario.","The authors devise a novel method for exact multi-task Gaussian process (GP) sampling that dramatically improves time costs, from multiplicative to additive in the combination of tasks and data points. Their method works by exploiting Kronecker structures in covariance matrices and then applying Matheron's identity. This allows them to perform Bayesian optimisation (BO) for tens of thousands of correlated outputs, which was hardly possible previously.",This paper presents the idea of scalable multi-task Gaussian processes using a trick known as Matheron's rule. Efficient sampling from multi-task GPs is essential in solving many black box optimization problems with high-dimensional outputs including multi-objective constrained BO and composite functions. Matheron's rule is used to reduce the dependency of the number of tasks from a multiplicative factor to an additive factor. Experimental results are presented validating the benefits of the approach wrt. the computational resource requirements and also sample complexity.,0.1839080459770115,0.16091954022988506,0.20689655172413793,0.171875,0.3125,0.19402985074626866,0.25,0.208955223880597,0.20689655172413793,0.16417910447761194,0.22988505747126436,0.14942528735632185,0.2119205298013245,0.18181818181818182,0.20689655172413793,0.16793893129770993,0.26490066225165565,0.16883116883116883
497,SP:6e2b3f934a0f7bb2066d9743ee9b3c04e6b36fb0,"The authors focus on the limits of the Shapley value-based explanation that cannot explain the synergy effect among more than two features. To resolve this, they provide a new measurement called Shapley residual, which measure excess of adding a feature to some set of features. Experimental results show that the Shapley residual can capture more information that the Shapley cannot explain.","This paper introduces Shapley Residuals, a new measure that can be calculated in addition to SHAP values. This measure quantifies in some sense the information lost by Shapley Value due to interactions or correlations between features. This measure can help practitioners to identify scenarios when a score given to a certain feature by SHAP may not correctly quantify the effect of this feature on the prediction due to interactions or correlations. The paper provides theoretical grounding for the residuals, along with toy examples and an experiment with UCI dataset. ","In a cooperative game in characteristic function form, the value function $v$ maps from all subsets of agents, $N = {1, \ldots, n}$, producing an image with $2^N$ elements.  Shapley's value (perhaps the most widely used measure of feature importance in handling ML's attribution problem) ascribes a number to each agent, for a set of $N$ values.    As $N < 2^N$, Shapley's value does not - in general - fully encode the game.  An exception occurs when the game is _inessential_, so that $v(S) = \sum_{i \in S} v(i)$.  Specifically, when features _interact_, Shapley's value may be restrictive.  This paper builds on Stern and Tettenhorst's 2019 _Games and Economic Behavior_ paper, which provided a way of assessing a game's deviation from _inessentiality_, by means of Shapley residuals, $r_i$.  Definition 4 defines _Shapley residuals_, $r_S$, for sets of agents/features.  A series of results (Proposition 1, Corollaries 1, 2) then allow the norms of $r_S$ to be interpreted as a deviation from essentiality and - thus - a measure of Shapley's value's failure to fully encode the game.  Lemma 2 then makes precise the relationship between the absence of interactions between variables and inessentiality.  Section 5 compares the Shapley residual to the Shapley-Taylor interaction index.","This paper presents Shapley residual as a measure of player interactions for the interpretation games arising in machine learing applications.  The definition of Shapley residual is based on a classical notion of ineseentiality in cooperative games.  Intuitively, it captures how much deviation a game is from being inessential (or modular in combinatorics) .   ",0.1774193548387097,0.3064516129032258,0.12903225806451613,0.21348314606741572,0.12359550561797752,0.06074766355140187,0.12359550561797752,0.08878504672897196,0.15384615384615385,0.08878504672897196,0.21153846153846154,0.25,0.14569536423841056,0.13768115942028986,0.14035087719298245,0.12541254125412543,0.15602836879432624,0.09774436090225563
498,SP:6e438d8de1443f49af05067a2fb38ec58ef60375,"This paper proposes a sparsity-aware quantization (SPARQ) method, in which n-bit quantization takes place by picking the most significant n bits from the 8-bit value representation. Also, SPARQ is implemented on a systolic array(SA) or a tensor core(TC) DP unit. Finally, compared with the previous PTQ work, this paper evaluates the quantization effect of this method for various image classification models, and achieves the most advanced results.","The authors propose a post-training quantization scheme comprising two orthogonal ideas: bSPARQ and vSPARQ.  bSPARQ a dynamic quantization technique. Instead of rounding values to the required bitwidth (equivalent to keeping the MSBs), the technique keeps the most significant consecutive non-zero bits (i.e. the sequence of bits starting from the first non-zero bit). This is equivalent to making a power-of-two adjustment to the scale factor for each individual value.  vSPARQ is a dynamic sparsity technique meant for activations. First, adjacent pairs of activatons are grouped (e.g. [x1, x2, x3, x4] -> [x1x2, x3x4]). In each pair, if one of the two activations is zero, the other can make use of its compute resources. The idea is detailed in Equation (2) and can be realized in hardware with the architecture in Figure 2.  The authors demonstrate SOTA accuracy results on CNN image classification for very low activation bitwidths (4/3/2-bit activations and 8-bit weights). However, the technique also incurs significant area overhead: 22% for the 3opt design point.","This paper presents a sparsity-aware post-training quantization approach, which considers the sparsity on both the digit representation level and the activation level. The algorithm thus can select the most significant bits from the original 8-bit value representation, and dynamically adjust the bit budget allocation for the activation pairs during multiplication. The authors also discuss the hardware implementation for the proposed approach based on systolic array and Tensor Core DP unit. Experimental results show the effectiveness of the proposed method on various network architectures. Nonetheless, the paper still has some unclear issues with the experimental results and hardware implementations.","This paper proposes a post-training sparsity-aware quantization algorithm, SPARQ, for neural networks. SPARQ leverages the bitwise sparsity by skipping leading zero-value during quantization. SPARQ also leverages the elementwise sparsity by dynamically increase the quantization precision if one in the activation pair is zero. This paper also presents a practical hardware implementation for supporting the SPARQ. The experiments show a 0.18% accuracy drop in 4-bit quantized ResNet50 on ImageNet.",0.2777777777777778,0.4305555555555556,0.2361111111111111,0.14857142857142858,0.10857142857142857,0.19801980198019803,0.11428571428571428,0.3069306930693069,0.2328767123287671,0.25742574257425743,0.2602739726027397,0.273972602739726,0.16194331983805668,0.3583815028901734,0.23448275862068965,0.1884057971014493,0.15322580645161288,0.22988505747126436
499,SP:6e9d644774963b8ca46930cb07709e2c73e7164c,"The paper studies Goal-conditioned Hierarchical RL (GCHRL) and proposes a new algorithm called Hierarchical Exploration approach with Stable Subgoal representation learning (HESS) to improve the stability of subgoal representation learning and strengthen the exploration at high level. HESS is built on previous method LESSON. The instability of subgoal representation learning is alleviated by a representation regularization which is utilized to encourage the representation to be stable for the states with relatively lower triplet losses (originated from LESSON). Further, this paper proposes an active exploration method for the high-level learning. The method is built on the definitions of Novelty and Potential of states, which corresponds to accumulated visit counts of high-level state trajectory and a negative distance to the perturbed subgoals. Extensive experiments are conducted in a few MuJoCo environments with sparse reward, demonstrating the superiority of proposed algorithm and the effectiveness of different ingredients.","This paper proposes a new algorithm for goal-conditioned hierarchical reinforcement learning that is able to succeed in tasks with sparse rewards (differently from most other methods in the field). It does so through two innovations: (1) a representation learning procedure that is more stable, and (2) a exploration strategy that takes into consideration not only novelty but also reachability.  Specifically, the representation learning procedure is based on what is now a standard a contrastive loss, but it is augmented by a regularization term that make the learning procedure stable where the representation is already satisfactory, allowing goal sampling to be more effective. Figure 6 is a particularly nice visualization of the impact of this regularization term.  The exploration strategy to sample goals to be visited is also novel. Instead of using goal visitation counts, this paper proposes the idea of using expected sum of state visitation counts from that state onwards, capturing some notion of long term novelty. Moreover, the exploration bonus also has a potential term that captures how promising each goal state is in terms of how far from the goal state the agent is expected to end up. Quantitative impact is reported in Figure 7, but I particularly liked the intuitions/visualizations provided in Figure 8.","1. This paper investigates learning stable subgoals within a deep hierarchical reinforcement learning setup.   2. Two controllers are learned from the same experience replay buffer. The high level controller serves as a meta controller and the low level controller serves as a goal-achieving agent. The high level controller communicates abstract goals to the low level controllers.   3. The high level controller is optimized using an extrinsically specified reward function. The low level controller optimizes the intrinsic goal communicated by the high level controller. The subgoals are changed after a deterministic time length (known option termination).  4. The subgoals are designed with the key insight that ""desirable novel subgoals should be reachable and effectively guide the agent to unexplored areas."". Typically count-based, predicted or successor feature based rewards have been used as novelty measures. However, these fall short in terms of reasoning about reachability of states. To handle this, a potential measure for subgoals is proposed which regularizes the novelty measure.  5. To go to unexplored states, a directional goal is synthesized/imagined using the current state and a directional vector. The potential function makes sure that this is approximately reachable by formulating reward as the expected negative distance between the ending state and imagined goal. This is similar to Feudal networks (Vezhnevets et al.) but goes beyond it to handle diversity and reachability.   6. The approach is validated on a set of hard to explore continuous environments with reasonably strong and relevant baselines.","The authors propose a hierarchical RL algorithm which augments an existing contrastive learning-based subgoal representation objective with heuristics for exploration. The proposed algorithm seeks to reduce representation drift over the course of learning by penalizing the learner for modifying $\phi(s)$ for states $s$ with low contrastive loss. Furthermore, the authors propose exploration heuristics that encourages the learner to explore in promising areas of latent space by combining count-based novelty and potential measures. The proposed algorithm is demonstrated to have the desirable properties, and outperforms existing methods. The analysis is complemented by an ablative analysis that disentangles the effects of each proposed mechanism.",0.25170068027210885,0.23809523809523808,0.19047619047619047,0.18095238095238095,0.1,0.11020408163265306,0.1761904761904762,0.14285714285714285,0.26666666666666666,0.15510204081632653,0.2,0.2571428571428571,0.20728291316526612,0.17857142857142855,0.2222222222222222,0.16703296703296705,0.13333333333333333,0.1542857142857143
500,SP:6eab0173ee6bff639f813b5c7036bc1f9ced5c58,"This work develops an approach to unsupervised single-object segmentation. The main idea is to segment the scene into $C$ (hyperparmeter) segments in one frame and to learn a flow field w.r.t. another frame such that it can be reconstructed by translating the segments from the original frame. This simple strategy provides competitive accuracy on standard object video segmentation benchmarks, as well as shows promise in unsupervised saliency detection (DUTS) and representation learning (VOC12). ","Paper proposes a novel approach to unsupervised object segmenation based on temporal consistency in video sequences. The authors show good results across a range of video datasets, and that the models trained without supervision generalise well to figure/ground segmenations on image based datasets.  The method itself is conceptually straightforward, decomposing the image into a fixed number of parts so that each part has a low photometric error when propegated to a future frame, using a constant velocity model.","This paper proposes to decouple the motion and appearance by two pathways: the appearance pathway is designed to learn object segmentation masks, while the motion pathway aims to predict the optical flow per region mask. The whole model is learned in a self-supervised manner by frame-reconstruction loss. While such region-wise, which is in contrast to the typical way of learning pixel-wise dense optical flow, is interesting, the overall novelty is not sufficient for NeurIPS.  ","This paper presents a method that segments the image into a number of segments and estimates an offset vector for each segment. Different from prior two-stream appearance+motion based methods, this approach does not leverage a pre-trained flow network but learns to estimate motion offset vectors implicitly by warping (segmented) regions across frames based on the flow estimates. Reconstruction error (inspired by self-supervised optical flow methods) is used as a loss function and provides a supervisory signal. Thus, the proposed method can be trained using unlabelled video. ",0.14473684210526316,0.17105263157894737,0.14473684210526316,0.13924050632911392,0.17721518987341772,0.20512820512820512,0.13924050632911392,0.16666666666666666,0.12222222222222222,0.14102564102564102,0.15555555555555556,0.17777777777777778,0.14193548387096774,0.16883116883116883,0.13253012048192772,0.14012738853503184,0.16568047337278105,0.19047619047619047
501,SP:6eb16cad8cedffabea3333f8f41192c0680416c0,"This paper expands on prior works in self-imitative goal-conditioned RL by proposing to selectively filter the target policy which is used during the behavioral cloning phase. By ensuring that the target policy reaches goals more optimally than the behavior policy, the paper shows that under certain conditions, performance improvement can be guaranteed. The algorithm first collects trajectories under some exploration policy, then for fragments of these trajectories, the algorithm evaluates the current policy (starting in the first state in the fragment) to see if the fragment represents an improvement on the agent's ability to reach the state. If so, it is added to the replay buffer for behavioral cloning. In experiments, the algorithm (SPLID) performs well in deterministic, goal-conditioned environments.","The authors propose an approach for goal-conditioned reinforcement learning via self-imitation. The paper first outlines a meta-algorithm which defines conditions under which an instantiation of the algorithm constitutes a valid policy improvement operator. Following this, the authors propose a concrete instantiation of the algorithm which perturbs the policies parameters, selects policies which decrease the hitting time on randomly chosen goals and then distills the selected behavior into a new policy  ","The authors consider the Goal-Conditioned continuous control RL task, which is difficult due to reward sparsity. The authors propose a method for training deep RL agents in a more sample efficient manner through self imitation on policy traces that have been ""distilled"", i.e., randomly perturbed and argmax'd according to some metric like ""First Hit Time"", then using hindsight experience replay to relabel traces (handling reward sparsity). This improves of over the previous state of the art which did not use the policy distillation step, simply relabeling with the most recent policy, prone to error propagation. The authors demonstrate that their policy distillation step improves the state of the art on 3 RL baseline settings, and also give theoretical backing to their approach and conditions for convergence.  ","This paper presents a self-imitation learning method for goal-conditioned continuous control tasks. The key idea is to identify a $\delta$-distilled policy that performs better than the current policy. Then a mixture policy is generated if a $\delta$-distilled policy is identified. Finally, the parameters are updated with behavior cloning to the relabeled data. Experiments on three goal-oriented tasks demonstrate the efficiency of the proposed method.",0.12096774193548387,0.1774193548387097,0.1532258064516129,0.2328767123287671,0.1917808219178082,0.13953488372093023,0.2054794520547945,0.17054263565891473,0.2753623188405797,0.13178294573643412,0.2028985507246377,0.2608695652173913,0.15228426395939085,0.17391304347826086,0.19689119170984457,0.16831683168316833,0.19718309859154928,0.18181818181818182
502,SP:6ec81fb71d58d19a31e0b9bc27ea48fcb984fb17,"Based on the Euler-Lagrangian framework, this manuscript develops the theoretical method to model the learning dynamics of DNNs as symmetry breaking of the likelihood by the kinetic energy.  The model is expressed as the time evolution of Noether-current. As a result, the author found that the learning dynamics of batch normalization are similar to RMSProp from the viewpoint of symmetry breaking. Their findings deepen our understanding of batch normalization. This study is unique not only because of the findings on batch normalization mechanics but also because it links the symmetry-breaking process of the likelihood function with the learning theory.","The authors identify a ""kinetic asymmetry"" in the learning dynamics under gradient flow, where the kinetic energy is not invariant to the symmetries of the potential energy. An example is scaling: even when the network function is invariant to scaling or ReLUs or batch norm inputs, the learning dynamics are not. They derive a genealized version of Noether's theorem which takes this asymmetry into account and use it to study the implict adaptive optimization exhibited by batch normalization, showing that it is the same as that of RMSprop. ","The gradient descent with a momentum can be regarded as the Newtonian equation with damping. The output of the batch normalization is invariant to the scale of the input (i.e., a scale symmetry). When a BN is inserted after a convolution, the whole network is invariant to the weight scale. A typical gradient descent algorithm ignores (i.e., breaks) this symmetry. Thanks to this symmetry breaking, the learning rate is implicitly adapted, and the adaptation is equivalent to RMSProp's. ","The paper considers the role of symmetries in the learning dynamic by considering a continuous time formulation of the learning process in Lagrangian formalism. In such formulation there are ""kinetic"" and ""potential"" components to the dynamics, the former arising from the learning rules employed during optimization and the latter generated by the loss function landscape. Authors argue that for a large class of practically relevant network architectures there are symmetries that are present in the ""potential"" component, but not in the ""kinetic"" part.  Starting with Bergman Lagrangian formulation of the learning dynamics, authors derive the equations for time evolution of the Noether's charge associated with symmetry transformation (assumed to be present) in the network architecture.  For the case on scale symmetry introduces by BatchNormalization, authors observe that in case of training with gradient descent + momentum + weight decay; the training dynamics effectively reduces to a learning dynamics with an adaptive scale where the scale is related to the associated Noether's charge. This suggests that kinetic asymmetry induces an implicit adaptive property to the optimization symmetry. A connection between RMSProp and implicit adaptation is established based on continuous time learning model.  Lastly, authors test theoretical predictions by training a set of models verifying theoretical predictions on Tiny ImageNet dataset. For comparison, the adaptive rescaling effect is artfully removed by renormalizing the parameters throughout training. ",0.23529411764705882,0.18627450980392157,0.27450980392156865,0.2696629213483146,0.30337078651685395,0.2962962962962963,0.2696629213483146,0.2345679012345679,0.125,0.2962962962962963,0.12053571428571429,0.10714285714285714,0.2513089005235602,0.20765027322404372,0.17177914110429449,0.28235294117647053,0.17252396166134185,0.15737704918032785
503,SP:6eec75a67b4ac252b00f2848030dcc49ab816ef5,"In this paper, the authors propose eXploit-Then-eXplore (XTX), a training strategy for agent solving human-generated text-based games.   XTX consists of two training phases:  1. In the exploitation phase, the agent samples high quality experiences (in terms of score and trajectory length) from its replay buffer. Using the sampled trajectories, an action generation module is trained. At a certain game step $t$, the action generation module takes the observation $o_t$, as well as the two most recent past actions $a_{t-1}$ and $a_{t-2}$ as input, and generates the new action $a_t$ in a word-by-word auto-regressive manner. This process is referred as self-imitation by the authors.   2. In the exploration phase, in addition to the Q-learning loss as used in DRRN, the authors use two auxiliary losses to encourage the model to capture useful representations. First, the inverse dynamics loss $L_{inv}$ optimizes a module that predicts an action $a_t$ given two consecutive observations $o_t$ and $o_{t+1}$, where $o_{t+1}$ is resulted by $a_t$ given $o_t$. The second loss $L_{dec}$ is a regularizer that optimizes a module reconstructs an action $a_t$ from its encoding $f_a(a_t)$.  During training, the two phases take control in an (almost) alternate manner, however, there is a coefficient $\lambda$ controls the interpolation between the phases. The authors show that it is beneficial to not having the exploitation take control solely.   On a subset of games from the Jericho suite, the authors show their agent outperform prior works.  ","This paper introduces an agent with a built-in exploration strategy that is aimed at text adventure games, or more generally, environments with large action spaces and sparse rewards. The exploration strategy is constructed from two independent policies: one trained with self-imitation learning on successful trajectories, and one trained on an inverse dynamics intrinsic reward. The agent plays episodes by starting with the exploitation policy for a number of steps that depends on the experience collected up to that point, and then switching to the exploration policy. The paper is well-written, describes the contributions clearly, and places itself in the context of the existing literature on exploration. It includes results on a number of text exploration games from a recent benchmark, where it shows by and large a significant improvement relative to the baselines included.","This paper presents a new exploration algorithm, eXploit-Then-eXplore (XTX), for text-based games which require extensive exploration. The authors propose an algorithm that explicitly disentangles exploitation and exploration strategies within each episode. XTX first learns the exploitation policy that imitates the promising trajectories from past experiences, then uses exploration policy to discover the novel state-action spaces. Finally, the authors demonstrated the outperforming results in the Jericho environment. ","Summary:  The paper proposes a multi-stage directed exploration algorithm, XTX. It first imitates previous high score trajectories and then switches to an exploration policy with novelty bonuses. Conceptually, XTX is a method that extends Go-Explore which only acts randomly after reaching the frontier of familiar states. The paper argues that with novelty bonuses, the agent will be encouraged to explore more promising actions. This can especially be helpful when the action space is large like text-based games. Empirically, XTX shows strong performance on a large set of text-based games.",0.1169811320754717,0.09433962264150944,0.07924528301886792,0.15328467153284672,0.1386861313868613,0.21428571428571427,0.22627737226277372,0.35714285714285715,0.22580645161290322,0.3,0.20430107526881722,0.16129032258064516,0.15422885572139303,0.1492537313432836,0.11731843575418995,0.20289855072463772,0.16521739130434784,0.18404907975460122
504,SP:6f9e85e5afa7e658d17e289100b460289677dd5b,"The paper considers the problem of reward identifiability in a maximum entropy inverse optimal control setting when access to the expert policy is available. It is shown that the reward function can only be recovered up to a state-dependent shaping term (which relates to the temporal credit assignment). If access to the optimal policies for two variants (different discount factor or different transition model) of a given MDP is available, it is shown that the reward function can be recovered (up to a constant). The results are validated on numerical experiments on a finite MDP, with known transition model policies.","This work presents theory and experiments on the problem of reward function non-identifiability in IRL. The key result is Theorem 1, which entails that the reward function (in the entropy-regularized MDP setting) cannot be uniquely identified given an unknown value function. Following this line of reasoning, the authors argue that it is possible to identify the reward function (up to a constant) provided we can disentangle immediate rewards from preferences over future states (e.g., with data from two agents with different discount factors in the LQG setting). This result is also demonstrated empirically using a small example. ","This paper studies the reward identifiability problem in inverse reinforcement learning, under a special family of MDPs -- entropy-regularized MDPs. The authors provide some mathematical deductions to show the identifiability is still an issue given entropy-regularization and side information from another policy with a different transition probability or a different discount factor but a consistent reward function may be sufficient to resolve it. They then construct a simple linear-quadratic Gaussian MDP to support their theorems and conduct numerical simulations. The empirical results validate their claims. ","The authors consider the question of recovering the exact reward function from trajectories in inverse reinforcement learning. To this end, the authors consider the formalism of IRL with a maximum-entropy regularized objective which can be seen as a generalization of the MaxEntIRL objective. Using this view, the authors show that the degrees of freedom of the reward correspond to an arbitrary, desired value function. The authors show that the reward function can be fully recovered if the policy is known in two distinct MDPs with sufficiently different transition-dynamics and/or discount factors and give conditions for which this is the case. Similar theorems are given for finite MDPs as well as the linear quadratic regulator.  ",0.24752475247524752,0.21782178217821782,0.25742574257425743,0.16,0.27,0.25287356321839083,0.25,0.25287356321839083,0.2222222222222222,0.1839080459770115,0.23076923076923078,0.18803418803418803,0.24875621890547264,0.2340425531914894,0.2385321100917431,0.1711229946524064,0.24884792626728108,0.2156862745098039
505,SP:6fb19b762505bd8f71a185fad232435d0fd73ec6,"This paper proposes a decoupled policy approach to state-only imitation learning, wherein the policy is decomposed into a next-state prediction network, which aims to predict the desired next state given the current state under the expert policy, and an inverse dynamics module, which given the current state and the desired next state, predicts the action. The utility of this split is that it allows for additional supervision during policy optimization, as the state transition network can be trained to match the observed expert transitions, and the inverse dynamics module can be trained on the transitions in the trajectories obtained during policy optimization. These supervised losses are placed on top of a standard policy gradient method, as gradients to both components of the decoupled policy can also be obtained through a standard likelihood-ratio policy gradient applied to a reward function corresponding to likelihood of the executed state-transitions being drawn from the expert policy. This reward function is obtained by jointly training a discriminator to classify sample transitions from the learned policy from those from the expert data. ","This paper tackles the state-only imitation learning problem by decomposing it into two components: a state-transition predictor and an inverse dynamics model. Specifically, it argues that the problem is difficult because there are potentially multiple policies that could match the same state trajectory of the expert.   Instead, it proposes to learn what they call a hyper-policy, which is a family of policies that lead to the same distribution over state trajectories. Then, after recovering the optimal hyper-policy, it learns an inverse dynamics model to recover the actions, and this model can be learned by interacting with the environment.  However, to actually achieve good performance in practice, they propose two regularizations, i.e., a multi-step prediction loss and a cycle training loss. The parameters of the hyper-policy and the inverse dynamics model are also trained with a policy gradient objective where the reward is based on a discriminator (which is trained to predict whether the transition (s, s’) is from the expert). ","The paper proposes a new method for state-only imitation learning, based around explicitly representing the transition from one state to the next (based on state-only expert demonstrations) and an inverse dynamics model providing feasible actions to produce that transition (based on rollouts from a sampling policy). Theoretical analysis attempts to show that this reduces policy ambiguity. Empirical results show improved performance relative to baselines, with the most striking results in a gridworld and on real-world driving data: on standard MuJoCo continuous control tasks, the mean of the method tends to be higher, but the confidence intervals overlap with baselines.","The paper presents a method for state-only imitation learning (SOIL) that decouples learning the expert state-transition distribution from the actual policy needed to reach desired states. In particular, given a dataset of expert state sequences, the method trains two models: one that predicts next state given the current state, and another one that learns inverse dynamics, i.e. which action is needed to reach the next state from the current state. Where the state prediction can be directly learned from the state-only expert data, environment interactions are collected using a sampling policy to learn the inverse dynamics model. Additional techniques, such as multi-step optimization and cycle training are used to improve prediction accuracy of the state-transition model. In addition, training is improved by introducing a discriminator with a GAN-like reward that can provide intermediate reward signals to the policy. The method is evaluated on several MuJoCo continuous control environments, where it is shown to outperform other SOIL baselines, and on a real-world human driving dataset where it can learn effective driving policies just from human driving trajectories. ",0.25555555555555554,0.17222222222222222,0.26666666666666666,0.16167664670658682,0.25748502994011974,0.3431372549019608,0.2754491017964072,0.30392156862745096,0.2608695652173913,0.2647058823529412,0.23369565217391305,0.19021739130434784,0.2651296829971182,0.2198581560283688,0.26373626373626374,0.2007434944237918,0.245014245014245,0.24475524475524482
506,SP:6fc049b9df962442aa104106bc52d459c940ee0e,"This paper considers the problem of solving smooth strongly convex strongly concave saddle point problems over both centralized and decentralized networks. The main novelty is the consideration of the similarity between local functions in terms of second-order information. It provides better complexity lower bounds for both centralized and decentralized cases when the similarity of local functions is better than the smoothness of the objectives, as well as a near-optimal algorithm for the centralized case.","In this work a distributed (strongly)-convex-(strongly)-concave saddle point problem (SPP) is studied, over two different types of networks, i.e., master/worker, and meshed networks. In both cases lower bounds are derived for the number of communication rounds required  to reach an $\epsilon$ optimal solution. Differently than previous works on distributed SPP, the new analysis takes into account the similarity between the local functions. The lower communication complexity for reaching an $\epsilon$-optimal solution is shown to be in the order of $\Omega \left( \frac{\Delta \delta}{\mu} \log \left( \frac{1}{\epsilon}\right) \right)$ for a master/worker network and  $\Omega \left( \frac{ \delta}{\mu \sqrt{\rho}} \log \left( \frac{1}{\epsilon}\right) \right)$ for a meshed one; the parameter $\delta$ captures the function similarity while $\Delta, \rho$ depend on the properties of the network. Then, two algorithms are developed, one for each network type, that match these bounds, up to logarithmic factors. Finally, the proposed algorithms are evaluated on a robust linear regression problem over synthetic and real data.",This paper studies the distributed convex-concave saddle point problem under the relative smooth condition. Lower bounds are proved for strongly convex-strongly concave problems over centralized and decentralized networks. Optimal methods are proposed mathching the lower bounds.,"The paper studies saddle-point problems with objective being a sum of strongly convex-concave functions, $\min_x \max_y f(x,y) = \sum_{m = 1}^M f_m(x,y)$, in the setting of distributed communication. Specifically, each component in the sum of functions is local to an agent $m$, and communication between agents is expensive, which forces algorithms of two forms: master-worker type, and meshed grid type.   The paper's chief assumption is that the functions $f_i$ are $\delta$-similar, which means an upper bound of $\delta$ on  the operator norm  of the difference between the Hessians of $f_m$ and $f$, evaluated with respect to $(x,x)$, $(x, y)$, and $(y, y)$ directions. This assumption is reasonable in the case of robust linear regression and in proving adversarial robustness of neural networks.   The paper provides lower bounds and algorithms meeting these lower bounds for solving the saddle point problems to $\epsilon$ accuracy. ",0.3684210526315789,0.23684210526315788,0.2631578947368421,0.08620689655172414,0.1724137931034483,0.3157894736842105,0.16091954022988506,0.47368421052631576,0.12738853503184713,0.39473684210526316,0.1910828025477707,0.07643312101910828,0.224,0.3157894736842105,0.17167381974248927,0.14150943396226418,0.1812688821752266,0.12307692307692307
507,SP:6fc7ea70cfb44c921400ad2bbf8ed4213e68d89f,"The paper proposes an approach to learning sparse deep neural networks (neural networks where most of the weights are set to zero) using annealing: starting by training a large overparameterized neural network to fit the training data perfectly, then adding weight penalties according to a prespecified schedule. A theoretical argument is provided to argue why this is reasonable, with two new theory proofs provided which extend previous work presented in Sun et al. 2021. Experiments on synthetic data show that the proposed algorithms perform feature selection better than other existing algorithms. Experiments on CIFAR10 show the algorithms are competitive in terms of the accuracy vs. pruning tradeoff.","1. Theoretical contributions: the authors extend the theory established by Sun et al. [2021] and study the asymptotic behaviors of sparse deep learning. 2. Methodological contributions: the authors propose prior annealing algorithms, one from the frequentist perspective and one from the Bayesian perspective, to implement the corresponding sparse neural nets. They show the performance of the prior annealing algorithms on one synthetic example and the CIFAR10 datasets.","The submission addresses the issue of sparsifying trained deep neural networks in an optimal way, preserving performance and improving calibration. It builds on the formalism and results of Sun et al. (2021), proves additional results and proposes two prior-annealing algorithms, which they demonstrate on synthetic and real data, comparing favorably to existing sparsifying algorithms. ","Overall, the work presents an interesting direction into NN pruning. However, the paper is quite hard to follow and overly complex in notation, with little effort to assist the reader. Further, the paper severely lacks useful devices, such as figures or in-depth description, to explain the methods to readers, making the work challenging to follow.  Last, some of the experimental evaluation is somewhat questionable, as specified in the details comments.  Though the direction seems useful, additional work is needed in communicating the results and in experiments in order to warrant acceptance. ",0.12149532710280374,0.1308411214953271,0.1308411214953271,0.208955223880597,0.1791044776119403,0.16363636363636364,0.19402985074626866,0.2545454545454545,0.15217391304347827,0.2545454545454545,0.13043478260869565,0.09782608695652174,0.14942528735632182,0.17283950617283947,0.14070351758793967,0.22950819672131145,0.1509433962264151,0.12244897959183675
508,SP:6fe0c3ccc4daee43d1396259fafb9cb0fcdcfad8,"This paper introduces a novel task, noisy correspondence of cross-modal retrieval tasks. The noisy correspondence is similar to the noisy label in object classification tasks, where the noisy correspondence scenario assumes that some image-caption pairs are incorrectly matched. To solve the problem, this paper proposes a Noisy Correspondence Rectifier (NCR), which could be considered as the novel extension of Co-teaching [9] to cross-modal retrieval tasks. Note that directly applying Co-teaching to cross-modal retrieval is non-trivial as the authors discussed in the paper (L105-115), because noisy label methods assume the existence of the prediction score function of the given input, while cross-modal retrieval methods usually do not have such per-sample score function. Hence, the proposed NCR first selects (co-divides) clean and noisy samples by using triplet loss values and the 2-mean Gaussian Mixture Model (GMM). Using the divided samples, NCR co-rectifies the noisy labels by the prediction function, where the prediction function is defined by the value of triplet distances. Using the extracted clean/noisy samples and rectified labels, NCR applies the hardest negative mining triplet loss to train the model, while the margin value is adaptive to the rectified labels. The experimental results show the effectiveness of NCR in terms of robustness to the noisy correspondence, e.g., achieving nearly dropped retrieval performances when 50% of correspondences are incorrect.","This paper proposes a noisy label learning framework for cross-modal learning, where the training data consists of wrongly matched modalities. The proposed NCR first divides the data into clean and noisy splits based on the difference of loss distribution. Then NCR proposes to rectify the labels with a prediction function and further recast them into soft margins in the triplet loss. Extensive experiments and comparisons are conducted to verify the effectiveness of the method.","This paper focuses on cross-modal matching with the presence of noisy correspondence. The author proposes a novel method for learning with noisy correspondence to achieve robust cross-modal matching based on the SGR. In brief, it first splits the data into clean and noisy subsets in the manner of co-teaching and rectifies the labels with an adaptive prediction function. Extensive experiments on three datasets show the effectiveness of the proposed method.","The authors propose a method for cross-modal retrieval in the presence of noisy image-text pairings. While there has been a lot of work in learning in the presence of noise in this space, existing works mainly handle the problem of ""label"" noise - i.e. when semantic categories / labels on the data are available, but are noisy. In contrast, the authors tackle the unsupervised setting where all that is available are image-text pairs harvested from the web. Some of these pairs may be noisy and some may not be. The idea of the paper is straightforward. The authors use a technique commonly used to address noisy labels - i.e. co-teaching. They train two networks, A and B on the same data. After each epoch, the authors collect the similarity scores computed by each of the networks and use it to train a Gaussian Mixture Model. The reason for this is the GMM allows one to calculate the probability that a sample is from a given distribution. They model it with two Gaussians. The idea is that the GMM will fit one to the set of data that has lower similarity (hence noisy) and a separate Gaussian to the one with high similarity (hence clean). They then use the learned similarities predicted by each model to ""correct"" the labels used for training the other model (for example, treating some data that was considered clean as noisy or vice versa) based on the co-teacher's predictions. Finally, they integrate the similarity / confidence that the model is clean or not into the triplet loss as an adaptive margin, rather than use a static margin. The authors experimentally validate their method on MSCOCO, FlickR30K, and Conceptual Captions and compare against a number of baselines showing reasonable improvement.",0.13793103448275862,0.14224137931034483,0.21120689655172414,0.38666666666666666,0.3466666666666667,0.3424657534246575,0.4266666666666667,0.4520547945205479,0.16498316498316498,0.3972602739726027,0.08754208754208755,0.08417508417508418,0.20846905537459284,0.21639344262295082,0.18525519848771266,0.3918918918918919,0.13978494623655915,0.13513513513513514
509,SP:700d1d30284a98ca459977e3a7a513d9a35323cc,This paper proposes an approach for training agents that are capable of ad hoc coordination with humans. The method combines an entropy based objective with a method for prioritizing partner selection. Results on the Overcooked domain show improved performance over a series of baselines.,"The paper aims to address the important problem of training AI agents in multi-agent games. Standard self-play training leads to agents that overfit to a particular partner, and even naive population based training may not help much if the population is not very diverse. A number of recent works have looked into generating diversity in the partner strategies so that population based training methods can work better. By covering a diverse set of trajectories/policies, it is more likely that coordinating with a new partner (perhaps a human) at test time will fall in-distribution relative to our training set partners.  The paper proposes a population diversity metric based on cross-entropy between different pairs of agents in the population. They optimize a lower bound to this metric based on the population entropy. They also propose a 'prioritized sampling' procedure that determines which partner in the population the ego-agent should train with. They experiment on the game of Overcooked, compare with TrajeDi, and run a human experiment to check performance of coordination with humans at test time.","This paper tries to find a new approach by enforcing the diversity in multi-agent RL  via the maximum entropy to address the zero-shot human-AI coordination problem. More specifically, in the proposed Maximum Entropy Population-based training (MEP) framework,  the authors choose population entropy as an efficient surrogate objective and use the prioritized sampling. The empirical results on the overcooked show that MEP outperforms other baselines with both simulated and real human players.  ","The authors focus on training agents for zero-shot human-AI coordination on Overcooked. They propose to first train a population of agents with a population entropy bonus, and then train another agent as the best response to the population, using a prioritized sampling approach that focuses more training on the worst performing pairs. They evaluate their agent and some baselines and ablations both with a human proxy model (i.e. an agent trained on human-human gameplay data with behavioral cloning) and also a smaller subset of their agent and baselines with real humans on Mechanical Turk. Their method performs as well or better in terms of game score than the baselines and ablations presented.",0.36363636363636365,0.3181818181818182,0.25,0.14444444444444443,0.16111111111111112,0.25333333333333335,0.08888888888888889,0.18666666666666668,0.09482758620689655,0.3466666666666667,0.25,0.16379310344827586,0.14285714285714285,0.23529411764705882,0.13749999999999998,0.20392156862745095,0.19594594594594597,0.19895287958115182
510,SP:7029f2103f85fb31db4373ddfeb02cc7816d9d9c,"This paper studies the exploding and vanishing gradient problem (EVGP) in the sequential modelling with chaotic dynamics. Theoretical analysis of the EVGP, based  on the relationship between loss gradients during RNN training and Lyapunov spectrum of RNN-generated orbits, is provided. Inspired by this analysis, an alternative to BPTT training algorithm is proposed, named sparsely forced BPTT, that forces the diverging dynamics to conform to the true trajectory, at regular time interval provided by the  Lyapunov spectrum.","The paper connects the gradient behavior of recurrent neural networks with its dynamics through the maximum Lyapunov exponent of a trajectory, which is generated by the respective RNN. In particular, several connections between RNNs which produce specific dynamics (e.g. with stable fixed points or limit-cycles) and the mitigation of the exploding gradient problem are proved. Moreover, the paper shows that RNNs generating a chaotic trajectory always suffer from the exploding gradient problem.  That being said, the paper emphasizes that in order to approximate chaotic systems, the RNN has to be able to produce chaotic trajectories, which is ruled out by design by several state-of-the-art RNNs that are constructed to solve the exploding and vanishing gradient problem. However, given that these RNNs - at least the moment the optimizer finds parameters such that the resulting RNN produces chaotic trajectories - will always run into the exploding gradient problem, the paper suggests an algorithmic fix for that, by forcing the hidden-states of the RNN to be the pseudo-inverse of the underlying ground-truth trajectory at equidistantly distributed points in time during training. This cuts off the gradients at these points in time and forces the output states to be close to the ground-truth trajectories. The interval for that is chosen based on the maximum Lyapunov exponent. In the paper, three experiments are provided, where two of them are based on approximating a chaotic dynamical system (Lorenz and Rössler), while one is based on real-world data from temperature forecasting.",The authors provide a study of gradients of the recurrent neural networks using Lyapunov exponents. They claim to offer a simple and effective training for the chaotic data. Some simulation results are provided.,"The paper looks at the asymptotic behavior of the Jacobian of various RNN variants (standard, LSTM, GRU, PWL-RNN) when realizing stable vs chaotic dynamics. In particular, the paper shows that in the chaotic setting, the gradients asymptotically explode, i.e., when learning chaotic dynamics, the gradients have to explode (asymptotically). The paper proposes to overcome this limitation by truncating the BP length and applying a teacher forcing method that periodically projects the observation onto the hidden state during training.",0.3116883116883117,0.1038961038961039,0.16883116883116883,0.051181102362204724,0.1141732283464567,0.21212121212121213,0.09448818897637795,0.24242424242424243,0.1625,0.3939393939393939,0.3625,0.0875,0.14501510574018125,0.14545454545454545,0.16560509554140126,0.09059233449477351,0.17365269461077842,0.1238938053097345
511,SP:70c96a9840083caba1a301577bd6d7c5f42fb121,"This paper presents a variational autoencoder model for learning representations of tokens in a sequence of text. The authors build upon prior work which argues that the point-estimate representations learned by Transformer-based language models are degenerate, i.e. they only populate a narrow subspace. The authors address this problem by learning Gaussian-distributed representations (instead of point-estimates) which are regularized using a KL divergence loss. Empirical results on classification tasks are competitive with point-estimate baselines.","To obtain an isotropic word embedding space, the authors proposed a new transformer-based autoencoder in which each word is represented as a normal distribution, and a variational loss is used to make it closer to an isotropic normal distribution. By carefully adjusting the weights of the variational loss (magnitude of variance), the learning converged, and an isotropic latent representation was obtained. They also tried to show some of the advantages of the proposed approach: 1. The proposed model can variationally sample sentences with slightly different meanings from the input sentences. 2. In some cases, the proposed model can generate a semantic ""interpolation"" between two given sentences. 3. Solving sentence-level tasks using word representations obtained by the proposed model outperforms MiniBERT.","This paper proposes to add a variational loss for each token, in the transformer architecture, to increase isotropy of deep language models. To achieve stable training, the paper proposed to adjust $\sigma$ of the prior Gaussian distribution, and obtain a smoother training curve. The paper evaluated the proposed method from different aspects, including variational sampling, interpolation, semantic textual similarity and semantic classification tasks.","Provide a brief summary of the paper and its contributions.  This paper proposes Variational Auto-Transformer to encourage isotropy in the latent representation space. The resulting encoder-decoder architecture allows interpretable embeddings. On various tasks, VAT shows better performances than other language models.  Contributions:  - A novel architecture based on Transformer with a token-level variational loss. - Extensive evaluations on the effectiveness of Transformer.",0.1518987341772152,0.189873417721519,0.1518987341772152,0.13114754098360656,0.09836065573770492,0.1746031746031746,0.09836065573770492,0.23809523809523808,0.19047619047619047,0.25396825396825395,0.19047619047619047,0.1746031746031746,0.11940298507462686,0.2112676056338028,0.16901408450704225,0.17297297297297295,0.12972972972972974,0.1746031746031746
512,SP:70d963a83f76df6022563ec3f4e4fdacd7943d8a,"The paper focuses on the sample complexity of learning to select Chvatal-Gomory cuts for integer linear programming. We assume that there is an unknown distribution that generates ILP instances. CG cuts are parametrized by a set of weights, one per constraint. How large should the set of training instances be for one to accurately estimate the ""goodness"" of a given parametrization? This is the main question tackled here.  Using the data-driven algorithm design framework of Balcan et al. [8], this paper shows that three flavors of the learning problem can be analyzed effectively. The main contribution is to show that there is structure to the cut generation process as its parameters vary; the space of possible cuts can be partitioned, the form of the boundaries that determine the partition is identified, and the behavior of the cut generation is constant within each region. These can be plugged into a very general PAC learning bound from Balcan et al. [8].  Additionally, the sample complexity of generic tree search is analyzed. It is shown that variable, node, and cut selection can be parameterized simultaneously, each with its own additive scoring function, and sample complexity bounds can be derived accordingly. This result generalizes a previous branching-only bound from Balcan et al. [5]. ","Linear integer programming is an incredibly important algorithmic tool that is used widely in practice.  In general the problem is NP-hard and so we have to resort to heuristics. These heuristics often use the branch-and-cut framework: we branch on ""guesses"" on variables and we cut by adding valid inequalities that tightens the linear relaxation of the integer program.  When dealing with such heuristics, an obvious question arises: which cut should I add and on which variable should I branch? The goal here is to limit  the size of the search tree and therefore getting a good running time. There has been a growing amount of experimental work on using ML algorithms to guide this choice.    The paper under submission considers this problem from a more theoretical perspective. The main results bound the sample complexity for finding ""good"" cuts complementing and generalizing a prior work that studied this question for variable selection.","This paper studies the sample complexity of cutting plane selection in branch-and-cut algorithms for integer programming problems. The authors study a handful of different settings, with the goal of characterizing the intrinsic difficulty of learning: (i) the tree size upon adding a single cut at the root, (ii) the tree size upon adding a sequence of cuts at the root, (iii) the tree size upon adding a series of waves of cuts at the root, and (iv) the score of cuts according to a set of predefined scoring rules. Finally, the authors generalize the results to bound the sample complexity of generic tree search policies with sequential operations that can be scored.","Combinatorial optimization solvers often add cutting planes during solving, which are additional constraints in the problem that accelerate solving. These cutting planes have a major impact on the size of the final branch-and-bound tree, and therefore of the speed of the overall solving, but their impact is badly understood theoretically. At minimum, it is empirically understood that the size of the branch-and-bound tree is a complicated and sensitive function of the parameters of the cutting planes that were added during solving: a slightly different cutting plane might have been much more, or much less, effective.  In this paper the authors look at the problem of trying to learn these functions that relate the size of the branch-and-bound tree (a measure of solving performance) as a function of the instance data (A, b, c) when cuts with given parameters u_1, ..., u_k are added to the problem. They compute an order of magnitude of samples required to learn well this function, both for a single cut, a sequence of cuts, or rounds of sequences of cuts (as are used in practice). They also compute the number of samples required for the generalized problem of predicting tree size as a function of the instance data (A, b, c) for an algorithm that would pick the cut parameters to maximize a convex combination of scores, for given algorithm hyperparameters (the convex weights.) Finally, they do the same for more general algorithms that would pick an action, during solving, by maximizing a convex combination of scores, such as when doing node selection or variable selection.",0.13679245283018868,0.1792452830188679,0.17452830188679244,0.16233766233766234,0.22077922077922077,0.30701754385964913,0.18831168831168832,0.3333333333333333,0.13805970149253732,0.21929824561403508,0.12686567164179105,0.13059701492537312,0.15846994535519124,0.2331288343558282,0.15416666666666665,0.18656716417910446,0.16113744075829384,0.1832460732984293
513,SP:70e7b407d616050340c1035969577b6fa0aa9fa5,"The paper discusses binary classification when the labels can be corrupted by noise in an active learning scenario. In this direction the authors show that in a benign corruption setting the, a known algorithm, which the authors call RobustCAL, can achieve pretty-much the same label complexity as in a non-corrupted setting. However, the algorithm may fail in general and the authors propose a variation of this algorithm in order to be able to deal with the general case. This latter algorithm, though it requires more labels compared to RobustCAL, nevertheless it does come with good theoretical properties.  ","The authors consider robust active learning. They demonstrate that an existing algorithm Robust CAL can be tricked into permanently discarding the optimal hypothesis by large amounts of corruption early on during learning (followed by no corruption to keep overall levels low). The authors repair this flaw by modifying the algorithm to do a soft-elimination so that no hypothesis is permanently discarded. However, the cost is a higher unlabeled sample complexity, which also creates a weakness to increasing corruption. ","The authors study the problem of active learning in binary classification with the presence of agnostic noise in the streaming model. The adversary chooses a priori some corrupted distributions $D_t$ for each round $t$. The goal is to design an algorithm that outputs a hypothesis $h$ that minimizes the expected risk using a few labeled samples. In this work, they assume that they do not have any a priori information on corruption. The authors proceed by first analyzing the ERM for the passive learning for using it as a benchmark for the rest of the algorithms. The authors analyze two algorithms the RobustCAL (which has been analyzed extensively in previous works) and the CALruption which is the new algorithm they propose. For RobustCAL they show that as long as the corruption is bounded enough (the number of corruptions is less than $t/8$) in all the $t=1,2,4,...,2^k$ then the classic RobustCAL along with a regularization term will output a hypothesis that achieves expected risk competitive with the passive learning, i.e., the RobustCAL will not eliminate the optimal hypothesis in any elimination step. Moreover, they show that the regularization term is necessary by showing that without this term the best hypothesis will be eliminated. In order to remove the assumption on the bounded corruptions, the authors suggest a new algorithm CALruption, that overcomes this issue. However, this algorithm in the general case outputs a hypothesis $h$, such that $R(h)-R^* \leq \epsilon +O(\bar{C}/n)$ where RobustCAL gets $R(h)-R^* \leq \epsilon +O(R^* C/n)$, $\bar{C}$ is the number of corruptions in each epoch with some weights if the corruptions are large or small. Furthermore, if the corruptions are bounded enough in each epoch, then the difference of expected risk can be improved to $\epsilon +O(R^*C/n)$. ","This paper considers active learning with an ""oblivious adversarial model"": before the start of learning, an adversarial chooses a series of distributions $P_t(Y\mid X)$ , and at time t the label $Y$ would be drawn from that distribution. The adversarial level is characterized by $C_{total}=\sum_t \sup_x |P_t(Y\mid X=x)-P(Y\mid X=x)|$ where $P(Y\mid X=x)$ is the distribution over which the error for the model is calculated.   It first shows an error bound for passive learning with ERM. Then, it shows a slightly modified version of CAL (disagreement based active learning) could tolerate around $C_{total} <= n/8$ and achieve an error of $R^* + \epsilon + O(R^* C_{total}/n)$ where R* is the best error achievable without corruption and n is the number of unlabeled samples. Its label complexity is similar to that of the original CAL.   To tolerate any level of adversarial, instead of shrinking the version space permanently, it proposes a new algorithm that queries examples with some carefully designed probabilities. It gives its label complexity bounds, and shows that (1) its label complexity matches standard CAL without corruption; (2) it can tolerate higher corruption ($C_{total}>n/8$) while the previous modified CAL cannot; (3) its label complexity is slightly worse when the adversarial corruption is not high (e.g. the mispecification case where the probability that the adversarial corrupts the label in each round is uniformly upper bounded).",0.1414141414141414,0.3434343434343434,0.20202020202020202,0.27848101265822783,0.21518987341772153,0.15483870967741936,0.17721518987341772,0.10967741935483871,0.08097165991902834,0.07096774193548387,0.06882591093117409,0.19433198380566802,0.15730337078651688,0.1662591687041565,0.11560693641618497,0.11311053984575835,0.10429447852760736,0.17235188509874327
514,SP:7109d442003054fff3d0c8f90b0d76520aaef858,"This paper studies how to ""fairly"" select a single candidate from a repeated i.i.d. classification perspective ""selection process"". They contribute a new fairness notion that is unique in the selection setting, however, it is not that different from previously contributed group fairness notions. They provide sound technical methods to post-process predictors to satisfy the fairness notion and consider selection in the private attribute setting.  However, the selection setting is not interesting enough without decisions at time step i affecting those at time step i+1, moreover, as the paper studies the  single selection setting, it is not that different from just classification.  The paper is novel and interesting in it's combination of all those elements, but there is no sufficient novelty in each direction alone to stand out. The paper is clearly written and easy to follow and technically sound. I think if the paper had a slightly more realistic selection setting (any sort of dynamics from one step to another), then it would be a clear accept. But without that, I think the paper is borderline.  ","This paper introduces a new fairness notion called Equalized Selection Rate (ESR) for sequential selection problems in which individuals are seen by the decision-maker sequentially, and there are a limited number of positive decisions available. For example, in a job acceptance setting, individuals are applicants submitting resumes sequentially, and the process ends when the decision-maker has accepted $m$ applicants. The authors show that a pre-trained model able to satisfy statistical parity or equal opportunity can be unfair in this sequential setting. A post-processing method is provided to circumvent this issue. Moreover, the authors show that when sensitive attributes are perturbed according to a local differential privacy notion, ESR fairness is still obtainable. Lastly, an empirical analysis is performed comparing equalized odds and ESR over two real data sets. ","The paper proposes a notion of algorithmic fairness for the setting of sequential selection. Namely, a situation where candidates arrive one after the other, and a learner aims to select some number of whom. The authors propose a notion they term “Equalized Selection Rate (ESR)"" which requires that the probability of an accepted qualified candidate is the same across groups in the distribution. The authors further suggest a post-processing approach for deriving a predictor that obeys the ESR constraint via solving a linear program (for fair selection using a binary classifier) or using a Bayesian optimization approach (for fair selection using qualification score). Additionally, the authors consider a case where for privacy concerns, the protected attribute is only accessible after noise is added (for predicted individuals), and show that perfect ESR is still attainable (when training the model on individuals for which the unnoisy protected attributes are included). The paper concludes with empirical evaluation of the proposed approaches.","This paper introduces a new task of making fair decisions in sequential selection problems. The authors have shown that existing fairness notions are not suitable for this problem and could lead to discrimination. Then the authors propose a new fairness notion called ""Equalized Selection Rate (ESR)"" for sequential selection problems and a post-processing approach is designed to satisfy the ESR constraint. The paper considers 2*2 different settings, i.e., whether the pre-trained model provides a binary decision or a continuous score, and whether the actual or noisy sensitive attribute is accessed. The paper also conducts experiments on FICO and Adult datasets.",0.12154696132596685,0.13259668508287292,0.143646408839779,0.21212121212121213,0.22727272727272727,0.2138364779874214,0.16666666666666666,0.1509433962264151,0.25,0.1761006289308176,0.28846153846153844,0.3269230769230769,0.14057507987220447,0.1411764705882353,0.1824561403508772,0.19243986254295534,0.25423728813559315,0.25855513307984795
515,SP:712a078db23923f8273098918729300a3521e967,"This paper proposes an approach, Sageflow, that jointly handles stragglers and Byzantine users in FL. Sageflow groups the users based on their staleness to handle stragglers and uses entropy-based filtering and loss-weighted averaging to provide Byzantine-robustness. This approach depends on the assumption that the server has access to a public dataset or the users share part of their data with the server to to identify the Byzantine users. Several experiments show that effectiveness of the Sageflow against some of the prior approaches.  ","The paper addresses a scheme to protect federated learning against stragglers and adversaries at the same time. For straggler resistance, the authors propose to group and weight the model coefficients according to their age. For resistance against adversaries they propose entropy-based filtering and loss-weighted averaging. Combining these schemes leads to their proposed Sageflow approach. They also provide an upper bound on the expected loss function error after a fixed number of operations.","This paper proposes a robust FL approach to deal with stragglers and adversaries in federated learning. Model grouping and staleness dependent weighting of models is done at the server to deal with stragglers. For model poising attacks, entropy-based filtering is proposed, where a client model entropy is calculated on a public dataset stored at the server. Same dataset is used to calculate loss of different client models at the server to identify clients contributing data-poisoned models. Theoretical bounds on convergence of the proposed method are obtained. Experimental results with three image classification datasets show that the proposed approach outperforms existing solutions to deal with stragglers and adversaries in FL. ","This paper studies and the interesting problem of presence of both stragglers and adversaries in Federated Learning (FL), and proposes a solution that simultaneously addresses both issues. There's no question that the challenges considered in this paper are important to the FL community. The solution is based on grouping the late model update from stragglers, incorporating them into the model update, and performing an entropy-based filtering to find the adversarial updates. The paper has really interesting and comprehensive experiments, and it is backed by theoretical findings. ",0.2235294117647059,0.3176470588235294,0.2235294117647059,0.24324324324324326,0.17567567567567569,0.18018018018018017,0.25675675675675674,0.24324324324324326,0.2159090909090909,0.16216216216216217,0.14772727272727273,0.22727272727272727,0.2389937106918239,0.2755102040816327,0.21965317919075145,0.1945945945945946,0.16049382716049385,0.20100502512562815
516,SP:714e476870475da51a041760d22dfc0c6e0aa20e,"This paper proposes online moment selection, an approach for choosing which data sources to draw from in an online setting. The goal is estimation of causal effects, the structural causal model is encoded as moment conditions, and the data sources drawn from are chosen to minimize the variance of the estimate. Explore then commit and explore then greedy approaches are suggested and analyzed. A limitation is that the structural equations need to be parametric and have a clear noise term. Improvement is shown over simple fixed policy baselines.","The paper proposes online moment selection (OMS), a framework in which structural assumptions are encoded as moment conditions. This algorithm allows users to actively acquire data (variables) given the very moments that identify the functional of interest. The paper shows that there is substantial empirical gains in doing so.","The asymptotic property of GMM estimator is proposed by some previous papers. In this paper, from the theoretical aspect, the authors generalize that to the setting with multiple domains and apply the results in the online estimation of causal effects. They evaluate the regret under three online policies to select which dataset to observe. ","Given multiple data sources, each related to a subset of variables, authors study the problem of deciding which data source to sample from at each time step. Then, by using the samples, the goal is to estimate the model parameters as good as possible. The proposed framework is based on the generalized method of moments. Their contributions are as follows:  1. In Section 4, it is assumed that each instance has the same cost across all data sources. Then, authors first show that any fixed policy that differs from the optimal one will attain a constant asymptotic regret. To overcome this issue, two adaptive methods are proposed OMS-ETC and OMS-ETG. Here, OMS-ETC resembles the classical explore-then-commit algorithm in the context of bandits, where there is an exploration step to estimate the model parameters and then the model commits to a data source that minimizes the asymptotic variance of the target parameter. In contrast, OMS-ETG keeps updating the model parameters as more data is gathered. Under certain conditions, authors show that both adaptive methods will attain a zero asymptotic regret.  2. In Section 5, both adaptive methods are modified as to attain zero regret in the case where instances have different costs across data sources.  3. Experiments are provided in Section 6, where authors depict the behavior of the regret for different causal DAGs. Semi-synthetic dataset are also provided to showcase the usability of their method.",0.19318181818181818,0.13636363636363635,0.3068181818181818,0.16326530612244897,0.2857142857142857,0.35185185185185186,0.3469387755102041,0.2222222222222222,0.1115702479338843,0.14814814814814814,0.05785123966942149,0.07851239669421488,0.24817518248175183,0.16901408450704225,0.16363636363636364,0.15533980582524273,0.09621993127147767,0.12837837837837837
517,SP:71663f246b1afd9c100b4bd3dbee48a0ce038aab,"This paper provides a practical guide on how to train an NN-based controller for locomotion tasks with robustness and efficiency.   The tasks are categorized into motion primitives including running, jumping, and crawling, each with a manually designed objective function. Considering the periodic nature of locomotion, the author proposed to use sine activation layers and periodic signals in different phases as input. The experiments show that a larger batch size can lead to faster convergence. Compared to SGD, this paper points out that Adam is better for the tasks. The supplementary video presents that the trained networks enable real-time control of complex skills. ","This paper proposes to use differentiable simulators for learning locomotion skills for deformable characters. The key contribution is the experiments and analysis of a large number of design choices, including activation function, reward design, optimizer, batch size, etc. The paper finds that by combining differentiable simulator with the right design choices, the character can learn general and complex skills with one policy. The paper also shows that this combination can significantly outperform PPO.","The paper proposes to learn neural network controllers in a differentiable simulator for locomotion tasks. A set of technical improvements are made and make the controllers more robust and require fewer samplers to learn. These include: using periodic activation functions, design of loss functions, and using large batch size. The paper shows learned locomotion skills of running, jumping, and crawling for different soft robots, which can be interactively controlled. ","This work proposes to learn locomotion skills on soft robots made of springs. It proposes to leverage differentiable physics along with a NN controller based on SIREN, which allows to directly learn policies by minimizing loss functions defined on trajectories. The methods allows to learn locomotion and jump behaviors on a various set of 2D and 3D mass-spring systems.",0.17307692307692307,0.14423076923076922,0.09615384615384616,0.2191780821917808,0.1506849315068493,0.18840579710144928,0.2465753424657534,0.21739130434782608,0.16666666666666666,0.2318840579710145,0.18333333333333332,0.21666666666666667,0.2033898305084746,0.17341040462427743,0.1219512195121951,0.22535211267605632,0.16541353383458646,0.20155038759689922
518,SP:718ba6d20ecab8e69e960b7aa91082353eec5987,"This paper proposes a generalization and extension to deep matrix factorization as presented in the former paper from [Arora et al, NeurIPS 2019]. The extension allows more complex model which include inverse problems. The generalization part is build on a ""vanishing"" regularization which leads to better dynamics and convergence. These results are given by a theoretical analysis which highlight the effect of the proposed model. Finally the experiments illustrate the advantage of the model compared to state-of-art methods. ","In this manuscript, the authors consider matrix completion problems. Leveraging recent advances on implicit regularization in (deep) matrix factorization problems, a new architecture for matrix completion is proposed. Specifically, the authors parameterize the unknown low-rank matrix as a deep linear network, which has been shown to exhibit a low-rank bias when learned via gradient descent. Additional regularization terms based on the Dirichlet energy are added to encourage other structural priors in the recovered solution, such as self-similarity between columns or blocks. The underlying Laplacian matrix is parameterized by learnable weights. The authors analyze the dynamics of gradient flow applied to their optimization problem and show empirically that this approach improves performance in certain settings.",This paper studies the matrix completion problem where the goal is to recover the matrix from partially observed elements.  The proposed approach involves parameterizing the unknown matrix by deep matrix factorization and adaptive regularizers that are parameterized with deep neural networks. The authors studied the Adaptive and Implicit Regularization of the proposed approach which is called AIR-Net. Experiments are provided to demonstrate the effect of the proposed approach. ,"This paper studies the problem of matrix completion using neural networks and deep matrix factorization as implicit and explicit regularization. The paper proposes a general framework and studies a specific case, namely, when the regularization is a form of Dirichlet Energy on the rows and columns of the matrix, and the matrix is formed as a product of L matrices. The results indicate improved performance of matrix completion under a variety of corruption methods and for a number of data sets. ",0.2,0.2125,0.2625,0.15384615384615385,0.18803418803418803,0.2608695652173913,0.13675213675213677,0.2463768115942029,0.25925925925925924,0.2608695652173913,0.2716049382716049,0.2222222222222222,0.16243654822335027,0.22818791946308725,0.2608695652173913,0.1935483870967742,0.2222222222222222,0.24
519,SP:7191fd3ad3b0e1a20bc4b2ad76ed7fca60c73809,"The paper proposes to have a closer look at (logit matching) knowledge distillation, and explores whether the gains are uniform across all portions of the test data or if instead there are gains on some portions of the data but damaged performance on others. This is justified by the potential of KD to amplify biases. The authors quickly show that indeed the gains are not uniform, some classes being affected negatively by KD so that the bad performance of the teacher on some classes is amplified on the student. Then the authors conduct a number of experiments to study the impact of different aspects of training (e.g. architectures involved and aspects of data like class imbalance) over this amplification of class-specific poor performance. They are then able to identify what classes are likely to be negatively affected and follow that with the proposal of two modifications of the basic KD formulation with the aim to improve KD effect on the worse-performing classes. Experiments focus on CIFAR-100 and ImageNet, including long-tail variants.","This paper identified an underexplored problem with distillation, fairness of distillation on different subgroups. This paper further proposed two methods to tackle this challenge. Experiments are conducted on CIFAR-100, ImageNet, and their long-tailed variants.","- This paper is established by a discovered phenomenon that in distillation, not all the classes’ performance is improved although the student is improved significantly.   - Thus, the authors explore a method that focuses on promoting the performance of the poorly-performed labels, which also lead to the overall promotion while distillation.  - The authors propose two methods to achieve it, one is the distillation with adaptive mixing weights and the other is the distillation with per-class margins. ","In this work the authors examine the impact of distillation on certain sub-groups of the data. More specifically, they demonstrate that even though distillation can increase the overall accuracy of a model, it can harm the accuracy on certain subclasses. This includes both self-distillation, as well as regular distillation setups. To overcome this, the authors propose using class-specific weights, that have been tuned according to a holdout set, to tune the importance of each class during distillation, as well as using softmax cross-entropy with margins tuned according to the difficulty of each class. Furthermore, it is also demonstrated that these phenomena can also impact the fairness of the model. ",0.08522727272727272,0.13636363636363635,0.14204545454545456,0.25,0.2222222222222222,0.25,0.4166666666666667,0.3157894736842105,0.22123893805309736,0.11842105263157894,0.07079646017699115,0.168141592920354,0.14150943396226412,0.19047619047619047,0.17301038062283738,0.16071428571428573,0.10738255033557045,0.20105820105820105
520,SP:71a736c1a398f37564ca3c1f0ce909cd8835de1f,This paper presents an anomaly detection and localization method based upon sparse dictionary learning on the activations of a pretrained neural network.  The method uses the K-SVD algorithm and modified OMP to solve the sparse coding problem.  The method is compared on industrial anomaly datasets and shows state of the art performance on detection w/ respect to other published work.    The contributions of the paper are the use of dictionary learning on the output of a pretrained CNN. State of the art anomaly detection on industrial datasets. Extensive justification of hyper parameter/layer decisions of the model.,"This paper tackles image anomaly detection by CNNs as feature extraction and sparse dictionary learning as its classifier solved by KSVD. In reference, the sparse coding can be effectively solved by OMP and its reconstruction errors denote the anomaly scores of images. As this paper states,  it is the first work to use a sparse dictionary learning algorithm to handle deep network features. Also, the proposed method achieves the sota performance.","This paper proposes an anomaly detection algorithm based on dictionary learning on the representations obtained by pre-trained deep convolutional networks. More precisely, the authors consider location-specific signals from a collection of training images, extracted via pre-trained neural networks of different depth and types. They then train a dictionary via K-SVD to obtain sparse representation for these patches, which are then modeled via multivariate Gaussian distribution. At testing time, the representation for each patch is computed, and a metric is computed over all patches to declare an image as an outlier/anomalous, or normal. ","This work combines sparse representation learning and a pre-trained neural network for classification purposes. Firstly, meaningful representation features are extracted and a sparse dictionary of the most expressive ones is built, which only come from the images without anomalies. Then, the images having anomalies will either contain a non-sparse representation as linear combination of the dictionary elements or a high reconstruction error. ",0.21428571428571427,0.20408163265306123,0.16326530612244897,0.2112676056338028,0.15492957746478872,0.15463917525773196,0.29577464788732394,0.20618556701030927,0.25,0.15463917525773196,0.171875,0.234375,0.24852071005917162,0.20512820512820512,0.19753086419753088,0.17857142857142855,0.16296296296296295,0.1863354037267081
521,SP:71f1f19f7a2203ad85976543ff46c35c469b9be1,"This paper has made the following contributions. Firstly, this paper illustrates how simply tuning hyperparameters based on non-private training runs can leak private information. Second, this paper provides privacy guarantees for hyperparameter search procedures within the framework of Renyi Differential Privacy. Their results improve and extend the work of Liu and Talwar (STOC 2019).","This paper studies the problem of hyper parameter tuning in the setting of differentially private training of machine learning models. The paper first shows that the hyper parameters used to train a model and the corresponding utility can leak information about training (through experimenting with outliers). They then go on to introduce and analyze ways that would help release DP models trained with the ""best"" set of hyper parameters, with small privacy leakage. If for hyper parameter tuning, $m$ models are trained with DP, then the leakage for releasing  the best model would be $m\epsilon$ with simple composition. The  method introduced by the paper, however, builds on prior work by Liu&Talwar and improves this to become $2\epsilon$, through randomly choosing and running hyper parameter settings. ","The paper provides an considerable improvement to the DP analysis of hyperparameter tuning of DP algorithms (such as DP-SGD). The analysis is carried out using Rényi differential privacy (RDP), and the DP bounds are RDP bounds that contain the RDP parameters of the underlying mechanisms (that give the private candidates). Most importantly, the paper considerably improves the state-of-the-art of Liu and Talwar (2019). Also, a nice counterexample using SVMs is constructed, that shows the importance of this problem. ","The authors make the following contributions regarding differentially private hyperparameter tuning : * As an example, the authors train an SVM with a weight penalty and show that, in presence of an outlier, a membership inference attack can be employed to infer from the hyperparameter whether or not the outlier was a part of the training set. *The authors provide an algorithm for private hyperparameter tuning, which consists of running a learning algorithm that satisfies Rényi differential privacy (RDP) for a *random* number of times ($K$), each with a hyperparameter drawn uniformly at random from a finite candidate set. The authors first prove RDP guarantees of the algorithm when $K$ is sampled from a truncated negative binomial distribution and Poisson distribution. Then they proceed to prove RDP guarantees for any distribution of $K$ supported on $\mathbb{N}\cup \{0\}$ ¹. The authors propose a way to measure the utility of the algorithms by looking at the expected quantile of the output. The results of their utility analysis, coupled with an experiment on MNIST, show that the algorithm with Poisson distribution performs better than those proposed by Liu and Talwar (2019) in an intermediate range of privacy budget ($\varepsilon$).  ¹ From what I understand, we can obtain a tighter generic bound by going through the proof of Lemma 7. But the authors opt to use (7) since the postprocessing often leads to a bound that is independent of $q$ and $q'$ when plugging in a specific distribution. ",0.2909090909090909,0.2545454545454545,0.32727272727272727,0.140625,0.21875,0.3855421686746988,0.125,0.1686746987951807,0.07407407407407407,0.21686746987951808,0.11522633744855967,0.13168724279835392,0.17486338797814208,0.20289855072463767,0.12080536912751678,0.17061611374407581,0.1509433962264151,0.196319018404908
522,SP:7225beb458ed9c79d8af081c15d74a63e56655ab,"This paper proposes a bilevel optimization method based on unrolled gradient-descent iterations of the lower-level (LL) problem, while providing a theoretical connection between the ideal bilevel optimization and the unrolled one when LL problem might not be convex. The main idea is two-fold: Regarding the initial point (i.e., z) of LL variable as another variable in the UL problem, and taking the pessimistic value from the unrolled iteration, i.e., max_k F(x, y_k(x, z) ). The theoretical derivation presents that the ideal bilevel problem (min phi(x)) is asymptotically equivalent to the unrolled version (min phi_K(x, z)). The resulting algorithm is very simple: (1) perform a few unrolled steps (with computational graph) for LL, (2) take the pessimistic estimate (y_k) from the steps, (3) and perform gradient descent for x and z for the UL problem based on the y_k(x, z). The paper also proposes a Nesterov version (for LL iterations). The proposed method shows state-of-the-art performance for a toy problem and two applications.","This work proposes a truncated unrolling type method for solving a bilevel optimization problem with non-convex lower-level problem and the convergence of the proposed method is also shown. Compared to the results in the existing literature, the convergence analysis in this work does not require any convexity assumptions on both upper level and lower-level objective functions. Efficiency of the proposed method is shown in the numerical experiments of artificial problems and real application problems.","This paper presents a first-order algorithm for solving bilevel optimization problems where the lower-level problem is non-convex. The proposed algorithm, named as IAPTT-GM, in my understanding, is roughly alternating projected gradient descent in the upper level and lower level variables, combined with two key ideas, namely initialization auxiliary and pessimistic trajectory truncation.  This paper provides theoretical analysis of IAPTT-GM, supported by sufficient numerical experiments, both on toy problems and real datasets.   This paper is well motivated and well written. The theoretical part is clean and concise, while the numerical experiments are well conducted, together with texts to describe the main takeaways.  In general, I like this paper and want to see it published. However, I am a bit skeptical about the theory. I may downgrade my score if there are severe theoretical issues. See main review.","The paper introduces a gradient-based bilevel optimization algorithm called IAPTT-GM that does not require the non-convex inner-level assumption. Specifically, the authors propose two methods: (1) initialization auxiliary that guides the optimization dynamics and (2) pessimistic truncation operation to build theoretical convergence. The authors theoretically show convergence of IAPTT-GM that does not assume the non-convexity of the inner-level problem and empirically show that the proposed method has improved performance on few-shot classification and hyper cleaning compared to previous approaches. ",0.1340782122905028,0.1452513966480447,0.12290502793296089,0.33766233766233766,0.2597402597402597,0.1702127659574468,0.3116883116883117,0.18439716312056736,0.2558139534883721,0.18439716312056736,0.23255813953488372,0.27906976744186046,0.1875,0.1625,0.16603773584905662,0.23853211009174313,0.24539877300613497,0.21145374449339208
523,SP:7267976d63243268534c227b3670320a5842042d,"This paper studies the smoothness of the functions represented by single variate two-layer neural networks at minima with different flatness, and connect this smoothness with the implicit bias of (stochastic) gradient descent to flat minima. Firstly, for a twice differentiable minimum found by SGD, an upper bound for the leading eigenvalue of the loss function's Hessian matrix (with respect to parameters) is derived depending on the learning rate. Then, a weighted L1 norm of the second derivative of the model function (with respect to input data) is upper bounded by the Hessian's leading eigenvalue. Combining these two bounds, the weighted L1 norm of the model function's second derivative is bounded by a quantity depending on the learning rate. This result implies that SGD with bigger learning rate finds flatter minima, and thus finds smoother functions. The result only considers the minima found by optimization algorithms, hence it is independent with initialization.   For neural networks with ReLU activation, minima are sometimes not differentiable. The analysis above is extended to this case and similar bounds are also provided. The theorem works in the mildly over-parameterized case in which the number of neurons is slightly bigger than the number of training data.   Numerical experiments show that GD with bigger learning rate can indeed find smoother functions, especially in the region where training data are densely distributed. This observation is consistent with the theoretical results. ","The goal of this paper is to extend existing results on stable minima found by SGD to the non-differentiable setting of ReLU networks.   In particular, in its main contribution (Theorem 1) the paper shows that the solution found by SGD for 1-dimensional ReLU networks has bounded second derivative, under a weighted total variation norm. Moreover, this weighted norm is controlled by the step-size of SGD.  This implies that also for ReLU networks, SGD with large learning rate is biased towards smooth solutions. "," This paper attempts to answer the implicit bias of SGD via a stability analysis on a single hidden layer univariate ReLU network. The main result is that a stable solution that can be found by a larger learning rate must be smoother. In detail, they show the weighted L1 norm of the second derivative is upper bounded by roughly $2/\eta$, where $\eta$ is the learning rate. ","This paper studies a single hidden layer univariate ReLU network. The authors show that stable solutions learned by SGD will be functions whose second derivative has bounded weighted L1 norm. Besides, the learned function gets smoother as the learning rate increases.",0.1228813559322034,0.11440677966101695,0.0847457627118644,0.2235294117647059,0.16470588235294117,0.31343283582089554,0.3411764705882353,0.40298507462686567,0.4878048780487805,0.2835820895522388,0.34146341463414637,0.5121951219512195,0.18068535825545173,0.17821782178217824,0.1444043321299639,0.25,0.2222222222222222,0.3888888888888889
524,SP:7278c9fb91ea682240a947c760d23e4a1aa1701d,"The paper deals with the analysis of the learned optimizers (LO). In particular, it tries to isolate their properties and distinctions with respect to the existing baseline optimizers.  It is true that a nascent field of learned optimizers, while enjoying good performance, currently lacks in depth analysis of why the results work. This paper doesn't answer all the questions, but it attempts to bridge the gap between a mysterious performance of LO and known well analyzed optimization techniques like momentum, gradient clipping, LR schedules and adaptation.  The paper uses 4 different tasks for the analysis (Linear regression, Rosenbrock, Two moons and MNIST) and uses a simple RNN optimizer for a LO.   The presented empirical result is analyzed from a theoretical perspective of a general LO-state update function. The theory has a very mild assumptions on the LO for analysis (essentially only that they have to converge to the fixed point).   The paper is mostly well written, however, some parts are hard to read, misleading or not explained well (see bellow).","In this work, the author got trained learned optimizers on four different optimization tasks, and analyzed their behavior. They discovered that learned optimizers learn a plethora of intuitive mechanisms: momentum, gradient clipping, schedules, forms of learning rate adaptation. While the coarse behaviors are qualitatively similar across different tasks, the mechanisms are tuned for particular tasks.","This submission focuses on analyzing the behaviors of learned optimizers. The goal is to elucidate properties and patterns learned by such learned optimizers, in order to better analyze, understand, and lay groundwork to potentially improve the behavior of learned and/or manually-designed optimizers. A novel reverse engineering approach is taken and identifies learned interpretable and fundamental behavior such as momentum, gradient clipping, learning rate schedules, and learning rate adaptations.  The submission trained learned optimizers on each of four representative tasks: Linear Regression, Rosenbrock, Two Moons, MNIST . These tasks were selected because they are fast to train, cover a range of loss surfaces (convex and non-convex, low- and high-dimensional, deterministic, and stochastic). Additionally, three baseline optimizers (momentum, RMSProp, and Adam) were tuned individually for each task. The learned optimizers outperformed the baseline optimizers on the meta-objective.  The submission uses two primary methods to analyze optimizer behavior and mechanisms: 1) visualize optimizer update function at a particular optimizer state. 2) analyze optimizer dynamics around fixed points.  The study was able to show that learned optimizers implement classical momentum and a soft form of gradient clipping. The paper visualizes and provides a careful explanation for the best learned optimizer for Rosenbrock (others in appendix) for both. The report also analyzes learned behavior for learning rate schedules and learning rate adaptation. The study isolated the behavior of tuning per layer and parameter type in the CNN for the MNIST problem.  The submission includes a detailed Appendix that illustrates the same behaviors for all the other learned optimizers not included in the main paper, along with other materials such as the meta-learning approach. ","The authors devise tools for investigating the behaviour of learned, gradient-based, optimization algorithms in order to understand better how they achieve their empirically validated high performance.   To this end, they visualize the update function, i.e., the change in parameters in dependence of the gradient input, which allows to identify mechanisms employed also by hand-designed optimizers, such as gradient clipping, momentum, and learning rate adaptation. The authors demonstrate that this visualization is particularly insightful when analyzing the update rule using the linearized optimizer state dynamics around fixed points. Furthermore, the authors identify that learned optimizers encode learning rate schedules through autonomous state trajectories.   Using their tools, the authors analyze one existing type of learned optimizer (the RNN-based approach proposed by Andrychowicz et al., ""Learning to Learn by Gradient Descent by Gradient Descent"") on four relatively simple problems (linear regression, Rosenbrock, two moons, MNIST). They find that (depending on the problem) the learned optimizer can exhibit well-known mechanisms like gradient clipping, momentum, learning rate adaptation, and learning rate schedules. Furthermore, they provide clues that the optimizer can learn parameter-type (bias/weights) and layer-specific learning-rate schedules in neural-network based problems.",0.11046511627906977,0.25,0.1686046511627907,0.43636363636363634,0.3090909090909091,0.17216117216117216,0.34545454545454546,0.1575091575091575,0.14795918367346939,0.08791208791208792,0.08673469387755102,0.23979591836734693,0.16740088105726872,0.19325842696629214,0.15760869565217395,0.14634146341463417,0.13545816733067728,0.20042643923240938
525,SP:72963364a461b1d3e5633e7f680e1a5966966924,"The paper presents Evolution Gym, a benchmark for the evaluation of design and control co-optimization algorithms on simulated voxel-based soft robots. It contains a back-end soft-body simulator (written in C++ with bindings in Python), a set of 30 evaluation tasks of various types and difficulty levels, 3 design optimization algorithms and 1 control optimization algorithm. The algorithms are evaluated on the benchmark tasks and the results are reported in the paper.","The paper proposes a framework for benchmarking algorithms for optimising both bodies and controllers of soft-bodied robots. It also presents three optimisation solutions and evaluate them on a suit of benchmark tasks. The authors also evaluate some hand-coded solutions; the performance of these is worse for some tasks, while about the same for others.","The paper proposes a benchmark for Evolving Soft Robots. The benchmark is designed to facilitate research in co-design of control algorithms and robot morphologies. The proposed framework relies on a custom physics engine which allows a simple mass-spring-damper connection of individual modules. The proposed benchmark is suitable for both manipulation and locomotion even though restricted to 2D environments only. Authors provide also an initial evaluation of SoTA design optimization algorithms (genetic algorithm, bayesian optmisation, CPPN-NEAT) on the proposed benchmark. ","In the paper, ""Evolution Gym: A Large-Scale Benchmark for Evolving Soft Robots"", the authors introduce a new framework for the benchmark for the evolution of the design of soft robots. This framework, called ""Evolution Gym, contains 30 environments (only 10 are described in the main paper), which cover different types of tasks (locomotion, manipulation, and a mixture of both) and different level of difficulties. In addition to the introduction of the framework, the paper also presents experimental results in which the authors have implemented three baseline algorithms (Genetic Algorithm, Bayesian Optimisation, and CPPN-NEAT) for the optimisation of the robot's morphology and PPO for the optimisation of the controllers.",0.2,0.29333333333333333,0.3333333333333333,0.2857142857142857,0.3392857142857143,0.3253012048192771,0.26785714285714285,0.26506024096385544,0.22522522522522523,0.1927710843373494,0.17117117117117117,0.24324324324324326,0.22900763358778628,0.27848101265822783,0.2688172043010753,0.2302158273381295,0.2275449101796407,0.27835051546391754
526,SP:72b2158dfc32d9843e947f9798818e9dd49f71fa,"The paper proposes a meta learning approach to select the best trade-of between memory usage and bit precision for DNN training. This approach is evaluated on 87 datasets and 99 bit-precision configurations. The result shows that in general 6 bit floats (1,4,1) for activations and weights and 14 bit floats (1,6,7) for optimizer parameters is the best choice in term of memory and accuracy trade-off.  ","The paper proposes a two-step methodology for evaluating the error-memory trade-off when employing low-precision training/inference in neural networks. Although low-precision training/inference in deep learning is a widely studied topic, this paper seems to be the first one to study this error-memory tradeoff without exhaustive search.   The proposed idea is inspired from the multi-objective optimization field. The authors propose to identify the Pareto frontier that would allow a user to identify the optimal precision (lowest error) given a certain memory constraint. The proposed system has two phases: training and testing.   At training time, given a set of datasets, a network architecture and a set of low-precision configurations, the method samples dataset-configuration pairs to train and computes the misclassification error for each selected pair. Matrix factorization is used to compute low-dimensional embeddings for each configuration.   At test time, a subset of configurations is chosen for evaluation (using Design Experiments with Matrix Factorization) and the computed errors are used as input together with the configuration embeddings to a linear regression model in order to estimate the errors on the non-evaluated configurations. For both training and testing, the memory matrix is fully computed based on the network architecture and the low-precision format.  The method is evaluated in terms of convergence and Hypervolume difference of the true and estimated Pareto frontier and it outperforms in most of the cases other methods such a Bayesian Optimization or Random Selection with Matrix Factorization. ","The paper aims to find Pareto frontiers for the precision given memory budget. The overall objective is to allow ML engineers to select optimal precision that lets them train the desired model with less energy. To do this, the paper formulates the problem into multi-objective optimization problem and use meta-learning to minimize the number of low-precision training data points for this optimization. Then, the paper uses the term ""meta-test"" that refers to transferring the information from known tasks to estimate Pareto frontiers of low-precision training on new unseen tasks.","To achieve the goal of efficiently selecting the best low-precision configuration within the memory budget, this paper proposed Pareto Estimation to Pick the Perfect Precision (PEPPP) by using matrix factorization to find non-dominated configurations (the Pareto frontier) with a limited number of network evaluations. Although this paper presents a solution for low-precision training that has recently gained a lot of attention, there are some concerns as a result of my review of this paper. (Please refer to my main review)",0.3194444444444444,0.19444444444444445,0.1388888888888889,0.112,0.1,0.19148936170212766,0.092,0.14893617021276595,0.12048192771084337,0.2978723404255319,0.30120481927710846,0.21686746987951808,0.14285714285714285,0.16867469879518074,0.12903225806451613,0.1627906976744186,0.15015015015015015,0.2033898305084746
527,SP:72c36e855b8755e88465552b3a07c89ed0d99716,"This paper focuses on the multi-modal interaction problem, that multi-modal models tend to rely on just one modality while under-utilizing the other modalities. Since conditional utilization rate cannot be computed efficiently during training, they introduce an efficient proxy based on the pace at which a DNN learns from each modality, which we refer to as conditional learning speed. So they propose a training algorithm, balanced multi-modal learning, and demonstrate that it indeed addresses the issue of greedy learning. The proposed algorithm is found to improve the model’s generalization.","This paper investigates the modality greediness of learning in multi-modal deep neural networks (DNNs). The authors hypothesize that multi-modal DNNs learn to rely on one of input modalities (it could learn faster), which is expressed as a “greedy"" behavior.  The greediness is quantified with the difference between conditional utilization rate between modalities. They claim the followings: (1) conditional utilization rate can be surrogated with conditional learning speed, (2) the strong regularization in the training process encourages the greediness, and (3) the greedy behavior often lowers generalization performances.  As a remedy of performance decreasing phenomena, they propose balanced multi-modal learning methods, which control the training speeds of all of modalities to be within certain criterion via giving more steps considering the log-ratio of sum of the magnitudes of gradients of loss functions between of fusion model parameters and of unimodal model parameters. The paper provides experimental results to validate their claims and the effectiveness of the proposed methods. ","This paper hypothesizes that due to the greedy nature of deep learning, these models tend to rely on just one modality while under-utilizing the other modalities. The authors empirically observe this phenomenon on several dataset with a proposed metric. They propose an algorithm to achieve balanced training, which forcing the model to update only one of the uni-modal branches when the two uni-modal branches are imbalanced. This method is applied to three datasets and gain some progress.","The paper talks about efficient multi-modal training. They hypothesize that multi-modal training is greedy and perform experiments to empirically show the same. They propose metrics such as conditional utilization rate and conditional learning speed which serves as a proxy for the former to measure the imbalance in utilization between modalities while training. Based on conditional learning speed, they propose a multi-modal training algorithm that mitigates the imbalance in modality utilization. Through experiments, the authors empirically show that it is true and their training paradigm performs better than vanilla multi-modal training.",0.34408602150537637,0.3010752688172043,0.27956989247311825,0.15527950310559005,0.18012422360248448,0.2,0.19875776397515527,0.35,0.2765957446808511,0.3125,0.30851063829787234,0.1702127659574468,0.25196850393700787,0.3236994219653179,0.27807486631016043,0.20746887966804978,0.22745098039215686,0.18390804597701152
528,SP:72c7561b6b4b719e0bedfaec0eefb13917d81492,"This paper discusses how to transfer the knowledge from known step-by-step algorithms to learn new algorithms with only input and output pairs. The authors reach the conclusion: transfer learning does not help, but multi-task learning helps with the systematic generalization. The main contributions of this paper are: 1. A new benchmark for transferring algorithmic knowledge 2. Study on training and loss functions case by case for different graph algorithms and setup 3. Shows standard transfer learning technique fails on algorithm learning, but multi-task learning helps.","The paper studies a transfer learning setting for learning graph algorithms, where we do not have the execution traces for the target algorithm, but we have execution traces for a related algorithm. The paper uses an adapted version of NeuralExecutor (Velickovic et al., 2019) and investigates transfer learning within two families of graph algorithms: sequential algorithms and parallel algorithms. The paper finds that standard transfer learning techniques don’t help in this setting, but multi-task training is useful.","The paper studies the problem of transferring algorithmic reasoning knowledge from a task to another similar task. The authors first propose a set of benchmark tasks to evaluate transfer learning for algorithmic reasoning on graphs.   The authors then propose modifications to the Neural Extractor model by replacing the node encoder with a latent edge encoder. Along with this, they propose using two latent encoders, one with a linear layer and one with a non-linear layer to allow the model to learn more types of tasks. Also, a separate encoder is used for each task.   The authors experiment with several transfer-learning and Multi-task learning (MTL) approaches on the proposed benchmark tasks.  The authors conclude that standard methods for transfer learning are ineffective on transferring algorithmic reasoning knowledge. They further show that the MTL approach effectively transfers knowledge thus improving systematic generalization.  ","The authors studied how to transfer algorithmic reasoning knowledge. Particularly, they aim at using algorithms for which we have access to the execution trace to learn to solve similar tasks for which execution trace is not available. The authors examined two classes of graph algorithms, parallel algorithms and sequential greedy algorithms. They tested the hypothesis that standard transfer techniques are not sufficient and showed that multi-task learning can be used instead to achieve the transfer of algorithmic reasoning knowledge. ",0.20224719101123595,0.2247191011235955,0.21348314606741572,0.25316455696202533,0.3037974683544304,0.18181818181818182,0.22784810126582278,0.13986013986013987,0.2375,0.13986013986013987,0.3,0.325,0.21428571428571427,0.1724137931034483,0.22485207100591714,0.1801801801801802,0.3018867924528302,0.23318385650224216
529,SP:730701a807f10850ee758dc628bde081314b85cd,This paper essentially studies pure exploration in nonparametric bandits setting and provides a simple regret guarantee. The authors generalized the setting to nonparametric policy class. Some new ideas based on minimizing a risk upper bound are proposed. ,The paper studies a pure exploration bandit problem in a setting where the reward and policies are assumed to  be in RKHS spaces. The main result is a passive strategy based on experimental design and bounds on the simple regret. The results are extended to the cumulative regret setting via a standard explore-then-commit argument. In the latter setting the results are (depending on the kernel) about competitive with the state-of-the-art (more on this later).,The paper addresses the problem of reward learning by learning a reward model from human feedback using the optimal design of the queries. Authors address this by essentially framing the problem in the flavor of GP-Bandits that models rewards and policies as non-parametric functions belonging to subsets of Reproducing Kernel Hilbert Spaces (RKHSs) where the learner receives (noisy) oracle access to a true reward and is expected to output a near-optimal (reward maximizing) policy. More precisely they analyze the framework of doubly-nonparametric bandits for theoretically studying the reward learning problem. ,"This paper is motivated by learning optimal actions in tasks where both the reward function, and policy (actions) are nonparametric. Previous literature has typically only considered one of these two components as being nonparametric. The main focus is on reliably identifying a policy with low instantaneous regret/risk via as few queries as possible, where all queries are specified in advance (i.e. the passive query setting). The proposed approach selects query locations based on an eigenvalue decomposition of the policy space, sampling repeatedly along a set of top eigenvectors to ensure reliable estimation of the reward via a plug-in ridge-regression-based regression. This estimated reward function is then minimised over the policy class to return a suggested policy/action. The accompanying theory shows the decay of the risk of this suggested action, and shows that in the setting of the Gaussian Process bandit, this can enjoy a better rate than existing adaptive approaches such as GP-UCB. ",0.40540540540540543,0.21621621621621623,0.35135135135135137,0.22784810126582278,0.34177215189873417,0.23404255319148937,0.189873417721519,0.0851063829787234,0.08125,0.19148936170212766,0.16875,0.1375,0.25862068965517243,0.12213740458015267,0.1319796954314721,0.20809248554913296,0.22594142259414227,0.17322834645669294
530,SP:73d7fc915934a87b99a53182a6a96bc2af294d5e,"The current training paradigm applied in most time series forecasting approaches jointly learns feature representations and the prediction function. This paper argues that this paradigm may lead to several issues, e.g., overfitting problems, obtaining false relations of the unpredictable noise in the data, and entangling representations.  To tackle these challenges, this paper proposes a new framework (CoST) to first learn error-free feature representations, and then fine-tune the representations through a simple regressor. In the feature learning stage, a pretext task is constructed by using data augmentation and contrastive learning. The goal is to learn disentangled seasonal and trend representations. Moreover, time domain and frequency domain contrastive losses are incorporated to learn discriminative trends and seasonal representations, respectively. ",This paper proposes a novel way to represent time series learnt with contrastive losses in both the time and frequency domain for the forecasting task. The approach is novel and timely and is clearly described. The authors make a connection to causality which they use also for designing data augmentation schemes.,"This paper introduce a framework of learning disentangled trend and seasonal representations of time series and its application to forecasting tasks. It employs contrastive losses to distill discriminative trend and seasonal representations. DFT is used in obtaining frequency domain information. Empirical results show the proposed framework is able to outperform end-to-end trained forecasters and other representation learning methods. In addition, the learned representations can be clustered to distinguish multiple trends or seasonalities.",The paper proposes a contrastive learning framework for time series forecasting CoST. This is done by detangling trend and seasonal features than using the learnt feature representations with a simple regression. Trend feature disentangler consists of a mixture of autoregressive experts followed by average pooling. While the seasonal feature is done by changing the features into the frequency domain via FFT and then applying a linear layer with weights to every different frequency. The overall loss is the weighted time-domain contrastive loss from trend feature disentangler and frequency domain contrastive loss which contains both phase and amplitude loss from seasonal feature disentangler. The proposed method was evaluated on 5 datasets in both a multivariate and univariate setting. CoST was compared against different representation learning and end-to-end learning appraoches. ,0.10833333333333334,0.15,0.21666666666666667,0.29411764705882354,0.3333333333333333,0.28378378378378377,0.2549019607843137,0.24324324324324326,0.1984732824427481,0.20270270270270271,0.1297709923664122,0.16030534351145037,0.15204678362573099,0.18556701030927836,0.20717131474103587,0.24000000000000005,0.18681318681318682,0.2048780487804878
531,SP:73ec978df787768c5cce958451c1a7d895914303,"This paper addresses masking-based saliency methods for providing visual model explanations. It defines several new metrics for evaluating saliency methods: ""completeness"", ""soundness"", and ""consistency score"". It proposes a saliency method that is a variant of prior work. The paper shows through experiments that TV regularization improves soundness of saliency-based explanations.   ","The paper tackles the problem of model explainibility from the perspective of heatmap-based visualization approaches. It argues that achieving a good heatmap to explain the model decision is possible by considering two notions, completeness and soundness, borrowed from logic. In fact, the completeness condition is what already considered during the formulation of similar methods. Thus the novelty lies on studying and analyzing the usefulness of soundness. The paper, thus, proposes a metric to evaluate both completeness and soundness that is used to learn a mask that explains the model prediction.","This paper discusses the completeness of saliency maps and proposes the concept of soundness. Based on this, they propose consistency score to predict the possibility that the quality of the saliency map is consistent with the classification probability. Besides, the author uses these definitions to implement the intrinsic evaluation of masked-based saliency methods.","This paper introduces two intrinsic evaluation criteria for saliency-based model explanation: completeness and soundness. While completeness indicates a saliency method's ability to justify the correct label,  soundness requires the method to be unable to find a masked input that significantly increases the probability of an incorrect label. A consistency score is introduced to simultaneously evaluate both completeness and soundness. Further, taking into account the soundness factor, a saliency method is proposed to maximize the probability for a given label, rather than all labels. Experiments demonstrate that the heuristic TV regularization method can help with soundness, as suggested by high consistency scores.  ",0.23076923076923078,0.21153846153846154,0.34615384615384615,0.16483516483516483,0.25274725274725274,0.2962962962962963,0.13186813186813187,0.2037037037037037,0.17475728155339806,0.2777777777777778,0.22330097087378642,0.1553398058252427,0.1678321678321678,0.20754716981132074,0.23225806451612904,0.20689655172413796,0.23711340206185566,0.2038216560509554
532,SP:747b35a42006e384ba7ef70c77612581885a8a31,"This paper argues that language models (LMs) require mechanisms to retain information about context words for better next word prediction. Several free recall experiments have been conducted to determine if such models can exactly retrieve/recall words from context. Specifically, different LMs are given sequences that contain texts in the following order: a preface text, a list of $n$ target nouns ($n\leq 10$), an intervening text, a short prompt string, and a second list of nouns, which are either identical to the first list or different from it (i.e. similar words but permuted or completely different words). Reduction in surprisal from the first lists to the second lists is used as a metric to quantify and compare memory of different LMs. Surprisal of each list is estimated by the ""median"" of the surprisal scores obtained for non-initial nouns in the list. Several transformer models and LSTM-based language models of different sizes are compared in terms of reduction in surprisal. Results show that transformer models are superior in retrieving word identity and order from small/long-range intervening text (from a few to more than 400 tokens). Also more training data and greater model depth lead to better performance in case of transformer models. ","The paper attempts to investigate the ability of Transformer and LSTM based LMs to retrieve information about the prior context. To do so, they provide the LMs some prompts, a list of nouns and check their ability to reproduce the list by comparing the probabilities assigned to the same list of nouns (checking the reduction in surprisal).    The premise of the work is that making use of prior context is an important functionality of language models and while several prior works have demonstrated that LMs do so to capture certain types of dependencies, their work aims to understand the extent to which Transformers and LSTM based LMs can retrieve words from the prior context.    Via their experiments, they try to evaluate three things: (1) How well can such LMs reproduce the noun list from memory verbatim, (2) the effect of intervening tokens on an LM's ability to access, and (3) whether the ability to access memory depends on semantic coherence of the prior information.    They considered two versions of LSTM models and a few versions of Transformer models including GPT-2. Based on their results, it seems like Transformer based LMs performed consistently better than LSTMs on the defined task. The GPT-2 model performs much better than others. They also did some experiments to evaluate the role of attention weights by comparing with a model with shuffled attention weights.  ","This paper studies how LSTMs and Transformers represent prior context. In particular, this work adapts benchmark tasks for human working memory to neural language models. In this paradigm, the model is presented with <preface text> <a list of words> <intervening text> <the same list of words>, and perplexity / surprisal is measured for each of the words when the list of words appears a second time at the end of the input. Their results lead to a variety of insights about how LSTMs and Transformers trained on varying amounts of data with varying amounts of depth handle context.","This paper studies the short-term memory of NLP models. The authors design a memorization task of the format: <prefix> <list1 of words> <infix> <list2 of words>, where list1 and list2 are either identical, a permutation of one another, or unrelated. They train several models (an LSTM and several transformer variants, including GPT-2) with the language modeling objective, and then measure the median LM loss of each model across all the words on list2. They find that their LSTM does not show any memorization skills, the medium size transformer is able to memorize to a certain extent, and that GPT-2 memorizes almost perfectly, at least when list1=list2. Further analysis studies the factors that influence this memorization, including the length of the list, the length of the infix, and the depth of the model. ",0.19806763285024154,0.1497584541062802,0.12077294685990338,0.12987012987012986,0.12554112554112554,0.25773195876288657,0.1774891774891775,0.31958762886597936,0.18382352941176472,0.30927835051546393,0.21323529411764705,0.18382352941176472,0.18721461187214614,0.2039473684210526,0.1457725947521866,0.18292682926829268,0.15803814713896458,0.2145922746781116
533,SP:755c6337558c749bbb5486fe0a71b82ca766090f,"This paper proves under what conditions the equivalency between a generalized hypergraph (a hypergraph with node weights $Q_1$ and $Q_2$ for two-step random walk) and an undirected graph holds. The theory applies to ordinary hypergraphs as a special case with $Q_1=Q_2=H$. Then, the authors derive the stationary random walk distribution, the hypergraph Laplacian, as well as the generalized spectral hypergraph convolution form leveraging the Laplacian. To alleviate oversmoothing, they further propose to use a diffusion kernel to build a simple hypergraph spectral convolution. Experiments on four tasks show better performance than state-of-the-art methods.","This paper considers the problem of showing equivalency of Generalized Hypergraphs to undirected graphs (in the sense that a natural random walk on the hypergraph is equivalent to the natural random walk on a weighted undirected clique graph). They do so via establishing a Hypergraph Laplacian and identifying its properties that help prove this equivalence. They leverage this equivalency to build a Hypergraph  convolution neural network by viewing the weighted undirected graph as a lower-order encoder of hypergraphs. They use empirical studies to show that the constructed Hypergraph Convolution NN on four different network based classification task, where the underlying network structure forms a hypergraph.","In this paper, the authors aim at building the equivalency between hypergraphs and undirected graphs. They introduce the concept of generalized hypergraphs and demonstrate their equivalency with undirgraphs in terms of random walk. They further proposed two spectral-based convolution operations for the undigraphs.  ","Real-world relational datasets (e.g., academic data, protein data) contain group relationships and can be modelled via hypergraphs.  One way to capture group higher-order relationships is to define a hypergraph with edge-dependent vertex weights (EDVWs) and such a hypergraph has been shown to be equivalent to directed graphs [Chitra et al., ICML'19]. The main contributions of this paper are: 1) Proposal of a generalised hypergraph capturing EDVWs with equivalence to undirected graphs, 2) Proposal of a Laplacian based on the generalised EDVW and exploration of existing spectral convolutional methods, and 3) Empirical evaluation on academic, protein, and visual object datasets.",0.2524271844660194,0.1650485436893204,0.14563106796116504,0.1320754716981132,0.1509433962264151,0.20454545454545456,0.24528301886792453,0.38636363636363635,0.14423076923076922,0.3181818181818182,0.15384615384615385,0.08653846153846154,0.24880382775119617,0.23129251700680273,0.14492753623188404,0.18666666666666665,0.1523809523809524,0.12162162162162163
534,SP:75616489f0d75d0aeeb6697a689ce69fb9a0799e,"The paper presents DeepSITH, a new approach to time series modeling that addresses the limitations of current models, such as LSTMs. The major limitation that is addressed is the difficulty of modeling long-range temporal dependencies, often due to exploding/vanishing gradients. DeepSITH addresses this challenge by projecting the input time series data onto a logarithmically spaced time axis, followed by a fully connected layer. This ""SITH"" cell is then repeated multiple times. The logarithmic representation of time can then allow the model to represent features in the far past more effectively than other methods (which the authors demonstrate with experiments). The authors also introduce a new time series modeling task (Hateful 8) in order to demonstrate the advantages of their method.","In this paper, the authors propose a neural architecture that can effectively capture long-range dependencies in sequence data. Unlike traditional LSTMs and their variants, the proposed model DeepSITH relies on applying several temporal filters at different scales to the input (implemented as a convolution) and using learned weights to combine information across filters. Moreover, the authors implement a hierarchy of timescales by increasingly broadening the range of temporal scales in each layer. They test the network on 5 different tasks-  Pixel MNIST, permuted MNIST, Adding problem, Mackey-Glass Prediction & their own Hateful-8 dataset (encoding morse code based on activation patterns and appending actual data with variable length noise patterns). For each task, DeepSITH is compared to 2 other models also known to capture long-range information (LMU & CoRNN) and a vanilla LSTM. Overall, they find that DeepSITH performs at par or better in all 4 tasks and often reaches the optimal solution in far lesser epochs. Additionally, the experiments demonstrate that in tasks like the adding problem, DeepSITH is the only network to capture dependencies as far as 5000 time steps back.","The paper introduces DeepSITH a biologically-inspired neural network. The goal of DeepSITH is to handles problems that depend on long-range dependencies. DeepSITH consists of a series of layers, each layer includes a scale-invariant temporal history (SITH) followed by a dense layer.  Scale-invariant temporal history (SITH) layer projects input into logarithmically compressed axis using Laplace transform.   The paper also introduced a new task based on Morse code, called the Hateful 8. The paper compared DeepSITH with LSTM, LMU, and coRNN on 4 different tasks. DeepSITH showed results similar to that of the different RNNs while being biologically inspired. ","This paper presents a deep-learning architecture (DeepSITH) that facilitates the learning of long-range temporal dependencies in data using a technique that (i) exponentially filters hidden features with a range of time constants and (ii) reconstructs the history of the features with an approximate inverse Laplace transform over the filtered features. This process is stacked by inserting a dense layer over the reconstructed feature history to obtain the next set of features. By using a geometric spacing of the time constants of the exponential filters, the method implicitly encodes the assumption that the exact timing of events that happened in the distant past is less crucial to know than that of recent events. The method is evaluated on a number of tasks that feature long-term dependencies in the data, most of which feature in previous works to test the same capability (Permuted/Sequential MNIST, the Adding problem, Mackey-Glass prediction), and one that is designed in this work (Hateful 8), which requires the model to maintain decoded information about a Morse-code signal in the face of signal-like noise that follows it. DeepSITH is shown to outperform relevant baselines (e.g. CoRNN, LMU) on all the tasks, either in terms of final performance or speed of convergence. ",0.18032786885245902,0.22131147540983606,0.27049180327868855,0.13043478260869565,0.23369565217391305,0.26732673267326734,0.11956521739130435,0.26732673267326734,0.15714285714285714,0.2376237623762376,0.20476190476190476,0.12857142857142856,0.1437908496732026,0.242152466367713,0.19879518072289157,0.16842105263157892,0.21827411167512692,0.17363344051446944
535,SP:7591a1d18a63f8cad2dfe950195a1ae10d1eba23,"In this paper, the authors study fair classification in the presence of an adversary who is capable of choosing an arbitrary-fraction of the training samples and arbitrarily perturbing the corresponding protected attributes. Theoretically, the authors propose an optimization framework to learn fair classifiers with provable guarantees on accuracy and fairness in the adversarial settings. Empirically, the authors evaluate the classifiers produced by our framework for statistical rate on real-world and synthetic datasets against different adversaries.  ",This work studies learning fair classification under the assumption that an arbitrary eta-fraction of training samples are perturbed on the protected attributes. The authors propose an optimization framework for eta-Hamming model which has the provable guarantees on accuracy and fairness. The experiments with linear models on synthetic and real-world datasets are used to evaluate the proposed approach. ,"This paper presents an optimization framework to learn fair and accurate classifiers in the setting where an adversary can perturb the protected attributes. The contribution is mainly theoretical and focuses on Theorem 4.3, which gives guarantees on fairness and accuracy from a solution to their ErrorTolerant program, along with additional impossibility results (4.4, 4.5) which provide tightness and optimality guarantees for the proposed approach. There are also results from a small set of experiments on one simulated and one real dataset showing that the proposed approach generally outperforms baseline methods under the perturbation model.  **Update**: I have read the author response, and it addresses my concerns. Looking at the other reviews and the responses as well, I will keep my initial rating.","They consider the problem offFair classification in the adversarially perturbed setting — that is, the adversary gets to choose eta fraction of the sample arbitrarily and change their protected attributes arbitrarily.   They show an optimization problem that ensures fairness and accuracy guarantees under a mild assumption and show a mild assumption is necessary for those guarantees.   Finally, they have some experiments in which they consider two well thought out adversaries.",0.38961038961038963,0.2597402597402597,0.22077922077922077,0.35,0.23333333333333334,0.12,0.5,0.16,0.2463768115942029,0.168,0.2028985507246377,0.21739130434782608,0.43795620437956206,0.19801980198019803,0.2328767123287671,0.227027027027027,0.21705426356589147,0.15463917525773196
536,SP:75d80f1f01481c386f2fc681e4126088b36fce7f,"The paper presents a novel perspective on deep ensembles (DE) that aims at providing a Bayesian justification to DE. The proposed approach builds a Gaussian process (GP) using the ensemble members and performs variational inference in the functional space. Moreover, the paper introduces a regularization method that works directly in the function space. The proposed algorithm DE-GP shows better results compared to DE an other well known Bayesian deep learning methods. ","The authors propose using Deep Ensemble (DE) as basis functions to train a Gaussian Process (GP) with the primary motivation of making DEs more Bayesian. The Evidence Lower BOund (ELBO) being maximized is now optimized via variational inference (VI) over the functional space of bases (i.e. fELBO from [3] Sun et al.) defining the mean and covariance of a GP similar to NN-GP [4]. The paper evaluates the proposed method on standard UCI regression and small-scale MNIST / CIFAR10 image classification tasks in terms of accuracy, likelihood, and uncertainty estimation.","This paper presents a GP model with a deep kernel defined in terms of a (finite) deep ensemble (DE). A variational approximation and a regularization scheme were introduced to optimize the GP. The proposed methods were then demonstrated on several benchmark regression/classification datasets.   Here, the claimed contributions are (1) a DE-parameterized GP posterior that provides a Bayesian justification for DE; and (b) a regularization scheme that improves the generalization of the trained GP.","The motivation for using fully Bayesian methods over ensemble methods has been a contentious topic in recent years - while ensemble methods are prized for their relative simplicity, they lack the theoretical framework that grounds fully Bayesian approaches. In this work, the authors propose an interpretation of Deep Ensemble models (DE) under a variational Bayesian framework. In particular, the authors demonstrate how VI can be carried out in the function space, and leverage posterior regularisation on functions to incorporate prior knowledge into the model architecture. The benefits of the proposed approach are verified via an extensive evaluation covering a variety of different problem settings, whereby it appears that the DE-GP consistently yields predictions having superior uncertainty calibration, and without compromising on predictive accuracy.",0.2638888888888889,0.25,0.2361111111111111,0.16304347826086957,0.20652173913043478,0.24,0.20652173913043478,0.24,0.13821138211382114,0.2,0.15447154471544716,0.14634146341463414,0.23170731707317074,0.24489795918367346,0.17435897435897435,0.17964071856287425,0.17674418604651163,0.1818181818181818
537,SP:76ae86cb0be31feacf85f95c6c90fdd19aee85a3,"This paper addresses the classical topic of directly optimizing non-decomposable loss functions. Since these metrics can be computed via linear programs, it is sufficient to compute gradient through the LP solver. To that end, the authors propose to use a particular method for solving linear programs. In experiments, the resulting implementation outperforms the cross-entropy loss and mildly outperforms one recent baseline.","This paper approximates several nondecomposable functions (AUC and F1-score) as linear programmings and uses them as loss functions for network training. In the linear programmings, the constraints are indeterministic at each mini-batch, the number of constraints increases quadratically to the number of training samples, so some previous works are inapplicable here. So does the primal-dual based forward pass and the corresponding implicit differentiation for a backward pass.","This work proposes to relax some commonly used discrete metrics into a Linear Program (LP). By using the Newton LP method, the algorithm is able to backpropagate through the LP and thus to train a neural network end-to-end while directly optimizing for the metric. In particular, this submission describes how the AUC, multi-class AUC and F-score can be casted as LPs, and how the resulting problems can be solved with a Newton LP method that can be differentiated through. Experiments are presented on Cat&Dog, CIFAR-10, CIFAR-100 and STL.","This paper present a novel approach to perform direct optimization of non-decomposable objective functions (such as AUC or F-measure) in the context of Deep Learning. They propose a general LP-based framework describing these objective functions and provide a general algorithm to solve this problem (and get the corresponding gradients). Finally, this new approach is compared to other methods trying to optimize these objectives directly and surrogate loss functions that are commonly used in practice.",0.1746031746031746,0.25396825396825395,0.19047619047619047,0.15714285714285714,0.15714285714285714,0.1368421052631579,0.15714285714285714,0.16842105263157894,0.15584415584415584,0.11578947368421053,0.14285714285714285,0.16883116883116883,0.16541353383458646,0.20253164556962025,0.17142857142857143,0.13333333333333333,0.14965986394557823,0.1511627906976744
538,SP:76d423655f3bb6ca009c2e7f06a49abecb8c210e,"This paper introduces Activity Parsing for the understanding of complex human activities based on a hierarchical structure describing human actions at 4 different levels. As an improvement from traditional pairwise entity representations, the Action Hyperedge is used to represent the more complex relationships between multiple humans and objects. Also the Action Hypergraph provides better interpretability for reasoning of high-level actions or activities. Followed by the concept of the hierarchical Activity Parsing, a new, meticulously-annotated video dataset, MOMA, is released and an HGAP network is proposed. In the experiment, the HGAP network achieves comparable results with the state-of-the-arts in several tasks such as video and role classification.  ","The paper defined a new action representation called action hypergraph which connects multiple entities with hyperedges, the role classification for actors, and the  temporal tracking for entities. A new dataset (MOMA) following the defined representation is proposed. The paper also proposed a HyperGraph Activity Parsing (HGAP) network for tackling multiple activity understanding tasks simultaneously, e.g. activity recognition, sub-activity recognition, atomic action recognition, etc. Results for these activity recognition tasks, as well as the hypergraph construction evaluation results (hyperedge classification and role classification) are reported on MOMA, together with some simple baseline results. ","This paper proposes that video understanding be approached by trying to recognize at several different levels of granularity, simultaneously. At the highest temporal granularity is the ""activity"", which consists of a sequence of temporally-localized ""sub-activities"". Below that are the ""atomic actions"" performed by individual actors, and at the bottom an ""action hypergraph"" that is a scene hypergraph showing the relationships between actors & objects at the frame level.  The authors introduce a video dataset, MOMA, and a task ""Activity Parsing"" which aims to infer all of these annotations from a video together with ground truth person and object tracks. Further they propose a HyperGraph Action Parsing Network, and compare it to 3DCNNs and graph convolutional networks on the Activity Parsing task. ","The paper introduces a new dataset for video-based activity parsing. Instead of assigning a single action label to a video, the paper decomposes an activity into sub-activities and atomic actions. Instance-level annotations of actors, objects and their relationships are also annotated. Based on this dataset, the paper proposes a new model, action hypergraph, for activity parsing. Experimental results demonstrate the potential applications of the dataset and the effectiveness of the proposed method.",0.1981981981981982,0.22522522522522523,0.21621621621621623,0.18085106382978725,0.19148936170212766,0.16393442622950818,0.23404255319148937,0.20491803278688525,0.32,0.13934426229508196,0.24,0.26666666666666666,0.21463414634146344,0.21459227467811157,0.25806451612903225,0.1574074074074074,0.21301775147928995,0.2030456852791878
539,SP:76fb3771dc4eddd0166117f39525f41f139b1a82,"The work proposes RotoGrad, a multitask layer that rotates a shared feature space in a deep neural network to align gradients and mitigate negative transfer effects. The proposed method works within models with multiple prediction heads and a shared backbone, and deals both with magnitude and directional imbalance between task gradients. Authors provide multiple experiments in the computer vision space to show efficacy of the proposed method against other multitask baselines.  ","The paper proposes a method to reduce the optimization difficulty for multi-task learning. The key idea is to homogenize task gradients in terms of both magnitudes and directions. Specifically, they rescale task gradients to maintain similar convergence rates over tasks. At the same time, they introduce new network parameter R_k for each task k and train it so that the angle between task-specific and average gradients are minimized when task-head parameters are rotated using R_k. The method improves baselines and outperforms existing methods on DIGITS, CIFAR10, NYUv2, and CelebA datasets.","The paper proposes a novel method for the multitask learning environment, which deals with the conflicting gradients problem (AKA negative transfer). The method comprises two parts for dealing with gradient magnitudes and directions. The latter part is quite novel, well-motivated (with a cartoon example from Fig. 1), and theoretically sound. The method is tested against a number of MTL benchmarks and is shown to produce better-performing models. Overall, the paper presents interest to the MTL community.","This paper proposes RotoGrad, a multi-task optimisation strategy that normalises the gradient magnitude and (conflicting) direction in a unified manner, to avoid the negative transfer. The proposed method is intuitive, theoretically motivated by the two player-game, and hyper-parameter free (except for learning rate required for updating rotation parameters).   The paper extensively evaluates the proposed method comparing with other baselines, both in single domain MTL setting NYUv2, and in multi-domain MTL setting CIFAR-10, and CelebA, and have showed improved performances in all settings.",0.19718309859154928,0.23943661971830985,0.2676056338028169,0.17894736842105263,0.18947368421052632,0.21794871794871795,0.14736842105263157,0.21794871794871795,0.21839080459770116,0.21794871794871795,0.20689655172413793,0.19540229885057472,0.1686746987951807,0.22818791946308722,0.24050632911392406,0.19653179190751446,0.1978021978021978,0.20606060606060608
540,SP:7727eeb7b17ad94ddfa0cf24e64a9626d83a8876,"This paper tries to address the possible underestimation issue of double Q-learning. Specifically, it shows underestimation bias may lead to multiple non-optimal fixed points, so it adds another target value from abstracted MDP to construct an integrated estimator, named doubly bounded estimator. It conducts serval experiments based on the standard Atari benchmark tasks, and the proposed doubly bounded estimator has shown promising results in bootstrapping the performance of double Q-learning algorithms.","The estimation bias is a quite general problem in Reinforcement Learning originally examined in Q-learning and due to the fact that the Bellman operator has a max operation which causes a systematic overestimation of state-action values. Double Q-learning had first been proposed as a way of mitigating this maximization bias, at the expense however of an opposite minimization bias. This papers tackles the estimation bias in particular of Double Q-learning by proposing a new way of analyzing the problem from the perspective of the fixed points of a stochastic Bellman operator (where the stochastic components models the approximation error in minimizing the Bellman error). The main theoretical message of the paper is that the estimation bias is the result of suboptimal fixed points of the stochastic Bellman operator. Loosely based on this observation the authors then propose a ""Doubly bounded estimator"" which they show has a variance reduction effect. Although they do not provide any theoretical guarantees in regard to bias  reduction,t hey then test their algorithm on a few tasks of the Atari 2600 benchmark, empirically demonstrating that their algorithm results in faster rewards maximization compared to DQN, Double DQN.","This paper proposes a new problem of double Q-learning, that is, the multiple fixed points caused by underestimation bias. This problem may lead to the non-optimal model after value iterations. Meanwhile, the authors provide many mathematical approvals. To solve it, the authors integrate another value estimator to escape the saddle points and approve its effectiveness with experiments. ","This paper studies the effect of the underestimation induced by Double Q-Learning, a well-known method to solve the overestimation problem of Q-Learning. The paper shows that Double Q-Learning might incur in suboptimal fixed points under an approximated setting, e.g. Double DQN, and proposes a method, i.e, Doubly Bounded Q-Learning, to curb this problem. The method consists of computing the target of the Q-Learning update as a maximum operator between the application of the Bellman operator on an approximation of the value function, and the application of the Bellman operator on the optimal value function computed on a discrete abstraction of the original MDP. The method is evaluated on several Atari games.",0.25675675675675674,0.25675675675675674,0.25675675675675674,0.09183673469387756,0.1989795918367347,0.3050847457627119,0.09693877551020408,0.3220338983050847,0.15966386554621848,0.3050847457627119,0.3277310924369748,0.15126050420168066,0.14074074074074072,0.2857142857142857,0.19689119170984457,0.14117647058823532,0.24761904761904763,0.20224719101123598
541,SP:774bcec23fedd4eadede71cc7c0f4626fcca6a9c,"This paper proposes BARTScore, a method to evaluate NLG system outputs by using a NLG system's output likelihood score, in particular different variants of BART. By changing what BARTScore conditions on and what it must score, as well as the prompts which are fed to the BARTScore model, variants of the evaluation method can be derived (faithfulness, precision, recall, F), and be adapted to measure different generation qualities such as coverage, coherence, factuality, fluency, informativeness, and adequacy. Experiments are run on a number of NLG tasks including MT, summarization, factuality, and data-to-text, where higher correlations are found between versions of BARTScore and human judgments. There are also several extra analytical experiments on MT focusing on issues such as reference length and the choice of prompts.","This paper proposes a different paradigm of evaluation of text generation by modeling evaluation as a text generation problem. This is built on top of the BART text generation algorithm, hence, the metric is called BARTScore. Similar to BERTScore, which is based on BERT, the metric proposed in this paper requires no training (with the reference human judgments) as the pre-trained BART model is used. BARTScore considers various aspects of matching generated text with reference (like factuality, coherence, etc.). In addition, BARTScore is shown to improve with using textual prompts and with fine-tuning on downstream domain tasks. BARTScore shows promising results across the board and fine-grained analysis shows effectiveness across different perspectives. ",This paper tries to propose a new generation-based BARTScore to evaluate text generation problems. The authors want to fill the gap between pretrainining objectives and the down stream feature extractors.  The main idea is that BART can achieve better score when the generation results are better. The BART Score is an unsupervised metric and have better correlation with different perspectives. The paper also check the influence of different textual prompts and fine-tuning process. The paper conducts experiments from 7 perspectives on 16 datasets. ,"This paper introduces BARTScore, a new metric for generation based on the BART model. BARTScore frames the evaluation of generated text as a text generation problem. Additionally, the authors show prompts can be used to improve the metrics. BARTScore is evaluated on various generation tasks, including summarization, machine translation (MT) and data-to-text. The results are convincing: BARTScore and its variants outperform unsupervised metrics in nearly all cases and are comparable to the best supervised metrics on MT.  ",0.15625,0.1328125,0.203125,0.2,0.1826086956521739,0.24705882352941178,0.17391304347826086,0.2,0.3291139240506329,0.27058823529411763,0.26582278481012656,0.26582278481012656,0.1646090534979424,0.1596244131455399,0.25120772946859904,0.22999999999999998,0.21649484536082475,0.25609756097560976
542,SP:77c669d9438c4219f9161afa4725cc028bf6d137,"The paper highlights two limitations of current approaches for Robust RL:   1. Robust optimization methods are computationally demanding. 2. They do not account for uncertainty in the model dynamics.  The paper proposes to work around these limitations by learning robust MDPs using regularization. Their contributions are the following:  1. Showing that regularized MDPs are an instance of Robust MDPs with uncertain rewards; thus, policy iteration on reward-robust MDPs has the same time complexity as on regularized MDPs  2. Extend point 1 to MDPs with the uncertain transition. 3. Generalize regularized MDPs to twin regularized MDPs to retrieve robust MDPs.",This paper formalized robust MDPs as general MDP with regularization. They first show that regularized MDPs are equivalent to MDPs with uncertain rewards. They also derive a regularized MDP formulation for robust MDPs with uncertain transitions. And they finally generalize a regularized MDP that can handle both reward and transition uncertainty. They argue that their regularized MDP framework provides better computational complexity and scalability compared to traditional robust optimization techniques.,"The paper studies the connection between robust and regularized MDPs. The authors first show that solving robust MDPs with uncertain rewards is equivalent to solving regularized MDPs with a suitable policy-dependent regularization. Then, they extend this result to MDPs with uncertain transitions, obtaining the equivalence with both policy and value regularization. Finally, they propose R^2 MDPs, i.e., MDPs with both value and policy regularization for which, under certain assumptions, they guarantee good properties (eg contraction) for the corresponding Bellman operators and thus convergence of a modified policy iteration scheme. This allows solving robust MDP problems with the same computational cost as regularized ones.","The paper establishes an equivalence between regularized MDPs and robust MDPs. The first main result shows that policy-regularized MDPs are equivalent to reward-robust MDPs, gives examples of equivalences between common regularizers and uncertainty sets, and gives a policy gradient algorithm for reward-robust MDPs. The second main result extends the equivalence to robust MDPs with both reward and transition uncertainty. The third main result introduces R^2 MDPs, which extend regularized MDPs with a value regularizer. It is shown that under certain assumptions on the uncertainty sets, the equivalent R^2 MDP can be solved with MPI.",0.21,0.22,0.23,0.32857142857142857,0.32857142857142857,0.3018867924528302,0.3,0.20754716981132076,0.23232323232323232,0.2169811320754717,0.23232323232323232,0.32323232323232326,0.24705882352941178,0.21359223300970873,0.23115577889447236,0.26136363636363635,0.27218934911242604,0.3121951219512195
543,SP:77eec5c26588047bb5a21067dcb085362c8d4101,"The paper proposes to utilize the zero-shot/few-shot generalization ability of frozen language models in a multi-modal setting, by projecting visual input features into the text input feature space, through moderate pre-training on Conceptual Captions (the LM is frozen during pre-training).   The paper designs several interesting experiment settings to show the model’s ability of task adaptation, utilizing pre-trained encyclopedia knowledge, and novel concept binding.   My main concern is that it is hard to judge whether the results in these experiments are significant or somewhat trivial. But given the novelty of the setting and the promise, I would lean towards acceptance but I look forward to the author’s response.","The paper proposes a new GPT-2 style image-language model, Frozen, which is trained in two stages:  1. common GPT-2 pre-training on huge text-alone corpora 2. training of a Resnet image encoder while freezing the GPT-2 and text embedding weights, on the task of image captioning (i.e. language modeling conditional on two image encoding vectors with the same dimension as token embeddings).  Since GPT-2 exhibits few-shot abilities in language-alone tasks, the idea is Frozen can also benefit from the GPT-2 language pertaining and exhibit *multimodal* few-shot abilities by the minimal design of adding a visual encoder. Note that the freezing does not make training more efficient, since gradients have to pass through all Transformer layers to the visual encoder.  The Frozen model is applied to zero/few-shot learning of two downstream tasks: VQA and few-shot image classification. ","The paper proposes a new way to reuse large pre-trained language models and condition their generation on visual input. The authors build on the idea of pretext tuning and make pretext a (CNN) function on an image. By training the CNN for VQA, keeping the language model decoder fixed, the authors bring visual information into the input space of the language model. The authors propose several benchmarks for few-shot classification and captioning and show that their model demonstrates promised capabilities and achieves better than random results.","This paper proposes a way to train a few-shot image-text neural network in which the text encoder weights are frozen, but used to train the image encoder from scratch. It provides an interesting training framework and evaluates few shot performance on image classification and vqa tasks, also studying several qualities of interesting, e.g. rapid task adaptation, encyclopedic knowledge, and fast concept binding.",0.1810344827586207,0.1724137931034483,0.15517241379310345,0.16,0.11333333333333333,0.1590909090909091,0.14,0.22727272727272727,0.27692307692307694,0.2727272727272727,0.26153846153846155,0.2153846153846154,0.15789473684210528,0.19607843137254902,0.1988950276243094,0.20168067226890754,0.15813953488372093,0.18300653594771243
544,SP:782dcf8d863531c1f918516c2c5bfd64360b0741,This paper considers the problem of learning to sufficiently explore an MDP in order to effectively provide a generative model that can be used for downstream learning tasks by algorithms that require a generative model. An algorithm is given that leverages SSP results in order to force exploration of certain state-actions that may be required by the arbitrary down-stream algorithm. Sample complexity upper and lower bounds are proved to demonstrate the samples required to meet the budget prescribed by the downstream algorithm. Several example tasks that require non-trivial exploration are shown to make use of the algorithm.,"The authors introduce a general technique for exploring the MDP in an SSP problem. Their approach extends the notion of reward-free exploration to  an ""objective-agnostic"" scenario, in which the goal is to generate a prescribed amount of samples from any state and action. The authors analyze the proposed algorithm and provide a sample complexity bound.  Then, the authors show this technique can be leveraged to solve specific scenarios which fit the communicating MDP setup. For these scenarios, the authors show the superiority of their approach with respect to previous works.","The paper showcases a polynomial sample complexity for the problem of simulating a generative model through online interactions with a communicating MDP. The approach works as follows: It first prescribes the samples to be taken in each state-action pair for a specific objective, and it actively collects these samples by addressing a sequence of SSP problems. The paper includes a numerical validation in illustrative domains.","The paper proposes to decouple the exploration problem in RL into an objective-specific part with access to a sampling oracle, and an objective-agnostic exploration part. For the objective-agnostic part, the paper proposes the GOSPRL algorithm with bounds on its sample complexity to meet the sampling requirement. Then GOSPRL is then combined with some sampling oracle-based algorithms to provide sample complexity for three types of RL problems.",0.19,0.15,0.17,0.16304347826086957,0.1956521739130435,0.18181818181818182,0.20652173913043478,0.22727272727272727,0.24285714285714285,0.22727272727272727,0.2571428571428571,0.17142857142857143,0.19791666666666669,0.18072289156626506,0.2,0.189873417721519,0.2222222222222222,0.1764705882352941
545,SP:7833a74d817ab59e201440fd08c6b65032251839,"This paper proposes a new neural implicit representation called object-centric scattering function for scene compositing application. The major extension is to add the lighting direction as an input so that the new representation can be used for relighting. To train this representation, the authors propose minimizing the rendering loss of images rendered from different views under different point lighting. Given this representation, the authors propose a standard volume path tracing framework to render different objects and scene structures together, with indirect illumination and shadow being modeled. Experiments show that compared with nerf, the proposed method achieves better accuracies in object compositing and can handle changes of illumination. ","The paper proposed a method to composite objects parameterized using implicit functions into realistic scenes. The idea is to first capture the representation for each object separately, and each object is represented using neural scattering function, which predicts the outgoing lighting transport conditioned on the input lighting direction, viewing direction and 3d location. The objects are composed into the full scene by doing volume rendering along the ray, and the radiance of each sampled point in the ray is calculated via integrating on a sphere to obtain the lighting radiance (including secondary (indirect) light effect, the shadow effect is obtained from iterating the ray from light position to the  sampled point). The results on both synthetic scene and real scene shows improvement over baselines (o-Nerf, o-Nerf S) that didn't consider the lighting transportation. ",This paper proposes a NeRF based method for composing photo-realistic scenes from captured images. The proposed method learns object-centric neural scattering functions (OSFs) to implicitly model per-object light transport using a lighting- and view-dependent neural network. Multiple objects can be rendered with volumetric path tracing. The proposed method has been evaluated on both synthetic and real-world datasets.,"The paper proposes a decomposition of a 3D scene into object-level neural radiance fields. This allows to apply rigid transforms to each object independently and to rearrange the scene. Crucially, light transport is modelled, which turns the radiance fields into scattering functions. Direct lighting and shadowing, as well as indirect illumination from several light bounces are taken into account, such that the illumination of the modified scene looks correct. As long as the scattering fields are trained on a sufficient number of light positions, the lights can also be moved around at test time.",0.21296296296296297,0.16666666666666666,0.1574074074074074,0.13970588235294118,0.16176470588235295,0.20967741935483872,0.16911764705882354,0.2903225806451613,0.17894736842105263,0.3064516129032258,0.23157894736842105,0.1368421052631579,0.18852459016393444,0.2117647058823529,0.16748768472906403,0.19191919191919193,0.19047619047619047,0.16560509554140126
546,SP:784641faaab4b5f7118d0af0a18ad4ed3846abf2,"**N.B.** I have reviewed a previous version of this paper. This version does not address many of the main concerns I raised in that review, so I will mostly repeat those.  In this work, the authors formulate Bayesian network structure learning (BNSL) as a quadratic unconstrained binary optimization (QUBO) problem. The main contribution of the work is a novel encoding of BNSL as a QUBO respecting the bit limitation of a particular digital annealer platform. Experiments on standard benchmarks show the formulation leads to mildly higher likelihoods compared to baseline methods.","The paper considers the problem of structure learning in Bayesian networks but on alternative hardware to the classic von Neumann computers. In particular, a digital annealer is considered and consequently the structure learning problem is encoded into a quadratic unconstrained binary optimisation problem (QUBO). Digital annealers are known to be very good at (approximately) solving these kinds of optimisation problems. Experimental results on standard Bayesian networks demonstrate the effectiveness of the proposed approach compared with standard methods that run on classical hardware.","The premise of the paper is both novel and interesting, in regarding Bayes network structure learning as a combinatoric optimization problem amenable to simulated annealing, specifically with current quantum computing tools.  The paper offers an approach to converting a score-based Bayesian network structure learning into quadratic unconstrained binary optimization. The improvement in representation of Bayes networks structure over current published methods turns on identification of candidate parent sets, and that this shows improvements compared to ""ordering space search algorithms"" ","The paper concerns a method for learning BNs from discrete data. A score-based approach is taken where the score is BDeu with effective sample size = 1. Table 1 shows the learning problems attempted here. The steps are: (1) Choose a limit on parent set size (here simulated data is used, and the chosen size is that of the biggest parent set in the true BN). (2) Generate candidate parent sets (CPSs). Since the authors claim that it is ""infeasible to identify exact candidate parent sets by searching the power set P(X \ {X i }) in a realistic timeframe."" a greedy approach it taken (Algorithm 2). (3) Decompose the CPSs into Cartesian products (another greedy algorithm is used - Algorithm 1). (4) Use a digital annealer to search for a high-scoring BN. (Comparisions with some other algorithms are also done). ",0.20652173913043478,0.18478260869565216,0.17391304347826086,0.2073170731707317,0.18292682926829268,0.2125,0.23170731707317074,0.2125,0.11428571428571428,0.2125,0.10714285714285714,0.12142857142857143,0.21839080459770116,0.19767441860465115,0.13793103448275862,0.20987654320987653,0.13513513513513511,0.15454545454545457
547,SP:786a0f7442b213c42cfa735c5a9c8eceee59a64c,"The authors study the problem of using machine learning techniques to 'automatically' learn voting mechanisms. Specifically, the authors use three permutation-invariant neural network architectures: DeepSet, Set Transformer and Graph Isomorphism Network and show that they can learn voting rules as well as maximize certain social welfare functions.  The first contribution is to show that these networks are universal function approximators of permutation invariant functions. Voting protocols typically fall within this class, as a permutation of votes should not lead to different outcomes: in other words, the identities of the voters do not matter.  The second contribution is to develop a learning procedure and evaluate the learned voting rules produced by these three algorithms (as well as a standard multi-layer perceptron, for reference). The way the training worked is to consider a population of voters and candidates such that the utility each voter has for the candidates is sampled from a distribution. Specifically, if there are $n$ voters and $k$ candidates, the utility of each candidate for voter $i$ is sampled by a Dirichlet distributions with all $k$ parameters set to $\alpha_0$. Different values of $\alpha_0$ correspond to different voters: some may prefer a single candidate or many of them equally. These values are then converted into an ordering, which the voters submit to the learned mechanism. The mechanisms' output is compared with the voting rule we are trying to learn (or the option which maximizes social welfare) a loss is incurred and a gradient descent step follows. On synthetic data, the learned mechanisms perform very well for all voting protocols tested, which are: Plurality, Borda, Copeland, Maximin and Kemeny.  Finally, the same network architectures perform even better on real data sets, such as the Netflix Prize Data.","The paper studies two problems related to using neural networks as single winner voting rules. (1) Training networks to mimic well-known existing rules, and (2) training networks to act as novel rules that select utility-maximizing alternatives.  The major novel contribution of the paper is in their choice of network architecture. ""Permutation-invariant networks"" are used, which are agnostic to the order of input data, thus ensuring that the resultant voting rules are anonymous by default.  Experiments show that the 3 PIN architectures considered are able to learn the studied voting rules with high accuracy after a large amount of training. These results are shown for testing done on each of real and synthetic data. Similarly, when the networks are trained with an oracle to learn the welfare maximizing alternative they perform quite well.",The Authors report a set of experiments with using neural networks with a particular architecture to learn optimal voting rules. They provide a concise theoretical description of the problem and prove that the chosen type of NNs are universal approximators of voting rules. They demonstrate experimentally that NNs can be trained using voter's ranked preferences only.  The experiments are of two kinds:  1. teaching the NN known classical voting rules  2. learning a new rule to maximise a given social welfare function  Their results show that NNs can learn known voting rules very well and that they can discover new voting rules which select optimal candidates more often than classical voting rules.  --- EDIT ---  Increased the score to 7 after the clarifications.,"This paper empirically studies the ability of PIN (Set Transformer, GIN, DeepSets and MLP) applied to predicting the (top-1) winner of voting rules (Plurality, Borda, Copeland, Maximin and Kemeny) and predicting the social-welfare-maximizing (utilitarian and egalitarian) candidates. The PINs are trained on synthetic data, agents whose utility over candidates are generated from Dirichlet distributions. The PINs are tested on synthetic data (Table 1 & 3), synthetic data with more voters than in training time (Table 2) and real world data (Table 4). ",0.12714776632302405,0.11683848797250859,0.09278350515463918,0.1925925925925926,0.13333333333333333,0.12295081967213115,0.2740740740740741,0.2786885245901639,0.32142857142857145,0.21311475409836064,0.21428571428571427,0.17857142857142858,0.17370892018779344,0.16464891041162225,0.144,0.20233463035019458,0.1643835616438356,0.14563106796116504
548,SP:78c573561cc23d53eb479f204d93579bd2cc3416,"In this paper, an algorithm for multi-source domain adaptation is proposed. The algorithm is developed in a source-free regime to address the concern for privacy. A loss function is designed to integrate the source-specific end-to-end classifiers using solely the target domain samples, which preserves privacy. The loss function consists of four terms to benefit from enforcing source-specific transferability, anchor-induced pseudo-labels, and class relationship-aware consistency. A PAC-learning style theoretical analysis is provided to demonstrate that using multiple sources may improve the generalizability on the target domain. Finally, experiments on five standard UDA datasets are provided to demonstrate that the method is effective.","This work considers unsupervised domain adaptation issue. A novel framework called CAiDA is proposed which uses the pre-trained source models rather than source data which are commonly used in traditional methods. To demonstrate the effectiveness and superiority of the proposed method, the authors did theoretical analyses as well as numerical experiments. The numerical experimental results show the proposed model can do better than state of art methods.","This work presents CAiDA for Multi Source-Free Domain Adaptation in the absence of source data. The core idea is to define confident anchors to obtain pseudo labels for target domains. In the presence of multiple source domains, the contribution of each source domain for the target sample is measured using source-specific transferability perception modules. Further a class-relationship aware consistency loss is proposed to maintain semantic consistency across domains. The proposed method consistently performs better than prior arts on several datasets.","This paper addresses the multi-source free domain adaptation problem where multiple pretrained source models and unlabeled target data are given. Theoretical analysis aims at proving that multiple source domains improve the probability to ensure a generalization bound. The method is developed based on mutual information maximization, pseudo labeling and consistency losses. Experiments show that the proposed method achieves slightly better results. ",0.13513513513513514,0.25225225225225223,0.17117117117117117,0.23529411764705882,0.22058823529411764,0.2289156626506024,0.22058823529411764,0.3373493975903614,0.3064516129032258,0.1927710843373494,0.24193548387096775,0.3064516129032258,0.16759776536312848,0.28865979381443296,0.21965317919075147,0.2119205298013245,0.23076923076923075,0.26206896551724135
549,SP:78d42d6bffafa0409345ccef469588aecea459dd,"Authors propose a method that allows communication in federated learning to run in parallel with local gradient computation; practically removing the communication delay. Authors provide theoretical convergence bounds and compare their method with two different but similar algorithms (only in writing). Authors provide formulation for both with and without momentum on local gradient updates.  Authors also provide experimental analysis on non-FL datasets (CIFAR, ImageNet, Shakespeare) that are sharded to adapt to FL settings. Authors also provide results from an implementation of the algorithm on eight raspberry pi devices.","The work proposes to overlap the local gradient computation step on each client in a federated ML setup with the latency cost associated with communicating the averaged gradients from all clients. This is done by allowing the clients to proceed with computation of local gradients on K mini-batches for a new round (t+1) using the latest local parameter until the gradient average from the previous round (t) is received with a delay (aka, *Delayed Gradient Averaging*). In doing so, the client is said to have performed D extra local updates which are combined with the delayed gradient average. The work is said to be the first of its kind to consider high latency (>1s) situations to perform scalable federated learning without sacrificing performance.",This paper propose a delayed gradient averaging method to hide communication latency behind computation in federated learning scenario to achieve better speedup. Theoretical and experimental results all show its advantage over FedAvg in some scenarios. The authors also carefully design simulate real federated learning environment to evaluate their method.,"This paper presents a new distributed federate learning method, DGA, to overcome long communication latency. The authors provide theoretical analysis about DGA's convergence rate, which shows that DGA makes same convergence as FedAvg, and empirically show that DGA can tolerate long network latency without hurting the accuracy. DGA delays the gradient averaging to a future iteration so that the communication can be overlapped with local update computation, which enables scalability even under long latency and network stragglers. In detail, in DAG, a training is divided into communication rounds, and each round consists of K local updates and one synchronization (gradient averaging). DGA allows gradient averaging at i-th round to D-th updates in a future round. DGA achieves high speed-up without hurting the accuracy by allowing the same communication frequency while reducing latency by controlling D knob.",0.15730337078651685,0.15730337078651685,0.1797752808988764,0.08,0.16,0.24489795918367346,0.112,0.2857142857142857,0.11428571428571428,0.20408163265306123,0.14285714285714285,0.08571428571428572,0.13084112149532712,0.2028985507246377,0.13973799126637557,0.11494252873563218,0.15094339622641512,0.126984126984127
550,SP:79669ef55faa1f00a2d340af03b26f22213017e3,"The authors introduce an extension to the new, but popular, data shift benchmark WILDS data sets called U-WILDS.  U-WILDS increases the number of instances in these data sets significantly (by a factor 3.5-14x times, depending on the data set), but includes no additional labels.  Such an extension allows unsupervised domain adaptation techniques to now be evaluated on the various WILDS data sets.  The authors evaluate a sampling of the state of the art in unsupervised domain adaptation on these new data sets.  The main finding is that most of the techniques performed poorly, except data augmentation in the image problems.  The authors suggest that this could motivate the need for better data augmentation techniques for other modalities.","The authors propose U-WILDS, which extends the WILDS benchmark (typically used for domain generalization or subpopulation shift) to the unsupervised domain adaptation scenario. They select eight datasets from WILDS, spanning a variety of data modalities, and add in additional unlabelled examples from a variety of data sources. They benchmark a comprehensive set of algorithms which make use of unlabelled data, and find that most methods do not significantly outperform ERM, except for some limited cases which the authors characterize in detail.","The authors present U-WILDS, an extension of the multi-task, large-scale domain-shift dataset WILDS. They provide a large quantity of unlabelled data complementing 8 of the existing multidomain labeled datasets in WILDS. They propose an extensive array of experiments evaluating the ability of a wide variety of algorithms to leverage the unlabelled data to address domain-shift. They present reasoned conclusions, and open-source datasets and implementations. ","### New large scale dataset for unsupervised transfer learning  The paper proposed an extension to the popular WILDS benchmark dataset by augmenting the different domain data with additional unlabeled examples. The dataset consists of data of various modalities including images, graph and text from various domains. Additionally, many recent methods that leverage unlabeled images to achieve generalization are bench marked on the proposed datasets. ",0.1652892561983471,0.14049586776859505,0.14049586776859505,0.23170731707317074,0.18292682926829268,0.17142857142857143,0.24390243902439024,0.24285714285714285,0.2698412698412698,0.2714285714285714,0.23809523809523808,0.19047619047619047,0.19704433497536944,0.1780104712041885,0.18478260869565216,0.25,0.20689655172413793,0.18045112781954886
551,SP:79a9b42ae4985b493cb0b00c862a9ce1a0343165,"This paper introduces SegFormer for semantic segmentation. The SegFormer consists: (1) A Transformer encoder that extracts multi-scale features. The authors claim their Transformer encoder to be novel in the sense that it is ""positional-encoding-free"" and ""hierarchical"". (2) A ""MLP"" based decoder that aggregates information from different layers. Although the authors claim it to be ""MLP"", the decoder is essentially a stack of several 1x1 convolutions with bilinear interpolation to upsample low-resolution features.  The authors further explore scaling the Transformer encoder which ends up with a series of models with different number of parameters and FLOPs. The authors show their models are competitive with SOTA on multiple semantic segmentation datasets and claim that their model is more robust on corrupted images than DeepLabV3+ (on Cityscapes-C).","This paper introduces the SegFormer model, a Transformer-based model for semantic segmentation (i.e. dense pixel classification) in images. The model uses prior architectural innovations such as a pyramidal structure (progressive downsampling) and a factorized version of self-attention for computational efficiency. The main novelties over prior work on Transformer-based semantic segmentation models and image classification models are: 1) the use of convolutional filters to propagate position information from the image boundaries (as opposed to using explicit positional encodings) and 2) skip-connections between individual layers directly to the output layer, using spatial upsampling. Experimental evaluation demonstrates improved mIoU scores on default semantic image classification benchmarks compared to a baseline SETR (Segmentation Transformer) model, while achieving computational benefits thanks to the adoption of a more efficient Transformer architecture.","This work proposed an efficient Transformer-based semantic segmentation model called SegFormer. In essence, it consists of a hierarchical Transformer encoder and a lightweight MLP decoder. Compared to ViT, the transformer encoder employs overlapped patch merging, self-attention with reduced (key, value) for efficient computation, and a depthwise convolution inserted between two MLP layers in FFN to replace position embedding. The authors also conducted an analysis based on effective receptive field and revealed that transformer's encoder with large receptive field enables relative lightweight decoder design. Extensive experiments demonstrate that SegFormer achieves strong speed-accuracy trade-off and better robustness.","This paper presents a simple, efficient yet powerful semantic segmentation framework which unifies Transformers with lightweight multilayer perceptron (MLP) decoders. The proposed SegFormer has two appealing features: 1) SegFormer comprises a novel hierarchically structured Transformer encoder which outputs multiscale features. It does not need positional encoding, thereby avoiding the interpolation of positional codes which leads to decreased performance when the testing resolution differs from training. 2) SegFormer avoids complex decoders. The extensive experiments show that the proposed method achieves the state-of-the-art performance on semantic segmentation task.",0.18604651162790697,0.17829457364341086,0.1937984496124031,0.14615384615384616,0.13076923076923078,0.15,0.18461538461538463,0.23,0.2808988764044944,0.19,0.19101123595505617,0.16853932584269662,0.18532818532818535,0.20087336244541484,0.22935779816513763,0.16521739130434784,0.15525114155251143,0.15873015873015872
552,SP:79c810b2a774a5c4f545e14110eb1a5d39b97275,"This paper proposed a method to run adversarial attacks on a k-NN classifier based on a breadth-first search. The proposed algorithm gradually expands the radius of searching until an adversarial example is found. The authors theoretically studied the optimality of the proposed method and proposed necessary approximations. The limitation and advantages of this method are also addressed. Finally, the experimental results show the effectiveness of this method.","This paper focuses on the problem of generating adversarial perturbations for $k$-nearest-neighbor classifiers. Focusing on the geometric nature of $k$-nearest-neighbor classification, the proposed scheme GeoAdEx works with order-$k$ Voronoi cells and facets to search around the test point being attacked by gradually increasing the search radius around it and updating the upper and lower bounds to the optimal adversarial perturbation norm. Beyond this novel algorithm to generate the optimal perturbation, the paper focuses on making this algorithm meaningfully scalable in high dimensional data with $k>1$ nearest-neighbors. The first optimization is the observation that the task of finding the neighboring order-$k$ Voronoi cells to the current cell in question can be solved by finding the neighboring order-$1$ Voronoi cells -- this is an improvement though finding order-$1$ Voronoi cells in high dimensions is still a very significant bottleneck, and hence needs to be approximated by just using the $m$ nearest neighbors of all the $k$ points in the current cell. This introduces a significant approximation that appears to be necessary to make the proposed algorithm tractable. Without this approximation, the proposed algorithm is guaranteed to find optimal perturbation; the above approximation does not have any such property. Empirically, the proposed algorithm is evaluated against existing baselines, with significant improvement, however, with an order of magnitude more computational cost. The empirical evaluation also study the effect of the class separation on the relative performance of the proposed GeoAdEx against baselines, demonstrating the intuitive result that GeoAdEx performs better with small class separations since the baselines tend to miss the closeby small order-$k$ Voronoi cells which GeoAdEx almost exhaustively searches over. ","Prior adversarial attacks on k-nearest neighbor (k-NN) do not scale well with the increase of k and the number of data. This paper proposed a set of better heuristics towards speeding up this problem. Theoretically, they show that their method still finds the optimal solution if no time limit. Empirically, they show that their methods can provide stronger attacks than prior works. ","The paper describes a ""white box"" adversarial attack on k-NN classifiers that is based on the geometry of the Voronoi diagram. The paper first defines the relevant geometric elements - Voronoi cells and their facets (which reside on bisectors - hyperplanes that are equidistant to two data points) and when they are considered *adversarial*. The paper then proposes an algorithm (GeoAdEx) for searching for the nearest adversarial facet for a given test point, using a priority queue in a BFS manner. GeoAdEx is shown to provide an optimal (min L2) attack. To allow scaling GeoAdEx to k>1 and large data, several approximations are suggested (which may result in a suboptimal attack). The experimental section compares the mean attack size for seven datasets against three other kNN attacks (for k=3, 5, 7) and shows that GeoAdEx results in a smaller-norm attack.",0.34782608695652173,0.21739130434782608,0.2898550724637681,0.07194244604316546,0.11510791366906475,0.1875,0.08633093525179857,0.234375,0.14084507042253522,0.3125,0.22535211267605634,0.08450704225352113,0.138328530259366,0.2255639097744361,0.18957345971563985,0.11695906432748537,0.1523809523809524,0.11650485436893204
553,SP:79f8ea63cb68c5f3f839755358b4b3ca8d52c31d,"Authors propose D-CODE, a framework to approximate closed-form ODEs using symbolic regression. The method first estimate the trajectory by smoothing the observations (using any convenient smoother) before inferring a closed form (finite sequence of operations) transition function. This allows for better interpretability of the system.   Authors test D-Code on 5 simulated datasets, where they show high performance of the method, and on one real dataset where they use D-Code to model the temporal effect of chemotherapy on the tumor volume, and compare it to Total variation regularized differentiation (SR-T), the closest method in the literature. ","This paper proposes a new methodology to infer symbolic ODE representation from observed time series. In contrast to previous methods, they bypass the inference of the time derivative and rather proceed by first estimating a continuous approximation of the trajectory and then optmizing a novel objective function. The first step can be performed with the interpolation method of choice (GP or splines). The second step can use any optimization scheme that does not require derivative (such as genetic algorithm). The authors then evaluate their approach. on a series of dynamical system and show improved performance compared to the baselines under consideration.",This paper suggests a variational formulation for discovering ODE systems in a closed-form. The main advantage of such a formulation is that it can find the system based on the observed trajectories (and some analytical testing functions) rather than estimating (possibly noisy) derivatives of them. The variational objective is equal to the distance between estimated and true velocity vector fields on the manifold when the estimated trajectory converges to the true one in L2 sense and there are infinitely many testing functions (while a finite number of functions are okay empirically). The proposed method outperforms its counterparts for various ODE systems and tumor volume dataset (whose true dynamics is unknown).,"The paper addresses the problem of learning closed form ODEs from observational data, when the observation can be noisy and not frequent enough that instantaneous derivatives can be estimated with low enough error. The work uses a variational criterion on the solution of the ODE, circumventing the need to evaluate instantaneous derivatives. It uses existing symbolic regression techniques but instead of the regression loss, it minimizes a loss term quantifying the violation of the variational constraints. It is a work combining existing results in a novel way, but offers new perspective and there are synergies between the different parts applied.",0.18,0.18,0.16,0.2079207920792079,0.16831683168316833,0.18018018018018017,0.1782178217821782,0.16216216216216217,0.16,0.1891891891891892,0.17,0.2,0.17910447761194032,0.17061611374407584,0.16,0.1981132075471698,0.1691542288557214,0.1895734597156398
554,SP:7a0d5ef34393c18c76eefef957b7137c5392d9e0,The paper proposes an new approach for exploration in reinforcement learning (RL). The idea is to give an exploration bonus to visiting with the new policy state-action pairs which have low visitation density compared to previously visited state-action pairs. The paper provides a convergence proof assuming a perfect oracle provides visitation densities for a chosen mixture policy. The proposed approach is compared in experiments to RL baselines in both continuous and discrete control tasks with function approximation and in tabular benchmarks.,"The authors propose adding a regularisation term, that encourages a policy to visit states, less visited by previous policies, to the RL problem to address the exploration-exploitation tradeoff. The result is a convex regularised optimisation problem in the state occupancy of the policy. In addition, the authors propose an algorithm to solve this problem that is based on the Frank-Wolfe algorithm, similar to Hazan et al. i.e., they iteratively find a policy that has a state occupancy with maximal correlation with the negative gradient of the objective. Experiments are performed in several domains and include a comparison with various baselines from the reward free literature. ","This paper introduces a new exploration technique called MADE that computes a bonus that depends on the ratio between the visitation densities of the current policy and policy cover of previous policies. This incentivizes the agent towards it has not been before and leads to more exploration. The authors provide a convergence result for this bonus and evaluate it on grid worlds, Mini Grid and Mujoco environment to show that it leads to better exploration than existing techniques.","The paper proposes an alternative exploration strategy by MAximizing the DEviation (MADE) of the next policy from explored regions of prior policies by adding an adaptive regularizer (function of state-action visitation counts) to the primary RL objective at each iteration to balance between exploration the new state space and exploit the visited states.  In addition, MADE has been theoretically proven to converge to the global optimum given access to an approximate planning oracle.  The authors show that MADE applies some simple adjustments to the Hoeffding-style count-based bonus. Moreover, experiments in three different RL algorithms (Value iteration, PPO, Q-Learning) show the superiority over the state-of-the-art UCB-bonus methods (Hoeffding and Bernstein bonus). In addition, the authors demonstrate that MADE can practically work both in Model-based (Dreamer) and Model-Free (IMPALA, RAD) RL algorithms, significantly improving sample-efficiency over baselines.",0.18072289156626506,0.1566265060240964,0.25301204819277107,0.1388888888888889,0.2222222222222222,0.21794871794871795,0.1388888888888889,0.16666666666666666,0.14383561643835616,0.19230769230769232,0.1643835616438356,0.11643835616438356,0.15706806282722513,0.16149068322981364,0.18340611353711792,0.16129032258064516,0.1889763779527559,0.15178571428571427
555,SP:7a10a17bfab78983e550446ab98bc57689a9787d,This paper presents a nice framework of test-time fine tuning through self-supervision for adversarially trained networks with the purpose of improving the robust accuracy on test data. A meta adversarial training strategy is also proposed that strengthens the correlation between the self-supervised and classification tasks to provide a good starting point for test-time fine tuning. The proposed method improved robustness performance against different attack methods including adaptive attacks where the attacker has knowledge of the fine tuning technique.,"Propose to improve the generalization and robust accuracy of adversarially-trained networks via self-supervised test-time fine-tuning. Introduce a meta adversarial training method to find a good starting point for test-time fine-tuning, which  incorporates the test-time fine-tuning procedure into the training phase. Observe consistent improvement under different attack strategies for both white-box and black-box attacks.","Fine-tuning the parameters of an adversarially-trained model at test-time with self-supervised tasks improves robustness to standard adversarial attacks. By incorporating meta-learning into the adversarial training stage, the test-time fine-tuning achieves marginally higher accuracy (though not reduced optimization time, or at least this is not reported). The test-time optimization approach requires the training data be kept available for joint updates on training batches and test data. The self-supervised tasks considered are rotation prediction, a common choice, and vertical flip recognition, which is a close relative of it. Results are reported on the standard adversarial defense benchmark of CIFAR-10, and additionally models are also evaluated on STL and Tiny ImageNet. Design choices are ablated and an adaptive attack is designed and experimented with, but there other checks against obfuscated gradients (for instance those suggested by Athalye et al. 2018) are not pursued, such as attacks in expectation, or decision-based attacks. No comment is made about the computational cost of test-time fine-tuning.","The paper introduces a meta adversarial training method to find a good starting point for test-time fine-tuning. It incorporates the test-time fine-tuning procedure into the training phase and strengthens the correlation between the self-supervised and classification tasks. Results on CIFAR-10, STL10 and Tiny-ImageNet show consistent robustness improvement.",0.2804878048780488,0.2804878048780488,0.24390243902439024,0.3333333333333333,0.47619047619047616,0.14534883720930233,0.36507936507936506,0.13372093023255813,0.37037037037037035,0.12209302325581395,0.5555555555555556,0.46296296296296297,0.31724137931034485,0.1811023622047244,0.2941176470588235,0.17872340425531913,0.5128205128205129,0.22123893805309736
556,SP:7a181bdade047875c1dd26d4fdfbe3de02804bb7,"This paper proposes Signed Supermask,  an extension of the original Supermask work (Zhou2019) for finding more efficient untrained subnetworks. Instead of learning a binary mask, Signed Supermask claims and shows that adding another dimension -1 to the masks leads to higher sparsity with higher accuracy. The main contribution of this paper is the introduction of weight flipping, as an improvement over the existing approaches. The method is simple and effective. The empirical experiments validate the effectiveness of the proposal. ","This paper discusses the signed supermask that can improve the model accuracy of untrained neural networks significantly while enhancing sparsity compared to the original supermask idea. The authors introduce ""-1"" as an additional mask value to enable flipping a sign of an initialized weight and suggest related activation functions and fixed threshold hyperparameters to achieve sparse networks. Analysis on sparsity and corresponding accuracy is given for various CNN models including ResNet models. The role of batch normalization for large models is also studied.","This paper introduces ""signed supermasks"", which builds on top of the supermasks line of work of finding binary masks on untrained networks that result in good performance. This work extends the original supermask by adding the ability to flip the sign of the weights. The proposed method learns parameters of a mask, and converts the mask parameters into a tertiary mask through the use of two thresholds. The paper further proposes to use ELU activation function and a ELUS initialization scheme that is more tailored to supermask training. Performance is compared against fully-trained models and prior work on supermasks on MNIST, CIFAR10, and CIFAR100, and the proposed method shows competitive results.","This work extends to (Zhou et al., 2019) and (Ramanujan et al., 2019) to allow sign flipping in the supermasks, without updating the magnitudes of weights. The main technical contributions are the new thresholding-based training method and the new weight initialization scheme that considers the supermasks. Empirical results show better performance on small conv nets and also residual networks on CIFAR-10/100 datasets.",0.21518987341772153,0.27848101265822783,0.16455696202531644,0.25301204819277107,0.12048192771084337,0.15178571428571427,0.20481927710843373,0.19642857142857142,0.2,0.1875,0.15384615384615385,0.26153846153846155,0.20987654320987653,0.23036649214659682,0.18055555555555555,0.21538461538461537,0.13513513513513514,0.192090395480226
557,SP:7a6ab0993dbc9b8258c39f1fe22ea8fe88b8e0ae,"This paper proposes a method for compressing the word embeddings. Though use cases of the compressing the (word) embeddings are much wider, this paper only focuses on the embeddings in pre-trained language models.  The main idea of the proposed method is to introduce the sub-embeddings shared among vocabulary. In detail, this paper proposes two variants for sharing the sub-embeddings, namely, random and cluster-based methods.  The experiments are conducted on GLUE and XNLI benchmarks. The results show that the proposed method provides to reduce the model size with the small performance degradation.","This paper proposes an embedding compression method which replaces an $N$ x $d$ embedding table with $k$ distinct $M$ x $(d/k)$ embedding tables (where $M = N^{1/k}$). Specifically, the $d$-dimensional embedding for each token in the vocabulary is replaced by $k$ different sub-embedding vectors $(e_i)_{i=1}^k$, each of dimension $d/k$, where $e_i$ is drawn from the $i^{th}$ codebook. It proposes two ways of selecting these sub-embeddings: 1) arbitrarily assigning the sub-embeddings to each token (based on its index), 2) using k-means to do this assignment procedure.  It shows that very high compression rates (e.g., 99.4-99.9% compression) can be attained using this method, while typically only sacrificing 0%-4% absolute accuracy. ","This paper proposes a method for reducing the memory footprint of word embeddings by representing each word not as its own unique embedding vector, but rather as a concatenation of K lower-dimensional sub-embeddings, where the sub-embeddings can be shared between different words. By properly choosing K, the number of unique sub-embeddings M, and the dimension of the sub-embeddings, the number of parameters can be reduced drastically. Two techniques are given to map the words to their respective sub-embeddings: one is essentially a uniform mapping which completely ignores any semantic meaning of the words, and the other approach uses k-means clustering to preserve some of the semantic structure present in the original word embeddings. These new word embeddings are then tested on a few tasks. As expected, the performance of the uniform mapping is much worse than that of the original word embeddings, since all semantic structure was lost. The performance of the k-means sub-embeddings approaches that of the original embeddings, while requiring far fewer parameters. ","This paper aims to reduce the size of word embeddings in pretrained language models. The main idea is to replace the lookup table embeddings of size $N$ with only $k$ sub-embeddings of $N^{1/k}$ size and a mapping that allows to combine entries of the sub-embeddings to produce unique embeddings of the same size via concatenation. Two different types of mappings are explored, one random and one derived from clustering word embeddings from a pretrained model. For evaluation, the procedure involves pretraining a language model with the proposed embedding from scratch and then finetune it on downstream tasks. The  results show that the embedding can be reduced by 99.9% while maintaining performance within ~1.4% difference on a XNLI benchmark and within ~4.8% difference on GLUE benchmark.",0.2,0.3368421052631579,0.3157894736842105,0.1889763779527559,0.18110236220472442,0.1781609195402299,0.14960629921259844,0.1839080459770115,0.22727272727272727,0.13793103448275862,0.17424242424242425,0.23484848484848486,0.17117117117117117,0.23791821561338292,0.2643171806167401,0.15946843853820597,0.1776061776061776,0.20261437908496732
558,SP:7a7e19b8700810f1092b886220a9c640f11cf6f3,"The paper present a novel method for imitation learning aiming to stabilize training and mode coverage compared to existing methods. The proposed method (SLIL) extends behavioral cloning by an additional objective of maximizing a state-only reward that is defined by a density estimate (trained once beforehand) of the expert's state distribution. For density estimation SLIL uses a continuous normalizing flow where the expert states are corrupted by Gaussian noise, where the noise-level is decreased during training. The noise is introduced to tackle the ""Manifold Hypothesis Challenge"", that is, the inability of normalizing flows to accurate approximate distributions that lie on a lower-dimensional manifold.   The main contributions are: 1. A novel imitation learning method that combines the BC loss with a RL loss for reaching expert states 2. A slightly modified approach to tackle the manifold hypothesis challenge (Kim et al. [2020] used Gaussian noise where the noise-level was sampled from a uniform distribution, independently for each data set, rather than using the same noise-level for all data and annealing it during optimization)","This paper proposes a straightforward non-adversarially-trained imitation learning method. The goal is to avoid mode collapse and training instability that can occur in some adversarially-trained imitaiton learning methods, while still addressing the distribution shift problem that basic Behavior Cloning is subject to.   The paper: - Motivates a two-stage learning procedure by starting with a likelihood-based state-action distribution matching objective that would require bi-level optimization - Presents a lower-bound of the objective in which optimization can occur serially of two components: MLE of the expert's vistation distribution, and then policy optimization over two terms, one of which is the LL of visited states under the the modelled visitation distribution, and the other is the behavior cloning loss. - Presents a proof that it is a lower bound.  - Proposes a concrete approach to performing this optimization problem by using a denoising conditional normalizing flow for modeling the expert's visitation distribution, where the CNF seems to be motivated because of its ability to model densities of complex distributions, and the denoising is motivated to enable stable training in situations in which the data lie in a lower-dimensional manifold.  - Performs experiments on 10 mujoco environments that illustrate that the proposed approach achieves better mode coverage than some comparable methods, that the denoising is empirically effective for generalization, that the algorithm is more robust than GAIL to variation in learning rate, among other hyperparmaeter variations.","This work proposed Stabilized Likelihood-based Imitation Learning (SLIL) which iteratively estimates the expert state distribution by using Denoising Continuous Normalizing Flow (DCNF) and maximizes the policy learning objective in Eq. (3) that matches expert policy and state distribution. Compared to other baselines such as BC, DRIL, GAIL, SLIL was shown to find out better expert state distribution (in terms of seeking multiple modes, Earth Mover's Distance (EMD) while achieving the empirical return close to that of the expert (which is widely used in the imitation learning literature). The strength of using DCNF instead of using Continuous Normalizing Flow (CNF) is well desribed and supported by the experiments evaluating the test log likelihood in Figure 9. Authors also analyze SLIL's hyperparameter sensitivity that supports the claim on the SLIL's stability.  ","The paper proposes an approach to the problem of state-action based imitation learning, in which an agent aims to solve a specific task given state-action expert trajectories. Their objective SLIL combines (or interpolates between) generative modeling and behavioral cloning in order to avoid mode collapse of the agent policy (common issue of adversarial approaches to IL like GAIL) and to mitigate distributional shift which BC typically suffers from.   Contributions: - An extension of continuous normalizing flows, coined denoising continuous normalising flows (DCNF), which is claimed to alleviate the manifold hypothesis, and is trained via maximum likelihood estimation. - A strong imitation learning algorithm consisting in estimating the expert's state distribution, and learning a policy that recovers the expert policy and that has an occupancy distribution close to the distribution induced by generative model learned via the DCNF. ",0.23595505617977527,0.12921348314606743,0.15730337078651685,0.12605042016806722,0.13445378151260504,0.17293233082706766,0.17647058823529413,0.17293233082706766,0.2028985507246377,0.22556390977443608,0.2318840579710145,0.16666666666666666,0.20192307692307693,0.14790996784565916,0.17721518987341775,0.16172506738544473,0.1702127659574468,0.16974169741697415
559,SP:7ad387a7c3ee9371f8f84de22f5844bdd021e1d6,"The paper proposes a graph neural network based forecasting algorithm for battery state of charge. Building on prior work, the paper proposes a regularization based on reconstruction loss. Evaluation on a battery dataset shows improvements.","This work proposes a new method based on graph neural network for Lithium-ion batteries parameters estimation. In a typical time-series data, one is given some past timestamps and is asked to predict some future timestamps. In this problem, you have multiple time series of battery parameters. The goal is to predict multiple future states for all the time series data. Since the time series data could be correlated, this work introduces a graph neural network where each time series source is a node on the graph, and the node relations are learnt through training. The graph is used in the recurrent graph convolution and graph autoencoding. The loss function has a future prediction component, and an autoencoding reconstruction component to regularize the learning. The method is validated on a Lithium-ion battery dataset. It outperforms the compared state-of-the-art method.",This paper studies a new problem of battery parameter estimation. The paper proposes a new graph autoencoder time series estimation approach that learns the underlying relationship between variables in battery measurements. Experimental results show that the new method outperforms the SOTA method and achieves satisfactory results.  ,"The paper proposes to incorporate a graph-autoencoder method from the field of causal structure learning for improving graph-based forecasting, with a focus on battery parameter estimation. In particular, the authors include a regularization term based on graph-autoencoder in the forecasting model to learn the variable dependencies. The authors demonstrate empirically that the proposed method has an improved performance over the baseline considered.",0.42857142857142855,0.22857142857142856,0.37142857142857144,0.125,0.125,0.32608695652173914,0.10416666666666667,0.17391304347826086,0.2,0.391304347826087,0.27692307692307694,0.23076923076923078,0.1675977653631285,0.19753086419753085,0.26,0.18947368421052635,0.1722488038277512,0.2702702702702703
560,SP:7b25050fda09ae3dd8454c2b5ee66325c3cd74be,"This paper considers the problem of training image classification for meta-learning/few-shot learning models with large images. The authors point out that many categories of meta-learning algorithms have difficulty using large images because of memory constraints brought up by computation of gradients computed during meta-training. They decompose 3 major categories of meta-learning models (CNAP, MAML, and Proto-Nets) into a common gradient update that is troublesome w/r/t memory. Their idea to make training more feasible is to use all the support set items in the forward pass but approximate the backward pass using a small random subsample of the examples in the support set. They compare training their versions of memory-efficient, large input meta-learning models against recent work on ORBIT (a few-shot object recognition task) and VTAB+MD (a newer benchmark for few-shot classification that is based on Meta-Dataset and VTAB datasets). On both benchmarks, they show that training using larger images using their proposed method improves performance.",The authors propose a method that enables few-shot learners to be trained on 1 GPU without memory issues. This is very important since most few-shot learners need to process the whole train set of a task before evaluating the loss on the task. They show mathematically that their method gives an unbiased estimation of the true gradient (if all samples in training were back-propagated to update the model). They evaluate their method on CNAPS and ProtoNets and show that it can achieve better results since it can be trained with larger images.  ,"Meta-learning algorithms are not memory friendly since they have to load all the support samples during the inner-loop. When the support data are high-resolution images, or need huge support samples (i.e. 1000), the meta-training is hard to work on a single GPU. As a result, this paper aims to propose an efficient meta-training algorithm which only rely on the gradients of subsets of the support data. And they show the can achieve competitive results against transfer learning approaches but at a fraction of the test-time computational cost on multiple benchmarks.","Many meta-learning approaches have been trouble with the scalability issue due to the large memory-required computation during the training process. To overcome such a problem, this paper proposes LITE that is memory efficient episodic training scheme allowing meta-learning methods to use the benefit of large-size images of tasks on a single GPU. On two benchmark datasets (ORBIT and VTAB+MB), meta-learning methods combined with LITE outperform meta-learning methods without LITE and Simple CNAPs (meta-learning method) + LITE shows competitive performance compared with transfer learning methods.",0.1411764705882353,0.12352941176470589,0.1588235294117647,0.15789473684210525,0.1368421052631579,0.21649484536082475,0.25263157894736843,0.21649484536082475,0.2967032967032967,0.15463917525773196,0.14285714285714285,0.23076923076923078,0.1811320754716981,0.15730337078651685,0.20689655172413793,0.15625,0.13978494623655913,0.22340425531914893
561,SP:7b4e9cd03685cb2714e3551b11bdf40765ea127b,"The authors investigate the self-attention module architecture and provide a theoretical analysis for the limitation of using softmax function for normalization in the module. In this regard, an unconstrained normalization function is proposed as alternative in the transformer. The authors compare it with baseline transformer and several variants to validate the effect of the proposed normalization function in transformer.","This paper looks at one specific aspect of transformer architectures - the normalization step that constrains the attention vectors in a probability simplex. The authors argue that this restriction has some limitations including limited information flow, makes the models biased towards local information at initialization and increases the sensitive to hyperparameters. To mitigate this, the authors propose an architecture that replaces the softmax in the attention mask with a simple normalization. Then, with several experiments on a synthetic dataset, the authors demonstrate that the proposed architectural change leads to models that are robust to hyperparameters, and less sensitive to biases in the data. ","The authors explore the implications of using softmax to implement attention mechanisms in NLP models, particularly transformers. They highlight the theoretical limitations of using softmax-attention and propose a normalization-based attention mechanism (NAP) that overcomes these constraints.. The authors conduct a set of synthetic experiments to compare different attention mechanisms’ performance and demonstrate that the proposed mechanism performs well compared to the traditional softmax attention while being  less sensitive to hyperparameters change.","This paper starts from the observation that current self-attention modules is sensitive to hyperparameter changing. The authors conjecture that this is due to the softmax operator in self-attention, and give some intuitive examples to support their hypothesis (eg., bias towards local information, gradient vanishing). To solve the problem, the authors propose to replace the softmax with a normalization operator. They show that normalized attention is more robust to hyperparameter in a synthetic dataset, as well as real-world settings.",0.26666666666666666,0.25,0.23333333333333334,0.19607843137254902,0.2549019607843137,0.1917808219178082,0.1568627450980392,0.2054794520547945,0.1728395061728395,0.273972602739726,0.32098765432098764,0.1728395061728395,0.19753086419753085,0.22556390977443608,0.19858156028368795,0.22857142857142856,0.28415300546448086,0.18181818181818182
562,SP:7b6c5a2384a8f0f5d1da09a92691ec0b4eb36b94,"**Summary**  The authors propose a white-box model that uses a similarity score in a space that is constructed based on topological transformation. In other words, two data points are close to each other if you can transform them into each other. To achieve this, they first compute the topological distance with training data points while minimizing the distortions (this part was a little bit unclear to me. I would appreciate it if the authors provide more context or examples of this). They use an interesting idea of using gradient descent to find a sequence of transformation which is the main reason that contributes to having a white-box model. I found this part of the paper very interesting and novel. Finally, it is not possible to apply this method at the pixel level. So they propose using a chain of lattices to be able to apply their technique in higher abstractions.    ","The paper on hand tackles the problem of learning from very few samples, which is of high relevance for many machine learning problems. To this end, an approach to model transformation-based topological similarity is introduced, allowing for covering many kinds of invariants in image data. The approach is demonstrated for well-known benchmark datasets for different classification models, demonstrating that in this way just using a small number of samples competitive classification results can be obtained.","This paper proposes a novel white-box model for one or few-shot learning, which tries to simulate the human recognition ability for “distort” objects. The authors use transformation-based topological similarity to build the model and propose an anchor system with gradient descent to train their models. Extensive experiments on standard character recognition benchmarks demonstrate the proposed method outperforms the classical machine learning methods with very limited training data, such as less than 20.","This paper proposes a method that models the visual difference between images as a topological transformation for images. Using this model and computing the topological distance by minimizing the distortions between imagers, one can find neighbors that are conceptually similar to the input image. Evaluation results show that this simple method can achieve strong performance when only very few samples are available and no pretraining is allowed.",0.1118421052631579,0.10526315789473684,0.125,0.12987012987012986,0.12987012987012986,0.17333333333333334,0.22077922077922077,0.21333333333333335,0.2835820895522388,0.13333333333333333,0.14925373134328357,0.19402985074626866,0.148471615720524,0.1409691629955947,0.1735159817351598,0.13157894736842102,0.13888888888888887,0.1830985915492958
563,SP:7b83b7e230498a219c2f37f4b35f1ea9c89cc5a7,"This paper considers a problem where one is trying to find an input that minimizes the expected value of a function that one has sample values of. Its goal is to show that algorithms that only have the ability to query the gradient of the average of the samples at specified points may need more queries than a function that can check the gradient on individual samples. In order to do that, the paper finds a class of probability distributions over functions for which any algorithm that gains information on the functions only by querying the gradient of the average of the sampled function needs $\Omega(1/\epsilon^4)$ queries to get within $\epsilon$ of the minimum expected loss."," The paper provides a lower bound on the iteration complexity of any full batch algorithm in stochastic convex optimization. In particular, they show that for every T, there exists a function (which depends on T) for which the suboptimality in terms of excess risk for any full batch algorithm run for T steps is lower bounded by 1/T^{1/4}. The proof technique extends upon the lower bound construction of Amir et. al. 2021 [1] for the iteration complexity of GD algorithm, and is based on a span based argument.  ","This work studies the generalization performance of algorithms in stochastic convex optimization. Specifically, the authors study the full-batch algorithms where only full gradient and function value information are accessible. By constructing a hard instance of the objective function, the authors show that any full-batch algorithm needs at least $\Omega(1/\epsilon^4)$ iterations to obtain an $\epsilon$-risk function, while stochastic algorithms such as SGD only need $O(1/\epsilon^2)$. ","In this paper the authors prove an error lower-bound for full-batch first-order algorithms. This is a generalization of the bound given in Bassily et al. [2]. This new bound of the authors - is true for a wider class of functions : GD and SGD, non-smooth and convex  - implies that to get an $\epsilon$-risk, full-batch methods require at least $\Omega (1/\epsilon^4)$ iterations This shows the advantage of stochastic methods like SGD which only require $\Omega (1/\epsilon^2)$ iterations to reach the same risk.",0.16806722689075632,0.17647058823529413,0.16806722689075632,0.17582417582417584,0.1978021978021978,0.3698630136986301,0.21978021978021978,0.2876712328767123,0.2222222222222222,0.2191780821917808,0.2,0.3,0.1904761904761905,0.21875000000000003,0.19138755980861244,0.1951219512195122,0.19889502762430936,0.3312883435582822
564,SP:7b93579cbf1d15262e1a5d7838ade8f8776e8570,"This paper mainly shows two insights: 1) When optimizing in the terminal phase of training, multiplicative logit adjustment is critical. 2) Compared with multiplicative adjustment, additive adjustment can speed up the convergence at the beginning of the training. Motivated by the insights, a new vector-scaling (VS) loss is proposed to exploit the advantages of both multiplicative adjustment and additive adjustment, which is further introduced to group-sensitive settings. What’s more, a generalization analysis of the VS-loss on binary over-parameterized Gaussian mixtures is presented, revealing tradeoffs between balanced/standard error and equal opportunity.","This paper studies the training loss used by neural networks for class-imbalanced learning/group-sensitive. It wants to investigate among two terms, multiplicative and additive, added to the weighted cross-entropy loss, how do they contribute to the class-imbalanced/group-sensitive learning and in what phase. By studying this problem, the paper did some experiments to get some insights and proved the equivalence of the solution achieved by the weighted cross-entropy loss with the additional terms, and the solution achieved by the cost-sensitive SVM on a linear model. The conclusion is drawn that the multiplicative term is more effective than the other one. On the other hand, it discusses that at the initial phase, the additive term may take effect. By additional distribution assumption of GMM model, the paper also derives generalization analysis and the trade-off between balanced error and imbalanced error. Finally, it shows some experimental results.  ============ Thank you for the rebuttal. I think the contribution of the paper is that it studies from the theoretical aspect a commonly met problem, and the insights drawn from theoretical studies can be used to design better loss functions.  In this way, I would keep my positive rate for this paper. ","This paper investigate the prediction problem under the settings of label-imbalance and group-sensitive. Previous literature propose several methods to solve it, such as weighted Cross-Entropy, additive/multiplicative adjustments on logits. This paper explore the mechanism and effect of these methods, and analyze the relationship to SVM and CS-SVM. It propose to combine additive/multiplicative adjustments to enjoy the benefit of both methods. The experimental results demonstrate the effectiveness of VS-loss which combine additive/multiplicative adjustments.","This paper studies loss functions in deep learning under imbalanced data. It focuses on the training and testing behavior after the training error is zero. It systematically reviews the previous loss functions, builds a connection between various loss functions and linear (cost-sensitive) SVMs, and then proposes a general way to set loss functions that recovers previous loss functions as special cases. It is claimed that this generalization is beneficial in improving the performance in the balanced test set for deep learning under imbalanced data beyond overparameterization. Specifically, the paper analyzes the performance metrics in fair machine learning like balance error and difference in equal opportunity and shows that the combined strategy in the loss function setup is beneficial for achieving more fair results.  ",0.2604166666666667,0.16666666666666666,0.20833333333333334,0.12254901960784313,0.15196078431372548,0.2,0.12254901960784313,0.2,0.16129032258064516,0.3125,0.25,0.12903225806451613,0.16666666666666663,0.1818181818181818,0.1818181818181818,0.176056338028169,0.1890243902439024,0.1568627450980392
565,SP:7bc236446a7104e576d235f1120e82f20943fbf4,"~~~~~~~~~~~~~~ Thank the authors for the clarification. Some of my former comments were nicely addressed and hence I am willing to increase my rating to 6.  ~~~~~~~~~~~~~~ This paper considers the offline setting of the contextual bandit with neural network function approximation. The key idea of the proposed NeuraLCB is to use neural network to learn the reward function and use a pessimism principle via a lower confidence bound (LCB) for decision making. In theory, the proposed approach is shown to learn the optimal policy with an error bound $O(\kappa \tilde{d}^{1/2} n^{-1/2})$ where $\kappa$ measures the distributional shift and $\tilde{d}$ is an effective dimension of the neural network. The empirical effectiveness of the proposed method is shown in a range of synthetic and real-world off-policy learning problems.  ",This paper is the first study considers offline policy learning for contextual bandits with neural networks. The authors proposed NeuraLCB algorithm that used neural network to model the rewards and followed pessimism principle with lower confidence bound in policy learning. It is a very intuitive combination. The algorithm works with mild assumptions with theoretical guarantee on its suboptimality. Experiments also showed that NeuraLCB outperforms other baselines.,"The paper studies the problem of offline contextual bandits, where policy learning can only leverage a fixed dataset collected a priori by behavior policies. Using a pessimism principle, the authors propose a new algorithm called NeuralLCB with overparameterized neural networks and provide theoretical regret guarantees based on the analysis framework of the neural tangent kernel. Experiments on both synthetic and real-world data are conducted, which confirms the theoretical results.","This paper proposes a neural network based contextual bandit algorithm in the offline setting where a dataset of contexts and rewards are given by a logging policy. The goal of the proposed algorithm is to learn an optimal policy from the offline dataset. The proposed algorithm NeuralLCB is similarly structured as the NeuralUCB algorithm (Zhou et a., 2019) in the online setting. The difference is that it uses a lower confidence bound for estimating the reward function instead of an upper confidence bound, and that the optimization procedure for learning the neural network representation is based on the loss on one data point instead of the whole historical data. The authors established an upper bound of the optimal gap of the learned policy and evaluated the algorithm on both simulation and UCI datasets.  ",0.1791044776119403,0.13432835820895522,0.23880597014925373,0.21212121212121213,0.2727272727272727,0.2857142857142857,0.36363636363636365,0.2571428571428571,0.24060150375939848,0.2,0.13533834586466165,0.15037593984962405,0.24,0.1764705882352941,0.23970037453183518,0.2058823529411765,0.18090452261306533,0.19704433497536944
566,SP:7c02a5348854a24f228456a383a21bc9eacdb10f,"This paper studies cyclical step sizes for heavy ball momentum. It is shown that under some conditions, it is possible to improve the convergence rate of heavy ball by relying on cyclical step sizes with K = 2. The result is obtained by a careful examine of the spectrum of the Hessian. The cases for larger K are also discussed. In general, the results are nice but also has its own limitation.","The authors study a variant of the heavy ball method, where the stepsizes are chosen in a cyclical order. The paper considers the problem of minimising quadratic functions (there is a paragraph on local analysis for more general functions, but this is reduced to  the quadratic case  by linearisation). In the case where the cycle is 2 and the intervals of the eigenvalues of the quadratic functions are known, the authors show how this cyclic stepsize choice can lead to acceleration. In the more general setting of cycle $K$, their analysis is subject to some polynomial.   ","This paper studies the cyclicial step-sizes to improve the worst-case convergence of the heavy ball method (deterministic version). More precisely, the authors 1) achieve a faster asymptotic worst-case convergence  rate compared to the polayk heavy ball method for the strongly convex and L-smooth quadratic objectives,  2) show how to select the optimal momentum and step-size cycles, and 3) extend the local convergence to the non-quadratic case. Furthemore, numerical experiments on quadratics and logistic regressions have been conducted to demonstrate the main strength of the cyclicial heavy ball method.","This paper studies using cyclical step-sizes of Heavy Ball for solving strongly convex quadratic problems. The authors first study setting the step size to two specific values periodically (period=2), where the values are related to the spectrum (i.e. the eigenvalues) of the underlying quadratic matrix. Assuming the spectrum can be divided into two groups. The authors prove that the resulting asymptotic rate of using the cyclical step size can be faster than that of the standard step size of Heavy Ball. Then, the authors discuss the extension to the case that the step size can have K values and the step size is set to the possible step sizes in a cyclical fashion. ",0.22535211267605634,0.2535211267605634,0.29577464788732394,0.20833333333333334,0.2708333333333333,0.26595744680851063,0.16666666666666666,0.19148936170212766,0.1810344827586207,0.2127659574468085,0.22413793103448276,0.21551724137931033,0.19161676646706588,0.2181818181818182,0.2245989304812834,0.21052631578947367,0.24528301886792453,0.2380952380952381
567,SP:7c675258d83f07a074b7f068e8d068766c27a121,"The paper studies the problem of computing a low rank decomposition of a matrix. Specifically, let $A$ be a $p \times r$ matrix and $B$ be a $X \times n$ matrix. Given the matrix $Y = AX$, the authors study the problem of computing the matrices $A$, $X$. Without further assumptions, the problem is ill-defined as for any decomposition $A$, $X$ and any invertible matrix $Q$, $AQ$ and $Q^{-1}X$ is another decomposition of the matrix $Y$.   The authors make an assumption here that the matrix $X$ is sparse in particular the matrix $X$ has iid entries where each entry is independently nonzero with probability $\theta$ and conditioned on an entry being nonzero it has gaussian distribution. Assume that $A$ has orthonormal columns. The authors argue that solving the maximization problem $\max_{q: \|q\|_2 = 1}\|q^T Y\|_4^4$ recovers a column of the matrix $A$ upto a sign with some error additive error. Then they show that for the general case of $A$, they can use a preconditioner computed using $Y$ that would reduce the problem to the case  orthonormal matrix and additive noise matrix of small norm.   They argue that any 2nd order optimization algorithm can be used by arguing that all critical points are either close to recovering the optimal column or that they are saddle points and that there is a direction in which the hessian is negative.  They show some experiments by generating a random matrix $A$ with orthonormal columns and generating a matrix $X$ from the above described Gaussian-Bernoulli distribution. They show that as the matrix $X$ gets sparser, the probability of recovery of $A$ increases as predicted by their theorems and also that as $n$, the number of columns of the matrix $X$ increases, the probability of recovery of $A$ increases.","This paper studies the unique decomposition of a low-rank matrix into sparse and full column-rank components. Explicit conditions under which such decomposition is possible (up to a signed permutation) are derived. Moreover, an initialization scheme is proposed that, together a generic second order method, guarantees the exact and unique recovery of the sparse and full column-rank components. The authors first study the setting where the full column-rank component has orthogonal columns. Then they extend their results to the general setting where this component is no longer column-orthogonal. To the best of my knowledge, the technical contributions of the paper are sound and correct.","The paper studies the problem of decomposing a low-rank p x n matrix Y into Y = AX, where A is p x r of full column rank and X is a sparse r x n matrix.  The paper first considers the case where A has orthonormal columns. It shows that by accessing Y only, one can recover approximately all columns of A up to the signs. It constructs a minimization programme involving the Schatten-4 norm, whose solution is an approximation to some column of A. The minimization problem can be solved by second-order methods. Then one can project A away from this solution and repeat the process. The paper gives quantitative error guarantees, provided that n is sufficiently large. The paper generalizes the result of the orthonormal case to a general A by preconditioning. ","The authors propose a new method and analysis to decompose a given matrix $Y$ as $Y=AX$ with $X$ sparse and $A$ full column rank. This is similar but different to sparse PCA and dictionary learning, and has interesting applications.  The proposed algorithm essentially recovers $A$ one column at a time (after deflation) and maximizes the $\ell_4$ norm of the overlap of a unit-norm vector with $Y$. As the authors also state, using the $\ell_4$ norm is not entirely new but it gives a benign optimization landscape (in particular, strict local minimizers that are close to the desired solutions).  The best results (theoretically and experimentally) are obtained when $A$ is an orthonormal matrix and $X$ is a random matrix from a Bernouilli-Gaussian distribution. The authors propose a procedure (called preconditioning) that generalizes their results (again theoretically and experimentally) to any $A$ but it is difficult to see how good the results are. In any case, the provided experiment clearly show that we do no longer have exact recovery in this case.  While the theoretical results seem -- I did not check the proofs in the 40(!) page appendix -- nontrivial and interesting, they only apply when recovering one vector and for a random $X$ from the Bernouilli-Gaussian distribution. No results are given for general $X$. In addition, the proposed deflation strategy is a typical technique to extend to multiple vector but typically performs quite poorly when $A$ is ill-conditioned (near linear depednent columns). The authors should therefore restate their contributions in the abstract and introduction to better reflect their main contributions.",0.10264900662251655,0.17218543046357615,0.17880794701986755,0.26851851851851855,0.28703703703703703,0.27007299270072993,0.28703703703703703,0.3795620437956204,0.2037735849056604,0.2116788321167883,0.1169811320754717,0.13962264150943396,0.15121951219512195,0.23690205011389523,0.1904761904761905,0.236734693877551,0.16621983914209112,0.18407960199004975
568,SP:7c816f5d7501d988b2cf8d932b16a22222ab994e,"This paper proposes a learning-based method to solve planning problems. The approach first identifies regions on the map using transformers to provide attention to map areas likely to include the best path, and then applies local planners to generate the final collision-free path. Experiments show that the proposed method can achieve performance comparable to the traditional planners with lower planning time for smaller maps, but the performance drops significantly as the map sizes increase.  Note that I previously reviewed this paper for NeurIPS 2021. The authors have made a few changes since the NeurIPS version and I have updated my review accordingly.","This paper proposes a transformer based trajectory estimator for 2D navigation. The proposed method is shown to not be limited by input size as other learning based approaches. In the experimental section, the authors show that their estimated trajectories result in speed up and performance boost for planning in comparison to traditional and other learning approaches.","The paper presents a Transformer-based planning approach that attempts to use an attention mechanism to reduce the search-space of a traditional planner such as RRT*. Approaches such as Informed RRT* constrain the search space of the planner, which allows much faster convergence time. In a similar way, the authors present an approach that uses attention on a 2D map to create a ""mask"" for a standard planner to draw samples from. The proposed Motion Planning Transformer (MPT) works by using sliding window on the original map, creating features that are then positionally encoded (which helps with generalisation) and then put through a transformer encoder and classified. The result is a prediction, for each patch, as to whether a planner should draw samples from it. When combined with a planner it is able to achieve state-of-the-art results in a reduced time.",This paper proposes a transformer based method for 2D motion planning. The aim of the method is to work in any resolutions of the 2D map and the attention of the transformer enable to get the best possible regions through which the desired path should exist. A  traditional planners is then used given the extracted region to generate the final collision-free path. The results shows the efficacy of the transformer guided path planning using RRT* in different resolution of maps.,0.14423076923076922,0.19230769230769232,0.25961538461538464,0.25,0.30357142857142855,0.16551724137931034,0.26785714285714285,0.13793103448275862,0.3333333333333333,0.09655172413793103,0.20987654320987653,0.2962962962962963,0.18749999999999997,0.1606425702811245,0.2918918918918919,0.13930348258706468,0.2481751824817518,0.21238938053097345
569,SP:7ca145296ee55be6bd61ef496fff4389feea744f,"This paper replaces usual weights and orthogonal matrices in orthogonal neural networks with an equivalent pyramidal circuit made of two-dimensional rotations.  By this, the authors proposed a training method for orthogonal neural networks that run in quadratic time, which is a significant improvement from previous methods based on Singular Value Decomposition. The authors also performed some numerical experiments to verify the abilities of their pyramidal circuit, on the standard MNIST dataset, which showed that their methods are efficient for learning a classification task.","The authors introduce a quantum pyramidal circuit to achieve an orthogonal layer of a neural network, which is fast and can maintain orthogonality. The angle gradients are derived. The authors implement the orthogonal NN on simulators and quantum machines to demonstrate the effectiveness.","A parameterization of NNs that guarantees orthogonality, and is easily implementable using classical as well as quantum computing. The paper proposes a ""pyramidal circuit"" as a model/architecture that can be implemented on quantum devices and, in the same form, on classical devices. On quantum devices, it is implemented using 2-qubit, 2-level gates (RBS gates), and on classical devices as a sequence of planar rotations. In both cases, there are $n(n-1)/2$ trainable parameters that also only need $O(n^2)$ in the backward pass.","The authors propose a new type of neural network layer called a Pyramidal Circuit. This layer implements an orthogonal matrix multiplication, and its claimed that it allows for gradient descent to be run while maintaining perfect orthogonality, and with the same asymptotic running time as an arbitrary fully connected layer. This algorithm is inspired by quantum computing, and it can be applied on a classical computer or a near term (NISQ) quantum computer. To add supporting evidence that the Pyramidal Circuit works, the authors include some experiments demonstrating that networks with Pyramidal Circuit(s) can achieve some level of accuracy on binary classification problems.",0.14285714285714285,0.11904761904761904,0.21428571428571427,0.23255813953488372,0.3488372093023256,0.19101123595505617,0.27906976744186046,0.11235955056179775,0.17307692307692307,0.11235955056179775,0.14423076923076922,0.16346153846153846,0.1889763779527559,0.11560693641618495,0.1914893617021277,0.1515151515151515,0.2040816326530612,0.17616580310880828
570,SP:7ccfbfa6b095dc3ec36ccb97e3c398b70b115e06,"This paper addresses test-time collective prediction. As far as I know, this is an underexplored topic, yet it is interesting. The paper proposes a decentralized consensus algorithm for multi-agent model aggregation. The paper includes both a nice theoretical analysis and empirical validation. Although I did not check the proofs, the theoretical analysis seems sound.","The authors present a decentralized algorithm for test-time model combination (collective prediction) wherein each party has an private model and dataset, and only has query access to other party's models. The algorithm is a novel application of DeGroot's consensus model which models how several agents iteratively influence each other's predictions based on mutual trust. The authors use the mean squared error on a local set of points as a proxy for mutual trust: each party queries each other party's models using points neighboring the test instance from its private dataset and measures the MSE on this set. The authors also show that the algorithm easily lends itself to a decentralized jackknife algorithm.  The authors prove several intuitive properties of their algorithm and prove that, asymptotically, it recovers an optimal inverse-MSE weighting. Empirically, they show that the method outperforms a naive equally-weighted average of the classifier predictions on synthetic and real data. Through synthetic experiments, the authors show that the proposed algorithm is particularly beneficial when there is significant heterogeneity in the private data or learners because of the adaptively computed weighting (based on MSE in a neighborhood around the test instance).","This paper considers collective prediction at test time, and proposes a novel decentralized mechanism by leveraging each agent's pretrained model and data without sharing them with other agents, inspired by human consensus in social science. In particular, for each test example, the proposed approach computes trust scores between two agents and aggregate the scores for a final collective prediction via DeGroot aggregation. In theory, this paper proves that the proposed DeGroot consensus mechanism is asymptotically optimal under some conditions. The efficacy of the proposed approach is empirically demonstrated by showing that the proposed scheme is better than an un-weighted consensus baseline.   The major contributions include the following:  (1) this paper introduces the concept of ""mutual trust scores among agents"" in social science (which is similar to PageRank algorithm in data science) for collective prediction at test time,  (2) the proposed approach is advantageous over existing methods since it does not require agents to share data or model parameters, which are beneficial in terms of communication cost and privacy perspective,  (3) the proposed approach is advantageous over existing methods since it does not require additional hold-out validation set to aggregate predictions of each agent,  (4) the proposed approach is advantageous over existing methods since it does not have any restriction on training procedure and a trained model of each agent,  (5) this paper theoretically proves the optimality of the proposed scheme (i.e., DeGroot consensus mechanism) is optimal under some conditions, and (6) this paper empirically demonstrates that the proposed approach is superior than the baseline approach (which does not weight the predictions of agents). ","This paper proposes an interesting approach based on DeGroot’s consensus model for combining predictions of multiple pre-trained models to collectively make a final prediction. The proposed method is useful in cases where data pooling, model sharing, or external validation is difficult. Experiments show that the proposed method is better than classical baselines of model averaging under heterogeneous conditions, without assuming access to additional labeled data for validation or learning a meta-model.  ",0.26785714285714285,0.3392857142857143,0.16071428571428573,0.17676767676767677,0.1111111111111111,0.09737827715355805,0.07575757575757576,0.07116104868913857,0.12162162162162163,0.13108614232209737,0.2972972972972973,0.35135135135135137,0.11811023622047245,0.1176470588235294,0.13846153846153847,0.15053763440860213,0.16176470588235292,0.15249266862170088
571,SP:7ce65cc7c85e7ca20a79eb344e5b85b3d2c0cd66,"This paper proposes a self-supervised approach that trains bug detectors by co-training a bug selector that learns to create bugs. A high-level framework is formulated, and then a python implementation using graph neural networks and relational transformers is provided. Experiments on a manually curated bug dataset demonstrate its effectiveness.","This work tackles programming bug detection and repair, introducing BugLab, which uses two models trained in an adversarial fashion together. One model is a bug detector that learns to identify and repair bugs, while the other is a bug selector that introduces buggy mutations in code that are difficult for the detector to identify. When trained together, the detector model is able to see more training data for bugs that it is worse at detecting, leading to better performance after sufficient training in this fashion. BugLab also uses semantics-preserving code modifications as a form of data augmentation. The experiments show that the BugLab training procedure leads to improved performance in bug localization and repair when applied to prior models (GNN and GREAT), compared to baselines of supervised learning and selecting buggy mutations at random instead of sampling from the trained selector model.","This paper describes an adversarial approach towards detecting and repairing local bugs in human-written software. The approach involves training in tandem two neural networks, the detector, which discovers and repairs bugs in a given piece of code, and the selector, which produces bugs that the detector is unable to detect. At test time, only the detector is used.  The architecture used for each of the networks is similar: first the program is represented by a graph with labeled edges, then each node is embedded using a language-like model (to take into account semantic information in variable names), then embeds the graph using either a GREAT relational transformer or a GNN. The code rewriting model then predicts a distribution over locations to rewrite using a pointer network, and a distribution of rewrites at that given location using a softmax over potential rewrites.  The types of bugs considered are local and shallow bugs such as variable misnamings or use of the wrong operator or literal in an expression. The authors create a corpus of code by crawling pypi and create a corpus of known shallow bugs by looking for version updates that include edits that fit their definition of a shallow bug. They additionally include a corpus of random generated bugs. In general, their approach works better on the random bug corpus than on the real bug corpus, and also works much better at repairing bugs given localization than it does at localization. They are able to achieve better performance than the special purpose CuBERT model using a F1 metric. Additionally, they were able to detect 19 bugs in open source projects, but with a TPR of 0.19% this is probably not yet a productionizable tool in this context.","This paper describes a self-supervised ML-based bug fixer that uses a combination of a bug localization and repair model, with a bug injection model. The two models are identical in architecture: they select a program location to mutate, and then one of a few possible mutations and its parameters (e.g., swapping a binary operator with another). Training is done concurrently from a dynamic corpus of buggy and bug-free examples. The bug fixer is trained on detecting bug-free examples, or fixing buggy ones. The bug injector is trained on selecting the location and type of bug to inject to maximize the bug fixer's loss.  The initial corpus is also augmented via a number of ""semantics preserving"" rewrites of examples (e.g., variable renaming).  The approach is shown to outperform bug fixers that don't have the benefit of a bug injector, as well as those that don't use data augmentation. Furthermore, the approach was shown to find previously-undiscovered bugs in open-source code, as well as perform better than previous SOTA models on a manually curated test dataset of bugs.",0.25,0.3076923076923077,0.38461538461538464,0.21678321678321677,0.21678321678321677,0.1413793103448276,0.09090909090909091,0.05517241379310345,0.10695187165775401,0.10689655172413794,0.1657754010695187,0.2192513368983957,0.13333333333333333,0.09356725146198831,0.16736401673640167,0.14318706697459585,0.1878787878787879,0.1719077568134172
572,SP:7d36f315b961d426934fdaec152c5ec6bcd86c54,"This paper proposes a general-purpose neural network architecture named Perceiver IO. With only small modifications on the query side, Perceiver IO can handle various inputs, such as languages, images, videos, point clouds, optical flows, or game agents. Perceiver IO is hugely dependent on the previous work, Perceiver (Jaegle et al.). The main difference between Perceiver and Perceiver IO is the final decoding module. Instead of using a classification head, Perceiver IO uses an additional cross-attention module with an output array. For example, if a user wants to make Perceiver IO be a classification model, the output array becomes 1 X # classes. Perceiver IO shows impressive results in the experiments compared to domain-specific designed architectures in various domains (language, optical flow, audio-video autoencoding, image classification).  Perceiver IO shows its flexibility on various input domains and domain knowledge. For example, Perceiver IO shows even comparable performances to the BERT baseline on the GLUE benchmark without the conventional tokenization but with only UTF-8 bytes (Perceiver IO with tokenization shows 81.1 and Perceiver IO with UTF-8 bytes shows 81.0 while BERT baseline with comparable FLOPs shows 81.1). On the other hand, in the vision domain, by using 2D convolution and max-pooling preprocessing, Perceiver IO shows better top-1 accuracy compared to 2D Fourier features (82.1 for conv preprocessing and 79.0 for 2D Fourier features). ","This paper proposes a general purpose architecture(Perceiver IO) which can take any arbitrary type of inputs and produce arbitrary outputs. The architecture enables this while scaling linearly with input embedding size and output size. Perceiver IO performs comparably with SOTA results on a variety of tasks from language, vision and multimodal domains, which speaks about the generalizability of this architecture. ","In this paper, the authors proposed a new general architecture called Perceiver IO for various tasks with different types of inputs and outputs. The Perceiver IO employes a read-process-write architecture, in which input arrays are first projected to the latent space through cross-attention and then the outputs are generated by querying the latent space through some output query arrays. Such a generic pipeline can be applied to various tasks spanning from single modality language tasks, multi-modal tasks, dense prediction tasks, and even symbolic predictions. The main benefit of the proposed Perceiver IO is that it decouples the inputs and outputs using a latent processing module so that it can cope with the arbitrary length of input arrays while outputting an arbitrary number of predictions. Extensive epxeriments are conducted on language understanding and masked language learning, optical flow estimation, image classification, multi-modal autoencoding, etc, and showed that the proposed Perceiver IO can achieve comparable performance to strong baselines in different domains.","This paper proposed perceiver IO, which is a general architecture for structured input & output. Perceiver IO is based on the perceiver architecture which scales linearly with the size of inputs and outputs and augments with a flexible querying mechanism similar to NeRF. Different from prior work, perceiver IO directly operates in the raw input space -- UTF-8 bytes for language, xy coordinates in optical flow, raw audio, etc. The output can be arbitrary in size and different structures, and with non-autoregressive decoding, the model can handle large input and output sizes. The proposed model is tested on variety of tasks, including language modeling, optical flow, video audio class autoencoding, image classification, and starcraft II, and achieves superior performance. ",0.09482758620689655,0.15086206896551724,0.1336206896551724,0.3770491803278688,0.3770491803278688,0.21212121212121213,0.36065573770491804,0.21212121212121213,0.2605042016806723,0.1393939393939394,0.19327731092436976,0.29411764705882354,0.15017064846416384,0.17632241813602012,0.17663817663817663,0.20353982300884957,0.2555555555555556,0.24647887323943665
573,SP:7d4547b74906922bd24abf172e50485d67caffd2,"This paper studies the boosting framework for classification. Weak classifiers are produced sequentially by an abstract learner. The popular α-CVaR (Conditional Value at Risk) is adopted. The 0/1 loss is considered. First, it is proved that the optimizer (among all the deterministic models) of ERM is also the optimizer of α-CVaR, and vice versa if α-CVaR could be smaller than α by some deterministic model. The authors also prove that with the same 0/1 loss, the randomized model achieves smaller α-CVaR. The optimization structures of α-LPBoost and α-AdaLPBoost are studied to construct randomized classifiers.","The authors of this paper propose robust boosting algorithms for minimzing the ""CVaR_{alpha} 0/1-loss"", defined as the worse case 0/1 loss over all alpha-sized sub-populations of the entire population. Models with low ""CVaR_{alpha} 0/1-loss"" help ensure fairness by not performing too poorly on any individual subpopulation.  The authors provide a number of contributions:  1. They show that while optimizing ""CVaR_{alpha} 0/1-loss"" is equivalent to minimizing average 0/1 loss when one is restricted to using deterministic classifiers, this is not the case if one is allowed to use randomized classifiers. And in fact, randomized classifiers can in principle always perform at least as well on CVaR_{alpha} 0/1 loss as deterministic ones.  2. They use the observation in #1 to motivate boosting algorithms that explicitly optimize the CVaR_{alpha} 0/1 loss. Here they provide 4 related learning algorithms   * alpha-LPBoost   * Regularized alpha-LPBoost   * alpha-AdaLPBoost   * AdaBoost + average  3. For the proposed algorithms, the authors give complexity guarantees that bound the ensemble size as a function of the required accuracy.  4. The authors compare their CVaR-optimizing, robust boosting algorithms to the baselines of stanard boosting with empirical risk minimization (ERM and ""AdaBoost + average"". Their experiments show that alpha-AdaLPBoost performs well on CVaR_{alpha} 0/1 loss across all values of alpha, in contrast to ERM which performs well when alpha is large and alpha-AdaLPBoost which performs well when alpha is small.  ","The paper attacks the problem of ""fair classification"", where the goal is to train a classifier that is ""good"" on any small fraction of the samples. Obviously, any deterministic classifier having less than 100% accuracy is arbitrarily bad on a small fraction of the distribution. The authors propose the usage of randomised classifiers to bypass this. i.e we prefer a classifier that is correct on all samples 80% of the time over a classifier that is correct on 80% of the samples wrong on the rest 20% all the time.   Firstly the authors observe the superiority of randomised classifiers for the CVaR loss over deterministic classifiers.  Then they propse the usage of a boosting framework to aggregate individual ""good but unfair"" classifiers into ""equally good but fair"" classifiers. In particular, the classic LP-boost algorithm is observed to be a good fit for this problem.  The authors then propose a more efficient version of the LP-boost algorithm where classifiers need not be trained for different levels of fairness (the worst fraction of samples being optimised for).   The algorithms are validated on classic classification datasets where they do better than standard baselines. ","The paper first shows that for deterministic models, optimizing for tail performance (in terms of 0/1 loss) is in fact equivalent to optimizing for average performance, and thus doesn't actually improve fairness of a model. As a mitigation, it is proposed to switch to non-deterministic classifiers, for which this property no longer holds. A boosting algorithm that can construct such a $\alpha$-CVaR optimized non-deterministic classifier based on training weighted deterministic unfair classifiers is proposed and demonstrated to provide good results.",0.3368421052631579,0.23157894736842105,0.1368421052631579,0.12955465587044535,0.09716599190283401,0.10880829015544041,0.12955465587044535,0.11398963730569948,0.15294117647058825,0.16580310880829016,0.2823529411764706,0.24705882352941178,0.18713450292397663,0.15277777777777776,0.14444444444444446,0.14545454545454545,0.14457831325301204,0.1510791366906475
574,SP:7db1647a8e860122a81917da15d6b132be8e35a1,"The authors use the predict-then-optimize framework to tackle prediction of MDP parameters based on input features. They achieve this by formulating MDP optimality KKT conditions and differentiating through them. In addition, the paper provides an ubiased sampling method for estimation of the KKT derivatives and a low-rank approximation to the high-dimensional sample-based derivatives. The authors name their approach decision-focused learning.","The paper proposes a learning problem in which a partially defined MDP is provided with missing parameter values, particularly a subset of the transition probabilities and reward values. Input features defining each MDP are provided, along with sample trajectories from its stochastic policy. The goal is to learn to predict the missing parameter values, by targeting the sample trajectories given the MDP features. The prediction model is trained end-to-end across several stages: Prediction of missing MDP parameters from feature values, followed by reinforcement learning to solve the MDP for an optimal policy, and finally evaluation of the policy as an expectation of some loss over its trajectories. The primary challenge is differentiation through the reinforcement learning layer, which is achieved by differentiating through the optimality conditions of the decision problem. Computational intractability of differentiating with respect to large state and action spaces is handled with one of multiple proposed gradient approximations. ","In this paper, the authors consider the problem of learning a predictive model for MDPs with missing parameters. Instead of a standard two-stage approach, an end-to-end approach based on differentiating through an optimality condition is proposed. To address the associated computation issues, a sampling method and a low-rank approximation method are introduced. Empirical evaluations on three synthetic settings are provided to show that the end-to-end approach outperforms the two-stage approach. ","The authors extend the ""predict-then-optimize"" framework to sequential decision problems. The framework which has been studied previously focuses on improving the quality of the decision-making process compared to the traditional two-stage prediction-optimization approach. In this paper, the sequential decision-making problem is formulated as a MDP, with the goal being to learn a predictive model that can predict missing parameters. At training time, a predictive model to map features to missing parameters is learned, and at test time, the learned model is used to make predictions without using trajectories. The predicted parameters are used to solve the test MDPs, yielding an optimal policy which is evaluated using an offline off-policy evaluation.  The authors study two types of optimality conditions: (1) a policy gradient based approach where the expected cumulative reward is maximized, (2) a Bellman based optimality condition where mean squared Bellman error is minimized. These optimality conditions are expressed using KKT conditions. However, due to the large state and action spaces involved in the optimization reformulation, and the high-dimensionality of the policy space, the authors propose a technique to sample an estimate of the first and second order derivatives to approximate the optimality and KKT conditions. To handle the second challenge, two options are proposed - approximating the Hessian by constant identity matrix, and using a low-rank Hessian approximation and application of Woodbury matrix identity.",0.2727272727272727,0.3181818181818182,0.4090909090909091,0.10457516339869281,0.2679738562091503,0.33766233766233766,0.11764705882352941,0.2727272727272727,0.11587982832618025,0.2077922077922078,0.1759656652360515,0.11158798283261803,0.1643835616438356,0.2937062937062937,0.18060200668896323,0.1391304347826087,0.21243523316062177,0.16774193548387095
575,SP:7df5387f0de3b9f313b6b36496f0c679a30d86e7,"This paper proposes Circa to reduce the communicational overhead of ReLU-based privacy-preserving machine learning using three modifications. First, the ReLU circuit is refactorized into two parts including multiplication and sign function, so that a GC-based ReLU is converted into a GC-based Sign function with a Beaver’s Triple’s based multiplication. Secondly, GC-based modular operation is removed from GC-based circuits at the cost of precision. Thirdly, quantization or truncation is used to further speedup GC-based circuits at the cost of precision (potential decryption errors). The authors show that inference accuracy won't be decreased a lot by this decreased precision due to the network’s error-tolerance ability. ","This paper focuses on minimizing the computation cost of the ReLU operation. Prior work uses Garbled Circuits (GC) to process ReLUs, which is computationally expensive. This paper proposes a novel method to reduce computation by decomposing ReLU operation into a sign computation and multiplication. The multiplication can be implemented by Secret Sharing and the cost of sign computation through GC can be reduced by approximation. This paper also provides adequate empirical evaluations to show the proposed methods can reduce runtime. ","The authors propose to improve the speed of modern Homomorphic encryption (HE) approaches which leverage GC for RELUs by moving as much computations as possible outside of the GC. The optimizations introduced in Circa (i.e truncation and removing the modulo addition) come at the cost of stochastic faults. This study demonstrates that modern deep networks are highly resilient to RELU fault behavior and that Circa (the proposed) approach can offer up to 3x faster predictions than Delphi, a state-of-the-art framework. ","The paper proposes several techniques to improve the trade-off between accuracy and storage/runtime for ReLU operations in private inference. The proposed techniques include refactoring ReLUs as a sign function plus a cheap multiplication, and replacing the sign function with a truncated stochastic sign that has runtime and storage beneﬁts (by sacrificing accuracy). Experiments on several datasets and network architectures show that the method has a better trade-off between accuracy and storage/runtime than prior state-of-the-art.",0.19130434782608696,0.12173913043478261,0.14782608695652175,0.1875,0.1875,0.2261904761904762,0.275,0.16666666666666666,0.2073170731707317,0.17857142857142858,0.18292682926829268,0.23170731707317074,0.22564102564102564,0.1407035175879397,0.17258883248730966,0.18292682926829265,0.1851851851851852,0.2289156626506024
576,SP:7e1803eb46ff385588ae1bc67b2026872549d652,"The paper addresses the problem of review calibration, i.e., to estimate true paper qualities from reviewer ratings, if they are monotonic functions in the true quantities. The authors model the problem as an optimization problem that aims to minimize input noise on the reviewer functions. The authors derive explicit linear programs for the case of linear reviewer functions with and without noise and sketch further variants. They discuss differences of their approach to linear regression and matrix seriation. In an experiment with synthetic data (and thus given ground truth), they compare their method against two baselines from the literature and show improvements both in the case of linear reviewer functions being the groundtruth as also for more general reviewer functions. ","The paper has proposed a new flexible framework, least square calibration (LSC), for selecting top candidates from peer ratings. The major contributions are summarized as follows: a.	Proposed a model which could handle different types of miscalibration in peer reviews, and also incorporate various levels of prior knowledge b.	Self-prove the effectiveness of LSC by both theoretically and empirically  ","This paper tackles the problem of calibrating ratings, focusing on the application of scientific peer review. The idea is to normalize reviewers who interpret the rating scale differently. If one reviewer tends to use low ratings and another tends to use generous high ratings, the calibration step will equilibrate them. The calibration requires the two reviewers to have some papers in common, or at least both have papers in common with a chain of other reviewers. The idea is that each paper has an inherent quality, each reviewer perceives that quality with independent zero-mean noise, and each reviewer translates that perceived quality into a rating using a monotone function. Given this model of how ratings are formed, the authors propose an optimization problem to find the hidden qualities, function parameters, and error rates that minimize the squared errors that reviewers perceive. They call this Least Square Calibration (line 133). The authors show how to do this for linear functions with and without noise, and prove some conditions under which the exact qualities can be recovered, including an if and only if connection to doubly connected review graphs with no noise. The authors also show how to do run the optimization for non-linear functions. The authors discuss similarities with linear regression and matrix seriation. The authors provide simulations to show that their method works.","This paper studies calibration in the peer review process. Unlike most previous work, in this paper, the structure of reviewer miscalibration needs to only be monotone (in the true score of the paper being reviewed).  The authors also study the cases where: miscalibration is linear, and there is added noise.  The main contribution is framing the calibration problem as a (specific) functional optimization problem. Theoretically, they prove that optimal solutions to their optimization problems yield the correct underlying ordering of the papers (by true score). In comparison to 3 baseline algorithms the proposed approach tends to perform best in simulated experiments.",0.09917355371900827,0.30578512396694213,0.19008264462809918,0.21666666666666667,0.2,0.1111111111111111,0.2,0.16444444444444445,0.22772277227722773,0.057777777777777775,0.1188118811881188,0.24752475247524752,0.13259668508287292,0.2138728323699422,0.20720720720720723,0.0912280701754386,0.14906832298136646,0.15337423312883436
577,SP:7e8cc139f828c69c1a25cecafe2fd88dc798afdd,"The authors proposed a local image-editing algorithm based on pre-trained StyleGAN2. The motivation is simple. Editing is performed based on two branches: generation and parsing. Parsing relies on pre-defined segmentation masks. The incremental editing vector is learnt by constraints to edited images and associated parsing masks. To handle the disentanglement, additional optimization is needed to perform refinement. ","This submission tackles the problem of semantic edition of images. The proposed method builds heavily on DatasetGAN.  To edit an image x, the method first finds the corresponding w in the GAN latent space (using a trained encoder) and then optimizes it so that the semantic branch output aligns to a target label map. The process effectively produces a modified image that retains the characteristics of the original image but with a semantic layout given by the target map. The difference between the initial w and the optimized w provides an edit vector, that can be applied to other similar images to perform similar edits in real-time. If needed, some further optimization can still be performed to remove edition artifacts.","The paper proposes to perform image editing via segmentation maps. First, a joint representation of image and segmentation map is learned with a GAN such that the GAN's latent representation w represents both the image and the segmentation map. This can be done with very few labeled datapoints. After this, a given image can be edited by optimizing the latent representation w to represent the edited segmentation map, which will implicitly also make the generated image follow the edited segmentation map. The authors perform all their experiments with the StyleGAN architecture and show that their approach works for several domains such as faces, cars, cats, etc. and in high resolution (up to 1024 px). Compared to previous methods this approach can do very fine-grained edits, the edits are much better disentangled, and only very few labeled samples are needed for training.","This work proposes EditGAN for semantic image editing with pre-trained GANs. It first introduces an additional branch beyond the original GAN generator to predict the semantic map corresponding to the image synthesis (this part is proposed by previous work). It then learns an encoder to project a target image to a latent code (this part is also proposed by previous work). At this end, given an image with the associated latent code and segmentation map, EditGAN enables semantic image editing by (1) modulating the segmentation map, (2) optimizing the latent code to fulfill the segmentation change, and (3) defining the semantic vector as the difference between the initial latent code and the optimized code. It turns out that such semantic vector is applicable to other images. ",0.21666666666666667,0.21666666666666667,0.26666666666666666,0.21487603305785125,0.24793388429752067,0.1958041958041958,0.10743801652892562,0.09090909090909091,0.12598425196850394,0.18181818181818182,0.23622047244094488,0.2204724409448819,0.14364640883977903,0.12807881773399016,0.1711229946524064,0.196969696969697,0.24193548387096778,0.20740740740740743
578,SP:7ecaa521c906012dc0bdee24471b6a6ccfd6bd3a,"This paper applies the idea of the collapsed variational bound studied by Teh et al., 2007 to BNNs. The bound is constructed by assuming that VB's Gaussian prior mean and variance are also part of the inference. (In contrast, one commonly sets them to some fixed values, e.g. $\mathcal{N}(0, 1)$.) The collapsed bound seems to have a similar computational cost but is tighter than the standard ELBO.  Extensive experiments show that the proposed objective works well, even on large networks, the regime which mean-field VB tends to underfit. It is also shown that the objective can mitigates the over-pruning effect that plagues VB.","Although marginal likelihood can be used to select hyper-parameters, the ELBO is generally regarded as too weak a bound to do this effectively. The authors propose applying collapsed variational bounds to variational inference of BNNs. They show why this maintains the tightness of the bound for parameter inference while also providing a tightish bound for hyper-parameter optimization by exploiting the independence of the two. They empirically show that their method performs well in a range of settings, including in tests of ood uncertainty.",This work introduces an improvement over the typical ELBO by deriving a tighter bound to the log marginal likelihood. It further shows that the ELBO can be used to learn appropriate hyperparameters which can help in situations where MF-VI would have underfit. A few variants of this method are derived where some combination of prior means/variances are learned. These methods were shown to be competitive with and at times outperform existing methods in experiments on standard benchmarking datasets. ,"This paper defines bayesian neural network models with hierarchical prior distributions. These models are trained efficiently using collapsed variational bounds, in which some of the variables of the hierarchical ELBO are integrated out. The authors show that these models outperform their hierarchical counterparts in a number of experiments and standard benchmarks.",0.13761467889908258,0.11009174311926606,0.10091743119266056,0.1411764705882353,0.17647058823529413,0.125,0.17647058823529413,0.15,0.21568627450980393,0.15,0.29411764705882354,0.19607843137254902,0.15463917525773196,0.12698412698412698,0.1375,0.14545454545454548,0.22058823529411767,0.15267175572519084
579,SP:7ee0167a89b58077dbbf043afd11e30be68b9cf6,"This paper presents a nonlinear module that leverages both positive and negative ReLUs for feature separation. This enables applications such as reflection removal, blind denoising and other layer separation tasks. The paper shows several variants of adding the proposed module into existing CNN frameworks. Results show superior performance on several benchmark datasets."," This paper presents a method that recovers reflection and transmission components from a given image degraded with reflections. The proposed network employs a dual-stream interactive strategy where the discarded information (by ReLU) from one branch helps improving the feature representation of the other branch and vice versa. Experiments are performed for several image processing tasks, including image reflection removal, denoising, demoireing, and intrinsic image decomposition.   ","In this work, the authors address the problem of how to get a good approximation of a scene affected by reflections produced by a pane of glass. To do this, the authors proposed a dual-stream decomposition strategy called YTMT, where the main idea is how to effectively exchange information to enforce a better approximation of the target scene (and also of the reflection layer). At the optimisation level, the authors use a composite loss function which involves the reconstruction term along with three additional terms to enforce better output quality based for instance in constraining the gradients of the image.  Experimental setting shows readily competing results with respect to the existing techniques for reflection removal. Moreover, authors show the potential of the technique for other tasks such as blind denoising.  ","This paper proposes a dual-stream network design equipped with an interactive strategy (coined YTMT) to tackle the blind source separation tasks, especially the single image reflection separation. The underlying design motivation comes from a very simple mathematical derivation, i.e., any information discarded from one layer might be beneficial to reconstruct another layer. They demonstrate performance improvement on the task of single image reflection removal. The possible extensions to other source separation tasks, e.g., the blind image denoising and intrinsic image decomposition, are also investigated. ",0.21153846153846154,0.19230769230769232,0.19230769230769232,0.3181818181818182,0.3333333333333333,0.17557251908396945,0.16666666666666666,0.07633587786259542,0.11494252873563218,0.16030534351145037,0.25287356321839083,0.26436781609195403,0.1864406779661017,0.10928961748633881,0.14388489208633093,0.21319796954314718,0.2875816993464052,0.21100917431192662
580,SP:7f160388affa492f33c54bd9dc7254765c2993d2,"This paper proposes a transformer-based abstractive multi-document summarization approach conditioned on attribute classification models, which check for sentiment and polarity attributes to ensure consistency of the generated summaries across multiple documents. The attribute conditioning classification models are built in a modular fashion to guide summary viewpoint consistency throughout the various stages of the summarization process and trained on different datasets than the dataset for which the downstream summarization task is evaluated on. Experiments and ablation study show the effectiveness of the decomposable conditioning component compared to several baselines without the conditioning component via using both automatic evaluation (ROUGE) and human evaluation.","To  make the generated summary be little affected by the conflicting information among documents, this paper introduces an attribute conditioned module for the task of abstractive multi-document summarization, which aims to generate a summary of the given multiple documents. The proposed module first applies an external classifier, i.e., XLNet, to predict the attribute of the input, and then improves the state-of-the-art model, i.e., GraphSum, for both the encoder and the decoder. For the encoder, the attribute score of each paragraph is computed and then used to measure the consistency among paragraphs for better learning the graph-based representation of the input. For the decoder, the attribute is predicted based on the partial predicted sequence generated so far for better generating the next token. The module is evaluated on both automatic and human metrics, and the experimental results show the effectiveness of the proposed module.","This paper presents a method for multi-document summarization that considers contradictory information present in the source documents. The presented method is an extension of GraphSum with attribute conditioning modules in the encoder and decoder. Attribute discriminators are incorporated into the model so that it can predict attributes when generating a summary. This study reports ROUGE scores, human evaluation, and ablation study on MultiNews dataset.",This paper proposes an attribute (sentiment or polarity) conditioned multi-document summarization framework to address of issue of conflicting information in input documents. Attribute classifiers are trained in the pre-stage and then are used to get the attribute information of the content. Experimental results show that the proposed framework can obtain better ROUGE than the baseline BART.,0.3106796116504854,0.18446601941747573,0.18446601941747573,0.14666666666666667,0.16666666666666666,0.26153846153846155,0.21333333333333335,0.2923076923076923,0.3275862068965517,0.3384615384615385,0.43103448275862066,0.29310344827586204,0.25296442687747034,0.2261904761904762,0.2360248447204969,0.20465116279069767,0.2403846153846154,0.2764227642276423
581,SP:7f173784629f216acbadf618af9441fed89748e9,"This paper proposes a new method for the semi-supervised learning of keypoints using multiview images. The main challenge addressed is finding exact correspondences betweenkeypoints in multiple views, since the inverse of keypoint matching cannot be analytically derived or differentiated. The proposed probabilistic epipolar constraint encodes a soft correspondence and geometric consistency in the correspondence field. A distillation-based regularization is also proposed to avoid degenerative cases. Each contribution is ablated and show significant improvement over the baseline and relative to other methods. ","This paper proposes a novel method to learn dense keypoints in a semi-supervised manner using unlabeled multiview images. They propose to leverage the epipolar constraint of dense correspondences given a known fundamental matrix. In order to make epipolar constraint learnable, they propose a probabilistic epipolar constraint, incorporating uncertainty in correspondences using soft correspondences. The proposed method is proven to improve dense keypoint detectors in multiple datasets over reasonable baseline methods. ","The paper proposes a semi-supervised approach for dense keypoints detection (assining a unique ID or a UV coordinate to an image of a human body). The key idea of the paper is an epipolar inconsistency loss function which can be computed and trained with multiview datasets. The approach has been evalauted on multi-view human datasets compared against single-view and multi-view techniques. Experimental results are OK, but not very great.","The paper presents a method for learning continuous dense keypoint fields with part-supervised/labelled data and part-unlabelled data. The authors derive a probabilistic matchability constraint and define a loss that includes multi-view consistency, photometric consistency and a supervised loss from the labelled data. The method is evaluated quantitatively by comparing to a few other baselines and doing ablation studies on various loss terms and train/test splits. Geometric consistency and reconstruction accuracy are used as metrics to measure the accuracy of the proposed method and a few qualitative results are provided. ",0.27710843373493976,0.1566265060240964,0.20481927710843373,0.16901408450704225,0.18309859154929578,0.2328767123287671,0.323943661971831,0.1780821917808219,0.18085106382978725,0.1643835616438356,0.13829787234042554,0.18085106382978725,0.29870129870129875,0.16666666666666666,0.192090395480226,0.16666666666666666,0.1575757575757576,0.20359281437125748
582,SP:7f1b4b1143979c9343b8be4b339ace0a7b2155a3,"The paper presents a goal-oriented RL framework based on state-transition graph. Compared to prior work like SoRB, GoalGAN, HIRO, the proposed method G2RL demonstrates higher success in exploration on discrete and continuous control benchmarks.   The paper addresses that inefficiency of randomly sampling past trajectories for policy learning by (1) modeling state-transition graph (2) relevance sampling of the nodes that are the current state or its neighbors (assuming that they have close goals or similar goal-oriented policies). The goal is sampled from the candidate set, selected by self-attention over sets of neighboring and uncertain nodes.  # Key ideas  1. Differentiable goal generator can be trained with optimal goals for supervision by planning algorithm, 2. (avg) out degree of the node in state-transition graph (also called certainty of a state in the paper) shows how much has the state been explored. 3. sampling relevant trajectories based on goal proximity. ","The paper proposes a goal generation procedure in order to improve goal-oriented reinforcement learning (GoRL) by relying on the proposition that the degree of exploration of a state/node can be related to its out-degree. They propose a solution that leverages this proposition while takin into account that we do not have access to the full graph. The paper also proposes to a method sample from the replay buffer, named relevance sampling, with the goal of improving the sample efficiency. The experimental setup considers continuous control domains as well a a visual navigation experiment on VizDoom. In these experiments, modest improvements are presented when compared to other GoRL methods. The authors also provide an ablation study to evaluate the effects of different components of the algorithm.","In this work, the authors describe a novel graph-augmented RL method for goal conditioned tasks. The results shown outperform a number of baseline methods. The method is well motivated and theoreticaly grounded.    ","This paper improves exploration and learning efficiency in Goal-oriented Reinforcement Learning (GoRL). In particular, a new framework graph-enhanced GoRL (G2RL) is proposed by leveraging the state-transition graph. (1) G2RL produces under-explored goals (or goal states) by planning on the “one-episode ahead” graph to conduct directed and effective exploration. In the actual implementation, the goal is generated by a differentiable generator trained in a hindsight manner.  (2) In addition, G2RL proposes to use “relevant trajectories” based on the graph neighborhood information to improve the efficiency of policy learning. By combining (1) and (2), G2RL demonstrated strong performance on various environments. Theoretical analysis is provided on deterministic MDPs to support the design of the algorithm.          ",0.15789473684210525,0.05921052631578947,0.17763157894736842,0.078125,0.2109375,0.24242424242424243,0.1875,0.2727272727272727,0.2288135593220339,0.30303030303030304,0.2288135593220339,0.06779661016949153,0.17142857142857143,0.09729729729729729,0.2,0.12422360248447205,0.21951219512195122,0.10596026490066225
583,SP:7f53317d1c63c90acc86c174e2ca403d06298062,"The authors study Federated Learning for time-evolving data. In particular, they study gradient based methods for learning parameter vectors and characterize their performance in terms of the approximation quality of a noisy gradient oracle. Numerical experiments compare the performance of the proposed method with existing FL methods.  ","The paper presents continual federated learning (CFL) to address the issue that data at clients in FL may vary over time. The main objective of CFL is to optimize the time-aggregated objective across multiple clients. While historical objective functions are usually not available, the paper considers a surrogate objective that approximates the original historical objective. A theoretical analysis is presented that takes into account the approximation error (referred to as information loss in the paper) and the time drift in local objective functions. Experiments were also conducted that show core set methods to perform well.","This paper proposes to study time evolving heterogeneous data, and proposes Continual Federated Learning (CFL) to address this problem. Their analysis achieves this by introducing time-drift to capture data heterogeneity across time. Convergence results are presented followed by numerical results on time-varying and heterogeneous settings.  ","This paper introduces the Continual Federated Learning framework that allows capturing time-evolving heterogeneity of FL. The authors introduce a new formulation of the problem and propose a carefully chosen approximation to work with the new problem. Also, the authors provide theoretical analysis for strongly convex and general convex settings. At the end of the paper, they provide experimental results for deep learning models.",0.22916666666666666,0.14583333333333334,0.25,0.13541666666666666,0.16666666666666666,0.2127659574468085,0.11458333333333333,0.14893617021276595,0.1875,0.2765957446808511,0.25,0.15625,0.15277777777777776,0.14736842105263157,0.21428571428571427,0.18181818181818182,0.2,0.1801801801801802
584,SP:7f54dd677e21e5fe05af71d6ad9e8f3b1eaff66e,"This paper presents ProTo (Program-Guided Transformer), a modified  Transformer-based architecture that aims to learn a (fully neurally-parameterized)  execution model for symbolic programs. The authors propose a parameterization scheme that encodes the program semantics (decomposed along minimal executable components of the symbolic program) and program syntax (using a transition mask defined based on the program tree); and describe multiple training regimes (varying from dense supervision based on ground truth or symbolic executions, to a fully RL-based distant supervision.)  The authors present empirical evaluations on two domains -- visual question answering (GQA) and policy execution (2D Minecraft) with respect to other comparable neural architectures that leverage symbolic program traces. They find comparable performance with respect to the SOTA, and demonstrate improvements in generalization to longer and novel programs.  ","This paper explores how learning agents can follow program guidance to solve visual reasoning and policy learning tasks. While most prior works are only designed to address one of the two tasks, this paper aims to propose a unified framework that can tackle both two problems. To this end, the paper proposes a Transformer-based framework (ProTo) that learns to interpret and execute programs on observed specifications by learning to execute a program in a latent space with cross attention and masked attention mechanisms. The experiments on the visual reasoning task (GQA dataset) show that ProTo outperforms baselines using different learning signals such as question-answer pairs, scene graphs, and programs. The experiments on the program-guided policy learning in a 2D-Minecraft environment show that ProTo outperforms baselines and generalizes better to long and complex programs. I believe this work studies a promising research direction and proposes a framework that can address program-guided tasks in two domains. Therefore, I am leaning toward accepting this paper yet still have some concerns (see below) which I hope to be addressed in the rebuttal.","This work proposes Program-Guided Transformer (ProTo) for program execution tasks. Specifically, instead of simply encoding the program as a token sequence, they disentangle the program representation into a semantic representation and a structural representation, and the structural representation takes the program control flows into accounts. They modify the attention mechanisms of the vanilla Transformer architecture to encode the disentangled program representation. They evaluate their approach on two tasks: (1) program-guided visual question answering on GQA; and (2) program-guided policy learning task proposed in [64]. They show that ProTo outperforms other baselines.","This paper proposes a new model for learning to execute programs on program-guided tasks. The model is based on transformer and uses cross-attention and self-attention mechanisms. It takes as inputs both the program structure (control flow information) and program semantics (through token embeddings), as well as task specifications. The model is evaluated on a visual reasoning benchmark and a 2D Minecraft RL benchmark. Experiments show that the proposed model achieves better performance than previous ad-hoc approaches for learning to execute programs in these domains. ",0.21705426356589147,0.18604651162790697,0.17829457364341086,0.15300546448087432,0.15846994535519127,0.19148936170212766,0.15300546448087432,0.2553191489361702,0.26136363636363635,0.2978723404255319,0.32954545454545453,0.20454545454545456,0.17948717948717952,0.21524663677130046,0.21198156682027652,0.20216606498194944,0.21402214022140223,0.19780219780219782
585,SP:7fc7e37c699a1bb738c65f0c6fa983203c6fd067,"The paper first identifies a starved edge problem in latent graph learning. That is, nodes that are far from labeled nodes may receive insufficient supervision signal during learning. To address this issue, the paper then proposes to leverage a self-supervision signal, which comes from denoising the node features of the graph. Through taking both the noisy node features and the currently learned adjacency matrix as input, the feature denoising process acts as a regularizer to the learning of the adjacency matrix. Experiments on benchmark tasks show that the proposed strategy outperforms several comparison baseline models with or without adjacency matrices given.   The identification of the starved edge problem in latent graph learning is interesting. The proposed solution that introduces inductive bias in adjacency matrix learning through leveraging node features makes sense to me. Nevertheless, I found that the impact of the starved edge problem to a graph model’s predictive performance could be better justified with stronger evidence and the empirical studies in its current form could be further improved. ","The authors propose SLAPS, a methodology for deriving latent k-NN graphs for usage as edges of a GNN. They argue theoretically and empirically that it is important to combine self-supervised learning with graph structure learning, to avoid the prevalent edge starvation problem. They validate an approach consisting of a denoising self-supervised objective and various graph derivation strategies against relevant latent graph inference benchmarks, achieving impressive results.","This paper combines the self-supervised learning with graph structure learning. A model named SLAPS is proposed to alleviate the ""supervision starvation"" problem of structure learning methods by denoising features using generated graph. Extensive experiments are conducted to show the effectiveness of the model.","This paper studies graph structure learning. Given a dataset with latent but unobserved graph structure, the goal is to infer the graph structure and help downstream tasks. The authors specifically design a feature reconstruction task as self-supervision to improve the structure learning.",0.09941520467836257,0.0935672514619883,0.08771929824561403,0.21739130434782608,0.13043478260869565,0.2727272727272727,0.2463768115942029,0.36363636363636365,0.3488372093023256,0.3409090909090909,0.20930232558139536,0.27906976744186046,0.14166666666666666,0.14883720930232555,0.14018691588785046,0.2654867256637168,0.16071428571428573,0.2758620689655172
586,SP:7fec24a0f869ec9f5c07b1cea62742e1937ae6ae,"In this paper, authors aim to untangle the disparities in performance between morphologically rich languages (e.g. RU) and morphologically poor languages (e.g. ZH). In order to untangle such disparities, the authors propose a highly controlled experiment setup, where they control for a number of potentially confounding factors (e.g. language mappings, data size, text encoding, etc.). After the controlled experiments, the main take-away of the paper is that vocab size seems to be the biggest indicator of how well or poorly a CLM will perform — since there can be several different granularities for the concept of a “word” for morphologically rich languages, and this can explain why modelling morphologcially rich languages can be more difficult.  They perform additional experiments with different representations of ZH, and were able to mimic the struggles of morphologically rich languages with ZH, and use this as further evidence to support the claim that the debate within NLP about the best segmentation is irrelevant.  Overall, this work is exciting, has has a lot of potential to help inform future language modelling for morphologically rich languages.  The very controlled nature of the experiment in the paper is what makes it most convincing, and this is the paper’s largest contribution. ","This paper asks: Are languages which have been traditionally considered morphologically rich (Arabic and Russian) and poor (Chinese) equally hard to learn by transformer-based conditional language model (CLM).  To quantify how hard it is for CLM to model a language $l_1$, this paper proposes to train a transformer-based encoder-decoder model to translate $l_1$ to another language $l_2$, and uses the \""unnormalized perplexity score\"" (the number of bits needed to encode the same development set for the target language) as an indicator for the \""hardness\"" for CLM to learn the $l_2$. For two languages $l_1$ and $l_2$, there exists **disparity** if their perplexity distributions are significantly different. This paper includes extensive experiments across 30 language pairs using three different representation level (including character, byte, and word) for different dataset sizes.  They have the following findings:  1.  They find that the performance disparity between morphologically     rich and poor languages cannot be justified due to morphological     complexity. Representation level matters more.  2.  They find that the hardness for CLM to model a language is related     to the level of representation used to encode the language, the     vocabulary size, and/or the sequence length.  3.  They find that it is equally hard to learn a certain target language     (that are tested in their experiment) using transformer-based     conditional language models, independent of the source language.","At its core, this paper focuses on testing whether languages with different morphological complexity equally challenging to language-model in a bilingual / MT-like setting using a 6 layer Transformer architecture.  Specifically, it tests the assumption whether the previous observations about languages with higher morphological complexity are difficult to language-model, using a controlled experiment with a n-way parallel corpus across 6 languages of interest. The key finding of this paper is a simple one : that the choice of input representation is highly important, given everything else is kept constant, and that differences in language-model perplexities can be normalized and made to disappear by choosing the same representation (word/character/byte)","This paper explores the performance of conditional language models for different languages with the aim of establishing ""fairness"" in NLP systems across languages. In particular, the paper focuses on perceived difficulty of ""morphologically rich"" languages for NLP.  The paper presents a series of experiments with 6 languages in the UN parallel corpus, using different units (words, sub-words, characters, bytes) for training the conditional language models. The paper concludes with the general observation that the use of characters or bytes as units instead of words removes (or reduces) the performance disparity between different languages. ",0.19902912621359223,0.11650485436893204,0.12135922330097088,0.11739130434782609,0.1,0.1504424778761062,0.1782608695652174,0.21238938053097345,0.26595744680851063,0.23893805309734514,0.24468085106382978,0.18085106382978725,0.18807339449541283,0.15047021943573669,0.16666666666666669,0.15743440233236153,0.1419753086419753,0.16425120772946863
587,SP:801a0418d0b33a850e9c164f8cb7991d2d471f70,"This paper considers an online bipartite caching problem which has a wide application in practice and therefore has been extensively studied. The problem can be expressed via a bipartite graph in which we have a set of vertices on the left, one for each user, and a set of vertices on the right, one for each cache. There is an edge between user vertex and cache vertex if the user can fetch files from the cache. Each cache has a capacity which upper bounds the number of files that it can store. At each time slot, every user may request at most one file to visit. If such a file is stored in one of the neighbors of the user at the current time slot, the objective function will be rewarded by one, otherwise zero. The algorithm needs to decide which files should be stored in which cache at each time slot to maximize the total benefit. The optimization objective is regret which represents the performance of the online algorithm compared to an offline fixed strategy in the hindsight. Upper bounds and lower bounds are shown with respect to regret based on some coloring argument and a previously developed stochastic smoothing framework. The two bounds seem not matching although the authors claim a ""tightness"".  ","This paper proposed an algorithm for the bipartite caching problem. The proposed algorithm has a follow-the-perturbed-leader (FTPL) flavour. The regret under the policy is proved to be optimal up to constant factors, assuming an adversarial request sequence. Further, it is proved if the request sequence is i.i.d. (or more generally satisfies a law of large numbers type property), then the cache stabilizes in finite time with probability 1. ","The paper studies the Bipartite Caching problem, from an online learning perspective. A bipartite graph describes how n users are connected to m caches, each of which supports C files. At each time t, N files are distributed across m caches (same file can be placed in multiple caches), and each user requests one file. Every cache-hit (a user that requests a file in one of its caches) counts as a +1 towards the total reward. The goal is to maximize the reward (or minimize the regret, which is the difference with respect to the reward of an optimal policy).  The file demands are assumed to be chosen by an ""oblivious"" adversary, which means that the file demands for the entire time horizon T are chosen by an adversary, but a priori (i.e., before seeing the cache allocations).  The paper proposes a cache allocation algorithm called LeadCache. The algorithm is based on the ""follow the perturbed leader"" policy. In essence, at time t, we aggregate all previous user-file requests until time t-1, add Gaussian noise to them, and then perform the file allocation to caches as if the resulting perturbed vector was the current file demand. The paper builds on previous works that study the problem of Online Linear Optimization and provide regret bounds for ""follow the perturbed leader"" algorithms. However, in order to apply these ideas in the bipartite caching context, several new ideas need to be introduced. In particular, the file allocation for time t is found by first solving a linear relaxation of the combinatorial optimization problem, and then using an interesting rounding algorithm, for which a performance guarantee can be proven.  The paper provides a careful bound on the regret achieved by LeadCache and shows that it is at most a O(n^(3/8)) factor away from a minimax lower bound. Furthermore, the paper analyzes the number of file fetches needed by LeadCache (i.e., how many times a cache needs to ""download"" a new file) and shows that, interestingly, the file fetches stop after a finite amount of time.  ","This paper develops online algorithms for a bipartite caching problem. The problem is considered in an adversarial setting and the regret of the proposed algorithm is analyzed. Since the underlying problem is combinatorial, an intermediate step is needed to facilitate the algorithm design and regret analysis. This is done by designing another approximation algorithm with approximation analysis. Also, the regret lower bound of the problem is obtained, which improved the existing regret lower bound that we proposed recently in e Bhattacharjee et al. [2020]. Last, the performance is empirically evaluated using multiple datasets and against multiple traditional and recent regret-based algorithms. ",0.102803738317757,0.2336448598130841,0.1308411214953271,0.3424657534246575,0.2465753424657534,0.0830945558739255,0.3013698630136986,0.14326647564469913,0.27450980392156865,0.07163323782234957,0.17647058823529413,0.28431372549019607,0.15331010452961671,0.17761989342806395,0.17721518987341772,0.11848341232227487,0.2057142857142857,0.1286031042128603
588,SP:802e6c669d8ac7671642c279694b4ee9486ca780,"This paper introduces a new control framework that is based on Koopman theory. A key advantage of the proposed approach is the guaranteed stability, yielding a robust closed-loop controller. The method is evaluated on several challenging control benchmarks, and it is compared to several state-of-the-art approaches. The results highlight the advantages of the approach. In particular, in the setting of large external disurbances, the proposed method achieves the best results. ","This paper exploits the duality between the Perron-Frobenius and Koopman operators to formulate a Deep Stochastic Koopman Operator (DeSKO) to control an unknown system.  In particular, in contrast to prior work in this space that assumes a deterministic system, the paper proposes to learn a distribution over observables (encoded via a probabilistic neural network) and linear dynamics that propagate this distribution forward.  Theoretically, it is shown that under certain assumptions, a model-predictive-controller (MPC) consisting of a nominal policy and a stabilizing feedback gain around the nominal trajectory can guarantee robust stability.  Empirically, extensive experiments are conducted across 8 (4 nominal + 4 perturbed) environments, showing that the proposed approach leads to favorable prediction error (Fig. 2), control performance (Fig.3), and robustness (Fig. 4) when compared to other Koopman and Model-Free RL algorithms.  Further an ablation study on the effect of an entropy constraint that is introduced is also performed (Fig. 5).","This paper leverage the Koopman operator theory to represent a nonlinear dynamics as a high dimensional linear dynamics. The map from the state space to the high-dimensional space as well as the Koopman Matrix (linear transition matrix) are learned from data. Similar ideas has been explored in existing works. The contribution of this work is bringing stochasticity into the learning of Koopman models. The paper assumes the data are noisy and try to uncover a Koopman model with noisy data. Another contribution is the joint learning of a control matrix in the Koopman setting and a robust control framework for the proposed method. The proposed robust control framework is based on model predictive control and is provably stable (thanks to the linear transition of the observables). The proposed method is tested on rigid body systems (CartPole and HalfCheetah), a soft robotic arm (SoPrA) and cell biology (GRN). The proposed method outperform existing Koopman-based method (DKO) and RL method (SAC).","In this paper, authors propose  ""Deep Stochastic Koopman Operator"" which can handle uncertainties in the system dynamics. The authors demonstrate that their approach can produce state loss similar to large capacity MLPs, yet maintains the linear structure in the projected space that is suitable to use linear MPC control methods such as LQR. The proposed method can achieve best tracking performance compared with previous non-stochastic version and SAC trained policies. ",0.22972972972972974,0.35135135135135137,0.17567567567567569,0.17419354838709677,0.12258064516129032,0.12422360248447205,0.10967741935483871,0.16149068322981366,0.18309859154929578,0.16770186335403728,0.2676056338028169,0.28169014084507044,0.148471615720524,0.22127659574468087,0.17931034482758623,0.1708860759493671,0.168141592920354,0.1724137931034483
589,SP:80364ca91750f4b46955f832374461ed78d3bc9a,"In this paper, the authors argue that existing graph unsupervised pre-training models may miss some important information when selecting positive samples, or the reconstructed graphs do not meet some specifications. Based on this motivation, the authors design a similarity-aware sampler to automatically select positive samples from the existing samples for graph contrastive learning. Experiments are also conducted to demonstrate the effectiveness of the method.",The authors of the paper proposed to select positive graph instances (in graph pre-training) directly from existing graphs in the training set in order to make them more legitimate.  The proposed scheme is based on pair-wise similarity measurements as well as sampling from a hierarchical graph encoding similarity relations among graph data sets. An adaptive node-level pre-training method is used to for noe-;evel pretraining tasks as well. Extensive experiments on 13 graph classification and node classification benchmark datasets from various domains are reported to demonstrate the usefulness of the methods. ,"- The position of this work relates to graph-level contrastive learning. The paper aims to introduce a better way to sample the positive instances, avoiding illegal or poor-quality positive instances.  - The authors propose to select positive graph instances directly from existing graphs in the training set, which can maintain the legality and similarity to the target graphs.   - The positive sample selection is done on domain-specific pair-wise similarity measurements, sampling from a hierarchical graph encoding similarity relations among graphs.   - A side idea using adaptively select nodes to mask for even distribution has been proposed.  - Extensive experiments on various graph classification and node classification benchmark datasets have shown the effectiveness of this simple design’s impact on the final pre-train performance. ","The authors propose a new method for graph pre-training based on instance contrastive learning. They exploit graphs in the training set, to select positive instances that are similar to target graphs. They also propose a new strategy for node-level pre-training.  ",0.2878787878787879,0.2727272727272727,0.19696969696969696,0.5157894736842106,0.16842105263157894,0.13008130081300814,0.2,0.14634146341463414,0.3023255813953488,0.3983739837398374,0.37209302325581395,0.37209302325581395,0.23602484472049692,0.19047619047619044,0.2385321100917431,0.4495412844036697,0.23188405797101452,0.1927710843373494
590,SP:807bac62c904e6e8fe569edb22777de96835e6de,"This paper presents a new few-shot learning framework COSOC that can focus more on foreground objects at both pre-training and evaluation stage. COSOC is a two-stage algorithm. At the pretraining stage, a clustering-based object seeker (COS) module is used to force the pretraining model to focus on found foreground objects by a fusion sampling strategy. At the evaluation stage, a shared object concentrator (SOC) module is used to seek for shared contents and filter out background, and then the foreground of testing images is matched with the recognized foreground objects of each class. Experiments on two benchmarks are conducted to verify the effectiveness of the presented method.",The paper proposed a few-short learning (FSL) method by excluding the negative impact of background noise common in the existing FSL framework. A few separate stages are employed here to achieve the goal: 1) Clustering-based Object Seeker: a pretraining stage to pretrain the network and hence the feature space of the foreground objects; 2) Shared Object Concentrator (SOC): get the representation of the foreground of each class (which is then used in test time to determine the foreground of a test image). The method showed good empirical results on selected datasets. ,"In this work, the authors conducted an emperical analysis and revealed that image background serves as a source of shortcut knowledge which is harmful for few-shot learning task. To alleviate this, they proposed a two-stage method, COSOC, that employs contrastive learning to draw the model's attention to image foreground during both pre-training and evaluation stage. Given the assumption that foreground objects from different images of the same class share more similar patterns as background, in the pre-training stage, a clustering is performed on randomly cropped patches to identify crops that best represent the foreground objects in the image. Then, fusion sampling is adopted, i.e. these crops are used to replace original image with a certain probability based on its foreground score during pre-training. In the evaluation stage, mean features maximally representing the shared contents among training images from the same class is obtained. Given this, the foregrounds in testing images can be located via matching.","This paper dives into the importance of learned “shortcuts” based on background similarly across the training samples in few-shot learning scenarios, at the detriment of generalization to images with a “category gap” - a background that does not match what was seen in training. They explore the effect of background on recognition and propose two simple algorithms based on contrastively-learned features (one method at train time, one at test) to find foreground vs background and learn more generalizable representations with little data. At training time they use clustering over contrastive-pretrained features extracted from a set of crops from images across the training data for each class to find crops that are highly likely to contain foreground and use those crops instead of the image (with some probability, based on the “foreground score” of the crop). At test time a representative feature per-class is generated using the training data, and simple feature matching is used to determine the class.",0.27927927927927926,0.3063063063063063,0.22522522522522523,0.23655913978494625,0.25806451612903225,0.20987654320987653,0.3333333333333333,0.20987654320987653,0.15527950310559005,0.13580246913580246,0.14906832298136646,0.2111801242236025,0.30392156862745096,0.24908424908424903,0.18382352941176472,0.17254901960784313,0.1889763779527559,0.2105263157894737
591,SP:80a6d854c0b143a971b4eaa70359aabbddc41d6c,"The paper describes an approach to train activation functions. The main idea is to rewrite ReLU as s(x)x (where s(x) is either 0 or 1 depending on the value of x), and then generalize this with a piecewise constant s(x). The paper also proposes a generalization where values at different neurons can be combined. Finally, this is evaluated on two artificial problems and two standard computer vision benchmarks.",The paper introduces a trainable matrix of activation functions. The authors propose to replace activation with a custom learnable piecewise linear function. The results are evaluated on a custom sine function with different oscillations as well as CIFAR-10 and CIFAR-100.   ,"This paper proposes a new type of activation function, called Trainable Matrix Activation Functions (TMAF), to replace the existing activation functions in neural networks, such as ReLU. TMAF is realized using matrix-vector multiplications, where entries of the matrices are trainable piecewise constants. Empirical studies show that TMAF can approximate the sin/cos type of functions better than ReLU/PReLU in terms of approximation error. TMAF also outperforms ReLU in MNIST/CIFAR classifications tasks.","The authors propose a new approach to activation functions in DNNs. More precisely, the authors present the trainable matrix activation function (TMAF), which is an activation realized by a matrix-vector multiplication whose entries are trainable. The authors then present a diagonal and tri-diagonal operators. Here, these matrices represent a coefficient that multiplies the output of the layer of the neural network and thus can represent any general piecewise linear activation including relu and leaky relu.  These matrices are trainable and therefore add a small number of extra parameters to the network depending on the architecture of the layers.  Finally the authors demonstrate the benefits of TMAF in synthetic and real world datasets on a couple of architectures.",0.18055555555555555,0.1388888888888889,0.20833333333333334,0.2857142857142857,0.30952380952380953,0.2972972972972973,0.30952380952380953,0.13513513513513514,0.12605042016806722,0.16216216216216217,0.1092436974789916,0.18487394957983194,0.22807017543859648,0.136986301369863,0.15706806282722513,0.20689655172413793,0.16149068322981366,0.22797927461139897
592,SP:80dc818ae9308991d6dfad45289090d6928d5fd4,"The paper proposes  a particular relaxation for the verification of neural network robustness under perturbation that can be phrased as functions over the probability simplex (e.g. $\ell_1$-norm).  Mathematically, the verification is first phrased as an optimization problem, that mirrors the layer structure of the neural network. For the ReLU (or other convex activation functions) they employ the well-known convex relaxation from [Ehlers, 2017]* (""Planet"") and improve upon it by a new upper bound, which utilizes the fact that the input (and all intermediate neuron values post-activation) are subject to a simplex constraint. This relaxation forms the convex hull of the possible values of a post-activation neuron and thus is tight (for a single neuron). The resulting optimization problem forms a linear program with 2 linear lower bounds, 2 linear upper bounds and a simplex constraint for each neuron. To allow for faster and more scalable evaluation the optimization problem is relaxed following the approach of [Zhang et al. 2018]. The 2 upper and lower bounds are relaxed into one each, that is a linear interpolation weighted by an $\alpha  \in [0, 1]$. In this formulation the optimum of the problem can be found by alternatingly computing the neuron bounds (as a function of fixed $\alpha$) and optimizing the values of $\alpha$ via gradient decent.  Experimentally the authors show that both, the relaxed version and the linear program version outperform Plant relaxations in terms of certified accuracy by a large margin on image classification (MNIST and CIFAR-10) under $\ell_{1}$ perturbations and robustness to text-perturbations for BoW-representations in an multi-modal setting (simplified Food-101). The relaxed version almost matches the linear program version in performance, but is significantly faster.  *Citations in the [Name, Year] format refer to citations in the paper. ","This paper proposes to use probability simplex specifications for robustness verification. With the simplex, the convex hull for relaxing activation functions can be made tighter using a newly proposed upper bound for relaxation. Then the simplex specification and the new relaxation are used in solving the verification problem in a manner similar to backward bound propagation in CROWN, and the relaxation parameters are optimized with projected gradient ascent. By applying the method to $\ell_1$ robustness verification, and also a multi-modal setting with text perturbation, experiments show that using the simplex specification with the proposed method leads to better verified accuracy and low verification cost. ","This paper proposes an efficient bound propagation method for verifying the robustness of neural networks under L1 (simplex) constraints. The authors show that for the L1 norm setting, the convex hull for a linear and relu layer can be exactly computed efficiently, unlike the setting for Linf norm. Then, the authors designed an efficient algorithm that uses a single pass of backward bound propagation to solve the resulting linear programming problem. The bound propagation algorithm has optimizable parameters that can be optimized to achieve the tightest bound.","The paper proposes a new convex relaxation for verifying neural networks with convex activations with specifications defined over simplex inputs. These convex relaxations are more precise than the triangle relaxation from Ehlers et. al and based on computing the convex hull of the set {(x,f(w.x+b))} where x belongs to a simplex and f is a convex activation function. The size of the resulting relaxation is linear in the number of neurons. Building on this, the authors develop a verification algorithm that propagates simplex input through the network and a solver for solving the verification problem posed as an optimization problem based on the obtained convex relaxations. The experimental results show that the verifier developed in this work is more effective than prior work.",0.12,0.10666666666666667,0.14666666666666667,0.22641509433962265,0.2169811320754717,0.27586206896551724,0.33962264150943394,0.367816091954023,0.3464566929133858,0.27586206896551724,0.18110236220472442,0.1889763779527559,0.17733990147783252,0.16537467700258396,0.20608899297423888,0.2487046632124352,0.19742489270386268,0.22429906542056074
593,SP:810d01f8621d6567d949f3c7c7a7b84a93f8646a,"The authors provide generalization bounds on GANs and some DNNs in terms of the Lipschitz continuity of the networks and loss functions. These bounds show that by decreasing the Lipschitz constant of the networks and loss functions, one can generalize better. Moreover, they show that with dropout and spectral normalization, one can escape the curse of dimensionality in terms of generalization error. Finally, they claim that this theory supports the empirical results we see when GANs which use Lipschitz penalization (e.g., WGAN, GP-WGAN, SN-GAN, etc.) perform well in practice.",The paper tries to bound the generalization error of deep learning models and generative adversarial networks with the Lipschitz coefficient of the model. The main idea supported by the paper's discussion is that bounding the Lipschitz constant of the neural networks will lead to good generalization performance. The paper observes the connection between spectral normalization and norm-bounded dropout with the Lipschitz coefficient of the neural network and extends the proven bounds to those regularization methods. ,"This paper analyzes the generalization and the consistency of a function with Lipschitz continuity. In particular, the generalization and consistency bounds of Neural Networks (NNs) with Dropout or Spectral Normalization are derived. The number of layers is logarithmic in sample size. Furthermore, the generalization of a GAN, whose discriminator and generator are both Lipschitz continuous, is given.  ","This paper's primary focus is to provide insights into the relation between Lipschitz-continuity and the generalization of DNNs.  Compared to previous work, tighter bounds connecting the Lipschitz-constant of the ""loss"" and the expected loss of the learned function are provided. The paper also provides insights into why SN and Dropout improve generalizability in DNNs. Some quite insightful theorems connecting the generalizability of GANs to various empirical tricks commonly used to improve their performance are also provided. ",0.20652173913043478,0.21739130434782608,0.1956521739130435,0.2077922077922078,0.24675324675324675,0.2807017543859649,0.24675324675324675,0.3508771929824561,0.22784810126582278,0.2807017543859649,0.24050632911392406,0.20253164556962025,0.22485207100591717,0.2684563758389262,0.2105263157894737,0.23880597014925373,0.24358974358974356,0.23529411764705882
594,SP:813f879a279d768769c99d68a58cd9fd530f1e1e,The authors proposed a novel approach for video understanding. They present temporally adaptive convolutions (TAdaConv) based on dynamic networks by adapting the convolutional weights on each frame with its temporal contexts. The experiment results on public benchmarks demonstrate the efficiency of the proposed approach.,"This paper proposes a modified version of convolution for videos where a 2D conv kernel is converted to a 3D conv kernel by multiplying the 2d weights by a per-frame value. This creates a set of weights for 3D convolution that are dynamic for each frame. The approach is evaluated on multiple video datasets, and shows some benefit over their baseline.","This paper proposes TAdaConv that calibrates kernel weights of convolutional layers adaptively according to the temporal dynamics of the input tensor. It is designed to incorporate both the local and global temporal context by using stacked two-layer 1D convolutional operations and global average pooling. Since it works exactly the same as the original convolutional layer at the initial stage, it is easy to insert TAdaConv into existing ConvNet architectures. On top of this, the authors construct TAda2D networks by introducing the temporal feature aggregation module that is based on a strided temporal average pooling. Many experiments performed on video/action classification and localization demonstrate the effectiveness of the proposed module.","This work seeks to improve performance of video understanding models though the use of spatial convolutions which are dynamically adapted in time when applied to a sequence of frames. The novelty of this work lies in the fact that the weights of the spatial convolutions are adapted, rather than applying a temporal excitation to feature maps to achieve the same purpose. To achieve this, a drop-in replacement for spatial convolutions is designed which uses pooled local and global frame descriptors to produce the temporal calibration weights. Furthermore, this operation is used as part of a temporal feature aggregation block that is used as part of 2D CNN architecture. The benefits of the approach include that it can make effective use of pre-trained weights and that it can improve model capacity for a marginal increase in computation. Experimentally, it is show large performance improvements over a baseline and match state of the art on significant datasets.",0.20454545454545456,0.3181818181818182,0.25,0.22580645161290322,0.27419354838709675,0.21621621621621623,0.14516129032258066,0.12612612612612611,0.07006369426751592,0.12612612612612611,0.10828025477707007,0.15286624203821655,0.169811320754717,0.18064516129032257,0.10945273631840796,0.16184971098265896,0.15525114155251143,0.1791044776119403
595,SP:81609c87e6a51511645f3fe236436378f520d6fa,"This paper studies the input-dependent randomized smoothing technique - it adds isotropic Gaussian noise with different variance on different input x to give robustness certificates.  The hope is that this can perform better than applying a uniform noise.  The main theoretical claim in this paper is that when adding input dependent isotropic Gaussian noise, the variance of the noise should not change too much among different inputs. Based on this, the authors propose an input dependent smoothing mechanism based on k-nearest neighbor and evaluate its performance on MNIST and CIFAR datasets.","The authors study input-dependent adversarial smoothing i.e. the setting where the smoothing distribution has noise level $\sigma$ dependent on $x$. They begin by deriving the robustness certificate in this setting, and show that it suffers from the curse of dimensionality -- in order to maintain non-vacuous certificates in high dimensions, $\sigma(x)$ must be prohibitively smooth. They then provide empirical experiments using an example function $\sigma(x)$ on CIFAR-10 and MNIST and show that performance is more or less the same compared to the fixed noise level baseline (Cohen et al. 2019).",The authors propose an input-dependent randomized smoothing method with non-constant input-dependent $\sigma(x)$.  ### contributions  - The authors generalize randomized smoothing with non-constant input-dependent $\sigma(x)$. - The proposed method successfully addresses some intrinsic limitations/flaws in the previous (input-independent) RS.,"This work extends the theoretical results from Cohen et.al to the input dependent setup. In particular, the derive the theoretical conditions for the validity of data-dependent randomized smoothing through a similar procedure to the one in Cohen et.al. At last, a function that optimizes the smoothing parameters per input was proposed and validated experimentally on MNIST and CIFAR10.",0.1956521739130435,0.15217391304347827,0.17391304347826086,0.1368421052631579,0.1368421052631579,0.22727272727272727,0.18947368421052632,0.3181818181818182,0.26229508196721313,0.29545454545454547,0.21311475409836064,0.16393442622950818,0.1925133689839572,0.2058823529411765,0.20915032679738563,0.1870503597122302,0.16666666666666666,0.1904761904761905
596,SP:8177c43096a6a35c7f64ccf714ae038b1ea4db7a,"This paper proposes a method for learning an object-centric symbolic representation of an environment that allows for planning. It extends the framework introduced in Konidaris et al. (2018), which learns a symbolic representation of an environment that can be expressed using a PDDL for planning. Importantly, the method presented in Konidaris et al. (2018) does not make assumptions about structure in the environment and how different states may relate to one another. As a consequence, the learned representations are highly tasks-specific and transfer/generalization to new tasks and across states is not possible.   In this paper it is assumed that the world consists of objects and that similar objects are common across tasks. This enables object-centric representations (and the associated model) to be reused across tasks, and organizing objects according to 'object types' that behave similarly when acted upon. This is expected to improve learning efficiency and generalization/transfer to new tasks.   The method presented here incorporates techniques for merging objects into object types, and integrating problem-specific information to ground these representations into specific tasks. The working of these techniques is validated experimentally, and it is shown how greater sample efficiency can be obtained when generalizing to new tasks.","This paper introduces a method for learning symbolic, object-centric abstractions from object-factored environment observations for long-term planning tasks. It extends the symbolic representation learning framework by Konidaris et al. (2018) by factoring the state into objects, learning object type abstractions, and “lifting” the model to operate on these object-centric, typed abstractions. The method is successfully demonstrated on three environments of complexity ranging from simple per-object discrete feature vectors to a Minecraft long-horizon planning task from (object-factored) pixel observations.","The paper presents an approach for object-centric representation learning for planning to accomplish complex tasks. The learned representations are at an abstract level, resulting in desirable knowledge transfer capabilities between tasks. The learned action knowledge is represented using PDDL. Experiments have been conducted using blocks world and Minecraft domains. Results show that the agent was able to learn useful operators (actions) and that learned actions can be applied to different tasks. ","- No Baseline Comparisons: Paper proposed a way to generate object level representation that can be used across the tasks with same objects. Authors claim that this should reduce the number of environment interactions required to solve the new task. However, there is no baseline comparison being done to figure out how sample efficient it is.",0.15763546798029557,0.12807881773399016,0.09359605911330049,0.15294117647058825,0.12941176470588237,0.125,0.3764705882352941,0.3611111111111111,0.34545454545454546,0.18055555555555555,0.2,0.16363636363636364,0.2222222222222222,0.18909090909090912,0.14728682170542637,0.16560509554140126,0.15714285714285714,0.14173228346456693
597,SP:822d55a13102e4ff73a808d2e32dbeb153c39df6,"The paper presents a method for inverse reinforcement learning for learning the non-Markovian (trajectory-based) rewards for symbolic transitions of a pre-speciefied (up to free variables that need to be learned) automaton. For this purpose, Symbolic Reward Machines (SRM) are introduced, which define symbolic states, transitions and rewards, whereby MDP-trajectories can be unambiguously mapped to SRM-trajectories. The method uses adversarial imitation learning, where the rewards that are computed by the SRM (for a given grounding of the free variables) are used by a discriminator to classify expert- and agent-trajectories. Following the framework of Bayesian GAIL, a distribution over groundings/rewards is learned. However, whereas Bayesian GAIL learns a particle approximation to the posterior, the submission learns a variational Gaussian distribution. The method is evaluted on (rather challenging) gridworlds, where it is able to learn rewards for concepts such as ""picking up a key after finding the goal"". ",The paper presents Symbolic Reward Machines (SRM) which helps in achieving interpretable and explainable reward functions. The paper also proposes a hierarchical Bayesian framework to concretize Symbolic Reward Machines using expert demonstrations. The author evaluates the proposed method on a Mini-grid environment with varying complexity and compares against state-of-the-art methods.,The paper presents a new IRL algorithm built on top of a reward machine (FSA-like) representation of a reward function.  The machine is lifted to relational predicates that may have free variables that need to be inferred.  The IRL procedure takes in a set of expert trajectories that are used to ground (or “concretize”) these variables and an RL algorithm (such as PPO) that can be used to learn policies based on reward functions.  A GAN approach is then used until the policy converges to behavior that matches the experts.  Empirical results are shown in 3 mini-grid scenarios.,"This paper deals with the problem of learning non-Markov task specifications (or rewards) encoded as a reward machine by inferring the real-value parameters of a reward machine (typically the edge transition rewards in an RM). The authors adopt a Bayesian deep learning approach to simultaneously learn a reward approximator, a distribution over the parameters of the reward machine, and the policy to optimize the most-likely reward instance.   The authors then go on to demonstrate in grid world environments that benefit from non-Markov rewards and policy representations that the inferred RM outperforms standard reward definition using standard RL approaches. And they further demonstrate that the inferred reward machines are beneficial  in larger instances within the same domain.",0.125,0.16447368421052633,0.14473684210526316,0.25925925925925924,0.25925925925925924,0.23,0.35185185185185186,0.25,0.18333333333333332,0.14,0.11666666666666667,0.19166666666666668,0.18446601941747573,0.19841269841269843,0.16176470588235292,0.18181818181818182,0.16091954022988508,0.20909090909090908
598,SP:8275b2e7787c420a6280e4d0d9b77f607581548f,The authors use a generic algorithm to embed molecular graph into 3D space using a distance matrix computed from PageRank. Then they use this 3D information as additional features to do property prediction. Results show that a) this approach is better than using GNNs without this information b) the augmented structures can even be applied to on Euclidean GNNs such as DimNet and get better results compared to simple GNNs.,"This paper focuses on the task of molecular property prediction and proposes novel approaches to build synthetic coordinates (geometric information) in molecules when atom positions are not available for advanced GNNs. Since the performance of those GNNs highly relies on atom coordinates, the applicable cases of advanced GNNs are limited without reliable and efficient approaches to generate coordinates before training.  The authors propose two ways to generate the synthetic coordinates: 1. Graph-based distance and angles via Personalized Page-Rank; 2. Molecular distance bounds and corresponding angles based on chemical knowledge. To embed the resulting distances and angles, the authors propose to convert the original molecular graphs to line graphs. With line graphs as input, regular GNNs work as directional MPNNs, which exhibit better performance in practice. Experiments on benchmarks validate the improvements achieved by using synthetic coordinates in molecular graphs with several baseline models. ","The paper proposes a method for estimating molecular properties without 3D information about the molecules. The main contribution of the paper is an approach to approximate the local distance and angular information. This may then be fed into other GNNs for calculating the various properties. The authors also explore improvements related to using line graphs.  Results are shown on QM9, ZINC and ogbg-molhiv datasets. ","This paper aims to improve molecular property prediction by generating synthetic 3D coordinates. The method is based on the key observation that the true 3D coordinates are difficult to obtain and oftentimes undecidable for molecular structures. The paper proposes synthetic coordinates, which then enables using GNNs without requiring the true molecular configuration. The experimental results on several data sets have shown very promising results using this method. ",0.17142857142857143,0.17142857142857143,0.15714285714285714,0.12413793103448276,0.13793103448275862,0.2153846153846154,0.08275862068965517,0.18461538461538463,0.16417910447761194,0.27692307692307694,0.29850746268656714,0.208955223880597,0.11162790697674418,0.17777777777777778,0.16058394160583941,0.17142857142857143,0.18867924528301888,0.21212121212121213
599,SP:828f5b0797f9ebd6102fde0cc970ab52ab3cb10b,The authors in the paper propose a method to train multi-layer neural networks. The standard back-propagation algorithm aims at computing the influence of the output neuron in a step wise chain rule fashion. The alternative proposed in this paper is to compute the influence of a mid-layer neuron on the output of the network using REINFORCE algorithm.  The proposal is to treat each neuron as a separate learning agent. The authors main contribution is to implement MAP propagation to control variance that comes with REINFORCE algorithm. The initial set of results show that this technique can be comparable to back-propagation in RL settings.,"The paper introduces a new synapse-local method for reinforcement learning, that is similar to the REINFORCE algorithm, but reduces the high variance caused by REINFORCE's need to assess many stochastic samples, at the cost of added computation.   IIUC, it does this by 1- making REINFORCE layer-local (Eq 2), 2- Replacing the gradient of probabilities of observed responses  by its expectation given the state and action (which is uncomputable) (Eq 4-5), and 3- Approximating this uncomputable expectation by replacing actual activations with their maximum a posteriori estimate, given the chosen action and observed state. The latter turns out to be computable, with some iterative process.  The algorithm is also shown to be equivalent, update-wise, to plain backpropagation with the reparametrization trick (this is how I understood Theorem 2 and Eq. 11, but I may be wrong, see below).   Various experiments show that this method learns faster than REINFORCE and about as fast as backprop with the reparametrization trick, in terms of number of steps (though each step is computationally more intensive).","This work introduces a local learning rule (with a global reward signal). The basic idea is just a neural network with stochastic units and REINFORCE update each layer (that is, the REINFORCE gradient estimate is not backpropogated through the network, each layer computes an update weighted by the reward. This, by itself, is (as recognized in the paper) an old idea.  They key addition here is an approach to reducing the (prohibitive) variance of this approach by conditioning the update of a particular layer on the state and action (eq 5). This lower-variance update is then approximated by eq 6. One interpretation of this is that the activations of the layer being updated are shifted to be more compatible (lower free energy) with the nearby layers.  They try this approach on some standard simple RL benchmark tasks and find that it performs comparably to backprop but much better then naive REINFORCE (per layer) or a previous method for reducing variance in this situation.",This paper proposes a local learning rule as a biologically plausible alternative to backprop. The core idea is to train each neuron using REINFORCE and mitigate the high variance issue of REINFORCE using their novel MAP propagation algorithm. They also show how a similar method could be used to train a value function for an actor-critic algorithm. And they are able to demonstrate the effectiveness of their method on few tasks.,0.2523364485981308,0.24299065420560748,0.1588785046728972,0.2057142857142857,0.11428571428571428,0.12804878048780488,0.15428571428571428,0.15853658536585366,0.2361111111111111,0.21951219512195122,0.2777777777777778,0.2916666666666667,0.19148936170212763,0.19188191881918817,0.1899441340782123,0.21238938053097345,0.16194331983805668,0.17796610169491528
600,SP:82bbfa7c9cceabf5be7d22bbfa2fa901c9a79148,"This paper deals with constructing coresets using $\ell_1$ Lewis weights. Such problems usually focused on $\ell_p$-regression problems but in this paper, a coreset construction scheme was applied to a fairly general class of functions, namely, ""nice hinge loss"" functions, including but not restricted to the RELU loss function, log hinge function, hinge function, etc.  As for applications, this paper generalizes existing work on logistic regression (without the regularization) using lewis weights. It also discusses a coreset construction for the hinge loss. The authors claim the first result of coreset construction concerning ""nice hinge loss functions"" that do not depend on the sensitivity framework. This enables avoiding the extra multiplicative $O(d^2)$ term in the sample complexity.  In general, I like this paper, however, I have some comments concerning the Related Work (see below).","This paper studies construction of coresets for training linear classifiers with objective functions such as logistic loss and hinge loss. The coresets they construct are of size $d\mu_y(X)^2/\epsilon^2$ upto polylogarithmic factors where $\mu_y$ is the parameter that denotes the maximum ratio of the correctly classified points to the wrongly classified points by a linear classifier. The authors argue that in general as all the points in the example cannot be classified by a linear classifier exactly, the parameter $\mu_y(X)$ is low and therefore  can have small coresets. For constructing the coreset, the authors sample points independently from a distribution based on $\ell_1$ lewis weights which were earlier used to construct $\ell_1$ subspace embeddings with small number of rows.","The authors propose a novel way of building coresets for linear classification problems. They build upon the two following works:  - [MSSW18], in which the concept of the classification complexity measure $\mu_y(X)$ is proposed and first used in coreset theorems - [CP15], in which normalized $\ell_p$ Lewis weights are shown to be a good probability distribution for row sampling of tall matrices $A\in\mathbb{R}^{n\times d}$, when one wishes to preserve the $p$-norm of the product $Ax$ for any vector $x\in\mathbb{R}^d$.  The authors show that $\ell_1$ Lewis weights of the input data matrix $X\in\mathbb{R}^{n\times d}$, are useful to build a good probability sampling distribution over the data elements, in order to create efficient coresets for linear classification. To do this, they: - first show that normalized $\ell_1$ Lewis weights are a good probability distribution for sampling coresets when the classification's loss function is the ReLU function; as the proof is elegantly short in this case. This strategy guarantees coreset of size $\tilde{O}\left(d\mu(X)^2\right)$ whp. The proof is mainly based on  proof strategies found in [CP15]. - then, via the definition of so-called nice Hinge functions --basically functions that are similar enough to the ReLU, the authors generalize their results to the hinge loss or the log loss functions. The proof needs to separately take into account different subspaces of $\beta$'s depending on the value of $||X\beta||_1$.  Their coreset construction is an improvement (the guaranteed minimal coreset size is much smaller) over similar approaches that take into account this $\mu(X)$ parameter (basically, [MSSW18]).  The paper finishes with experimental comparisons with the state-of-the-art from [MSSW18] (that samples with a combination of uniform and square root of the classical $\ell_2$ leverage scores, an oblivious sketching algorithm, and also simple uniform sampling. The algorithm used to estimate the $\ell_1$ Lewis weights is that of [CP15].","This paper provides improved coresets for a wide class of linear classification problems, improving the seminal work of [MSSW18]. In particular, it improves the d^3 \mu^3 / \epsilon^4 bound of [MSSW18] to d \mu^2 / \epsilon^2, which is significant. The parameter \mu measures the imbalance of the data, and it was introduced by [MSSW18] and shown to be necessary for a small-size coreset to exist.  Technically, the improvement is achieved by using L_1 Lewis sampling which is different from the standard Feldman-Langberg approach commonly used in coreset literature. The experiments show that the new coreset can largely outperform the uniform baseline, and is overall comparable with [MSSW18] while slightly outperform it when \mu is large. ",0.1386861313868613,0.22627737226277372,0.1386861313868613,0.29457364341085274,0.2248062015503876,0.10843373493975904,0.14728682170542637,0.09337349397590361,0.15702479338842976,0.1144578313253012,0.2396694214876033,0.2975206611570248,0.14285714285714288,0.13219616204690832,0.14728682170542637,0.1648590021691974,0.23199999999999998,0.15894039735099338
601,SP:82e6dd26c69e983fc10fb60ce55e6fa0b2d5e9c9,"The paper introduces an upgrade of the existing research on the existence of low-loss paths connecting minima (or random points) on the loss surface of a neural network. The main idea is to bound the loss of the points along the path by the loss of the sparse subnetworks of the original network. The technique is demonstrated to be more widely applicable as compared to the existing dropout-stability technique. Moreover, the connection to memorization ability is shown with smaller requirements to the width of the layers, which is also demonstrated in the experiments (by the price of considering only SGD solutions and not the full parameter space). The theory gives an insight on the trade-off between quality of features and overparametrization of the layers. The empirical evaluation demonstrates that the proposed approach can find connection path on each level and with significantly better approximation than the dropout stability technique. Also it requires less overparametrization for precise estimations.","The authors propose a theoretical explanation to the mode connectivity phenomena observed in neural networks. Specifically, it is shown that any two points in parameter space may be connected by a piece-wise linear path of low loss.  Technically this is done by assuming that there exists a tradeoff between feature quality and over parameterization at every layer. The authors note that this tradeoff assumption is milder than the dropout stability assumption made in previous work. Given this assumption, the authors improve upon the conditions for the existence of level set connectivity by requiring that the last two hidden layers only contain O(\sqrt{N}) neurons, instead of O(N).","1. The authors prove a mild condition for mode connectivity for deep nets. The proof is a natural generalization of (Kuditipudi et al., 2019) and the condition required in this paper is provably weaker than the dropout stability. The authors also demonstrate in experiments that their condition can still hold when dropout stability fails.  2. The author provides a nice conceptual connection between their results and the memorization capacity of neural nets. In detail, they show if the product of widths of the last two layers is $\Omega(N)$, then solutions are connected via a low-loss path with minimal assumptions.","This paper studies the connectivity of two parameters such as two SGD solutions of the deep neural networks. The general result subsumes several conditions including dropout stability, memorization capacity, and linear separable features. In particular, the theory shows that the connectivity occurs when the last two layers have $\Omega(\sqrt{N})$ neuron. The theory is validated by numerical experiments. ",0.11875,0.15,0.1,0.23636363636363636,0.16363636363636364,0.16831683168316833,0.17272727272727273,0.2376237623762376,0.2711864406779661,0.25742574257425743,0.3050847457627119,0.288135593220339,0.14074074074074072,0.18390804597701146,0.1461187214611872,0.24644549763033177,0.21301775147928992,0.2125
602,SP:82e8c35836f5e70887dcc258df4610a2448fac14,"In this paper, the authors proposed to replace the classical activation units in MAML with stochastic local winner-takes-all (LWTA) units. The authors argued the biologically motivated LWTA units would lead to better performance in the few-shot setting where the support set in the target task is small, in which case the embedding representation of the target task would be noisy. The authors framed their proposal in the Bayesian setting, and proposed algorithms to estimate the distribution parameters.","The paper proposes to use neural networks with so-called ""stochastic local winner-takes-all (LWTA)"" activations in the place of standard ReLU activations. These stochastic LWTA activations result in a model with sparse representations. The sparse gating is treated as a random variable. The authors also treat the weights of the neural network as random variables. Both sets of random variables are learnt using variational inference.","This paper proposes a stochastic local winner-takes-all (SLWTA) approach to learn data-driven (stochastic) sparsity. This is motivated by the limited availability of training data, especiially in few-shot meta learning scenarios. The authors propose a meta-learning algorithm for their SLWTA approach. As this is a probabilistic approach, one can draw from the approximate posterior at inference to obtained the posterior predictive distribution. The authors compare their method against MAML, FOMAML and Reptile on the Omniglot and Mini-Imagenet displaying better performance than the origiinal MAML algorithm.",This paper proposed a novel meta-learning method by replacing standard nonlinearity functions in MAML with stochastic local winner-takes-all (LWTA) activations. The authors claim that such design results in sparse representations and benefit meta-learning. This method demonstrated superior performance over MAML and FOMAML over standard benchmarks such as Omniglot and Mini-ImageNet.,0.225,0.2625,0.2,0.2537313432835821,0.208955223880597,0.2222222222222222,0.26865671641791045,0.23333333333333334,0.2909090909090909,0.18888888888888888,0.2545454545454545,0.36363636363636365,0.2448979591836735,0.24705882352941178,0.23703703703703705,0.2165605095541401,0.22950819672131145,0.27586206896551724
603,SP:831ee132b504f1f111faae57feabd1d2964cdb5f,"The paper investigates the use of AdderNets (networks that do not use multiply operations) for object detection. Several modifications to the original AdderNets architecture are proposed including ""unfreezing"" batch norm statistics during inference as well as residual connections. Knowledge distillation as well as ""other tricks"" (L206) are used to further improve performance. Results are reported for some modified (replace traditional convs with adders) detection architectures are presented. The results show a moderate drop in performance, while supposedly using less power (estimated in mJ). ",This paper empirically explores the impact of adder-based network architecture for developing object detectors. It studies the impact of batch normalization and batch size on the performance of adder networks and proposes unfreezing batch normalization for adder networks. It also explored the effect of different neck architectures by investigating the limitations of adder networks and how to improve them. ,"The authors look at using AdderNets for object detection. AdderNets are neural networks which replace dot product between a weight filter and an activation window with the L1 distance between that weight and activation window instead. In hardware, AdderNets would replace multiplications with additions which are simpler and use less energy.  The authors use empirical experiments to discover a number of strategies that improve AdderNet AP, including: 1. Unfreezing the batchnorm statistics during downstream fine-tuning 2. Using direct gradients for the L1 distance (which are +`1/-1) instead of a continuous approximate gradient 3. Finetuning from a knowledge-distilled AdderNet backbone (as opposed to a regularly trained AdderNet) 4. Architectural changes specific to object detection, adding more skip connections  With these ideas, AdderNet versions of object detectors can approach 1-3% of their convolutional counterparts in AP while reducing multiplies by about 50% and energy consumption by `about 25%.","This paper is an empirical study of using adder networks for the task of object detection. These networks (mostly) use additions over multiplications, and are a good choice for improving computational performance for this task, specially for edge and embedded devices. I belive that the authors in this paper only consider adder networks only for the backbone and neck architectures of an object detector, leaving out the prediction heads which still use full convolution.  The contributions are: some tricks and strategies to train AdderNet's for object detection, a comprehensive evaluation on COCO and PASCAL VOC, a residual version of FPN which has slightly better performance, knowledge distillation for an improved backbone network for the object detector, and an analysis of freezing batch normalization and the effect of the batch size on the adder object detection networks. ",0.13253012048192772,0.21686746987951808,0.18072289156626506,0.21666666666666667,0.3333333333333333,0.14666666666666667,0.18333333333333332,0.12,0.10948905109489052,0.08666666666666667,0.145985401459854,0.16058394160583941,0.15384615384615385,0.15450643776824033,0.13636363636363638,0.12380952380952381,0.20304568527918782,0.15331010452961674
604,SP:8332ae1b0a6a8e1941636fadc4f543f7b4214eca,"The authors propose to use corpora generated from _emergent communication_ as a fine-tuning signal for NLP tasks (language modeling and image captioning in particular).  They show that, especially in small-data regimes, pre-training on an emergent language can yield significant performance boosts in both tasks.  (In particular, pre-training on emergent language performs on average better than synthetic hierarchical data, but not quite as well as a different natural language, and all of these pre-training methods do better than training from scratch.)  This is the newest of a small but growing body of literature that seeks to connect emergent communication with genuine NLP tasks.  The results are intriguing and promising and should be of interest both to the emergent communication community as well as to the broader community working on low-resource NLP.","This paper proposes a new way of using techniques from the emergent communication literature, where agents are trained to develop languages for communication on some shared task, to improve more typical supervised learning tasks in NLP, namely language modeling and image captioning. This is a question that is only just beginning to be addressed in some work (e.g. Lowe et al., 2021; Lazaridou et al., 2020), so this area is ripe for exploration. This paper proposes an interesting and novel method of using EC to improve supervised learning: generate emergent language from an image-based referential game with simple speaker/listener models. Then, using a much larger transformer, pretrain on this corpus before doing fine-tuning on the supervised task of interest, a la Papadimitriou and Jurfasky, 2020.  Both language modeling and image captioning tasks are explored, and results convincingly show that emergent communication corpora are surprisingly effective. Some ablation studies help elucidate where the benefits arise, although there are some questions here (see Weaknesses).  Finally the authors propose a new metric for evaluating the naturalness and usefulness of a corpus of emergent language: if you have natural language messages generated for some task, as well as true human messages, the authors literally just build a model for translating emergent languages into the corresponding human messages. The ROUGE score obtained by such a model is thus a ""transferability"" metric. This metric is not only useful for identifying whether an EC corpus will be useful for transfer for some task (more similar = more useful), but is possibly also a measure of humanness/compositionaliy of a language in general, and so this contribution will be useful even for those in EC who are not interested in supervised learning.  Overall, I very much like the philosophy of this paper. I think it helps fill an important gap in the literature, which is namely what to do with the recent deluge of emergent communication studies. The big elephant in the room with these studies is whether the languages are actually useful for some reason besides being studies of linguistic evolution with limited generality. This paper proposes a simple and interesting way of using the emergent languages for something more productive and more of interest to people actually working on real NLP applications. I do however see some weaknesses and lack of comparison which I would love to see answered and/or addressed in the final version.","This paper studies whether output from emergent communication systems might be useful as pre-training data for natural language tasks. The authors train an EC agent, generate from it, and use the generated IDs as data for language model pre-training. Compared to pre-training on Spanish wikipedia or a simple bracket language, pre-training on EC-generated data is better when you have 2M examples, but quickly becomes worse as the amount of data increases. In experiments on image captioning, the authors find that pre-training on either EC-generated or natural language captions improve over not pre-training, but that there is not much of a difference between the results when pre-training on EC-generated text and natural language. Finally, the authors propose EC -> natural language translation as a way of evaluating the quality of EC agents.","This paper explores the use of pre-training models on corpora of emergent languages to improve performance on downstream tasks (in this case language modeling and image captioning).  The authors compare against using other types of pre-training corpora, like synthetic or natural languages.  While not always optimal, the authors demonstrate that there is clearly an advantage to using emergent languages in this way, if one is otherwise considering training from scratch.  They propose a new metric for evaluating whether languages have structure, doing so by training a translation model to map emergent language to natural language on the same data, and validate the measure by showing it corresponds closely to task accuracy. ",0.3161764705882353,0.22794117647058823,0.18382352941176472,0.08436724565756824,0.09181141439205956,0.19285714285714287,0.10669975186104218,0.22142857142857142,0.22123893805309736,0.24285714285714285,0.3274336283185841,0.23893805309734514,0.1595547309833024,0.2246376811594203,0.20080321285140565,0.1252302025782689,0.1434108527131783,0.2134387351778656
605,SP:836bbc79c4a4dbf6e46e7da2d8ae8191183fbf26,"In this work, the authors propose a novel framework for the generative commonsense reasoning (GCSR) tasks. GCSR are tasks where given a set of concepts (or a sequence of set of concepts), a model is required to generate sentences (or a short paragraph) that are both grammatically correct and plausible (follow commonsense).   The authors propose Imagine-and-Verbalize (I&V), which is a 2-layer system: 1. The imagination module is a transformer-based seq2seq model, takes a set of concepts (and sometimes text as context) as input, and generates a flattened graph (AMR tree, PENMAN serialization).  2. The verbalization module is another transformer-based seq2seq model, which takes the concepts, some context, and the generated scene knowledge graph (SKG, flattened) as input, and translate the scene information into natural language sentences (or iteratively, paragraphs).  On two GCSR datasets, the proposed method significantly outperform strong baselines and prior works. ","Descriptive sentences about arbitrary concepts generated by neural text generation models are often grammatically fluent but may be not consistent with common sense. This show that such generative commonsense reasoning (GCSR) skills are lacking in state-of-the-art text generation methods. Therefore, the authors propose an Imagine-and-Verbalize (I&V) method, which learns to imagine a relational scene knowledge graph (SKG) with relations between the input concepts, and leverage the SKG as a constraint when generating a plausible scene description. Experimental results demonstrate the effectiveness of I&V in improving language models on both concept-to-sentence and concept-to-story generation tasks.","This paper proposes a model for generative commonsense reasoning, taking a set (or sets) of concepts as input and producing a sentence (or sentences) as output. The primary innovation introduced in this model is the use of an intermediate ""scene knowledge graph"" (roughly equivalent to an AMR of the corresponding sentence) which is output by an ""imagination module"" that takes concepts (and textual context) as input. This SKG is then input, along with the concepts and textual context, to a ""verbalization module"" which outputs a sentence (or sentences). The authors show that this model outperforms most of the selected baselines (on tasks of generating single sentences as well as stories), with the exception of KFCNet, which the authors argue is trained on prototypes from a much larger corpus, and is supervised to produce sentences like these prototypes, possibly limiting the model's need to generalize at test. They also do ablations on the type of training data, and the size of base LM, as well as showing that their model learns better with smaller amounts of training data. ",The paper introduces an Imagine and Verbalize two-step method for generative commonsense reasoning. The system first imagines a scene in the form of a linearized SKG and uses that as input for a second model that verbalizes it into (more) human readable text. The method is tested on the Concept2Sentence and Concept2Story tasks and compared to multiple baselines drawn from related works.,0.18791946308724833,0.26174496644295303,0.1342281879194631,0.21904761904761905,0.1523809523809524,0.10674157303370786,0.26666666666666666,0.21910112359550563,0.31746031746031744,0.12921348314606743,0.25396825396825395,0.30158730158730157,0.22047244094488191,0.23853211009174313,0.18867924528301885,0.16254416961130744,0.19047619047619047,0.15767634854771787
606,SP:8390572468186e948fb942b218a7dae77b48454b,"The paper presents a general architecture which unifies fifteen previously separate ML4Code tasks and achieves state-of-the-art or near state-of-the-art results on them.  The inputs of the architecture are graphs where the nodes have labels and positions, and both the nodes and the edges have types. The outputs follow the ToCoPo output formulation where each output element can be a token, a copy, or a pointer. This makes the architecture flexible enough to accomodate most ML4Code tasks. ","This paper proposes a framework, called PLUR to unify a number of code-related tasks into a general form. Specifically, it pairs a graph encoder with a Transformer-style decoder augmented with pointers and a copy mechanism as a unified form and uses GREAT as the graph encoder.  On 15 different tasks e.g., function name prediction, program repair, variable misuse, PLUR has achieved at or above state-of-the-art results. Furthermore, PLUR also investigates multi-task learning by the proposed framework and yields promising results.","The paper proposed a general method called PLUR for solving ML4code problems. PLUR is based on mapping a graph to a sequence of tokens and pointers. The author showed that 15 different ML4Code tasks can be casted in the form of PLUR. Experiments show that PLUR achieves near or above state-of-the-art results for nearly all tasks. In addition, the unified framework enables the author to study a few important research questions, including 1) performance of different encoders (GREAT, Transformer, GGNN), 2) importance of the pointer mechanism, 3) effect of different graph representations, 4) effect of multi-task learning. To be more specific, the author showed that 1) GREAT and Transformer encoders outperform GGNN, 2) multi-task learning with PLUR improves the performance in most datasets, 3) copy mechanism and pointer network improves the performance. ","This work proposes a unified encoder-decoder framework, PLUR, that can cover 15 recently published source code tasks (e.g. code classification, code repair, function name prediction). The encoder generalizes sequence-based modeling (e.g. Transformer) and relation/graph-aware modeling. The decoder includes copying and pointer mechanisms. Their unified model, PLUR, outperforms task-specific models in previous literature, and also enables multi-task learning. The authors also study important research questions such as Transformer vs GNN. I think this paper deserves acceptance to NeurIPS.",0.15853658536585366,0.2682926829268293,0.10975609756097561,0.28735632183908044,0.21839080459770116,0.11678832116788321,0.14942528735632185,0.16058394160583941,0.10588235294117647,0.18248175182481752,0.2235294117647059,0.18823529411764706,0.15384615384615385,0.2009132420091324,0.10778443113772455,0.22321428571428573,0.22093023255813954,0.14414414414414417
607,SP:83d45a08ec41d05c4c980a741810a296b2e5ba4f,"The work proposed a simple framework that allows performing robust subspace recovery without requiring a priori knowledge of the subspace codimension. The proposed approach is based on Dual Principal Component Pursuit (DPCP), which is amenable to handling subspaces of high relative dimensions. Empirical results corroborate the developed theory and showcase the merits of the proposed optimization methods.","This paper discusses solving robust subspace recovery by solving parallel versions of $\min_b$ $\|X^Tb\|_1$ st $\| b\|_2=1$, for random initializations of $b$, using projected subgradient method. The argument is that using this  method, in the limit where the number of inliers greatly outnumber the outliers, that if the inlier dimension is c, then c random initializations will converge on identifying this dimension with probability 1. Experimental results and proofs are given to prove this idea.   ","This paper is concerned with the Robust Subspace Recovery (RSR) problem. That is, one tries to estimate an unknown subspace given some data, where some of the entries are corrupted. This paper studies the setting, where the subspace to be recovered has large dimension, i.e. the co-dimension of this subspace is small.   Whereas previous work tackles this question, it requires that the dimension of the subspace to be recovered is known a-priori. This work proposes a new algorithm, which is able to recover the subspace without knowing this dimension.  The contributions of this paper are threefold. 1. It proposes a new algorithm, which does not need a-priori knowledge of the unknown subspace. 2. Some theory for the new algorithm is developed. 3. The algorithm is tested on both synthetic and real data.","This paper studies robust sparse recovery when the dimension of the subspace is not known. It finds a basis of the orthogonal complement of the subspace via dual principal component pursuit (DPCP), but without explicitly imposing orthogonality between the columns. The authors propose a projected sub-gradient method with random initialization and overestimate of the dimension, and show theoretical results. Simulation experiments are provided.",0.17543859649122806,0.2982456140350877,0.2631578947368421,0.25316455696202533,0.189873417721519,0.14705882352941177,0.12658227848101267,0.125,0.234375,0.14705882352941177,0.234375,0.3125,0.14705882352941177,0.17616580310880828,0.24793388429752067,0.18604651162790697,0.2097902097902098,0.2
608,SP:842005dc93fb24f029d3db702fe535e621eeb278,"This paper presents a method for adapting a learned compression model to improve the lossless compression rate over a set of data to be transmitted, e.g. a set of images. The method assumes a generative model has been optimized over a large dataset, and several models are explored empirically including HiLLoC and IDF++. The model represents a distribution over data items that can be used as the entropy model for a lossless compression algorithm (Huffman coding, arithmetic coding, ANS, etc.). The core idea in OSOA is to divide the data to be compressed into separate batches and to adapt the pre-trained model on each batch before coding the next batch.  The authors show that this approach reduces total bit rate, outperforms training and transferring a separate model for each batch (at least in several cases), that the method can be adapted to FIFO and FILO algorithms (e.g. range coding vs ANS), and that there is a trade-off between runtime and compression rate based on early stopping of the update algorithm.","This work proposes an adaptive compression scheme that leverages a pre-trained deep generative model (DGM).  Specifically, the authors consider compressing a sequence of many (~thousands or more) samples and propose to periodically update the underlying DGM, so that the DGM gradually becomes a better fit for the dataset at hand.  The main contributions of this work are in its formulation as well as experimental results.","The paper considers the task of online adaptation of pre-trained lossless compression models, while simultaneously using the updated model to compress the target dataset. The resulting framework, and new algorithm called OSOA, is based on the key insight that when losslessly compressing many batches of data, one can update the existing model on the data batches on the fly, so that the compression model better captures the target data statistics. Together with a FIFO codec (such as arithmetic coding) or FILO codec with a caching mechanism, the decoder can decode the data batches while replaying the model updates, so that the model update comes with no extra bitrate cost. Experiments demonstrate significant time and space savings of OSOA compared to the baselines of re-training or fine-tuning. ","The paper introduces One-Shot Online Adaptation (OSOA) for lossless compression by generative models. This method allows a model to adapt to a test-time distribution that is different from the train-time distribution without having to transmit a re-trained model. The basic idea is that one can apply a deterministic learning procedure to the model for each batch in a sequence of batches being compressed. Since compression is done losslessly, the decoder can replicate the exact same training procedure, and thus have access to the same adapted model as was used for compression. The idea is validated in a thorough experimental study.",0.09195402298850575,0.16091954022988506,0.16666666666666666,0.22727272727272727,0.24242424242424243,0.20155038759689922,0.24242424242424243,0.21705426356589147,0.27884615384615385,0.11627906976744186,0.15384615384615385,0.25,0.13333333333333333,0.18481848184818483,0.20863309352517986,0.15384615384615383,0.18823529411764706,0.22317596566523604
609,SP:842877c0caf8f1d193dc997cbcc38580682edf41,This paper proposes an hierarchical semantic-visual adaptation framework to migrate the distribution and structure disagreement in generative zero-shot learning (ZSL) models. This network adopts hierarchical two-step adaptation. The task-specific adaptation is complemented by training two task-specific encoders with a supervised adversarial discrepancy module. The distribution adaptation is achieved by minimizing the Wasserstein distance between visual and semantic distributions. Quantitative results on four widely used benchmarks indicates the effectiveness of the proposed method. Qualitative visualization shows the network learns intrinsic common space that better bridges the gap between the heterogeneous feature representations. 	 ,"The paper proposes a new two-step, hierarchical adaptation approach for Zero-Shot Learning. The first step is structure adaptation where visual data and target/semantic data (eg attributes) are encoded into an aligned common space. This is similar to prior work, but is accomplished using a new module that adversarially minimizes the discrepancy between the visual and semantic-based classifiers. The second step is distribution adaptation which seeks to align the visual and semantic feature distributions using a common encoder.  The paper ablates the contribution of both structure and distribution alignment on several ZSL benchmarks, showing significant improvements. It also compares favorably against all prior common space ZSL methods on all benchmarks, and against all prior work (common and non-common space) on several benchmarks.","In zero-shot learning (ZSL) problem, feature representations of distinct semantic and visual domains are intrinsically heterogeneous, thus having both distribution and structure variations. To this problem, this paper proposes a novel HSVA framework that aligns the semantic and visual domains by structure adaptation (SA) and distribution adaptation (DA). SA, consisting of two encoders, aims to encode the visual data and semantic data into a structure-aligned common space, by using a SAD module. Then, DA further explores a common encoder to align the visual and semantic distributions. This paper also conducts extensive experiments to show the effectiveness of the proposed framework for both conventional ZSL and generalized ZSL settings. ",This paper proposes a new method for zero-shot learning. It enforces the classes are well aligned by using a domain-specific discrepancy on an intermediate space of the encoder. The method is evaluated on benchmark datasets and achieves good results.,0.25,0.23958333333333334,0.14583333333333334,0.24603174603174602,0.15079365079365079,0.10909090909090909,0.19047619047619047,0.20909090909090908,0.34146341463414637,0.2818181818181818,0.4634146341463415,0.2926829268292683,0.2162162162162162,0.22330097087378642,0.20437956204379565,0.2627118644067797,0.2275449101796407,0.15894039735099338
610,SP:844fd53bb10076aaab27bfd54ac76af07a2d09be,"This paper introduces functional neural networks for image restoration that are parametrized by a variable (e.g. super-resolution factor, noise level, JPEG quality level).  A functional network is one where the neural network parameters themselves are a function of the problem parameter known a priori.  This way, the neural network weights can adapt based on the problem, for example, a denoising CNN can apply stronger denoising when the noise level increases.  Different functions are explored including linear functions which require twice the storage but using a single model.  Compelling experimental results are shown across a range of parameters of the problem.","This paper proposes a network interpolation scheme for non-blind image restoration problems with a single parameter (e.g. noise level for denoising). The parameter is remapped to [0, 1] using an affine mapping.  Effectively, two networks are trained such that for any given problem parameter in [0,1], an interpolated network is obtained by linearly blending the two networks  parameters. The interpolated model is then applied to the input. ",The authors propose a technique to build image restoration networks that are robust to variations in the problem parameters (e.g. noise level or upscaling factor). The main idea is to derive the weights of the network as a learnable function of the problem parameter.,"The authors proposed a functional neural network for image restoration. They use a linear function with respect to the task parameter to adjust the weights of the network, to achieve an adaptive restoration for different parameter levels using a single model. The idea is similar to meta-learning on restoration, or weight modulation derived from instance normalization. The experiment shows its better quantitative performance compared with SOTA among three tasks, and better visual quality. ",0.18627450980392157,0.16666666666666666,0.18627450980392157,0.18571428571428572,0.22857142857142856,0.3333333333333333,0.2714285714285714,0.37777777777777777,0.25675675675675674,0.28888888888888886,0.21621621621621623,0.20270270270270271,0.22093023255813954,0.23129251700680273,0.2159090909090909,0.22608695652173913,0.22222222222222224,0.2521008403361345
611,SP:84c07d367d510a24de584cd0b66d00d6dc751933,"The paper proposes a novel automatic augmentation method. That is a method that learns based on a dataset, how to augment that dataset during training of a classifier.  The proposed method works by matching gradients based on cosine similarity on fully trained networks and uses this approach to build augmentation policies with multiple augmentations applied after one another. Similar to the recent usage of very deep RandAugment for EfficientNetV2s for example.","A more ""automated"" augmentation search method is proposed. The main contributions are:  - Default transformations like flipping and cropping are regarded as new augmentation candidates which could be optimized, making the algorithm more automated. - The gradient matching formulation is borrowed and seems to be effective for augmentation search. - MC sampling and a progressive search pipeline are used to reduce the computational complexity.",This paper proposed a learned data augmentation method. The proposed method is able to search for the augmentation policy with multiple layers by searching the next transformation operation conditioned on all previous augmentations. The authors also proposed to use cosine similarity between the gradients of the original and augmented data as guidance rather than e.g. the more commonly used classification validation loss.,"This paper provides an interesting finding that instead of injecting experts prior to hand-pick augmentations at the top layers, the entire search can be formulated end-to-end in a multi-layer (deep) way without hand-picked transformation optimized with gradient matching. The independent sampling at each layer with estimated mean gradient of training data is used to stabilize the training and make the optimization process more efficient. Empirical results on CIFAR-10/100 and ImageNet outperform previous best performing DAS methods.",0.15492957746478872,0.2112676056338028,0.11267605633802817,0.18032786885245902,0.14754098360655737,0.20634920634920634,0.18032786885245902,0.23809523809523808,0.0963855421686747,0.1746031746031746,0.10843373493975904,0.1566265060240964,0.16666666666666666,0.22388059701492535,0.1038961038961039,0.1774193548387097,0.125,0.1780821917808219
612,SP:8574365c50e6baea474d785d24a225e4346b5b4c,"This paper takes a closer look at the ability of language models to perform reasoning based tasks such as tracking states of entities and answering navigation based questions. The main question is: Can these models generalize outside of the training distribution, thus exhibiting the ability to learn underlying rules instead of learning superficial correlations that only work on in-distribution data. This question is studied through several different generalization splits. From results, we see some evidence that pre-trained models are able to generalize outside of the training distribution though in some cases, it’s inconclusive, (and I elaborate on why below). ","This paper investigates whether pre-trained language models (T5 in this case) learn priors which support symbolic reasoning tasks in the process of optimizing their purely linguistic objective.  The performance of fine-tuned pre-trained LMs is evaluated on two symbolic reasoning tasks -- object location across a number of boxes, and a room navigation task -- and compared with identical models trained from scratch.  The results touch on a number of topics, but generally show (1) good reasoning accuracy when starting from the pre-trained t5 model, (2) abysmal performance by the from-scratch models, (3) a good degree amount of generalization performance for t5 to reason beyond the fine-tune scenario, and (4) that learning subtasks improves performance on a compositional task. ","In this work, the authors investigate if large scale pre-trained language models (such as T5) can provide inductive biases that are useful for solving language-based symbolic reasoning tasks, and furthermore, generalize to unseen settings.   First, the authors define four types of tasks. While all the four tasks are formulated as sequence generation given prompts (prefixes), they require different kinds of reasoning:  1. Container. The prefix text describes an initial states of the world and a sequence of transitions, the target sequence needs to describe the ending states.  2. Navigation Route. The prefix text describes a map as well as two positions in the map, the target sequence is the navigation path from one position to the other.  3. Navigation Result. The prefix text describes a map, a starting position, and a navigation path, the target is the ending position.  4. Composite Task. A mixture of container and navigation tasks.   Then, the authors define a set of experimental settings targeting the generalizability of models, from a variety of aspects such as map sizes, object/container numbers, reasoning steps required as well as some linguistic properties (e.g., replacing nouns with words with other POS tags, or even made-up words).  The authors provide plenty of experiments, suggesting the pre-trained language models indeed have capability of learning and to some extent generalizing symbolic rules via language-based tasks. ","This paper aims to explore whether the inductive bias of pre-trained language models can support symbolic reasoning tasks. Three different types of tasks are designed for the above objective, including a container-based task, a navigation task and a composite task. In order to verify the generalization ability of the model, the paper also designed different kinds of generalization for probing, including cardinality generalization, object generalization, POS generalization and reasonable phrasing generalization. T5 is fine-tuned and evaluated on these tasks, with some interesting findings observed, which I think are the key contribution of this work: (1) for container-based task, T5 shows good generalization and prediction capability; (2) for navigation task, T5 can also do a good job but when the number of inference steps changes, its performance will drop. (3) for composite task, T5 can do a good job  by learning in a curriculum learning way.",0.19607843137254902,0.23529411764705882,0.20588235294117646,0.26229508196721313,0.28688524590163933,0.14847161572052403,0.16393442622950818,0.10480349344978165,0.14093959731543623,0.13973799126637554,0.2348993288590604,0.22818791946308725,0.17857142857142855,0.14501510574018128,0.16733067729083667,0.18233618233618235,0.25830258302583026,0.1798941798941799
613,SP:859d8d865df4c2b628c293e1fee7ef01969337e4,"This paper propose using a multidecoder architecture to to control the style of output summarization. In the designed model, each decoder learns and generates stylisticall-distinct summaries. The contribution of each experts are controlled by a gating mechanism. This multidecoder model can be trained without supervision or with guidance. This model is tested on CNN, NEWSROOM and XSum. When trained unsupervised, different decoders learn to produce summaries with different abstractiveness and specificity. The guided training shows the capability of asigning certain styles to a specific encoder. ","This paper proposes a new architecture for controllable text summarization, leveraging multiple decoders (with the bottom decoder layers  shared across decoders) with the contributions of each decoder controlled by a gating mechanism. This model is studied in two gating settings: unguided (where the network learns the gating weights) and guided (where gating is controlled manually). The networks are shown to allow some stylistic controllability, most notably in terms of abstractiveness and specificity. ","This paper proposes an architecture for controlled summarization, allowing some type of aspect-based summarization, as in - for example - ""I want long/detailed summary"". The control proposed here is on what the authors call _stylistic_ features (which also includes _quality_, arguably not a stylistic aspect) and not other aspect (as in for example: give a summary of Darius the Great focusing on his earlier years).  They obtain this by using a Transformer (BART-like) seq2seq model, but where the last layers of the decoder are multiplied and considered as independent expert which are combined in the end. That combination consists in a weighted average (no non-linearity) of the last softmax layers.  The proposed solution is simple (good!) although not very elegant. The analysis focuses on evaluating the control capacity and diversity with respect to other (smaller) architectures.","This paper proposes a neural sequence-to-sequence (Seq2Seq) model, called HYDRASUM for text summarization. HYDRASUM incorporates multiple decoders where each decoder learns and generates stylistically distinct summaries along dimensions such as abstractiveness, specificity, and others. HYDRASUM is fundamentally built upon the notion of mixture-of-experts (MoE), where a gating mechanism decides the contribution of each expert (individual decoder) to the next token's output probability distribution. The paper demonstrates that HYDRASUM automatically learns to generate contrasting summary styles using each decoder. The paper further proposes a ""guided"" training scheme that explicitly govern which summary style is partitioned between decoders, e.g. high abstractiveness vs. low abstractiveness or high specificity vs. low specificity, and also increase the stylistic differences between individual decoders. The paper also show that HYDRASUM is flexible, during inference, it can be controlled to generate a diverse set of summaries by sampling from individual decoders or mixtures of different subsets of the decoders.",0.26744186046511625,0.19767441860465115,0.3023255813953488,0.2777777777777778,0.3194444444444444,0.14492753623188406,0.3194444444444444,0.12318840579710146,0.16560509554140126,0.14492753623188406,0.1464968152866242,0.12738853503184713,0.29113924050632906,0.15178571428571427,0.2139917695473251,0.1904761904761905,0.20087336244541484,0.13559322033898305
614,SP:8689bb56a5097f6ba7c01de5c91cfa2aae509b2e,"This paper studies the advantages and drawbacks of node injection attacks to graph neural networks. The authors demonstrate that, in general, node injection attacks are more powerful than the node modification attacks when there is no defense. But when the model trainer adopts some homophily based defense, the node injection attacks suddenly become ineffective and even underperforms the node modification attacks. Based on the observation, the authors propose to add add homophily preservation as an additional regularizer for the attack to remain stealthy against defenses. Extensive experiments demonstrate that the proposed homophily indeed improve attack effectiveness against defenses. ","This paper studies the problem of adversarial attack in graph neural networks. It aims to improve the unnoticeability of graph injection attack which injects carefully-crafted nodes into the graph data. In detail, it proposes the concept of homophily unnoticeability and studies the power of graph injection attack (GIA). Further, it observes that GIA can be easily defended by homophily defenders and proposes a harmonious adversarial objective to preserve homophily. Extensive experiments have demonstrated the effectiveness of the proposed method under various defenders.","The paper focuses on graph injection attacks (GIA) in which an adversary introduces new nodes and links such that the predicted label by a trained classifier for a victim node is changed. They compare GIA versus graph modification attacks (GMA), in which the existing links are manipulated, and theoretically prove the superiority of GIA in causing damage. They further address the vulnerability of GIA against homophily-based defense and propose a GIA attack that preserves the homophily and is robust against such defense mechanism.","This paper first theoretically shows that graph injection attack (GIA) is more powerful than graph modification (GMA), and GIA will lead to great damage to homophily distribution, which makes GIA easily defended by homophily-based defense. To mitigate the issue, the authors further propose to add the homophily unnoticeability constraint to preserve homophily. The authors also theoretically show that with the GIA with homophily unnoticeability constraint is more powerful than GIA without the constraint.",0.22448979591836735,0.19387755102040816,0.22448979591836735,0.2289156626506024,0.21686746987951808,0.23809523809523808,0.26506024096385544,0.2261904761904762,0.2972972972972973,0.2261904761904762,0.24324324324324326,0.2702702702702703,0.24309392265193372,0.20879120879120883,0.2558139534883721,0.2275449101796407,0.22929936305732487,0.25316455696202533
615,SP:8697801d0f0913455bb17049812a9f2f55b19bec,"The authors suggest using repeated problem instances to learn input graphs for graph-based semi-supervised algorithms. They extend the concept of dispersion of one-dimensional algebraic polynomials to an arbitrary number of dimensions in order to provide uniform convergence guarantees for the gaussian graph kernel in unweighted offline training and a generalization bound for weighted graphs. They also offer a threshold graph approach for online training under the dispersity assumption that includes assurances in the form of a constraint on the algorithm's regret across repetitions. Finally, they show that the suggested offline technique leads to minimal regret across a large number of repetitions T.","The authors propose to learn the underlying graphs for graph-based semi-supervised learning problems. So far, graph-based SSL usually grounds on KNN-like graphs where distances are computed according to some measure. The present paper now learns the parameters of the measures (kernels) as well as a threshold to determine whether an edge is present or not. ","This work presents novel theoretical tools for data-driven algorithm design. In particular, the authors generalise a dispersion-based analysis from intervals to axis-aligned paths of arbitrary dimension.   Using these new results, the authors study graph-based semi-supervised learning from a data-driven perspective and achieve strong bounds in the online and PAC setting. Their goal is to perform well on a fixed distribution of semi-supervised problem instances (or minimising the regret in the online setting) by selecting the most appropriate graph from a parametrised graph family (in particular: threshold graphs and graphs with edge weights given by the polynomial or RBF kernel). Many additional results, such as first experiments and lower bounds, are discussed.","This paper proposes a data driven approach to construct graph for graph-based semi-supervised classification task. The authors realize the graph quality is important to learning performance and they aim to update the hyper-parameter when new samples come. They present the algorithms for two cases, online and distributional settings.",0.16037735849056603,0.16037735849056603,0.1320754716981132,0.288135593220339,0.1864406779661017,0.1271186440677966,0.288135593220339,0.1440677966101695,0.27450980392156865,0.1440677966101695,0.21568627450980393,0.29411764705882354,0.20606060606060606,0.15178571428571427,0.178343949044586,0.192090395480226,0.2,0.1775147928994083
616,SP:86dfaba85d1ea35c9b28eefcb82ef7f6a2f02eaa,"In this manuscript, the authors proposes an assistant module (AM) that is able to dynamically change the model structure of a GNN model by adjusting the depth and the number of neurons. The AM relies on the reinforcement learning framework and exploits the loss decrease rate and the accuracy results on the validation data set to select how to modify the GCN hyper-parameters. Exploiting this component and the capsule GNN model, the authors defined a novel GNN model dubbed Adaptive Graph Capsule Convolutional Networks.","because they use scalar-valued neurons, this paper improves on existing graph capsule network, CapsGNN by adapting the neural network architecture in the runtime. More specifically, the authors propose Adaptive Graph Capsule Convolutional Networks (AdaGCCN) which leverages Reinforcement Learning (RL) to design an assistant module for continuously selecting the optimal modification to the model structure through the whole training process. In addition, it determines the architecture search space through analyzing the impacts of model’s depth and width. Evaluations demonstrates that AdaGCCN achieves comparable accuracy results with the state-of-the-art GNN and outperforms CapsGNN almost on all datasets from bioinformatics and social networks.","This paper proposes a reinforcement learning based strategy for adaptively adjusting capsule graph convolutional network architectures throughout training. To that end, they introduce an assistant module which continuously adjusts the depth and width of the network based on the loss/validation accuracy reduction rate over a predetermined training epoch window.  The search space is defined by the authors as a list of m combinations of depth and width variables D and W, and an action a_t at epoch t consists of choosing one of the m depth/width architectural changes. The current training epoch with architecture depth D_t and width W_t is treated as the state, and the reward of an action at epoch t is revised via the relative decrease in training loss/validation accuracy over 3 epochs. ","This work proposed the Adaptive Graph Capsule Convolutional Networks (AdaGCCN), which utilizes RL to adjust the CapsGNN model architecture at runtime. Specifically, an RL assistant module is proposed to search the optimal architecture during training. Another focus of this work is to speed up the proposed method with parallel processing.",0.24705882352941178,0.24705882352941178,0.12941176470588237,0.2571428571428571,0.18095238095238095,0.09848484848484848,0.2,0.1590909090909091,0.22,0.20454545454545456,0.38,0.26,0.22105263157894736,0.19354838709677422,0.16296296296296298,0.22784810126582275,0.24516129032258063,0.14285714285714282
617,SP:86f09c5440219a19e0a45ad9f6a525fb37d3d730,"This paper deals with the goal conditioned reinforcement learning problem (GCRL). It proposes an incremental approach to learning good landmark states through successor features so as to enhance exploration and in turn improve the success rate of reaching arbitrary goal states. The authors discuss learning in navigational tasks, both tabular and pixel based, where the goal state changes randomly and exploration is hard. The idea is to learn a landmark graph, each landmark being a state itself which is sufficiently distanced from other landmark states. The notion of distance is provided by the successor representation (features in the pixel case). Planning policies to reach between different landmark states alleviates the exploration issue, and thus helps in improving GCRL performance. ","Summary: ============================= This paper aims to solve the long-term goal-conditioned planning problem by tackling strategies to explore state-goal pairs. The proposed methods improve the exploration performance by formulating successor features as landmark graphs and in exploration, use three key steps: (1) select the frontier graph node; (2) use a random policy to explore; (3) extend the landmark graph by a successor feature similarity metric.  Overall, this paper is well written, with methodology clearly explained and evaluation done in detail. So I recommend a clear acceptance.","This paper presents Successor Feature Landmarks (SFL), a new graph-based planning framework based on successor features for long-horizon goal-conditioned RL (GCRL). The key idea is to use successor features to define a new metric, Successor Feature Similarity (SFS), to estimate the distance between two states or state-action pairs in high-dimensional environments, such as 3D visual environments. By leveraging SFS, a landmark-based graph is built to represent the explored state space. Both guided/directed exploration and random exploration are deployed to help the agent traverse between landmarks and explore unseen states of the environment respectively. Combining all the components, SFL is more efficient and scalable than previous graph-based methods for long-horizon GCRL.   ","The paper proposes a new metric, called the successor feature similarity, that measures the similarity between two states as the dot-product between the respective successor features. The authors then use this metric to guide goal-directed exploration when learning a goal-conditioned policy by choosing a goal with the lowest count. Novel goals are stored by keeping a buffer of visited goals, where new goals are only added if they are beyond a certain distance of any prior goal. Rewards are given by the SFS between a state and a goal (where a state-only SF is computed as the action-averaged SF). The authors compare this overall method to existing graph-based GCRL methods and find that the resulting method outperforms prior work, particularly in challenging first-person maze-navigation VizDoom tasks that require moving around multiple walls.",0.17647058823529413,0.20168067226890757,0.18487394957983194,0.19540229885057472,0.1839080459770115,0.20168067226890757,0.2413793103448276,0.20168067226890757,0.15714285714285714,0.14285714285714285,0.11428571428571428,0.17142857142857143,0.2038834951456311,0.20168067226890757,0.16988416988416988,0.16504854368932037,0.14096916299559473,0.18532818532818532
618,SP:8793c31610bb1adb659072daefe0f395a4569101,The paper studies the relationship between Byzantine gradient attacks in distributed optimization and data poisoning. Some theoretical evidence is presented about the equivalence of these. A new gradient-based Byzantine attack is also presented.,"The paper formalizes the concept of PAC*-learnability for a generalized personalized federated learning model and provides a sufficient condition. Under this model, converging gradient attack is equivalent to data poisoning. The paper also proposes the counter-gradient attack that is effective and data-efficient.",This paper reveals the inherent equivalence between gradient and data poisoning attacks in personalized federated learning settings. The authors showed that any gradient attack can be transformed into data poisoning in a personalized federated learning system that provides PAC guarantees. This new insight challenges the view that (Byzantine) gradient attacks are unrealistic. The authors built this equivalence by constructing a model attack for personalized federated learning models. The authors showed the effectiveness of this attack both theoretically and empirically. ,"The paper studies data and model poisoning attacks in federated learning (FL). It argues that there is an equivalence between data poisoning and model poisoning attacks, by restricting attention to linear and logistic regression. The main technique is to leverage ideas from PAC-learning. Another key contribution is to propose a gradient poisoning attack.",0.29411764705882354,0.35294117647058826,0.3235294117647059,0.3111111111111111,0.2222222222222222,0.17721518987341772,0.2222222222222222,0.1518987341772152,0.2037037037037037,0.17721518987341772,0.18518518518518517,0.25925925925925924,0.25316455696202533,0.21238938053097348,0.25,0.22580645161290322,0.202020202020202,0.21052631578947367
619,SP:879e4af8012f31b4bc12387f8573889b7d341bd6,"This paper adapts a recently proposed analytical closed-form Bayesian approach to optimizing Neural Networks to Q-Learning. The proposed TAGI-DQN architecture is optimized without gradient descent. RL policies based on TAGI-DQN are shown to perform well on a collection of OpenAI Gym environments, including a small number of Atari games. ",This paper proposed a new Bayesian Deep RL algorithm that learns deep Q networks (DQN) without using gradient descent. The updates of DQN's parameters follows from the tractable approximate Gaussian inference (TAGI) algorithm. Then it studies the empirical performance of proposed algorithm with DQN learned by gradient based optimization in several Atari benchmarks.,"This paper proposes applying TAGI (Goulet et al, 2019), a framework for approximate Bayesian inference in deep neural networks, to deep q-learning. Using assumptions of Gaussianity on various distributions, TAGI infers both the neural network parameters and unit activations, without needing gradient descent. Because all distributions are assumed to be Gaussian, the means and variances can be updated analytically with linear computational complexity. TAGI is essentially used to infer the correct Q-value function, and the approach is applied to lunar lander, cartpole, and several Atari games.","The paper describes how to adapt the Q-learning algorithm so that it is compatible with Bayesian deep learning, as opposed to the standard (semi-)gradient-based deep learning implementation. It explains how to propagate uncertainties about network parameters through the neural network computations and ultimately to the q-values, which enables action selection using Thompson sampling. Finally, it presents empirical results on CartPole, Lunar Lander, and five Atari games.",0.22641509433962265,0.32075471698113206,0.20754716981132076,0.2777777777777778,0.18518518518518517,0.20454545454545456,0.2222222222222222,0.19318181818181818,0.15714285714285714,0.17045454545454544,0.14285714285714285,0.2571428571428571,0.22429906542056074,0.24113475177304963,0.1788617886178862,0.21126760563380284,0.16129032258064516,0.22784810126582275
620,SP:882fad532bed7d2abd0c73f42b27d9f903e35e3a,"This paper addresses the problem of doing unsupervised learning of visual representations, including clustering these representations to infer the existence of new categories. The paper focuses on more ""un-curated"" settings in which the data comes from samples of an environment that a real agent is passing through. This gives object distributions that are very different from typical, curated data sets, like ImageNet without labels. The authors compare their method with other popular self-supervised learning techniques like SimCLR.","This work studies an online version of self-supervised representation learning. It uses Gaussian mixture  of constant isotropic variance as a prototype memory as well as a uniform distribution to handle new unknown classes. The overall distribution evolves using an online EM algorithm that supports adding and removing prototypes. Using this memory, representation learning follows standard unsupervised losses, primalrily a distillation loss over predicted prototype assignment probabilities.","The authors propose a method for unsupervised learning of instance-based clustering which is more robust to data imbalance and non-iid distributions than existing approaches, such as SwAW. In particular, they propose an Expectation Maximization algorithm that operates in temporal episodes (supposed to correspond to an agent moving through an environment).  At every step an observation (image with an overplayed instance segmentation mask) is first encoded with a CNN and the resulting vector is assigned to one of the existing clusters based on the distance in the feature space, or a new cluster is created (E-step). This is formulated in a probabilistic framework. The cluster prototypes are then updated accordingly (M-step). The loss, computed after a sequence of such steps, consists of a standard contrastive loss (encourages assigning similar images to the same clusters) + entropy (encouraging confident assignments) + a term encouraging the creation of new clusters.   The method is mainly evaluated on RoamingRooms - a dataset with an agent moving through indoor environments with instance labels assigned to objects. The task is then to cluster different view points of the same object together. The main challenge is that the distribution of examples is non-iid, since in every episode mostly the same objects are seen. Additional evaluation on online variants of Omniglot and ImageNet is also provided.  When compared to offline and online contrastive learning algorithms the proposed approach demonstrates stronger performance especially with low batch size and non-iid distribution of examples.","This paper claims that in real world the data distribution is nonstationary, which is different from the current standard machine learning formulation. To solve this problem, the authors design an online unsupervised learning algorithm with Gaussian mixture model and EM algorithm. The experiment shows that the method can learn the online stream of visual input data.",0.13924050632911392,0.27848101265822783,0.13924050632911392,0.1791044776119403,0.13432835820895522,0.06938775510204082,0.16417910447761194,0.08979591836734693,0.19642857142857142,0.04897959183673469,0.16071428571428573,0.30357142857142855,0.1506849315068493,0.13580246913580246,0.16296296296296295,0.07692307692307691,0.14634146341463414,0.11295681063122924
621,SP:88314328bbea1e182ed1afc69195730423ab4dcf,"This paper introduces the ""Latent Causal Invariance Model"" (LaCIM), a structural variational autoencoder-like approach for learning to isolate causal factors from ""spurious"" ones in prediction to improve generalization to new environments. The authors assume a latent generative structure for the data, with separate latent constructs for causal and contextual factors. Under strong parametric assumptions and requirements on the number of source environments the authors prove identifiability of the causal factors. Experiments on imaging datasets (2 benchmarks and a small medical dataset) show LaCIM outperforms some existing domain adaptation baselines as well as Invariant Risk Minimization (IRM). Further, qualitative assessment of images generated by ablating the causal and contextual factors show disentanglement of these factors.","This work proposes an identifiable causal model with latent variables to model spurious correlations between labels and irrelevant context (such as background or light setting) in order to construct a classifier which is robust to certain distribution shifts. The authors show under which conditions the irrelevant context and important features can be disentangled from only input observations and labels. They propose a VAE-like approach to learn the model which they use to come up with an invariant classifier. The identifiability of the model is assessed on synthetic data and the robustness to dataset shift is evaluated on multiple real world data sets, drawing a favorable picture of their contribution. ","In this paper, the authors propose a method to estimate the invariant causal mechanism. By modeling the process as the causal graph in Fig.1 (c), the authors first provide the condition when disentangling causal factors is feasible. Based on the theoretical result, they present the method to infer causal factor and non-causal factor with X based on VAE.","To deal with the issue of distributional shifts between training and testing domains, the authors propose Latent Causal Invariance Models (LaCIM) consisting of both causal latent factors and non-causal latent factors and the extent of the correlations between them is governed by a domain variable. They theoretically show the identifiability of the causal latent factors and the ground-truth predicting mechanism in the proposed LaCIM. Based on the identifiability, they learn the model by reformulating VAE and then verify it on various real-world data. ",0.19130434782608696,0.1565217391304348,0.19130434782608696,0.13636363636363635,0.17272727272727273,0.2833333333333333,0.2,0.3,0.2558139534883721,0.25,0.22093023255813954,0.19767441860465115,0.19555555555555557,0.2057142857142857,0.21890547263681592,0.1764705882352941,0.19387755102040816,0.23287671232876708
622,SP:886b8083595d1d1e3290d9afb0f6704df6061682,"This paper develops an approach for option discovery using meta-gradient descent to optimize a set of option networks. These option networks are then used to define the reward, and termination functions. The option policies are learned through normal RL in a separate option-network which is used to define options for a network which learns the behaviour policy using options and the base actions. They restrict their setting to that of multi-task reinforcement learning, where they sample from distributions of tasks with shared underlying regularities. They test their approach on several environments.","The authors introduce MODAC, a method that relies on meta-gradients for option discovery in hierarchical multi-task settings. The authors extend the options framework to the multi-task setting and use meta-gradients learning to discover task-independent options that can be successfully re-used across tasks. Their architecture is based on the standard hierarchical agent from Sutton et al. (1999) where a manager, that is conditioned by the current state and the task to achieve, can choose between a set of primitive actions and temporally extended options. Adding primitive actions to the action space of the manager adds flexibility and expressiveness and matches the original hierarchical framework. MODAC also implements option-policies, option-rewards and option-terminations. The training corresponds to two nested loops. An inner loop updates the manager to maximize the sum of discounted environment rewards and the option policies to maximize the discounted sum of option rewards. An outer loop evaluates the updated manager and option-policies and updates the option-rewards and option-terminations by back-propagating through the inner loop updates. This training setting enables MODAC option policies to optimize for different objectives that are parameterized by task-independent option-reward and termination that are discovered to be directly useful across many tasks. The authors evaluate MODAC in a grid world environment and in  DeepMind lab navigation tasks. They illustrate that MODAC does not only call primitive actions but also makes use of the options and that the discovered options are diverse and useful for the task at hand. They also compare MODAC to two baselines and show that MODAC outperforms them either in terms of asymptotic performance or sample efficiency or both.",The authors tackle the problem of learning options that would be useful across a range of similar tasks (for example similar mazes with different difficulties). The authors use a meta-gradient approach with appropriately defined parameters and meta-parameters. The paper contains quantitative and qualitative experiments on variants of the FourRooms task and on the more complicated DeepMind Lab (as well as experiments on Atari in the appendix),"Authors propose a meta-learning algorithm for learning temporarily extended actions within the options framework. In addition to the commonly used option modules defined in the original options framework (high-level policy, terminations, sub-policies) the algorithm also uses an aditional learned option-rewards module ($\{r^{o^i}\}$) to reward sub-policies when they perform actions. The algorithm is designed to extract useful temporarily extended actions from multiple environments by using meta-gradients (backpropagating through gradient). Meta-gradients are taken wrt. parameters of termination functions and option-rewards whereas sub-policies and high-level policy are updated in the inner update. The intuition behind using option-rewards and terminations as meta-parameters is that they should define task-agnostic sub-goals.   Authors evaluate the performance of this new algorithm on one simple and one complex navigation tasks. The main performance metric considered is performance on a new unseen task (after pre-training on many tasks), although authors do show additional visualizations of learned options. Comparison is made with two additional algorithms for learning options  MLSH (designed for multi-task learning) and Option Critic (originally designed for single-task) and a single-task Actor-Critic baseline. In experiments, authors show that proposed algorithm outperforms all three baselines when options are transferred to previously unseen tasks. This suggests that proposed algorithm discovers options with better temporal structure.",0.3191489361702128,0.10638297872340426,0.2553191489361702,0.07857142857142857,0.16785714285714284,0.25,0.10714285714285714,0.14705882352941177,0.10666666666666667,0.3235294117647059,0.2088888888888889,0.07555555555555556,0.160427807486631,0.12345679012345678,0.15047021943573669,0.1264367816091954,0.18613861386138614,0.11604095563139932
623,SP:88a470a70070768cb7cfb04c228db0af63654763,"The authors propose a new post-processing group-aware threshold search procedure for fair classification. The procedure is efficient, amenable to different types of underlying classification models, and is empirically shown to be working with multiple fairness criteria. The authors also derive  Pareto frontier of their model for the fairness-accuracy trade-off. Lastly, the authors conclude with experiments on four datasets showing the efficacy of their method.",This paper proposes a threshold modification method to post-process the trained classifiers. The threshold learning is formulated as an optimization task and an efficient technique is proposed to learn the thresholds for groups. The proposed adaptive threshold learning is able to work with multiple fairness constraints.,"This paper introduces a method for finding fair and optimal thresholds for binary classification. It considers the mean squared error as the objective function. Instead of solving a constrained optimization problem, they add a  fairness loss to the objective function.   Then using Taylor approximation,  the optimization problem is reduced to an optimization function with a quadratic utility. The proposed method has low complexity ($\mathcal{O}(n+T)$). Moreover, it improves the fairness accuracy tradeoff as compared to existing works. ","The paper proposes a post-processing approach to achieve different notions of fairness. To do this, the paper first approximates the probability distribution of the output model. Group-specific confusion matrices can be derived by thresholding this distribution. The entries of such a matrix can be used to compute accuracy and various fairness metrics. The authors experimentally evaluate their approach on several datasets.",0.16176470588235295,0.19117647058823528,0.25,0.2978723404255319,0.2127659574468085,0.12658227848101267,0.23404255319148937,0.16455696202531644,0.2698412698412698,0.17721518987341772,0.15873015873015872,0.15873015873015872,0.19130434782608696,0.17687074829931973,0.25954198473282447,0.2222222222222222,0.18181818181818182,0.14084507042253522
624,SP:89e7a0b793893a9866f9e79bea2a035f3e0eeacf,Update at the end of the discussion session:  Discussion among reviewers and with the authors made me increase my score by a total of 2 points.   The paper presents a graph-neural network based approach to detecting objects in vector graphics. The paper works directly on the vector graphics representation by first normalizing the vector graphics to bezier splines and then building a graph representation of the vector graphics. Object detection happens directly on this graph representation. Experiments show a comparison to standard computer vision models. ,"The paper addresses the problem of object recognition, i.e., object localization and classification, in Vector Graphics. The input is a text document that describes the vector graphics, and the output is a bounding box corresponding to a localized object.  The proposed approach is supervised.  The main idea is to represent the input textual document describing the Vector Graphic (VG) object using a unified representation for different primitives described in the text document (such as a Bezier curve). The VG objects are represented using a Graph, where the nodes correspond to the start and end points of the Bezier curves.  A dual-stream Graph Neural Network (GNN) is proposed to aggregate feature representation for the input VG object, which is in-turn used for classification and localization.  The method is evaluated using Average Precision metric.   Two kinds of publicly available VG datasets are used: Floorplan and Diagrams. Comparison against object recognition techniques that employ a CNN has been done, and the efficiency of dual-stream GNN architecture is demonstrated.","In this paper, the authors proposed one framework for the object detection in the vector graphics instead of raster graphics (i.e., pixel images), where the main contribution maybe is to employ the graph neural networks in this area. The extensive experiments on two datasets show the effectiveness of this method, especially comparing with the methods on raster graphics. However, I do not think this comparison is fair, since the proposed methods employ more information contained in the vector graphics while other methods only use pixel information.","The paper present a method to perform object detection for vector graphics images directly from their description as order three Bézier curves. It proceeds as follows: 1. it convert a vector graphics object as a set of order 3 Bézier cuvres 2. it looks at this set of curves as a graph 3. it process this graph using a GNN 4. it aggregates the features of the GNN on proposal regions, generated form the vector description of the image and classify them.  The method is evaluated on two public datasets and compared to standard CNN baselines on the rendered images.",0.2558139534883721,0.18604651162790697,0.19767441860465115,0.1242603550295858,0.17159763313609466,0.1724137931034483,0.1301775147928994,0.1839080459770115,0.16666666666666666,0.2413793103448276,0.28431372549019607,0.14705882352941177,0.17254901960784313,0.18497109826589594,0.18085106382978722,0.1640625,0.21402214022140217,0.15873015873015872
625,SP:8a1b43649ca42a8b638c3c67f81de0e5cccb68de,"This paper empirically investigates the scaling and transfer of Transformer language model architectures (i.e., a unified LM architecture rather than encoder-decoder models) for machine translation. The paper examines choices of architectural designs wrt data scales and model sizes and evaluates by performance (BLEU) on bilingual, multilingual and zero-shot translation, as well as the power-law (Kaplan+20) of neural language models.  Experiments and analyses are extensive and findings are interesting.   ","This paper provides an in-depth examination of language model based translation models and the impact of various parameters in the learning process.   They pick the ""classic"" encoder-decoder based translation model as the baseline and propose several variations of language model based translation models to compare. This includes the two main categories of unidirectional and bidirectional language modelling of the source language. On top of that, they look into various setups such as which layers to use to pass the source encodings to the target generation phase of the translation, whether to share parameters between source and target, and changing the depth (number of transformer layers) and the width (feed forward dimension).  They evaluate on in-house and publicly available datasets and investigate how different setups compare in the supervised translation task, zero-shot learning, low-resource vs. high-resource language pairs, and bilingual to multilingual translation.  ","This paper gives a quite thorough comparison between language-model-based (LM) and encoder-decoder-based (EncDec) architectures for neural machine translation (NMT). The investigated LMs are varied: different masking strategies, different origins of source features, and so on. The authors conducted experiments on representative bilingual and multilingual benchmarks, summarizing some findings based on the experimental results. ","There are more and more work using language model to conduct translation between languages. This paper examines scaling and transferring laws when implementing translation with LM architecture through extensive experiments. Some of these findings are straightforward and intuitive but not experimentally verified before, and some are not so obvious. The latter is especially important for the community. These findings include: LMs are generally inferior to the EncDec architecture in terms of MT performance; it does not have the advantage of speed either; while on the zero-shot transferring aspect, LMs often performs better than EncDec models; different LM architectures have different scaling properties, among which architecture differences usually have a significant impact on the performance of small-scale models, but the performance gap narrows as the number of parameters increases, and so on. We can find more in the paper. ",0.2465753424657534,0.2465753424657534,0.2328767123287671,0.12162162162162163,0.1554054054054054,0.2631578947368421,0.12162162162162163,0.3157894736842105,0.12142857142857143,0.3157894736842105,0.16428571428571428,0.10714285714285714,0.16289592760181,0.27692307692307694,0.1596244131455399,0.175609756097561,0.1597222222222222,0.15228426395939085
626,SP:8a1d1af883e7a2e92f8889475970d2b75cefb57d,"The authors provide an extensive characterization of calibration and consistency for surrogates to the l2 robust 0-1 loss, along the lines of what Bartlett et al. and Steinwart provided for the standard 0-1 loss.  They demonstrate that convex losses and supremum-based convex losses are not calibrated, and further that calibration is in general not sufficient to guarantee consistency.  This corrects a previous incorrect assertion made by Bao et al. in a COLT 2020 paper.  They provide experiments which corroborate their results. ","This paper focuses on the H-calibration and H-consistency of loss functions in terms of adversarial robustness. The authors claim that the reason for the difficulty in achieving the adversarial robustness is related to the current convex surrogate losses. Theoretically, they showed the limitation of H-calibration of convex loss functions and surrogate loss functions. Furthermore, they rejected the result from previous studies which provides the relationship between H-calibration and H-consistency, and found a condition that the relationship working correctly. They proposed a non-convex loss function to satisfy H-calibration and experimentally showed the advantages of the proposed loss.",This paper provides a detailed analysis on the H-calibration and H-consistency of adversarial surrogate losses.   It shows that some common used losses for adversarial training are not H-calibrated.    Some theoretical results on the characterization of H-calibration are established.  The paper also proves that some surrogate losses are indeed H-calibrated.  Some additional theoretical results on H-consistency are established too.  ,"This paper provides an extensive analysis of the satisfiability of H-calibration and H-consistency for surrogate losses in binary classification, where the evaluation metric is not the standard 0-1 loss but the adversarial 0-1 loss. The adversarial 0-1 loss is a pointwise loss defined on an input x by calculating the supremum of the 0-1 loss value of a point that can perturb around x within a ball of a pre-specified radius. This paper reveals a negative result showing that many surrogate losses are not H-consistent, especially the convex losses.  This paper also reveals a positive result of a certain class of non-convex losses that can be H-consistent under some assumptions. Synthetic experiments were also provided to justify the relevance of theoretical findings.",0.19047619047619047,0.15476190476190477,0.2976190476190476,0.21359223300970873,0.27184466019417475,0.390625,0.1553398058252427,0.203125,0.1893939393939394,0.34375,0.21212121212121213,0.1893939393939394,0.1711229946524064,0.17567567567567566,0.23148148148148148,0.26347305389221554,0.23829787234042552,0.2551020408163265
627,SP:8a4577858cd5a05d51046c6edf8cefe8f95d0e42,"This paper considers a denoising setup for a linear model. Compared with the traditional supervised learning setup where the noise is added to the output, the denoising setup has noise in the input. The authors provide an asymptotically exact formula for the generalization error for rank 1 data and an approximation for higher rank data. Both the theoretical and numerical results validate that the generalization error has a peak when the number of samples is equal to the number of features, which is consistent with existing literature about the double descent phenomenon. Authors also derive a formula for the amount of noise in the training data for the best generalization performance, which leads to a novel conclusion that the optimal noise amount in the training data is not always equal to the noise level in the test data.","This paper identifies and studies a double descent phenomenon (wrt the training data size) for a denoising problem, where the goal is to recover a low-rank ground truth given a corrupted version of it (with added full-rank noise). The paper considers the linear regression estimator, and gives the exact formula for the test risk in the asymptotic regime for the rank-1 case; for a general rank r, the paper also gives a formula, but it's not entirely rigorous. Furthermore, the paper also studies the role of the training and testing SNRs.","The paper investigates a new double-descent in denoising task for linear predictors. As the number of samples increases the denoising loss exhibits a peak in the case of linear predictions, but the peaks are less pronounced in the case of neural networks, except for the case with rank 1 data. The authors calculate an asymptotical formula for their special rank-1 data model in theorem 1 which proves the explosion at (c=1). For the arbitrary rank data, the authors present an approximate formula too. ","In this paper, the authors study overparameterization of autoencoders in terms of varying sample size. They derive an analytic expression for the bias-variance decomposition of the loss for a bottleneck of size 1. Furthermore, they approximate the bias-variance decomposition for larger bottleneck sizes. This is done with all linear networks. They experimentally show double descent in nonlinear networks as well.",0.21014492753623187,0.18840579710144928,0.11594202898550725,0.23157894736842105,0.12631578947368421,0.1511627906976744,0.30526315789473685,0.3023255813953488,0.25806451612903225,0.2558139534883721,0.1935483870967742,0.20967741935483872,0.24892703862660942,0.23214285714285712,0.15999999999999998,0.2430939226519337,0.15286624203821655,0.17567567567567566
628,SP:8a71dd181c58014caafd6157493f27c1427c9a57,"This paper introduces ""CausalBALD"", a set of acquisition functions for use in Bayesian Active Learning when trying to estimate the Conditional Average Treatment Effect (""CATE"") associated with an intervention, $\tau(\mathbf{x}) = \mathbb{E}[Y^{1}-Y^{0}|\mathbf{x}]$.  The authors introduce multiple information-theoretic acquisition functions, and then show the improved performance (relative to random sampling of examples) of their acquisition functions for estimating the true treatment effects present in three simulated/semi-synthetic datasets.  Through a toy problem (Figures 2/3), they also provide some intuition for why their acquisition functions outperform random sampling.","Update (12th Aug 2021) - increased score to reflect that my comments were satisfactorily addressed by the authors in their rebuttal.  ----------- The authors propose new set of acquisition functions, dubbed Causal-BALD, based on Bayesian Active Learning by Disagreement (BALD) that consider overlap and information gain as a decision criterion for iterative improvement of a counterfactual estimator in settings in which direct treatment assignment by an algorithm is unethical or not feasible. The authors performed a clean evaluation in various settings and with multiple underlying models to show that their proposed approach outperforms the previous state-of-the-art.","This paper proposed a new acquisition function for applying active learning to observation data consisting of covariates, treatments and outcomes. The motivation of developing a new acquisition function is the existing ones could either query too many samples close to the distribution mode or query samples outside the overlapping region of the treatment and control group. Therefore, the key contribution of this work is to propose a acquisition function that 1) query samples in the overlapping region of case and control; and 2) reduce uncertainity of the model parameters.  The proposed function is built on the existing Bayesian active learning by disagreement (BALD).   First, \tauBALD aims to maximize the conditional mutual information between CATE and model parameters \Omega. However, since the control outcome and treatment outcome cannot be observed simultaneously, leading to irreducible uncertainity.   Second, to overcome this drawback, \muBALD was considered by maximizing the conditional mutual information between outcome Y^t and the model parameters \Omega. The proposed Causal-BALD improved \muBALD by considering both model uncertainty reduction and overlapping. Specifically, \mu\piBALD directly multiplies the propensity acquisition function and \muBALD.   Third, \rhoBALD can also handle overlap by maximizing the mutual information between outcome Y^t and the CATE computed from model parameter \Omega. \rhoBALD can also be combined to \muBALD to form \mu\rhoBALD, which can handle both model uncertainity reduction and overlap.  Experimental results on synthetic dataset and semi-synthetic data showed the proposed \mu\rhoBALD outperforms existing acquisition function for the application active learning to individual treatment effect estimation.",The paper proposes several acquisition functions for performing active learning in a causal setting. The paper explores the limitations of more naive acquisition functions and motivates its own proposals. The performance of the proposed acquisition functions is empirically compared to that of more naive acquisition functions. ,0.16494845360824742,0.23711340206185566,0.18556701030927836,0.21428571428571427,0.12244897959183673,0.06719367588932806,0.16326530612244897,0.09090909090909091,0.391304347826087,0.08300395256916997,0.2608695652173913,0.3695652173913043,0.1641025641025641,0.13142857142857145,0.2517482517482518,0.11965811965811968,0.16666666666666666,0.11371237458193979
629,SP:8a8aa5f245c2fb82beddb19c82dddb8d67f66f8a,"In this paper, the authors study a two-player zero-sum convex-concave game whose inputs are parameterized by nonlinear functions. This results in the overall game being nonconvex-nonconcave in the parameter space. This setting is similar to that observed in GANs where the game is convex-concave in function space of discriminator and generator but is nonconvex-nonconcave in the space of parameters. The authors call such games as Hidden Convex-Concave (HCC) games.","This paper studies a special problem structure in min-max optimization. Specifically, the paper considers the setting when the objective function can be reparametrized into a (strict) convex-concave game, or referred to as a hidden convex-concave game in the paper.   More formally, the considered function is of the form L(F(\theta), G(\phi)), where L is convex-concave. F and G are of separable structure, namely both the max and min player are from a cartesian product of scalar functions from disjoint sets of parameters. With the separable structure, the authors can thoroughly investigate the conditions for stable GDA dynamics and the good initializations for GDA to find the equilibrium of L.   Finally, the authors give some discussion on connecting the structures to GANs and evolutionary game theory/biology. ","In this paper, the authors introduce a class of games called Hidden Convex-Concave where a (stricly) convex-concave potential is composed with smooth maps. On this class of problems, they show that the continuous gradient dynamics converge to (a neighbordhood of) the minimax solutions of the problem. This is an exploratory theoretical paper which aims at better capturing the behaviors that can be observed e.g. in the training of GANs.","This paper extends ""hidden bilinear"" games [1]  to ""hidden convex-concave (HCC)"" games, and studies a special case of HCC games (denoted as HCC* from below) where output of each dimension of a nonlinear function depends on a seperate group of parameters .  For HCC* games, the authors utilize Lyapunov-type arguments and show that GDA dynamics stabilizes around the output-space Nash equilibrium (Theorem 3), whose parameter counterparts are named as ""von Neumann solutions"".  With an additional assumption that the game has one-sided strictness property, the authors further show that GDA converges to a ""von Neumann solution"" (Theorem 6), by combining Lasalle's principle and the fact that ""safe initializations"" reside in ROA of such solutions.  Additionally, the authors show that adding a regularization term can accelerate the convergence of GDA towards the perturbed equilibrium in the output space (Theorem 8).  Finally, the authors exemplify GANs and evolutionary games, and argue that such games can be nicely formulated as HCC games.",0.27631578947368424,0.23684210526315788,0.3157894736842105,0.1590909090909091,0.22727272727272727,0.3194444444444444,0.1590909090909091,0.25,0.14814814814814814,0.2916666666666667,0.18518518518518517,0.1419753086419753,0.2019230769230769,0.24324324324324326,0.20168067226890757,0.20588235294117646,0.20408163265306123,0.19658119658119658
630,SP:8b2284723ec02f8b455b945cea6a7574e0f95049,"This paper solves a constrained discrete black-box optimization problem that employs a surrogate model in modeling an unknown objective function. Unlike the formulation of standard Bayesian optimization, it constructs a surrogate model using a piecewise linear neural network. Under the assumption that a randomly-initialized neural network is able to produce an uncertainty for exploration (against exploitation), the proposed method optimizes a surrogate model directly with mixed-integer linear programming. It follows a spirit of Thompson sampling. Finally the authors conduct their method on several experimental circumstances and show the validity of their method.","The paper proposes a method using piecewise-linear neural networks as the surrogate model (and the acquisition function) in a black-box optimization framework. At each step, the acquisition function is optimized by casting the learned neural network (and a set of constraints excluding the already-visited data points) into a mixed integer linear program, which can be then solved using off-the-shelf solver. The proposed method is empirically evaluated on a number of unconstrained and constrained tasks.          ","The authors present NN+MILP, a framework for the optimization of an expensive to evaluate blackbox fuction with a discrete combinatorially constrained domain. The acquisition problem of finding the surrogate minimum is solved to global optimality by solving a MILP formulation of the acquisition problem. The MILP formulation limits the considered neural network surrogate class to networks with piecewise linear activation functions. However, it provides a simple declarative language for integrating the problem-specific constraints.   The experiments cover both the cases of unconstrained and constrained optimization of the acquisition function.  The unconstrained case compares NN+MILP to general purpose algorithms for unconstrained discrete blackbox optimization. It shows that the global optimization often achieves better results than a local search evolution based method for solving the acquisition problem, while also solving the inner loop problem faster. Additionally, it demonstrates that even with the restrictive function class of a single layer neural network, comparable performance to better-suited surrogate hypothesis classes can be achieved, thanks to the global optimization.  In the constrained case artificial subset-equality constraints are used to create a combinatorial domain for the blackbox function. NN+MILP performs similar to NN+ConEvo, a manually adjusted local search method that ensures feasibility of the proposals at every step. Other methods employing random search for inner loop optimization or local search with a different surrogate hypothesis class perform much worse. Finally, a case study for the NAS-Bench-101 benchmark is provided. A novel MILP formulation for cells of a valid architecture design is described, which consists of an MILP formulation for directed acyclic graphs with added null operations to allow DAGs with a reduced number of cells. Despite the generality of NN+MILP it outperforms a strong evolution based baseline.","The paper develops a technique to optimize an unknown, black-box function ""f"" by leveraging a combination of neural networks with mixed-integer linear programming (MILP) methodology. More specifically, authors encode an approximation of ""f"" using a neural network with piecewise-linear activation functions, which is optimized using its associated reformulation as an MILP with no-good cuts. Numerical results evaluate the approach with respect to other baselines and neural network optimization mechanisms. ",0.2,0.2736842105263158,0.16842105263157894,0.3037974683544304,0.20253164556962025,0.07241379310344828,0.24050632911392406,0.0896551724137931,0.2191780821917808,0.08275862068965517,0.2191780821917808,0.2876712328767123,0.21839080459770116,0.13506493506493505,0.19047619047619047,0.13008130081300812,0.2105263157894737,0.11570247933884299
631,SP:8b9abfa00de5708f3577d454081b1e77676e4160,"The paper introduce a new insight to diffusion generative models. Mainly, the paper re-write the forward and backward processes in new formulation which incorporate the SNR value into the diffusion process.   Previous diffusion process like DDPM can be regard as instance of the proposed model. The snr value in the proposed method is learned jointly with the model whereas previous approach use fix snr value.   The paper showed various theoretical results: (i) More steps is better, (ii) for continues time loss the model is invariance to the noise schedule, (iii) the training is impacted the start and end snr values.    The paper introduce to main novelties: (i) learning noise schedule and (ii) insert Fourier features as additional features ","The paper introduces a family of diffusion-based generative models for density estimations on image datasets. Here, the forward diffusion is directly defined by a set of conditional densities of diffused random variables for a given data. In particular, the paper proposes that $t$-th density for a given data follows a normal distribution; its mean and variance will be determined by data and a monotonic function of $t$, called $SNR(t)$. The generative models aim at learning diffusion processes reversed in time by maximizing its ELBO.  The paper first draws connections between the proposed models and previous diffusion-based models. For example, the authors show that the proposed method's discrete-time versions include previous finite-length Markov chain models as special cases. The paper then demonstrates that the ELBOs of the proposed continuous-time models exist. In here, the resulting ELBOs include an integration (wrt time) of a reconstruction loss weighted by the time-derivative of $SNR(t)$. Based on this, the paper shows that the losses of the previous continuous-time diffusion-based models follow the same integration form but with different weighting functions.  Second, the paper presents interesting properties of the proposed method due to $SNR(t)$. In particular, remind that $SNR(t)$ is invertible because of its monotonicity, and the continuous-time loss's integration contains the time-derivative of $SNR(t)$. The paper shows that the integral can be re-written wrt $SNR(t)$'s output by change-of-variable. In this form, the integral only depends on the range of the integration but doesn't depend on the shape of $SNR(t)$ within the range. As a result, acknowledging the invariant of the loss on the $SNR(t)$'s output space, the paper proposes to parameterize the score models conditioning on log-scaled $SNR(t)$ instead of $t$.  Third, the paper proposes to train $SNR(t)$ for the discrete and continuous-time models. For the monotonicity, $SNR(t)$ is defined as a monotonic network wrt $t$. For discrete-time models, the paper proposes to train $SNR(t)$ networks by maximizing the ELBO together with the diffusion models. For continuous-time models, $SNR(t)$ is trained by minimizing the variance of the Monte-Carlo estimation of the continuous-time ELBOs.  Furthermore, the paper introduces various practical techniques to improve training the models. For example, instead of iid sampling of $t$, the authors propose to use a low-discrepancy sampler of $t$ within a mini-batch. The paper also proposes to use the Fourier feature of inputs: feeding the concatenation of inputs and its Fourier feature to diffusion models.  In the experiments, the paper demonstrates that the proposed models achieve SOTA likelihoods on image datasets, such as CIFAR-10 and ImageNet.","The paper proposed an improvement to DPM. The main contribution is summarized below.  - Sec 3: The diffusion and reverse processes in DPM are re-written with the proposed SNR function. It can be seen as a high-level framework of the noise schedule.  - Sec 4: The ELBO (for both discrete and continuous DPMs) is then re-written in terms of SNR. The authors had a few other findings: e.g., a larger T is better, and equivalence between continuous DPMs.  - Sec 5: The SNR function is parameterized by a neural network and targets at minimizing the estimation variance of ELBO.  - Sec 6: Additional features computed from Fourier transformation are added to data. - Sec 7: Experiments on CIFAR and ImageNet show improvements on BPD.   ","The paper tackles density estimation with denoising diffusion-based generative models and achieves very strong, state-of-the-art likelihoods (bounds) on standard image modeling benchmarks. The formulation largely relies on previous denoising diffusion probabilistic models (DDPMs), but uses the exact ELBO objective without dropping weighting coefficients in the mixture of losses, as was done in DDPM. The paper also proposes to learn the noise schedule, either using the ELBO objective or a variance reduction objective, and adds additional Fourier features into the model, which brings a significant benefit. On the theoretical side, the paper demonstrates how a change of variables in the time variable can lead to an equivalence between models trained with different noise schedules and shows how the important aspect of the noise schedule is only its behaviour at its endpoints. ",0.3697478991596639,0.226890756302521,0.2184873949579832,0.09429824561403509,0.08771929824561403,0.17886178861788618,0.09649122807017543,0.21951219512195122,0.19402985074626866,0.34959349593495936,0.29850746268656714,0.16417910447761194,0.15304347826086956,0.2231404958677686,0.20553359683794467,0.14853195164075994,0.13559322033898305,0.17120622568093385
632,SP:8b9db14e9d750bc0dc2b9f4c10e30f4ac4806308,"The paper proposes Confidence-Aware Imitation Learning (CAIL) that learns from sub-optimal demonstrations collected by different policies. The main idea is to train an imitating policy from demonstrations weighted by an estimated confidence scores, while the confidence scores are simultaneously estimated by using the imitating policy and ranked demonstrations. The paper also presents the convergence proofs of the algorithm. Experiments on simulated and real-world environments indicate that CAIL is more effective at learning from sub-optimal demonstrations when compared to existing algorithms.","* This paper proposes a novel imitation learning algorithm, Confidence-Aware Imitation Learning (CAIL), to learn policy from suboptimal data.   * In particular, this proposed algorithm iteratively does two gradient updates:     1. learns a function, \beta, that maps (state, action) to a confidence score using an evaluation dataset (e.g., a limited amount of partial rankings) with an outer loss.     2. learn the imitation learning model parameters \theta by reweighting the full IL dataset (could be suboptimal, without labeled rankings) using \beta, and optimizing for an inner loss.   * The authors proposed a convergence rate of the proposed method as a bi-level optimization problem (Theorem 2).  * The authors conduct a comprehensive evaluation between the proposed algorithm with a set of baseline algorithms.   * Domains: MuJoCo reacher, MuJoCo ant, Franka Panda simulation, UR5e real robot.   * Baselines:     * Standard imitation learning algorithms: GAIL, AIRL.     * Confidence-based methods: 2IWIL, ICGAIL.     * Ranking-based methods: T-REX, D-REX, SSRR.   * Results:     * Reacher: CAIL outperformed other methods in expected return.       * All comparisons (between CAIL and another method) are statistically significant.     * Ant: CAIL outperformed other methods in expected return.       * The p-value between CAIL and the closest baseline method, 2IWIL, is 0.1405.     * Franka Panda simulation: CAIL outperformed other methods in expected return.       * The p-value between CAIL and the closest baseline, 2IWIL, is 0.0974.     * UR5e real robot: CAIL outperformed other methods in expected return.       * All comparisons (between CAIL and another method) are statistically significant.     * Demonstrations with Different Optimality       * CAIL consistently outperforms or performs comparably to other methods with demonstrations at different optimality.       * CAIL performs more stably while the baselines suffer from a performance drop at speciﬁc optimality levels.     * Learning from Only Non-optimal Demonstrations       * Ant: CAIL outperformed other methods in expected return.         * All comparisons (between CAIL and another method) are statistically significant. ","This paper considers the problem of offline imitation learning from demonstrations when these demonstrations may have been generated from a mixture of optimal, suboptimal, and even adversarial (i.e. worse than random) agents. To address this problem, the authors introduce Confidence-Aware Imitation Learning or CAIL, an imitation learning methodology where the policy is optimized jointly with a weight function that dynamically reweighs the ""expert"" demonstrations so that policy more closely imitates the high quality demonstrations. To enable the training of this weight function, CAIL requires, similarly to prior work, some additional supervision in the form of demonstration ranks, namely the quality of a small collection of demonstrations are ranked relative one another. Two theorems show that CAIL converges at a given rate under intuitive assumptions. Finally a collection of experiments show that, for both simulated environments and real-world robotic settings, CAIL outperforms competitive baselines. ","This paper investigates imitation learning under the setting where there are non-interactive state-action demonstrations with varying optimality, and among them a small percentage has trajectory-level ranking annotations. The authors propose an imitation learning framework that can leverage all demonstrations, although some of them are suboptimal or even malicious, and potentially outperform the demonstrators. The idea is to formulate learning a state-action confidence function (which weights demonstrations) as a bi-level optimization problem. This closely ties IRL and ranking matching. The authors prove the convergence rate of the resulting algorithm -- CAIL -- and show empirically both in simulation and in real-word that CAIL outperforms recent algorithms designed for similar settings.  ",0.34523809523809523,0.25,0.2619047619047619,0.09523809523809523,0.10884353741496598,0.1917808219178082,0.09863945578231292,0.14383561643835616,0.19469026548672566,0.1917808219178082,0.2831858407079646,0.24778761061946902,0.15343915343915343,0.18260869565217389,0.2233502538071066,0.12727272727272726,0.15724815724815724,0.2162162162162162
633,SP:8b9df88952388fc17c22b1fcbdac97d59031b872,"This paper focuses on the open-domain question answering task. They use the generate framework which first retrieve relevant documents for the given question and then generate the answer based on the question and the retrieved documents. They model the retrieved documents in the latent space. Previous studies usually consider only a single document as the latent variable. In this paper, they consider to use a set of documents as the latent variables. They then apply an expectation-maximization algorithm to update the reader and the retriever. The reader is optimized to generate the answer based on the question and the retrieved set of documents. The retriever is updated using posterior estimation of the retrieved results given the question and the answer. They conduct experiments on three open domain question answering tasks. The experimental results show that their approach outperforms other baselines by a large margin (2%-3% absolute improvement). They conducted ablation studies to show the performance of the model with different number of retrieved documents. They also compared different retriever initialization  strategies and showed that masked salient spans pretraining is helpful.","This paper makes end-to-end training work for multi-document readers (e.g., FiD style readers). The key idea is an EM algorithm to update the reader and retriever given the top-k selection of previous iteration models. The proposed approach shows good performance on multiple open-domain QA benchmarks.","The paper proposes end-to-end multi-document retriever-reader training for open-domain question answering. The proposed model is based on a pipeline approach where the retriever based on a dual encoder retrieves a set of documents from a large text corpus and a reader processes them and generates the answer. Different from previous approaches which independently train the retriever and the reader (Karpukhin et al, Izacard & Gave, and others), the paper jointly trains them using the EM algorithm. It is also different from previous work with joint training (Lee et al, Guu et al, and others) in that the generation of the answer is conditioned on multiple documents, not just a single one. In particular, this difference makes joint training more difficult, so the paper proposes to use the EM algorithm to iteratively update the parameters of the retriever and the reader. The paper also came up with clever initialization tricks and asynchronous refreshes of the MIPS index, which leads to successful training of the proposed model. The proposed model is evaluated on widely-used open-domain QA benchmarks (NQ, TriviaQA, WebQ). The proposed model outperforms a set of other, recently-proposed models by a large margin, which is impressive given that the baselines are highly competitive.","This paper proposes EMDR2, an end-to-end differentiable training method of multi-document reader and retriever for open-domain question answering. Based on expectation-maximization algorithm, EMDR2 iteratively estimates the set of relevant documents for a given question and uses the estimated set to update the parameters of retriever and reader. EMDR2 achieves new SOTA performance for models of comparable size on three benchmark OpenQA datasets. Contributions: -	An **EM-based** end-to-end training method of **multi-document** reader and retriever is proposed, which achieves SOTA performance on three benchmark OpenQA datasets. -	The observation that EMDR2 implementations with different retriever initializations achieve similar final retrieval accuracies may be instructive. ",0.10382513661202186,0.273224043715847,0.16939890710382513,0.47058823529411764,0.35294117647058826,0.18181818181818182,0.37254901960784315,0.23923444976076555,0.2818181818181818,0.11483253588516747,0.16363636363636364,0.34545454545454546,0.1623931623931624,0.25510204081632654,0.2116040955631399,0.1846153846153846,0.2236024844720497,0.23824451410658307
634,SP:8ba58693c1bba852dd6be6dd2b69ab148d69051d,"This paper studies the task of stylized dialogue generation, where the goal is to generate dialogue responses in a particular target style (formal / informal / Shakespeare-like). The authors operate in a setting where they do not have access to any dialogue training data in the target style, but instead have access to dialogue data in a complementary style, along with parallel data to transfer between the two styles. For instance, the authors study dialogue generation in formal style, and assume access to dialogue data in informal style, parallel data between formal / informal sentences, and a corpus of unpaired formal sentences.  The authors model this task as a three-way dual learning problem, a technique which has proven useful for machine translation [1]. They jointly training their model to perform 1) style transfer in both directions; 2) dialogue / inverse-dialogue generation to the complementary style; 3) dialogue / inverse-dialogue generation to the target style. 1 / 2 act as auxiliary tasks to aid (3) via shared models. Since no parallel data is available for (3), the authors perform sampling (using current version of model for 1 / 2) to create pseudo parallel data for (3). Since sampling can be noisy, the authors additionally use discriminators to weight samples, which judge whether the samples are real/fake (akin to adversarial training).  The authors evaluate their approach on formal dialogue generation and Shakespearan dialogue generation. They compare their approach to 5-6 baselines, along with human upperbounds. Both automatic and human evaluation is conducted. The authors additionally conduct ablation studies, analyze their discriminator, and show the effect of the size of parallel data.  [1] - https://arxiv.org/abs/1611.00179","This paper focuses on proposing multi-pass dual learning (MPDL) to build mappings among dialogue context and responses in two styles. The proposed model is reasonable and is the extension of the classical dual learning method, which usually maps between two domains. The idea is reasonable and novel, and the experiments show the improved performance in terms of both automatic and human evaluation for both datasets.  A main concern is about the paired style transfer data $D_{tra}$, which limits the flexibility of usage. Also, an intuitive baseline may be needed for fair comparison. The further suggestion is to remove the ""paired style transfer data"" constraint, considering that the proposed MPDL may still work.","The authors propose a method for learning stylized dialogue response generation: given a context, generate a response conditioned on a particular style domain (e.g. modern vs. shakespearean english). The method, Multi-Pass Dual Learning (MPDL), leverages large corpora of non-parallel data in tandem. Specifically, the framework involves several forward and backward models between the context and two styles (one which has labeled data from the context to the style, and one that does not); the models produce intermediate data that is used to train the desired style generation model. To make sure that the intermediate data is being used appropriately, the authors introduce two discriminators that evaluate the quality of the data and determine whether it falls in the respective style/domain. The scores of these discriminators weight the importance of the loss computed on that data to ensure that higher-quality pseudo tuples are more impactful in the learning process. The authors evaluate the framework on two style-transfer datasets, and show that the method outperforms 5 baselines, achieving state-of-the-art results in automated metrics and outperforming nearly all models in human evaluations. A case study and ablations are presented as well that highlight and support the authors’ design decisions.","This paper studies the stylized dialog generation problem, in which the problem setup contains contexts C, responses with the original style S0, and responses with the desired style S1. The contributions of this paper includes: - A novel multiple-pass dual learning framework that explores the interaction between the pairwise relationship among C, S0 and S1. - The authors propose an additional discrimination to determine the text quality generated by the backward models, which has been shown to be helpful for the task. - Evaluations are compared to state-of-the-arts on two large text style transfer datasets, and show good amounts of improvements on both automatic and manual metrics. - Ablation studies further demonstrated the necessity of each component, especially convinced the capacity of discriminator (which is pretty clear from Figure 3).  - The authors also release their code and data.",0.10909090909090909,0.1781818181818182,0.14909090909090908,0.2719298245614035,0.21052631578947367,0.17073170731707318,0.2631578947368421,0.23902439024390243,0.2971014492753623,0.15121951219512195,0.17391304347826086,0.2536231884057971,0.15424164524421594,0.20416666666666666,0.19854721549636803,0.1943573667711599,0.1904761904761905,0.20408163265306123
635,SP:8c4ea178f50c633301495454f47e0947f35376c2,"Learning-augmented algorithms have been recently proposed as a way to utilize machine-learned predictions in online algorithm design. This paper considers the related problem of how to learn from past data so that online algorithms can obtain improved performance. Specifically, the paper introduces a general online search problem and design online algorithms for it, both without using predictions and using them. The online search framework is fairly general and encompasses well-studied problems such as generalized ski-rental, load balancing and bin packing. Given a single real-valued predicted parameter, the authors design an algorithm that is $(1+\epsilon)$-consistent and $O(1/\epsilon)$ robust.   Secondly (and more interestingly), the paper designs a learning algorithm to efficiently learn such a parameter given sample access to a data distribution. The authors show that utilizing the structure of the online algorithm, one can define an asymmetric loss function such that the empirical minimizer of this loss function serves as a good prediction for the online algorithm. ","This paper focuses on using regression-based ML-predictions in online search problems. In particular, they construct a new loss function that incorporates the asymmetric behavior of the competitive ratio to generate these ML predictions. They show that their proposed algorithm has nearly tight sample complexity bounds under the standard model (when a perfect predition is available in $\mathcal{F}$), and can also achieve good sample complexity guarantees in the agnostic setting.","The paper considers an ""online search problem"", which includes ski rental and variants as special cases and is also applicable to a classical scheduling problem and online bin packing, and proposes an approach for solving these problems in a learning-augmented setting. The online search problem is very generic: An input sequence is known *offline*, but the actual online input is a prefix of this sequence of unknown length. The goal is to minimize some cost. Some assumptions need to be satisfied, e.g. if A and B are solutions to two subsequences then the combined solution of A and B is feasible for the concatenation of the two subsequences (and the cost of the combined solution is at most the sum of the individual costs). Essentially, this is an abstract version of a generalization of ski rental. Although scheduling and bin packing don't have the property of the online input being a prefix of a known offline input, they can be reduced to the online search problem at the loss of a 1.5 factor in the competitive ratio (the classical competitive ratios for the scheduling and bin packing problems are roughly 1.9 and 1.6 respectively, so a bit higher than those 1.5).  The paper first gives a 4-competitive algorithm for the online search problem (using a standard doubling technique) and a prediction-augmented algorithm with consistency 1+eps and robustness 5+5/eps for any eps (which is pareto-optimal up to a constant factor in the robustness). Here, the prediction is the optimal cost for the online input (or equivalently, the length of the online input). The competitive ratio as a function of the prediction error has an unusual asymmetric shape, which motivates the paper's main contribution:  The main contribution is a regression approach to generate this prediction: It is assumed that for any instance, some feature vector x can be observed at the start, and if z is the optimal value of the instance, then the pair (x,z) comes from some (unknown) probability distribution. Since z is needed as the prediction for the learning-augmented algorithm, the goal of the regression approach is to learn a function f that maps x to z. However, the loss function used for learning is not simply an l1 error or similar, but rather a carefully crafted loss function that serves as a proxy of the competitive ratio as a function the true optimal value z and the predicted value f(x). Such a non-standard loss function is needed because of the asymmetric nature of the competitive ratio as a function of the prediction error. Upper and lower bounds on the sample complexity are given so that the resulting algorithm has competitive ratio close to 1+eps with high probability, where epsilon (roughly speaking) is the loss of the best function f from the considered function class.","This paper studies learning augmented algorithms from the perspective of regression.  In particular, this paper studies both how to construct predictions (via regression) and how to use them online for the online search framework.  The online search framework captures problems such as ski rental, online scheduling on identical machines, and online bin packing.  The first main result of this paper consists of tight bounds (up to constants) for robustness and consistency for the online search problem with predictions of the input length.  Next, the paper studies how to predict the value of the optimal solution using regression under the assumption that each instance of the problem has a feature vector x associated with it.  To do this, a novel loss function is proposed that takes into account the competitive ratio of the algorithm utilizing the predicted values.  Nearly tight sample complexity bounds are given for the regression problem. ",0.10909090909090909,0.30303030303030304,0.24242424242424243,0.3333333333333333,0.2916666666666667,0.12783505154639174,0.25,0.10309278350515463,0.2702702702702703,0.049484536082474224,0.14189189189189189,0.4189189189189189,0.1518987341772152,0.15384615384615383,0.25559105431309903,0.08617594254937162,0.19090909090909092,0.19589257503949448
636,SP:8c562c0cd522174d61f24d9a05416b8d6f166ffd,"This paper proposes a method for causal discovery by aggregating data from different data sources into clusters. Those clusters are similar with respect to the underlying graphical structure that generates the data. Based on these clusters, the authors an efficient algorithm in terms of number of interventions required to estimate the underlying graph.","The main goal of the paper is to perform Collaborative Causal Discovery. The main contributions of the paper are the following * Define a new framework for collaborative discovery. The authors consider the case of several entities, each being associated to a DAG containing both observed and latent variables. The idea is to recover the so-called Maximal Ancestral Graph of each entities under a clustering assumption, stating that the MAG are more or less similar inside a cluster, and different when considering different clusters clusters * Under this clustering assumption, is provided an algorithm allowing to recover the MAG of each entity with a minimal number of interventions. The algorithm is a two step procedure, performing first a clustering of the entities and thereafter estimating the MAG of each entity * Numerical experiments complete the paper * In Appendix,  details about MAG are given as well as proofs on the minimality of the numbers of interventions to recover the causal graphs ","The paper proposes an approach to learn multiple causal graphs, in the presence of latents, collaboratively and simultaneously by means of clustering and by performing atomic interventions. First, the work proposes the property $(\alpha,\beta)$-clustering where graphs in different clusters are ""different"" by a factor of $\alpha$ at least, while graphs within the same cluster are ""different"" by a factor of $\beta$ at most. Using this property, an approximate algorithm is proposed to (1) learn the clusters such that the $(\alpha,\beta)$-clustering property is satisfied, and (2) learn one dominant graph in each cluster assuming the clusters are more homogeneous. Next, a special case of the clustering property is considered where all the graphs within the same cluster are identical, i.e. $\beta=0$, and a learning algorithm is proposed with a lower bound on the number of interventions required to learn the graphs than the general case. Finally, the proposed algorithms are evaluated using data generated from synthetic and real causal graphs.","The paper introduces an algorithm for learning causal graphs for multiple entities which share some underlying similarities with minimum atomic interventions. The goal is different from the well-studied problem of learning a causal graph from multiple environments in which the data distribution could differ and the aim is to recover a single global graph.  The problem is of learning independent causal graphs is posed from the perspective of finding clusters among the entities where entities in a cluster are closer in some distance metric in comparison to entities in other clusters. This property is termed as $(\alpha,\beta)$ clustering. The task is to minimize the number of interventions needed to recover the causal graphs while distributing the interventions across the clusters to minimize the total intervention cost. A lower bound of $\Omega(\frac{1}{\alpha})$ interventions is shown to be necessary for recovering the clusters as well as the minimum number of atomic interventions needed.  Experiments are performed on multiple datasets with a comparison to the standard FCI algorithm. Results indicate an overall improvement in the metrics for recovering the true causal graph.",0.32075471698113206,0.37735849056603776,0.3584905660377358,0.22784810126582278,0.25949367088607594,0.24242424242424243,0.10759493670886076,0.12121212121212122,0.10326086956521739,0.21818181818181817,0.22282608695652173,0.21739130434782608,0.16113744075829384,0.1834862385321101,0.16033755274261605,0.22291021671826625,0.23976608187134502,0.22922636103151864
637,SP:8cbb679f98e54d7f5c94094d1eb2894deb2fe820,"The authors study the problem of allocating a single indivisible item to a group of agents, without money. Motivated by kidney exchange, the item can only have two qualities: either good or bad. Every agent (as well as the mechanism designer) agree that the prior probability of the item good is $\mu = Pr[\text{item is good}]$. However, each agent i has their own private signal s_i about the item's quality. It is assumed that these signals are iid and depend on the true quality of the item: $q = Pr[s_i \text{ is good } |\text{ item is good}] = Pr[s_i \text{ is bad }|\text{ item is bad}]$, where this q is the accuracy of their signal. The utility of each agent is +1 upon receiving a good item and -1 for a bad one. The goal is to develop truthful mechanisms that maximize the probability that the item is correctly allocated: i.e., given to an agent if it is good or discarded if it is bad.  The authors study sequential allocation mechanisms, where the decisions of previous agents might reveal information about the item and affect following choices. This is empirically observed in Kidney Exchange, where if a kidney is rejected early, it is very likely that potential recipients might keep rejecting in, further decreasing the probability that someone will accept it.  The first result of the authors is to study the sequential offering mechanism. Here, the agents are ordered and the item is offered to each one, stopping and allocating the item the first time an agent accepts it. They show (roughly) that the item is allocated if $\mu > q$ or if the first two agents receive a good signal and it's never allocated for $\mu < 1-q$. Also, notice that his mechanism is not truthful. The reason the item might not get allocated is because the agents are actively updating their prior $\mu$.  The second (and main result) is the development of a *batched* version of the previous mechanism. In the simplest version, a batch of K agents is selected and asked to report their signals. If the majority considers the item good, then it is allocated at random to one agent reporting a good signal. Depending on $\mu$ and $q$, the authors show that there exists a range of batch sizes such that the mechanism is truthful. The second step is to greedily combine batches together: starting for a truthful batch size for $\mu$, if the item is not allocated then the next batch size would be a function of $\mu'$, the updated prior. Even though the game is sequential, the agents only participate once and only care about their current prior and the size of the batch they participate in.  Putting the two results together, the authors show that for $\mu > q$ there is no truthful mechanism and every mechanism is at most as good as the sequential one. This makes sense, as in this case the prior is more informative than the private signal. For $\mu < q$ however, their batched mechanism is incentive compatible and improves the guarantee. Finally, the authors have experiments on synthetic date showing the benefit of using 1 or 2 batches, compared to the sequential mechanism,  ","This paper studies how to allocate an object with unknown quality (only prior is known) to strategic agents. The strategic agents have their own private signals (drawn i.i.d. from another common prior) about the quality of the object and they would update their beliefs after observing other agents' decisions.   The authors point out that sequentially offering the object would lead to a cascading effect; in particular, only the first two agents decide the outcome of allocation. In contrast, offering the object to all the agents to elicit their private beliefs all together would lead to an efficient allocation, if without taking strategic behavior into account. The authors propose a novel mechanism, which offers the object to agents batch-by-batch, leading to an improvement in the allocation performance. Their results are supported and validated using simulations.","This paper presents an idea to improve the correctness of an allocation when the quality of the good must be inferred from agents who learn from each other. The authors set up a problem of allocating one object to a group of agents. The object is either of good or bad quality and each agent has a noisy IID signal of the quality. Together, they have a pretty good estimate of the quality. But each agent may have reasons to withhold their signal in order to avoid getting a bad object or attempt to acquire a good object. With incentives and learning together, the allocation can fail badly using a standard approach of offering the item to each agent sequentially to either accept or reject. The authors prove that, in their simplified model, only the first two agents play a role. If both of the first two agents reject the object, then every agent will reject the object, since they learn about quality from each other. Observing two rejections is enough to update every agents' belief to below threshold. The authors propose instead a clever algorithm. The planner offers the object to a batch of agents at a time. The batch size is calculated to maximize correctness subject to incentive compatibility. If every agent in the first batch rejects the object, the object is offered to a new, larger, batch of agents. The authors prove that, if the signals are more informative than the prior, then their mechanism is incentive compatible and is significantly more correct (has higher social welfare) than the sequential offer mechanism. The authors also prove that if the prior is more informative than the signals, then no mechanism can do better than the sequential offer mechanism.","The authors study a setting in which a central planner allocates a single indivisible good to agents showing up sequentially. The true quality of the item (good or bad) is unknown to both agents and the mechanism designer. The agents receive a signal about the quality of the item, and the planner aims to leverage those signals to learn the quality of the item and how to allocate it efficiently. ",0.08317929759704251,0.17375231053604437,0.07578558225508318,0.3333333333333333,0.13768115942028986,0.09655172413793103,0.32608695652173914,0.32413793103448274,0.5857142857142857,0.15862068965517243,0.2714285714285714,0.4,0.1325478645066274,0.2262334536702768,0.13420621931260232,0.21495327102803738,0.18269230769230768,0.15555555555555556
638,SP:8cc495b2c9707c95f245fd6e5d819ca55eb4749f,"This paper studies Markov games with large number of agents. The authors start by proposing a novel discrete-time formulation for graphon mean field games. They show that the mean field game exists, and that finite graph games converge to the mean field game when the number of agents goes to infinity. They also provide an algorithm that learns an approximate Nash equilibria. ","This paper proposes a discrete-time graphon mean-field game (GMFG) framework as a limiting model to approximate large population dense graph games. Existence of graphon mean-field equilibrium (GMFE) is proved by reformulating GMFG as a classical MFG, and a graphon mean-field approximation result is also obtained. The authors then propose two learning algorithms for computing GMFE, one based on discretizing the graphon index, while the other based on the aforementioned classical MFG reformulation. Some preliminary numerical experiments are also provided to demonstrate the theoretical findings and to validate the proposed algorithms. ","This paper studies discrete time dense graph mean field games (GMFG). It discusses the settings of both finite-agent graph game and GMFG and shows that under mild Lipschitz continuous assumptions, the Nash equilibrium of the GMFG exists. It establishes that the NE of the GMFG is a good approximation of the NE of an N-agent game and empirically verifies this result. It then proposes two algorithms for solving the NE of GMFG, one with the idea of discretizing the graphon index, and the other utilizing the equivalence of GMFG to a classical MFG with an extended state space. The proposed algorithms are tested on an SIS-Graphon problem and an Investment-Graphon problem to validate their performances. ","This paper studies a class of games with a continuum of agents that connected by a graphon. This corresponds to the limit of games with a finite number of players connected by a graph. In the recent literature, such games have been considered in continuous time, and here the authors focus on a discrete time version. They first analyze the game by showing existence of a Nash equilibrium (defined in a suitable sense), and they prove that using the equilibrium policy from the game with a continuum of players in a game with a finite number of players provides an approximate Nash equilibrium. Then, two numerical approaches are proposed, both based on fixed point iterations that alternate between updating the distribution and updating the optimal control. The first method relies on backward induction to compute the optimal control of representative agents in a set of equivalence classes. The second method relies on reinforcement learning to compute the optimal control. Last, numerical examples are provided. ",0.25396825396825395,0.30158730158730157,0.38095238095238093,0.32978723404255317,0.2553191489361702,0.25210084033613445,0.1702127659574468,0.15966386554621848,0.14634146341463414,0.2605042016806723,0.14634146341463414,0.18292682926829268,0.2038216560509554,0.2087912087912088,0.21145374449339205,0.2910798122065728,0.18604651162790697,0.21201413427561835
639,SP:8d29d19b8a4c43bd307bc3050ea41e2118a7a8e4,"This work studies the continual learning of a sequence of NLP tasks. A novel model called AFK is proposed, which contains a series of hand-crafted modules to prevent forgetting and help with knowledge transfer. A number of baselines are compared with, and strong CL performance are shown in the experiments.","This paper studies the problem of continual learning in a popular NLP task, text classification.  It proposes a framework called AFK, that utilizes capsule networks to dynamically route task-specific knowledge among task-shared knowledge. The author claims two major benefits of doing this: (1) this essentially establishes an attention effect that allows the model to more effectively conduct forward transfer, and (2) the assignment of new parameters for new tasks and the masking mechanism can mitigate catastrophic forgetting. Overall, the application of capsule network in continual learning is novel and the framework obtains good performance. However, I feel that more comprehensive analysis could further supported the presentation, and some could be critical for understanding these models (see below).","The paper investigates the problem of sequential learning of NLP tasks. Concretely, it proposes a novel method, AFK, to alleviate catastrophic forgetting and enable knowledge transfer across related tasks. The paper argues that most of the existing works solely focus on catastrophic forgetting problems and there is no explicit mechanism to facilitate the transfer across tasks. To address this issue, the paper designs a novel architecture as a part of the AFK method, CBA (Continual learning BERT Adapter). CBA consists of a capsule network to model different tasks and a transfer routing algorithm to support knowledge transfer across tasks. For experimentation, the paper considers three text classification datasets - Document Sentiment Classification (10 tasks), Aspect Sentiment Classification (19 tasks), and 20News (10 tasks) and evaluates AFK on task-incremental learning scenarios (separate classification head per task). In comparison with the considered baselines, the paper reports improved overall performance and minimal forgetting for AFK.","The authors proposed a new continual learning method AFK, aiming to prevent catastrophic forgetting and exhibit knowledge transfer. Capsule Network is used to share task-specific knowledge in the continual learning process. Experimental results show that AFK outperforms previous SOTA approaches on many different continual learning sentiment classification datasets.",0.3333333333333333,0.35294117647058826,0.23529411764705882,0.23529411764705882,0.1092436974789916,0.125,0.14285714285714285,0.11842105263157894,0.24489795918367346,0.18421052631578946,0.2653061224489796,0.3877551020408163,0.2,0.17733990147783252,0.24,0.2066420664206642,0.15476190476190477,0.1890547263681592
640,SP:8d4d04c137ea96e18deb20a4fd3516de963941fd,"This paper studies a class of reinforcement learning methods which utilize a value function which in addition to the state is conditioned on some representation of the current policy. The idea being that this can allow the agent to learn a value function that explicitly generalizes among policies as the policy changes over the course of learning and ultimately improve performance. The approach is named Policy-extended Value Function Approximation (PeVFA).  They present some theoretical results with essentially show that if we assume that lowering the error in the value function for one policy results in sufficiently lower error for nearby policies, and the policy does not change too much between iterations, we can bound the value error for a policy dependent value function in terms of distance between policies at subsequent iterations. They also show that assuming the policy dependent value function generalizes well it can provide a benefit over a policy independent value function in terms of value error during a policy iteration procedure.  They further present empirical results using policy dependent value functions with a variety of different techniques for conditioning the value function on the policy. Policy representation techniques tested include: Raw Policy Representation (RPR) which uses all the parameters of the policy network as input to a value approximation, Surface Policy Representation (SPR) which uses a set of state action pairs gathered from running the policy, and Origin Policy Representation (OPR) which again uses the policy network parameters, but preprocessed by a permutation invariant transform. In addition, they test combining these representation strategies with auxiliary losses encouraging policy recovery from eh learned representation.  They study the use of policy dependent value functions in two settings, a supervised learning like setting where a number of fixed policies are initially evaluated to train the value function and a reinforcement learning setting where a policy is adapted online while learning a policy dependent value function for the policies the arise along the way. Results indicate that in the supervised case the learned value function can generalize to unseen policies.  In the reinforcement learning case, policy dependent value functions appear to confer some performance improvement when used with PPO compared to standard value functions.","Update: See the comments. Review score remains at 7.  The paper considers learning value function approximators that take a policy representation as an input to the value function to allow generalization across different policies. There are several other recent works of this type. The aspect differentiating the current work is the type of policy representation used that allowed the method to scale to policies with 10^5 parameters.  The paper considered two types of data sources for the policy representations: 1. State action pairs obtained by the policy. 2. The policy parameters.  Then, the policy representation is formed by passing the data through a multilayer neural network that embeds the data into a lower dimensional representation. The embedding is fed into the value function as an input, and the network creating the embedding is trained end-to-end to minimize the value function prediction error. Optionally they also added auxiliary losses such as contrastive learning.  Experiments were performed using PPO combined with their new value function and compared to the standard PPO as well as variants taking a random policy representation and also a policy representation with no emebedding. They performed experiments on 6 MuJoCo tasks, and generally the new methods outperformed the baselines.  tSNE plots showed that the policies with similar representations had a similar performance at the task (when comparing the achieved rewards). Moreover, experiments showed that the value prediction error was smaller for the new method compared to a standard value function method, and also that the value function was accurate for a test set of new policies not encountered during training.","This paper studies value function approximators that explicitly take the policy as input, which the authors named policy-extended value function approximator (PeVFA). This is different from the common value function approximators which implicitly represent the value of the current policy. The authors discuss two scenarios where PeVFA could provide potential benefits via generalization: global generalization where the policies are sampled from a fixed distribution and local generalization where the policies are sampled from the policy improvement path. The latter is a typical scenario for policy improvement. The authors provide a theoretical analysis of the generalization properties of PeVFA. To apply PeVFA in practice, the authors also investigate several different ways of learning a compact embedding for policies: end-to-end learning, auxiliary loss of policy recovery, and contrastive learning. Empirical study shows that PeVFA improves over the conventional value function approximators on the Mujoco continuous control benchmark. The empirical results also provide some insight into the effectiveness of different policy embedding learning methods.","The authors propose PeVFA, an extension to value functions that include not only states (or state-action pairs), but a representation of the parameters of the policy. They apply their method to PPO and show experimentally that such policy representations help improving over standard PPO. Unfortunately, the theoretical results in the paper seem trivial and disconnected from the rest of the paper. The value functions presented in the paper were already introduced in previous work. The authors compare extensively these methods to their approach, but introduce their PeVFAs separately, creating some confusion on the novelty of the work. ",0.16712328767123288,0.14246575342465753,0.07671232876712329,0.16981132075471697,0.10566037735849057,0.1402439024390244,0.23018867924528302,0.3170731707317073,0.2857142857142857,0.27439024390243905,0.2857142857142857,0.23469387755102042,0.19365079365079363,0.19659735349716445,0.12095032397408208,0.20979020979020982,0.15426997245179064,0.17557251908396945
641,SP:8d4d303c2e45fa1ecb7abbd3fbb048bfe2e70765,"This paper theoretically characterizes the number of linear regions of ReLU neural networks at initialization. Compared to existing results, the authors take the data manifold into consideration, and the number of linear regions is measured on the data manifold. Moreover, for uniformly random sampled data on a compact manifold, the expected (geodesic) distance to the linear boundary is analyzed.  To further support the theory and also extend to the behavior through the training process, the paper provides synthetic data and real data experiments. There are indications of the number of linear regions does not deviate significantly from their initialization and is faithful to the data manifold dimension.","The authors study average-case representation capacity of random ReLU neural networks, as measured through their linear regions, as in Hanin and Rolnick 2019 -- the difference in the authors' setting is that they consider the input space to be defined on a low-dimensional manifold, whereas Hanin and Rolnick considered inputs on the solid cube. The authors derive analogues of each of the results of Hanin and Rolnick in the manifold setting -- these results include formulas for the ""average volume density"" of linear regions, a proxy for the exact number of linear regions, and the average distance to the boundary of the linear regions -- where the dimension dependence is on the intrinsic manifold dimension rather than the ambient dimension, and additional constants appear that depend on properties like curvatures of the manifolds. They provide toy experiments to verify the theory (involving testing linear region properties on a pair of 1D manifolds with different curvatures), as well as an experiment on 'manifolds of faces' generated by the StyleGAN model -- in both cases, the linear regions are studied throughout training rather than just at initialization (as the theory pertains to).  ",The paper investigates the number of linear regions of deep neural nets. More specifically how deep neural nets split the input data manifold into regions where it behaves approximately linearly.  It generalizes the results by Hanin and Rolnick 19’ to the manifold setting. Two theoretical bounds: 1) an upper bound for the number of linear regions and 2) a lower bound for the average manifold distance between points on the manifold to the linear boundary. Experiments were done on both toy datasets and MetFaces dataset. ,"This paper provides an analysis of the impact of data geometry on the regions of linearity in deep neural networks. To this end, it extends previous results by Hanin and Rolnick, which were established in terms of the Euclidean ambient feature space of input data, to account here for nonlinear structure when modeling the data as being sampled from a manifold submersed in this ambient space. The quantities related to the local linear structure of ReLU networks are directly related to the number of neurons, and theoretical results are validated empirically, albeit limited to simplified toy examples.",0.308411214953271,0.2616822429906542,0.2616822429906542,0.17553191489361702,0.13297872340425532,0.27058823529411763,0.17553191489361702,0.32941176470588235,0.28865979381443296,0.38823529411764707,0.25773195876288657,0.23711340206185566,0.22372881355932203,0.29166666666666663,0.27450980392156865,0.24175824175824173,0.17543859649122806,0.25274725274725274
642,SP:8da1bfe8ad08e6e6b5c1f73021931bc58941ae0c,The authors provide an algorithm for online learning in a non-cooperative configurable MDP. They propose a setting where a configurator can choose a transition model for the MDP from a finite set of models. The goal is then to observe the agent's behaviour (either the state-action sequences or a noisy reward) and to learn an optimal configuration that results in maximum configurator reward given that the agent behaves optimally with respect to its own reward function.  ,"This paper presents an extension of the existing Configurable-MDP framework to a non-cooperative setting; one where the agent and the configurator could also have different reward functions. This setting (referred to as Non-cooperative Configurable MDPs) allows for modelling a wider range of situations, which also include the scenario where the agent and configurator display non-cooperative behaviour. The problem-setting is modelled as a leader-follower game, with the configurator as the leader and the agent as the follower. The authors present two algorithms for the configurator: AfOCL- which assumes that during online interaction the configurator can observe the agent's actions; RfOCL- which assumes that during online interaction the configurator can observe the agent's actions and obtains noisy estimates of its reward as well. The authors go on to prove that AfOCL achieves finite expected regret, which scales *linearly* with the number of possible configurations. This is already an improvement over the standard UCB algorithm which yields a regret that grows indefinitely over time. They also show that RfOCL the regret does not depend on the number of configurations under the assumption that the agent has a non-zero probability in some time-step of the horizon to visit every state in the MDP. The experimental evaluations were carried out on two environments: A configurable gridworld, and a simulated teacher-student environment. In both settings, their algorithms obtain constant  cumulative regret, as opposed to the logarithmic regret suffered by the UCB algorithm, validating their result.","The paper studies learning in configurable MDPs with two-agents, a reinforcement learning agent and a configurator that can modify the environment parameters. In contrast to prior work, the paper considers a setting in which the configurator and the agent can have misaligned objectives. The setting is formally modeled as a Stackelberg game, in which the configurator is the leader that  selects a transition model and the agent is the follower that plays the best response policy under this model. The goal is to design a learning algorithm for the leader with provable regret guarantees. The paper provides two learning algorithms for the configurator and formally analyzes their regret properties, which turn out to be better than those of a naive solution based on UCB1. The paper supports its theoretical claims using simulation-based experiments.  ","This paper proposes an extension to the configurable MDP framework, which implicitly assumes a shared reward function by the acting agent and the configurator, to the non-cooperative case where the reward functions of the two agents may be different and unknown to the other. Modeling the problem as a leader follower game, the papers present two online regret-minimization algorithms based on a multi-armed bandit strategy to learn the best environment configuration under different conditions. In the first, the configurator has access only to observed state-action traces (AfOCL), and in the second the configurator additionally observes noisy reward signals (RfOCL). The core idea of the algorithms is to iteratively prune the space of ""plausible policies"" of the acting agent based on observed feedback to compute an increasingly accurate optimistic value function for each environmental configuration for the configurator at the start of each episode. The authors prove that both algorithms incur finite expected regret, and show that regret scales linearly in the number of configurations for AfOCL, and is independent of the number of configurations in the case of RfOCL. Finally, the authors provide empirical results of the performance of AfOCL and RfOCL against the baseline UCB1 in two experimental domains, a gridworld domain and a teacher-student domain. ",0.3291139240506329,0.2911392405063291,0.27848101265822783,0.164,0.288,0.2518518518518518,0.104,0.17037037037037037,0.10377358490566038,0.3037037037037037,0.33962264150943394,0.16037735849056603,0.1580547112462006,0.21495327102803735,0.15120274914089346,0.212987012987013,0.3116883116883116,0.19596541786743513
643,SP:8e3ee4e55a883396c2622d8c9a7e60e3def3f8da,"The authors consider the problem of deployment complexity in reinforcement learning problems, where we want to reduce the number of cycles an agent is typically deployed for interacting with the environment. In the context of a linear MDP, the authors prove an information theoretic lower bound. Further, they also propose algorithms which achieve this optimal deployment efficiency. ","The paper focuses on the theoretical properties of Deployment Efficient Reinforcement Learning, DE-RL, focusing on linear MDPs. Deployment complexity is defined as the number of times that a different policy is selected to collect data, and an algorithm is said to be deployment efficient if the number of deployments is as small as possible and the number of trajectories per iteration is polynomial.  In section 3, the authors provide a lower bound for the task, under both deterministic and stochastic policies.  In section 4, the authors provide detailed algorithms for DE-RL, as well as relevant upper bounds. In the deterministic case the upperbound matches lower bound up to constant factors, and in the stochastic case the upperbound matches the lower bound *up to log factors* and as long as $\nu_{\textrm{min}}$ is bounded.","This paper studies deployment-efficient reinforcement learning and provides a theoretical perspective. In this setting, this paper provides lower bounds and upper bounds respectively for determinisitc policies and arbitrary policies when the MDPs have linear structures. The deployment complexity is near-optimal.","The paper presents a theoretical perspective on deployment efficiency in linear MDPs. The paper formalizes the notion of deployment complexity and presents an information-theoretic lower bound for worst-case deployment complexity of any algorithm, identifying horizon as the main bottleneck for deployment efficiency (in addition to feature dimension when restricting to deterministic policies). The paper then presents two algorithms (for deterministic or stochastic policies) matching the lower bounds.",0.3333333333333333,0.14035087719298245,0.22807017543859648,0.11029411764705882,0.17647058823529413,0.2619047619047619,0.13970588235294118,0.19047619047619047,0.18840579710144928,0.35714285714285715,0.34782608695652173,0.15942028985507245,0.19689119170984457,0.16161616161616163,0.20634920634920634,0.16853932584269662,0.23414634146341462,0.19819819819819817
644,SP:8e62c6cb57b93a9623c8e96e6376538d950fb024,"This paper presents a psychophysical comparison between a peripheral summary statistics model (""texforms""), adversarially robust and non-robust neural networks. Images are generated from each model / network using gradient ascent, then presented to humans at different retinal eccentricities. The key finding is that images generated by non-robust networks don't look anything like natural images, and so are easily discriminable, whereas robust and texforms become more difficult to tell apart from natural images as they are moved further into the periphery. The main claim is that because of this similar performance falloff, training on adversarial noise may cause similar representations to be learned as in the human periphery. By comparing the generated images in terms of physical and perceptual distances, the claim is further strengthened as ""suggesting their models compute the same transformations"". The paper ends with the interesting conjecture that ""peripheral computation may implicitly act as a natural visual regularizer"".  ","This study draws a surprising connection between adversarially trained CNNs and human peripheral perception. Using the synthesized metamers, the authors show that the human ability to discriminate between natural images and their synthetic metamers drops in a similar fashion for metamers generated by inverting adversarially trained CNNs and for metamers generated by an inversion of a well-studied model of peripheral vision (i.e., ""Texforms"") as display eccentricity increases. This may indicate that adversarially trained models bear some resemblance to human visual processing at the retinal periphery.","The authors hypothesize that description of visual textures/objects using summary statistics (as may be doing the human vision in the periphery) may induce robustness to adversarial examples.   In the discussion they argue that this may be the case because imposing separability of classes with high-variance (responses of peripheral vision) leads to enlarged class separation of small-variance sets in foveal vision which have the same mean.   The authors check their initial hypothesis in an indirect way: they check that robust representations to adversarial attacks resemble human representations. They do so by comparing \tilde{x} (synthesized images from networks which are robust to adversarial examples) and \hat{x} (synthesized images from models of human peripheral vision). They argue that given the fact that these classes of images (\tilde{x} and \hat{x}) are equally discriminable by humans from the original images, x, the image representation of robust networks is similar to the (periphery) image representation in humans. The authors also compare the discriminability of images, x_s, generated from standard (non-robust) networks.  The specific way in which \tilde{x}, \hat{x} and x_s are compared is measuring the psychometric function (over eccentricity) so that these images are discriminable by humans from the original image. Interestingly, these images are also compared using subjective image distortion metrics.","This paper examines whether an adversarially trained (“robust”) ResNet-50 classification model might better resemble the representation of human peripheral vision than a non-adversarially trained (“non-robust”) model. It does so by means of a human perceptual experiment: test images from Restricted ImageNet (a collapsed subset of ImageNet comprising 9 super-classes) are resynthesized via gradient descent from random noise initial conditions by minimizing the L2 difference in the penultimate feature activation of both a robust and non-robustmodel.  In addition, the same stimuli are resynthesized according to previously validated models of summary statistic based peripheral computation (Texforms). A range of perceptual discrimination experiments (odd-one-out and two alternative forced-choice) are carried out with human subjects among different pairs of stimuli (original, robust, non-robust and texform) at varying degrees of peripheral eccentricity. The experiments show that the discriminability of robust and texform stimuli vs original stimuli degrade similarly with increasing eccentricity, while non-robust stimuli are relatively easy to pick out vs original stimuli at all levels of eccentricity.  ",0.125,0.17763157894736842,0.14473684210526316,0.19540229885057472,0.20689655172413793,0.13761467889908258,0.21839080459770116,0.12385321100917432,0.12716763005780346,0.0779816513761468,0.10404624277456648,0.17341040462427745,0.1589958158995816,0.14594594594594595,0.13538461538461538,0.11147540983606558,0.13846153846153847,0.1534526854219949
645,SP:8e6a07dcd70f24523be35d61e086ba2c73c97f8a,"The paper proposes a re-ranking algorithm for image retrieval, capable of improving the ranking results of an off-the-shelf similarity-based retrieval system, by representing each query by an affinity vector that measures similarities to a set of anchor images, and then refine this representation by aggregating it with contextual information obtained from other top-ranked retrieval results using a transformer architecture. The refined representations of the top-K images can then be used for re-ranking.   The paper shows improvements both in precision and time complexity over previous re-ranking algorithms. The time complexity improvements are mostly due to the introduction of a fixed set of anchor images, which avoids context affinity features that scale linearly with the number of top-k retrievals considered. The precision improvements are a result of a learnable context aggregation module (in the form of a transformer) and the data augmentations used for training it.  Experiments on several benchmarks show healthy improvements over prior re-ranking procedures.","The paper introduces a new re-ranking method for image retrieval. It proposes to learn an affinity vector for each of the top-ranked candidates w.r.t a set of anchor images, using a transformer encoder. These affinity vectors are then used to re-estimate the similarity scores between the top-ranked candidates and the query image. The proposed method is evaluated on the Revisited Oxford and Paris datasets and shown to be superior or complementary to other query expansion/diffusion based reranking methods.","In this paper, a visual re-ranking method is proposed. This method leverages the contextual similarity of the top-K retrieved images by comparing the original features with a set of anchor images, which is further refined by a transformer encoder. The refined features are then used to re-rank the top-K ranking list.  Moreover, a new data augmentation scheme is designed by employing different feature extractors, which enlarges the feature variety in the ranking list and improve the robustness of the re-ranking model.  Comprehensive experiments have been conducted on four benchmarks, demonstrating clear improvement and strong robustness.","Paper presents a method to perform top-k reranking on a (image) retrieval system. The method uses external data to learn, in a supervised manner,  a reranking module that produces contextualized representations (by means of self attention) of the top-k results, which are then used to rerank them.  The method seems inspired by the recent LattQE [16], that also uses a reranking module based on self attention and trained in a supervised manner. However, I see this paper has relevant contributions / differences: - The proposed method uses ""affinity"" representations as the input of the reranking module, that probably capture better the similarity space. - The form of supervision is different, computing a contrastive loss only amongst the top-k results.  - In addition, a ""fidelity"" loss and a different form of data augmentation is proposed.",0.1696969696969697,0.2,0.18181818181818182,0.32941176470588235,0.27058823529411763,0.22,0.32941176470588235,0.33,0.22556390977443608,0.28,0.17293233082706766,0.16541353383458646,0.22399999999999998,0.2490566037735849,0.20134228187919465,0.3027027027027028,0.2110091743119266,0.1888412017167382
646,SP:8e7c1ca81b52d8f95095cfe68010a9114fcf1804,This paper introduces monotic differentiable sinkhorn networks. Authors argue that mononicity is a desired property because it leads to better bounds in errors. Authors show their proposed approach beats the state of the art on sorting MNIST and SVHN digits,"This paper presents monotonic differentiable sorting networks. It argues that monotonicity is lacking in most differentiable sorting networks whereas this may lead to incorrect gradient signs and inconsistent outputs. The first part of the paper motivates the problem and provides a brief review of other differentiable sorting networks. Then the paper provides a necessary condition for monotonicity and study bounds on the error of continuous conditional swaps. Finally, experiments demonstrate that monotonic swaps outperform non-monotonic ones for training a network to order MNIST, especially on larger numbers. ","This work introduces methods to construct differentiable sorting networks that are provably monotonic and have bounded error. To provide such guarantees, the paper derives a set of necessary and sufficient conditions on sigmoid function used for the soft swap operators in the network. It then studies a set of candidate sigmoid functions for such construction, detailing their monotonicity and error guarantees. The experimental results show that the use of monotonic networks can achieve SOTA results when applied to multi-digit ordering on images constructed from MNIST and SVHN.","This paper proposes sigmoidal functions f for which, roughly speaking, x * f(-x) is non-decreasingly monotonic and bounded from above (this does not hold for the standard sigmoid). These are used to define continuous relaxations of min and max operations (not to confuse with softmax which is a relaxation for argmax) which are monotonic in their inputs.  By stacking multiple layers of these 'monotonic continuous conditional swaps' the authors obtain a sorting network for which all outputs are monotonic in all inputs. This ensures that the gradient is in the right direction when training a sorting network to sort data using only ground truth order.  Experiments on sorting 4-digit MNIST numbers or SVHN  show that the resulting network is able to predict the order more accurately than a number of alternatives, especially non-monotonic differentiable sorting networks.",0.3,0.35,0.3,0.22727272727272727,0.23863636363636365,0.2159090909090909,0.13636363636363635,0.1590909090909091,0.08633093525179857,0.22727272727272727,0.1510791366906475,0.1366906474820144,0.18749999999999997,0.21875,0.1340782122905028,0.22727272727272727,0.18502202643171808,0.16740088105726872
647,SP:8e7ea3ffc14606fcafd46135ed7c8bb2ce07b2b5,"This manuscript proposes CAGE, a method for determining causal relationships between attributes based on the learned representations of deep causal models.  The method is based upon relatively standard causal inference methods after structuring the deep latent representations between them.  The method is straightforward to apply and estimate the Generative Average Treatment Effect (GATE).  Results are shown on both synthetic and real datasets, and are used to infer relationships.",This paper proposes a novel framework CAGE (causal probing of deep generative models) for inferring the causal-effect relationship in deep generative models. The treatment is implemented by moving linearly along the hyperplane normal. Then the attribute treatment effect can be quantified via a proxy classifier. The authors evaluated the CAGE on low-dimensional synthetic and high-dimensional datasets. Results show the ability to infer causal relationships of the proposed CAGE. ,"The goal of this paper is to inspect causal relationships learned by a generative model. In particular, the paper studies how moving in the latent space with respect to a putative cause variable (e.g. age) affects the probability of the putative effect variable (e.g. baldness). This is done by training two linear classifiers in the latent space, one for each variable. The counterfactual for a given cause variable and datapoint is defined by changing the location of the data point in the latent space in the direction perpendicular to the linear classification boundary for the cause variable. Then, the paper inspects what happens to the putative effect variable, using its respective decision boundary. A null distribution is defined using permutation. The usefulness of techniques is demonstrated for inferring the causal relationships (including direction) for pairs of variables and for generating data augmentations for robust classification. ","The authors propose to discover causality for a trained man-made model. This is very unusual, as normally we would be interested in discovering causality the true underlying process that generates the true data.      More specifically, they propose a framework called CAGE to estimate generative average treatment effects (GATEs) for probing cause-effect relationships in the so called latent space of deep generative models. They use the average treatment effects (ATEs) to infer causal directions for deep generative models' attributes and use CAGE to generate counterfactual data. They evaluate the ability of CAGE both synthetic (a variant of MNIST called MorphoMNIST) and high-resolution face dataset (CelebaHQ). ",0.3235294117647059,0.22058823529411764,0.23529411764705882,0.28169014084507044,0.2676056338028169,0.1564625850340136,0.30985915492957744,0.10204081632653061,0.14953271028037382,0.1360544217687075,0.17757009345794392,0.21495327102803738,0.31654676258992803,0.13953488372093023,0.18285714285714286,0.1834862385321101,0.2134831460674157,0.18110236220472442
648,SP:8eb51c3303a0eca35019f9636ffaf93c537bcefe,"This paper presents a simple architecture based on MLPs, which only adopts channel projections and spatial projections with the spatial gating units. On image classification, the presented gMLP could achieve on par performance with DeiT and ViT, and better than previous MLP-based approaches, such as MLP Mixer. On masked language modeling tasks, gMLP could also achieve on par performance with Transformer on Perplexity, and gMLP could perform slightly worse than Transformer on MNLI.",This paper proposes and studies an interesting research problem whether we can remove the self-attention in Transformer based models. The authors devise a novel architecture called gMLPs by removing the self-attention layer and introducing a new Spatial Gating Unit into the FFN module to well model the interactions between independent tokens. Comprehensive experiments on CV or NLP tasks confirmed that the proposed gMLPs can achieve comparable performances with the standard ViT or BERT models.    ,"This paper proposes a MLP-based model that achieves comparable performance with transformers for vision tasks and language modeling. While this MLP-based model performs slighly worse on natural language understanding tasks (e.g. MNLI), introducing a small single-head self-attention module can compenate for most of the performance loss. Compared to other MLP-based models introduced in concurrent works, the proposed model uses a gating module before the spatial projection. Performance wise, the proposed method is stronger than the concurrent MLP variant MLP-mixer on vision and language modeling tasks, showing the advantage of the proposed gating methods  Overall I find the proposed method to be interesting and empirically strong. I am leaning toward accept but also wish the authors had done more work in understanding the proposed models. I offer some suggestions and have some questions below.",This paper proposes a simple mechanism: gated MLP and questions the necessity of self attention. gMLP achieves comparable performance as DeiT on the classification task. Experiments on NLP talk also support the validness of this method.   ,0.16216216216216217,0.25675675675675674,0.14864864864864866,0.2631578947368421,0.15789473684210525,0.1,0.15789473684210525,0.1357142857142857,0.3055555555555556,0.14285714285714285,0.3333333333333333,0.3888888888888889,0.16,0.17757009345794392,0.19999999999999998,0.18518518518518517,0.21428571428571427,0.1590909090909091
649,SP:8ebb5d3873148ab696bf5342d5a68bdf7639a46f,The authors present a novel method for imitation learning from a limited set of expert demonstrations. The method is heavily based on the DREAMER model-based RL framework and extends this with a discriminator to match a learner policy to an expert rollout. I find the paper overall well-written and a worthwhile contribution but there are some technicalities holding the paper back from being a top submission. I am overall still in favor of getting this work accepted since I believe most things can be addressed easily and I'm happy to change my rating if the authors can address some of my concerns.,"The authors propose V-MAIL, a variational model-based imitation learning algorithm. The transition dynamics model is learned by optimizing a relevant variational lower bound under the assumption of partial observability, while discriminators for imitation learning are trained over the latent-action space. The empirical results show that V-MAIL outperforms baselines including SQIL and DAC (where the data-augmentation technique is applied for both baselines). The authors additionally check the generalization ability (transfer learning) for V-MAIL. ","Motivated by several challenges that face the use of adversarial imitation learning (AIL) in practice, the authors propose V-MAIL—a model-based algorithm for visual AIL. V-MAIL addresses the challenges of sample inefficiency and difficult numerical optimization by learning and leveraging both a variational observation model and a variational forward dynamics model. The authors provide/prove interesting theoretical performance bounds with respect to the proposed formulation, and propose V-MAIL as a practical solution to that formulation.","This paper introduces Variational model-based adversarial imitation learning (V-MAIL). V-MAIL considers a model-based setup for adversarial imitation learning: a latent dynamic model is learned and used to generate on-policy trajectories for training the RL policy generally used in the GAIL setup. It manages to achieve state-of-the-art performance on multiple tasks from visual observations, including MuJoCo, classic car racing, and two other robot manipulation tasks. ",0.13333333333333333,0.13333333333333333,0.13333333333333333,0.24358974358974358,0.19230769230769232,0.21518987341772153,0.1794871794871795,0.17721518987341772,0.19444444444444445,0.24050632911392406,0.20833333333333334,0.2361111111111111,0.1530054644808743,0.15217391304347824,0.1581920903954802,0.24203821656050956,0.2,0.2251655629139073
650,SP:8ee33fdd3343ae6760e416ae34ac56eac406ce7f,"This paper contributes to the literature on efficient [transformer][] architectures. Specifically, papers that focus on reducing the quadratic computational complexity of the self-attention mechanism between tokens.  The problem this paper focuses on is the incompatibility of [relative positional encodings (RPE)][rpe] and sub-quadratic efficient attention mechanisms. It adapts the kernelized attention used in the [Performer][] architecture to allow the use of RPE. In the process, the authors find that this adaptation allows use of the FFT algorithm to perform the approximate attention mechanism, resulting in an $O(N \log N)$ algorithm, where $N$ is the number of tokens.  Benchmarks are provided on the GLUE dataset according to various metrics and on the ImageNet dataset using Image Transformer variants. Performance to competing methods chosen is better on most of the metrics presented.  [transformer]: https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf [rpe]: https://arxiv.org/abs/1803.02155 [performer]: https://arxiv.org/abs/2009.14794","There are two major contributions of this paper. First, it proposes a method to incorporate relative positional embeddings (RPE) into kernelized attention, which is based on Fast Fourier Transformation (FFT) and Toeplitz matrix multiplication and leads to a $O(n \log n)$ time complexity compared to $O(n^2)$ using the conventional self-attention with RPE. Second, it proposes to normalize the queries and keys by their `$\ell_2$ norms, which makes the optimization more stable.","The paper introduces a novel way to add relative positional encoding to efficient transformers with kernelized attention. The authors make use of the efficient matrix multiplication of Toeplitz matrices using the fast Fourier transform.  The authors perform several experiments on masked language modeling, machine translation and even vision transformers.","The authors propose a method to compute kernelized attention with relative position encoding (RPE) efficiently in $O(n\log n)$. They show that the RPE family of attention is richer than attention without RPE, and the usual $O(n)$ implementation of kernelized attention is no longer possible.  The derivation gives rise to a convolution, which the authors propose to compute in the Fourier domain, providing $O(n\log n)$ complexity, in contrast to $O(n^2)$  for a naive implementation.  The paper provides a comprehensive derivation of the algorithm, mathematically and in pseudo code. Interestingly, it is found that with the help of RPE training stability of kernelized attention can be improved, thus allowing for the first time to train certain models end-to-end.  Experiments with random toy data where performed to assess the computational load of the algorithm, and experiments on datasets coming from language pre-training, machine translation, and image classification are used to show the effectiveness and the improved training stability of the algorithm on real world problems      ",0.12025316455696203,0.10126582278481013,0.2088607594936709,0.17105263157894737,0.34210526315789475,0.40816326530612246,0.25,0.32653061224489793,0.19186046511627908,0.2653061224489796,0.1511627906976744,0.11627906976744186,0.1623931623931624,0.15458937198067635,0.2,0.208,0.20967741935483872,0.18099547511312217
651,SP:8f2e3e3cc87dafca8e979e04daa077d90d394bc4,The article presents a change point detection (CPD) method for multivariate data. The proposed methodology operates by monitoring the space generated by segments of the data and then assesses how this space varies. A sound theoretical treatment and convincing experiments are reported as well.  ,"The paper studies change-point detection in time-series using a combination multi-variate singular spectrum analysis (SSA) and CUSUM.   While multi-variate SSA and CUSUM are established techniques, the contribution of the paper is to (a) analyze the online CPD setting with a dependent (non-iid) timeseries. (b) Conduct rigorous analysis of the tradeoff of detection delay vs. false-alarm within the MSSA model.  (c) The exact version of MSSA is a bit different from the classical one. ",This paper presents a multivariate singular spectrum analysis to perform change point detection. A spatio-temporal model is introduced to model the dynamics and CUSUM statistic was used to perform online change point detection. The experiment results showed the effectiveness of the proposed method.,This paper studies the online change-point detection for multivariate time series and proposes a new cusum-like detection statistic based on singular spectrum analysis. Theoretical approximations for the ARL and EDD are provided. This paper also validates the performance of the proposed method using a variety of synthetic and real data sets.,0.20454545454545456,0.20454545454545456,0.2727272727272727,0.17721518987341772,0.22784810126582278,0.3181818181818182,0.11392405063291139,0.20454545454545456,0.22641509433962265,0.3181818181818182,0.33962264150943394,0.2641509433962264,0.14634146341463417,0.20454545454545456,0.24742268041237112,0.22764227642276422,0.27272727272727265,0.28865979381443296
652,SP:8f3cbf8d6730615fc5513c8437f3b658ef74604f,"This paper proposes Permuton-induced Chinese Restaurant Processes (PCRPs), a novel class of Bayesian nonparametric (BNP) models for rectangular partitioning. There have been several proposals for BNP priors for rectangular partitioning or more generally space partitioning problems. Many of them can be understood as a multi-dimensional extension of Dirichlet processes, but almost all of them only consider the infinite-dimensional representations (multi-dimensional stick-breaking like representations) which make posterior inference cumbersome. According to the authors, PCRP is probably the first try to introduce a CRP-like representation for rectangular partitioning, which comes with the benefit of handy posterior inference. The main idea is to use some tools developed in the literature dealing with the permutations; it can be shown that a permutation can be mapped to a specific class of rectangular partitioning, and a random permutation can be realized by the random measure called permuton (which is quite similar to graphon). In order to actually draw a rectangular partitioning, a partition is drawn from a plain CRP with table coordinates are sampled from the permuton. Then, the permutation induced from the table coordinates is turned into a rectangular partitioning. PCRP facilitates a convenient MCMC procedure which is demonstrated to converge faster than previous approaches in the experiments but is not projective like normal BNP models. The authors further propose a workaround to this and an MCMC inference for the workaround based on bridging.  ","The paper introduces the permuton-induced Chinese restaurante process (PCRP), a prior over rectangular partitions of 2D space, and applies the prior to analyze relational data. The sample paths of the prior (Figure 2) exhibit different behavior depending on the permuton measure gamma, which is one of the design choices in the prior. In numerical experiments, probabilistic models using the proposed prior perform competitively with existing choices of prior in terms of held-out perplexity.","This paper introduces  a multi-dimensional extension of the Chinese restaurant process (CRP) called permuton-induced CRP (PCRP). The paper discusses the permuton setting of the CRP to get a probabilistic model of rectangular partitioning of a matrix. A Permuton is a probability measure on [0, 1]x [0, 1], which can be regarded as a geometric interpretation of the scaling limit of permutations. The PCRP can be used as a unified stochastic process to represent different classes of rectangular partitioning.  The authors also propose a new representation of the BNP model to depict the PCRP as a bridging state to existing BNP relational models. Experiments involve the application of PCRP to Bayesian nonparametric (BNP) relational data and there are some promising results showcased.","This paper proposed a model for relational data -- permuton-induced Chinese restaurant process, which can be regarded as a multidimensional Chinese restaurant process. The permuton is used to fill in the input matrix according to a Chinese restaurant scheme. The underlying model is well designed for rectangular partitioning, and various empirical results are also provided.",0.09361702127659574,0.1574468085106383,0.09361702127659574,0.26666666666666666,0.16,0.1693548387096774,0.29333333333333333,0.29838709677419356,0.4,0.16129032258064516,0.21818181818181817,0.38181818181818183,0.14193548387096772,0.20612813370473537,0.15172413793103448,0.20100502512562812,0.18461538461538463,0.23463687150837992
653,SP:8f9df1d7b59569e71a39957a5b6fb50369b92c13,"The paper considers reinforcement learning problem in infinite-horizon, discounted reward MDPs with a generative model. The authors aim to obtain upper bounds on the number of samples required to efficiently learn the optimal policy or the optimal $Q$-function when number of states and actions are very large. In order to achieve this, the authors impose generalization by considering transition probabilities to be linear in known state-action features (a.k.a. the linear MDP setting). The authors further assume access to a number of anchor state-action pairs, whose features (essentially) form a basis of the feature space. Under these assumptions on the MDP, the authors propose both model-based and model-free algorithm for learning the optimal policy (or $Q$-function), and analyze their sample complexity guarantees. The authors prove that the sample complexities of both the algorithms scale linearly with the dimension of the feature space (which is also equal to the number of anchor state-action pairs). The sample complexity bound of the model-based algorithm is the first of its kind for linear MDPs, and matches the minimax lower bound of Yang and Wang, 2019. The sample complexity bound of the model-free algorithm, though suboptimal, improves over the previously known bound of Yang and Wang, 2019.","This paper considers reinforcement learning in infinite horizon discounted Markov decision processes with access to a generative model with linearly parameterized transitions.  At a high level, the authors consider an infinite horizon discounted Markov decision process with discount factor $\gamma$, finite state and action spaces, and known reward functions.  The high level goal is to query the transition kernel at various states in order to learn a policy $\pi$ mapping states $s$ to actions $a$ which maximizes the cumulative reward experienced across the trajectory.  It is widely known that learning in general depends on $|S| \times |A|$ which can be prohibitively large whenever $S$ or $A$ is large.  As such, the authors impose additional assumptions in the paper different from prior work with respect to the transition kernel (the only uncertainty present in the model) in order to avoid this worst-case scaling.  In particular, they assume that the algorithm designer has access to a feature function $\phi : S \times A \rightarrow R^K$ such that there are unknown functions $\psi_1, \ldots, \psi_k$ where $P(s' | s,a) = \sum_k \phi_k(s,a) \psi_k(s').$  This definition is the same structure imposed on the probability transition kernel in the linear MDP model (but doesn't also require that the reward function is a linear mapping of the features $\phi$).  This model encompasses tabular MDPs with no additional assumptions where $K$ can be taken to be $S \times A$.  In addition to the linear assumption, the authors assume access to a set of anchor state action pairs of size $K$.  With this assumption, they assume that the feature vectors can be expressed as a convex combination of the feature vectors of the anchor state action pairs (which additionally imposes that the transition kernel can be written as $P(s' | s,a) = \sum_i \lambda_i(s,a) P(s' | s_i, a_i))$ where $i$ indexes over the anchor state action pairs. The authors assume that the $\lambda$ function and the anchor pairs are known in advance to the algorithm designer.  This assumption differentiates the model from the typical infinite horizon discounted MDP to (hopefully) obtain improved regret guarantees.  In particular, the theoretical contributions for the authors follows via two separate strands.  First, the authors consider a model based reinforcement learning algorithm using the generative model.  In particular, their algorithm queries all of the anchor state-action pairs a given number of times to obtain independent samples from the transition $P( | s_i, a_i)$.  Using this, they can then construct an empirical transition kernel for all state action pairs using the linear combination arising from the anchor state action pair assumption.  With this estimate of the transition kernel, the authors can construct an empirical MDP where the true transition is replaced by its estimate (which can be done as the reward function is known), and use planning to find an optimal policy in this perturbed MDP.  Via this method the authors show that the sample complexity scales via $K / (1 - \gamma)^3 \epsilon^2$, which is minimax optimal up to logarithmic factors.  Second, the authors consider a model free reinforcement learning algorithm using the generative model.  This algorithm proceeds across time steps, where in each time step they sample a transition for each anchor state action pair, and update an estimate for the $Q$ function using the 'anchored' Bellman equations.  Via this method the authors show that the sample complexity scales via $K / (1 - \gamma)^4 \epsilon^2$, which is minimax optimal or 'sharp' under naive $Q$ learning based algorithms.  Lastly, the authors complement the theoretical results by a detailed discussion comparing the theoretical guarantees on the performance of their approaches to that in [61].",This paper derives sample complexity for both model-based RL algorithms and model-free RL algorithm (i.e. $Q$-learning) in the case where the underlying MDP admits a linear structure. The resulting sample complexity bounds in both cases are shown to be tight. ,"This paper considers MDPs with linear transition models. Assuming access to a generative model, under certain anchor state assumption, the authors prove the following two results:  1. A model-based approach achieves a sample complexity of $O~(K / ((1 - \gamma)^3 \epsilon^2))$ which matches the minimax optimal lower bound up to logarithm factors;  2. A model-free approach based on Q-learning achieves a sample complexity of  $O~(K / ((1 - \gamma)^3 \epsilon^2))$.",0.39436619718309857,0.07042253521126761,0.12206572769953052,0.029079159935379646,0.07431340872374798,0.25,0.13570274636510501,0.3409090909090909,0.3466666666666667,0.4090909090909091,0.6133333333333333,0.14666666666666667,0.20192307692307693,0.11673151750972761,0.18055555555555555,0.05429864253393666,0.13256484149855904,0.18487394957983194
654,SP:8fa16c85f0489747b8a56eb555f220a2260d3bb3,"The manuscript proposes a hybrid method for learning Granger causality that combines data-driven learning methods and theory-based behavioral models for the goal of modeling multi-agent behavior.   The main motivation is that current models either do not consider the data generation process behind the observed time series or if they do, they are tailored to specific animal species, thus having generalization limitations.  More specifically, a Self-Explainable Neural Network (SENN) -like model is used where scientific knowledge on multi-agent interactions is integrated. In this regard, key contributions of the proposed method include: a theory-guided loss function to regularize training and a motion and navigation function Experiments on synthetic and realistic data show the effectiveness of the proposed method.","In this paper, the authors propose a method for learning rules of interaction between individuals from　trajectory data of animals. Their method is based on the Granger causality methodology for time series data but differs from previous studies in that it introduces a new network architecture and a regularization term based on the theory of animal behavior, which enables more accurate learning. The method has been validated using synthetic datasets and then applied to real animal behavior data to gain new knowledge about animal behavior.","The proposed paper introduces a novel approach for learning the Granger causality based on augmented behavioral models. The objective of the discussed method is to extract interaction rules from real-world multi-agent trajectory data with neural networks. The main contributions of this work are a framework for learning Granger causality, methods for efficient and interpretable learning, and the decomposition of motion and navigation. The method is evaluated based on a number of experiments and based on datasets of mice, flies, birds, and bats.    ","This paper proposes a framework for learning Granger causality with the goal of finding interaction rules from trajectory data of multiple animals. The framework has an ""augmented behavior model"" based on a conceptual behavior model studied in [53]. The experiments are on a range of datasets, from synthetic to real data of mice, birds, and bats. The main baseline of comparison is GVAR [44], which this method compares against for synthetic data and on real mice data. Based on the experiments, the interaction rules found are in the form of attraction, no interaction, or repulsion between different agents.",0.16393442622950818,0.21311475409836064,0.1885245901639344,0.23529411764705882,0.29411764705882354,0.30952380952380953,0.23529411764705882,0.30952380952380953,0.23469387755102042,0.23809523809523808,0.25510204081632654,0.2653061224489796,0.19323671497584538,0.2524271844660194,0.20909090909090908,0.23668639053254437,0.273224043715847,0.2857142857142857
655,SP:9005af2967bae53554e254e396431edc3189face,"This paper is focused on training networks to solve problems via extrapolation, e.g. solving large mazes by learning to solve small mazes. In order to achieve this, two modifications to recurrent convolutional networks are proposed: a) adding a concatenating skip connection from the input to the recurrent layers that stabilizes extrapolation (Fig. 2), and b) using a modified training method and loss that encourages the network to learn computations independent of current iteration (Alg. 1). Together, these techniques lead to substantial improvements in results on the problems of computing prefix sums, solving 2D mazes and solving chess puzzles.","This paper proposes two extensions to the recently proposed recurrent 'thinking systems', in order to enable them to better generalize from training on simple problems to testing on more complex problems. The proposed extensions involve 1) giving the model access to a cure indicating the to-be-solved problem at each time step ('recall') and 2) a method intended to prevent the model from learning behaviors specific to particular iterations (so as to enable generalization to more complex problems via a larger number of iterations). When combined, these extensions enable generalization to significantly more complex problem instances across three separate task domains.","The paper proposes two modifications to recurrent neural networks that help improve generalization on three synthetic tasks. In particular, the authors implement a recall mechanism through residual connections to prevent the recurrent network from losing original input information after many iterations. In addition, they randomly apply truncated backpropagation through time to the computation graph to build an auxiliary loss, which diminishes overfitting to the number of recurrent steps. The trained network can solve significantly harder problems by performing more recurrent iterations, showing strong generalization ability. ","The paper proposes two modifications to recurrent neural networks that enable them to extrapolate to larger problems than seen during training.  The problems (generalizations) are  1) pathfinding in a maze (larger mazes),  2) binary prefix sums (larger bit strings) 3) Evaluate the best chess move given a position (harder positions).  The modifications are:   1) Add the initial problem features to every step of the recurrent computation, coined ""recall"" in the paper. 2) train on a combination of a regular loss with m recurrent iterations and with a loss with m=n+k iterations where the gradient is not tracked for the first n iterations.  The paper shows that the modified recurrent network learns an algorithm that converges to the correct result with more recurrent iterations, thus overcoming the problem of ""overthinking"" (divergence really) as coined in earlier papers.   The paper further shows that the learned convergent algorithm ""extrapolates"" to larger/harder problems. For instance, the networks are trained on 32-bit strings, and evaluated on 512-bit strings, and trained on 9x9 mazes and evaluated on up to 201x201 mazes.  The paper compares their method, ablations w.o. ""recall"" and modified loss, MLPs and MLPs with ""recall"", and show that only their proposed network learns a convergent solution that extrapolates to the larger/harder problem instances.",0.1919191919191919,0.16161616161616163,0.2727272727272727,0.19607843137254902,0.29411764705882354,0.3058823529411765,0.18627450980392157,0.18823529411764706,0.125,0.23529411764705882,0.1388888888888889,0.12037037037037036,0.18905472636815918,0.17391304347826086,0.17142857142857143,0.21390374331550802,0.18867924528301885,0.1727574750830565
656,SP:901ee3c99bec6363ff957abe93ab91503302717f,"The authors propose a model editing method that proceeds in two steps. First, they rely on off-the-shelf segmentation models and style transfer models to learn a data transformation $x \to x'$ that preserves semantic meaning (e.g., in a vehicle recognition task, they might convert a snowy road into a non-snowy road, which preserves the label of the vehicle). Second, they build on recent work by Bau et al. to use these transformed pairs $(x,  x')$ to learn an edit to the model $f$ such that, roughly speaking, $f(x)$ and $f(x')$ agree in their representations at some particular layer and above. They show that their procedure generalizes (astonishingly) well from edits learned on just a single exemplar. ","The paper presents a method to improve robustness of object classifiers to style-changes in components, termed “concepts” in the paper, which can include both background elements as well as parts of the object of interest. This is done by first localizing confounding concepts using existing instance segmentation methods, performing style-transfer on the localized concept using existing methods, and then using an existing method which performs a rank-1 update to (a layer of) network parameters to map the transformed-concept-region to the same value as the original concept-region was mapped to by the layer. Experiments illustrate that this pipeline is effective at making the network robust to style-changes for concepts identified to be potentially misleading.","The paper proposes an algorithm to change the behaviour of a trained model in the presence of some external interference in the presence of an external concept. The paper does it without using extra data in two stages. First, the algorithm utilises a rule detection framework using counterfactuals and then changes it using the technique from bau et. al. ","This paper proposes a method to directly edit the predictions of a trained neural network by (1) building “counterfactual” images by replacing objects in a scene, then (2) mapping activations from pre-transformed images to post-transformed images through the algorithm proposed in Bau et. al. The method is applied to well-known image datasets to show improvements in accuracy on images where transformed objects are present. In particular, the proposed approach generalizes better than naive baselines of fine-tuning on new data. The authors then apply the method to two real world contexts: driving scenes and typographic errors. ",0.1557377049180328,0.10655737704918032,0.13114754098360656,0.13333333333333333,0.14166666666666666,0.2711864406779661,0.15833333333333333,0.22033898305084745,0.16161616161616163,0.2711864406779661,0.1717171717171717,0.16161616161616163,0.15702479338842976,0.143646408839779,0.14479638009049775,0.1787709497206704,0.1552511415525114,0.20253164556962028
657,SP:902411db5807a16fe6ecb9dbeb0d90d0e6508353,"This paper gives a discretization analysis of the mirror-Langevin Monte Carlo algorithm, which is the analogue of mirror descent for MCMC. The prior works studying discretization of mirror-Langevin are as follows:  [1] Zhang et al., who studied the Euler-Maruyama discretization. They used stochastic calculus coupling techniques in the vein of works by e.g. Dalalyan.  [2] Ahn and Chewi, who introduced and studied an alternate discretization which simulates the diffusion step more faithfully. Their analysis uses Wasserstein calculus in the vein of Durmus, Majewski, and Miasojedow.  A key difference between these two works is that the result of [1] has a non-vanishing bias (even with vanishing step size), whereas the discretization of [2] does not.  This paper studies both the Euler-Maruyama and Ahn-Chewi discretizations, and introduces a third one which combines the Ahn-Chewi discretization with a proximal step to obtain better guarantees. The results obtained have non-vanishing bias for the Euler-Maruyama discretization, and vanishing bias for the latter two discretizations, which is in line with previous work. The techniques are inspired by the discretization analyses of Wibisono, who is inspired by the interpretation of sampling as composite optimization in Wasserstein space. Moreover, this paper emphasizes the use of a mirror-log-Sobolev inequality as the condition ensuring rates of convergence, which allows for treating examples of non-convex potentials (which are not handled by the prior two works).  The main contributions can be summarized thus: (1) by obtaining a similar dichotomy between the bias of Euler-Maruyama vs. Ahn-Chewi, it sheds further light on the effect of discretization of the diffusion in the presence of non-isotropic diffusions; (2) it shows how to adapt a third type of discretization analysis to the mirror-Langevin setting; (3) it introduces a proximal mirror-Langevin algorithm which had previously not been considered and gives guarantees.","This paper studies discretized mirror Langevin diffusions under the assumption of mirror-log Sobolev inequalities as well as relative smoothness assumptions. In particular, they show that a new backward method obtains perhaps the best dependence in terms of finite sample guarantees in this setting. On the other hand, Euler-Maruyama discretization has an irreducible bias and a different forward discretization has an exponential dependence on the diameter of the mirror set.","In this work the authors study the convergence of mirror descent langevin dynamics -- Langevin dynamics which have been transformed appropriately to satisfy certain constraints or evolve according to a specific non-euclidean geometry.   The dynamics can be characterised in various forms, including as a riemannian manifold overdamped langevin process with a specific choice of metric tensor -- but also as an SDE in the dual (unconstrained space) which is projected onto the original state space.  The paper considers the convergence to equilibrium of the mirror descent Langevin dynamics -- both for the original continuous time process, but also for various discretisations of the scheme.  This is done under the assumption of a Logarithmic Sobolev Inequality, thus generalising common convergence analysis which typically require strong-log-concavity assumptions.  It is demonstrated that the choice of discretisation plays a key role in achieving consistency under vanishing step-size for constraints which involve non-smooth contributions (e.g. constraint to lie within a box, etc), ultimately presenting a backward discretisation scheme which is far more stable to sudden changes in the geometry underlying the constraint.  These results are illustrated through a straightforward numerical example.    ",The paper under review mainly studies different discretization schemes for mirror Langevin dynamics. The main assumption is the non-asymptotic convergence/long time convergence behavior to the target invariant measure. The main feature of this type Langevin dynamic is that they have non-gradient drift and variable diffusion matrix. The author try to covey the importance of the underlying geometry which is introduced by the variable diffusion matrix. ,0.08333333333333333,0.13782051282051283,0.07371794871794872,0.23943661971830985,0.18309859154929578,0.10582010582010581,0.36619718309859156,0.2275132275132275,0.3382352941176471,0.08994708994708994,0.19117647058823528,0.29411764705882354,0.13577023498694518,0.17165668662674652,0.12105263157894737,0.13076923076923078,0.18705035971223022,0.1556420233463035
658,SP:90300ee76f9660901e534c7dcf3598e2ebee0acb,"This paper takes online learning algorithms designed for the Euclidean space (algorithms for online convex optimization with convex or strongly convex functions in the full information case. Or the convex case with bandit feedback) and generalizes them so they work in Hadamard manifolds with the corresponding notions of geodesic convexity. In the bandit case the manifolds are also assumed to be homogeneous.   A lower bound of \Omega(\sqrt{T}) on the expected regret is provided.   The regret bounds are greater than those in the Euclidean counterparts and in the bandit case (the main contribution) they are greater by a factor that depends on the dimension and that is not given in the paper ( its value is not worked out). This fact makes the claim in the abstract ""All the obtained regret bounds match the corresponding results in Euclidean spaces."" be false and misleading, in my opinion.",The paper proposes Riemannian variations of online optimization problems. Two algorithms (one in the general case and one for bandits) are proposed for (strongly) geodesically convex functions. Error bounds are derived under various regularity conditions on the manifold. The algorithm is evaluated numerically on computing Karcher means.,"The paper considers the problem of online optimization over manifolds of negative curvature (Hadamard) in the full-information and bandit setting, providing upper bounds for the regret which match the Euclidean case. Prior work has considered the same problem in a zero-order oracle regime and provided only asymptotic results. The theoretical analysis is an adaptation of [40] in the convex case and [18] in the strongly-convex case, using the law of cosines for geodesic triangles on Hadamard manifolds developed in [37] and new geometric results based on the extra assumption of homogeneity. Experimental results in the simple problem of Karcher mean estimation are also included. ","In this paper, the authors derived Riemannian online learning algorithms with first-order derivative information on Hadamard manifolds, with various problem settings (including full information geodesically-convex, full information geodesically-strongly-convex and bandit feedback). In each setting, the authors proved the regret of their proposed algorithms matched their vector space counterparts (up to Riemannian geometric constants). In deriving their proofs, the authors introduced new analytic tools and insights to Riemannian optimization research, which could facilitate further developments in this area.",0.0958904109589041,0.1917808219178082,0.1506849315068493,0.3191489361702128,0.1702127659574468,0.14953271028037382,0.2978723404255319,0.2616822429906542,0.2716049382716049,0.14018691588785046,0.09876543209876543,0.19753086419753085,0.14507772020725387,0.22134387351778656,0.19383259911894274,0.1948051948051948,0.12499999999999997,0.17021276595744678
659,SP:9070183afc9422af7dcef84aea785cb59bbba3ae,"This paper develops new stability bounds for SGD. The main difference from the existing studies is that they consider stability bounds for normalized loss functions where the parameters are normalized to have a norm of $1$. This paper considers both convex and nonconvex cases. For the convex case, the authors develop uniform stability bounds and high-probability bounds. For the nonconvex case, the authors develop on-average stability bounds for neural networks. Experimental results are also given.",This paper considers the generalization bound for stochastic gradient descent. The authors leverage normalized loss function to analyze the stability of SGD algorithms which further yields the generalization bound. They provide the on-average stability result for non-convex optimization under the ReLU neural network setting. The theoretical results deepen our understanding of the performance of the SGD algorithm and an experiment is provided to illustrate theoretical findings.  ,"This paper conducted a stability analysis of Stochastic Gradient Descent (SGD) for empirical risk minimization induced by the so-called normalized loss function. Here, the normalization is taken with respect to parameters involved in an individual loss; see (4) for the definition of the normalized loss function. The paper should be regarded as a theoretical paper. The main results are stability bounds of SGD for convex and nonconvex ERM schemes.   ","This paper considers the problem of understanding the generalization of SGD using the stability framework. The well-known result in this line of work is the paper by Hardt'16. In Hardt'16, the stability is measured using the difference between the ""actual"" weights of two copy of SGD which differ in a single data point. The main observation by the authors in this paper is that in many cases, the loss function is invariant to the scaling of weights. Then, they reformulate the stability analysis using the ""normalized loss function"" which is defined by  l^alpha(w,z) = loss(alpha*w/||w||,z) where alpha is a constant.  Their main results are the new stability analysis for this new notion for convex and non-convex settings. Specifically, for the convex case the analysis is very similar to the Hardt paper. For the non-convex the authors define a new measure for generalization ""zeta"" in Theorem 4 which governs the stability.",0.22077922077922077,0.23376623376623376,0.36363636363636365,0.27941176470588236,0.3235294117647059,0.35714285714285715,0.25,0.2571428571428571,0.17391304347826086,0.2714285714285714,0.13664596273291926,0.15527950310559005,0.23448275862068965,0.24489795918367346,0.2352941176470588,0.27536231884057966,0.19213973799126638,0.21645021645021642
660,SP:90f53b62d904d9cd6e74fa0208daade5e66333eb," The paper provides an analytical explanation for the over-squashing and GNN bottleneck phenomena. While Alon & Yahav (2020) demonstrated the over-squashing phenomenon empirically and provided mainly intuitions as to why it happens, this paper performs a deeper analysis and connects over-squashing to combinatorial curvatures. The paper shows that negatively curved edges are responsible for over-squashing. This is important because it allows to measure over-squashing, and pinpoint specific edges that are responsible for it in a given graph.  Further, the paper proposes a method (called SDRF) to re-wire the graph based on these insights and shows empirically how this method alleviates over-squashing. ",This paper analyzes oversquashing in GNN using geometric methods. The paper proposes a novel rewiring method based on negative curvature to construct graphs that are less susceptible to oversquashing. Though the theoretical contribution is strong the empirical evidence is not strong. Overall a good paper. ,"The authors propose a new graph rewiring approach that utilizes a discrete notion of Ricci curvature to mitigate over-squashing. This is motivated by a link between negatively curved edges and graph bottlenecks. The paper has a theoretical focus, but also provides a set of validation experiments to demonstrate the proposed approach.","In this paper, the authors work on the over-squashing effect that has been recently observed in the GCN literature, namely the effect that, in a message-passing paradigm and for a learning problem that necessitates long-range interaction between distant nodes, the existence of bottlenecks in the graph (for instance an edge e with very large betweenness centrality) will intuitively distort the information that needs to travel from distant nodes and thus hinder the GCN's performance.  One natural direction of research is to add edges (and sometimes remove others in order to keep the complexity under control) to alleviate the bottlenecks. This rewiring process may be done in different ways and the authors suggest one particular algorithm that they compare with state-of-the-art.  In particular, the contributions are: - a precise (even if somewhat arbitrary) definition of an over-squashing measure between two nodes $i$ and $s$ that is simply the Jacobian of the node representation at $i$ with respect to the entry $x_s$. The smaller this measure, the less $i$ ""feels"" $s$, the larger the over-squashing effect. The goal of the paper is to find a way to understand how to alleviate the bottlenecks responsible for this effect  - to this end, the authors suggest a new definition of Ricci-like curvature defined over the edges of the graphs. This new definition has one main property which is theorem 3: it lower bounds the Ollivier curvature. This then enables to obtain Cor. 4, and finally the main result Thm 5, that states, in a nutshell, that negatively curved edges are the ones causing bottlenecks.  - a concrete rewiring algorithm is suggested, in which two steps are repeated until convergence or max iteration is reached: a first stochastic step is performed where an edge is added to alleviate the edge that has minimal curvature, before a deterministic step where the edge with maximal curvature (typically those in cliques) is removed if it is larger than a threshold  - a tentative analysis, in the form of Thm 8, as well as experiments show the authors' method performs as well or outperforms other rewiring methods (they mainly compare with DIGL) while keeping the number of total edges after rewiring under control",0.102803738317757,0.14018691588785046,0.2897196261682243,0.17777777777777778,0.3333333333333333,0.4807692307692308,0.24444444444444444,0.28846153846153844,0.08355795148247978,0.15384615384615385,0.04043126684636118,0.0673854447439353,0.14473684210526316,0.18867924528301885,0.12970711297071127,0.16494845360824742,0.07211538461538462,0.1182033096926714
661,SP:913e32bb7ad5d78485748d91d97ccb344adbcafb,This paper gives low-regret algorithms for the causal bandits problem when the graph structure is unknown. The main contribution is in combining use of some recent results on efficient causal structure discovery with traditional UCB approaches. Some minimax results are given to demonstrate the necessity of the assumptions made. ,"Authors propose using the existing adaptive causal discovery algorithms to identify the reward variable in a causal graph efficiently and then run the existing bandit algorithms on this very small space of actions (interventions). They use the ideas from central node algorithm of Kristjan et al.: Identify a central node, either eliminate ~half the graph or find the subtree including the target node. They then have to check each child separately since either one of them can be the target node - which also serves as a method to check if central node itself is the target node. This takes ~dlogn interventions which can easily be converted to sample-complexity with the resorted assumptions.  ","This paper investigates the problem of 'causal bandits,’ a sequential decision problem where the agent can intervene on a causal DAG (setting the value of a variable) and receives a reward associated with an unknown variable’s value.  Unlike prior works, in this paper, only partial prior knowledge of the structure is known (essential graph).  In addition to the reward, the agent also observes the realization of all the variables; that information with the essential graph helps inform the agent of where in the graph the reward-generating variable is.   Although the agent need not learn the full causal structure for the task, prior work on causal structure learning provides an algorithmic foundation for approaching the causal bandits problem.  Regret bounds for the algorithms are provided for trees, forests, and then general graphs (which satisfy certain topological constraints).  Lower bounds are also provided.  ","The authors present a novel approach to tackle so-called causal bandit problems where the structure of the underlying graph is unknown. The algorithm is based on a ‘central node’ approach to find the direct parent of the reward as soon as possible. Under certain restrictions various regret bounds are derived for this algorithm that show the asymptotic behaviour outperforms standard multi-arm bandits by a significant margin, though no experimental verification is provided. ",0.22,0.34,0.26,0.1592920353982301,0.1504424778761062,0.1258741258741259,0.09734513274336283,0.11888111888111888,0.17567567567567569,0.1258741258741259,0.22972972972972974,0.24324324324324326,0.13496932515337423,0.17616580310880828,0.20967741935483875,0.140625,0.18181818181818185,0.16589861751152077
662,SP:91502097fb18778de12dc35cf33578998a5e5d18,"The paper proposes a method for representing vector graphic fonts with multi-implicit functions. The main goal is to allow the reconstruction of fonts at any resolution without loss of discontinuities such as corners and edges. Instead of representing a font shape as one SDF and then optimising the training process against this SDF, the paper proposes to represent a shape using multi-SDFs, and then differentiably rendering a shape against this multi-SDF representation and optimizing the input shape against this rendering. This allows the method to bypass the need for collecting a vast amount of vector graphics data for training.  The paper shows that this kind of multi-SDF representation helps in preserving discontinuities at corners and edges.","This paper deals with the problem of glyph generation in fontset design. Whereas previous works used raster-based or vector-based representations (and combinations of the two) to learn generative models for fonts, this paper proposes the use of what they call “multi-implicit representations”. In the proposed method, implicit functions (in the style of “NeRF”) are learned to represent subregions of a glyph by combining multiple simple shapes (eg two half-planes to create a corner). These subregion-implicit models are learned with local supervision, and a multi-Signed Distance Field model learns to compose them into the global glyph.  ","This paper aims to learn a representation for rasterized fonts that allow for re-scaling, reconstruction, interpolation, and generation by preserving local features such as sharp corners or smooth curves. The proposed method composes multiple implicit representations (i.e., signed distance fields) to model complex details observed in neutral and stylistic fonts. The core idea is to represent corners as an intersection of multiple curves, enabling the construction of sharp corners in arbitrary resolutions. This assumption is further supported by a local supervision objective for the corners. In the experiments, the authors present that their model is able to preserve sharp corners. The proposed model outperforms baselines both qualitatively and quantitatively.","This paper proposes multi-implicit neural representation for fonts given the observation that it is hard to represent complex fonts with single neural representation. The author introduces a local supervision method that can train multi-implicit functions without knowing ground truth channels. Experimental results in various settings, e.g. reconstruction, interpolation, and generation, show that it outperformed the previous methods such as rasterized representation VAE and a couple of neural vectorized representations.",0.15,0.175,0.13333333333333333,0.21782178217821782,0.12871287128712872,0.15315315315315314,0.1782178217821782,0.1891891891891892,0.2222222222222222,0.1981981981981982,0.18055555555555555,0.2361111111111111,0.16289592760180996,0.18181818181818182,0.16666666666666669,0.20754716981132076,0.15028901734104044,0.18579234972677594
663,SP:91647cb52a6ef0688001e9220e1f0a0a27505381,"This paper aims to refine the 3D human poses. To this end, the authors equip neural radiance fields with human skeletons, where they design a skeleton-relative embedding based on the human skeleton. By training the A-NeRF on the video, the human poses are optimized to fit the observations.",This submission presents a way for adding articulation to neural radiance fields. The main contribution is an encoding of the query points and view directions relative to a skeleton. The proposed formulation is trained per subject on a video sequence and optimizes for the 3D skeleton configuration in each frame and a global MLP which represents the NeRF. This defines the motion tracking throughout the input sequence and the recovered model can be reposed by rendering frames with new joint locations.,The paper presents A-Nerf which is a generative neural body model capable of rendering a human under novel viewpoints and poses. A-Nerf is an extension of Nerf to capture dynamic human bodies. It's achieved by conditioning Nerf on the skeletal body pose information. The paper demonstrates that naively conditioning Nerf on the body pose information does not lead to optimal performance and therefore proposes a set of relative encodings of the pose information that results in significantly better renderings without the need of a full-body mesh model. The experiments are performed for motion capture as well as novel view synthesis where the proposed method demonstrates superior performance as compared to existing methods. ,"This paper presents modelling articulated neural radiance fields for human dynamics from monocular videos. To model NeRF for dynamic human, it encodes 3d spatial points and ray directions w.r.t the articulated skeleton poses, which are fine-tuned together when training the NeRF field. Various skeleton-based feature encoding are experimented in consideration of both performance and memory. ",0.3,0.28,0.24,0.19753086419753085,0.1728395061728395,0.1206896551724138,0.18518518518518517,0.1206896551724138,0.2033898305084746,0.13793103448275862,0.23728813559322035,0.23728813559322035,0.22900763358778625,0.1686746987951807,0.22018348623853212,0.16243654822335024,0.19999999999999996,0.16000000000000003
664,SP:91648a45a51e9eedc1e92f0944a110b32b7209e3,"The paper addresses the problem of generating 3D molecular geometries. The proposed G-SphereNet takes SphereNet as the backbone to generate 3D molecular geometries from scratch. The generation is done in a step-by-step fashion, with each step generating a (distance, angle, torsion) triplet, conditioning on features extracted by an attention mechanism.","The paper proposes a new way to directly generate 3D molecular geometries from scratch. They output the atoms one by one, including the type of the atom and the related position from one chosen focal atom. They use a SphereNet (Liu et al. 2021d) - a spherical message-passing network and an additional multi-head attention network (Vaswani et al. 2017) to process the existed information in the partial generation results. They have shown in 2 different tasks that they are better than previous methods.","The paper introduces a new generative model for 3D molecular geometry, taking the form of a graph embedded in 3D where each node is an atom, and each edge is a bond. Differently from existing methods, the proposed pipeline attempts to generate the 3D molecule from scratch, instead of starting, e.g., from a 2D molecular graph as other methods do. The pipeline is sequential and is based on autoregressive flow models, which have been previously used for the generation of 2D molecular graphs, but not for the present task. The paper convincingly shows that the proposed model works better than competing approaches for the same task.","The paper focuses on a generative model for 3D molecules that generates full 3D molecular gemoetry, instead of generating a molecular graph and then generating the 3D molecular geometry conditional on the graph. The method using an autoregressive flow model, which has been applied to molecular graph generation but not full geometry. This enables some new applications such as molecular design to target a geometry-dependent objective function.  ",0.2641509433962264,0.2830188679245283,0.22641509433962265,0.2261904761904762,0.13095238095238096,0.22429906542056074,0.16666666666666666,0.14018691588785046,0.17647058823529413,0.17757009345794392,0.16176470588235295,0.35294117647058826,0.2043795620437956,0.18749999999999997,0.19834710743801656,0.19895287958115185,0.14473684210526316,0.2742857142857143
665,SP:91a25f7a91b38d2ba664fd3154598002341e3dc1,"This paper uses transliteration to build better multilingual language models. This is particularly useful for building downstream NLP models for low resource languages. The idea behind this is that script divergences between languages avoid pretrained models to pool resources across language, leading to poor performances on these scripted low-resource languages. Transliteration, i.e. using a common script for all the languages can help solve this issue.   For experiments, the authors focus on Indo-Aryan languages, where they first transliterate all the documents to a common script. Then, the ALBERT model is pre-trained on the transliterated corpora of these languages extracted from the OSCAR corpus. For downstream tasks (section title prediction, news category classification, NER and genre/sentiment classification), they perform fine tuning for each tasks independently, in a multilingual manner when possible. The transliterated models outperform the baseline systems (with no transliteration) on most tasks.  ",This paper hypothesizes that transliterating all the languages to the same script can improve the performance of multilingual language models (MLLM). Experiments are designed to validate this hypothesis by comparing the performance of ALBERT and transliteration-based ALBERT (“XLM-Indic”). The authors find that XLM-Indic outperforms ALBERT across many downstream tasks such as text classification and QA. The main contribution is that the authors empirically validate the effectiveness of introducing transliteration technique on improving the overall performance of MLLMs. The improvement is obvious especially for underrepresented languages.,"This paper talks about whether uniform script representation can alleviate the dominance of language-specific corpus size and therefore benefit cross-lingual language modeling. The authors train two ALBERTs on non-transliterated and transliterated (ISO-15919) corpus (filtered and normalized version of OSCAR). Those two models are further fine-tuned for four classification/sequence labeling tasks in IndicGLUE. The result shows that XLM-Indic achieves better or comparable performance in most settings. They also test the model’s zero shot capability in cloze style QA, and XLM-Indic also achieves better or comparable performance. By measuring lexical fertility, they claim that the success of shared scripts comes from that low-resource languages borrow better representation of shared lexical from other languages where those lexical are frequently used.","In this paper, the authors show explore the following question: is it beneficial to represent different languages in the same script for training pre-training LMs. They explore this question for related languages which share similar scripts (making it easy to transliterate between scripts). The study has been performed on Indo-Aryan languages, which is an important representative of this scenario. The comparison between single-script and multi-script ALBERT language models show the single script model outperforms a multiscript model. The analysis shows that the single-script model vocabularies have a higher level of subword sharing.",0.1360544217687075,0.10884353741496598,0.14965986394557823,0.22727272727272727,0.2159090909090909,0.11023622047244094,0.22727272727272727,0.12598425196850394,0.2268041237113402,0.15748031496062992,0.1958762886597938,0.14432989690721648,0.1702127659574468,0.11678832116788321,0.180327868852459,0.18604651162790697,0.2054054054054054,0.125
666,SP:91aa079afee2737fccc3d24deb5d250ae4b86839,"The paper addresses the task of hot-refresh model upgrade (ie. the task of deploying a new model into production while the gallery features from an older are still being re-indexed) in retrieval systems and studies the problem of model regression that happens when the similarity of new-feature-to-old-feature positive pairs is lower than new-feature-to-new-feature negative pairs. To solve this the authors propose a compatible training method that takes these flips into account, encouraging the new-to-old pair positives to be more similar than both new-to-old and new-to-new negative pairs (this is nicely illustrated in Figure 2). The paper also introduces an uncertainty-based backfilling (ie. the process of re-indexing the old gallery features with the new model) strategy that tries to follow a poor-first policy using the probability distribution of the training classes yielded by the new classifier.",This paper tackles the problem of model upgrades in large-scale training of image retrieval models. It proposes a new framework of “hot-refresh” updates with two main techniques:  1) by alleviating “negative flip” by a regression-free regularization term in the loss and  2) by prioritizing gallery images with the highest uncertainties during the backfilling process.  The results suggest improvement across three major retrieval benchmarks compared against baseline model-upgrade methods.,"The paper proposes a new technique for updating image retrieval models while not requiring full backfill of the index in order to allow deployment. This has the potential to make model upgrades more practical for real-world large-scale applications. The paper is the first to study the impact of such “hot-refresh” techniques to the image retrieval domain. The authors make observations about the reason behind negative flips, which motivate their newly-proposed loss function / training technique. The paper also proposes an uncertainty-based backfilling method to decide the ordering in which images in the index are backfilled. Experiments are conducted on standard image retrieval datasets.","The paper presents a method to learn compatible features with low negative flips rate. The paper also introduces a resampling strategy to gradually replace the gallery to ameliorate the performance. Resampling is based on feature uncertainty. The less discriminative a feature is, the sooner it is replaced.",0.11688311688311688,0.17532467532467533,0.1038961038961039,0.2222222222222222,0.1388888888888889,0.12149532710280374,0.25,0.2523364485981308,0.3404255319148936,0.14953271028037382,0.2127659574468085,0.2765957446808511,0.1592920353982301,0.20689655172413793,0.15920398009950248,0.1787709497206704,0.1680672268907563,0.16883116883116883
667,SP:91f70200c26770d11db1ef84ff1dbd49aa4a7279,The paper introduces a novel method for multi-view scene reconstruction and image synthesis. The authors combine volumetric and surface based neural rendering techniques to obtain highly accurate surface models without need for object mask annotation. The paper presents theoretical framework motivating the SDF-based density field parametrization and the adaptive sampling approach. The validation shows that the reconstructed shapes are surpassing the state of the art.,"NeRF and follow-up papers have led to impressive improvements in view synthesis when trained from multiple views of a single scene. This is achieved through volumetric rendering of an opacity field and a decoupled reflectance network. A limitation of NeRF is that the opacity field might not be faithful to the true geometry, as long as the rendered RGB images reproduce the correct appearance.  The proposed work builds on ideas developed in the NeRF literature and adapts them to 3D surface reconstruction. Contributions include deriving opacity from a truncated signed distance function, which explicity models the inside-outside topology of the surface. The validity of the signed distance function is enforced though an Eikonal loss term. The authors present a novel sampling technique for volumetric rendering, based on an approximation of the current opacity with bounded error.  ","The paper proposed a method to recover both geometry and appearance of objects. They  achieve this by combing both SDF representations and volume rendering. Given a set of  multi-view images, they use a network to predict the SDF and radiance filed of an arbitrary point, which  is mapped to the volume density using a bell-shaped analytic function. Then they perform  volume rendering to predict the color for the pixel. They minimize the color difference as  well as an Eikonal loss to train the networks. They also propose a sampling scheme to  adaptively sample points along the ray based on the derived theoretical bound. The results  show that the proposed method can generate both accurate 3D mesh and faithful view-dependent  appearance. ","This paper presents a neural approach based on NeRF, for the purpose of learning a 3D representation of a shape from captured images. Instead of allowing volume density to be freely allocated along rays, as in NeRF, this paper proposes concentrating volume density around a single surface. To do this, the method represents the shape as a neural SDF, and defines a volumetric density around the zero level set of the SDF using the CDF of the Laplace distribution. This choice enables the paper to derive a bound on the estimated opacity for any segment along a ray, and this is used to draw samples such that the error is limited to some epsilon. The paper experimentally shows that the resulting recovered surfaces are closer to the ground-truth geometry than surfaces extracted from level sets of NeRF's volumetric geometry, as well as meshes produced by COLMAP. Additionally, the paper experimentally shows that the proposed method is able to render novel views that are close in quality to those rendered by NeRF.",0.22388059701492538,0.2835820895522388,0.29850746268656714,0.18115942028985507,0.17391304347826086,0.22764227642276422,0.10869565217391304,0.15447154471544716,0.11560693641618497,0.2032520325203252,0.13872832369942195,0.16184971098265896,0.14634146341463414,0.19999999999999998,0.16666666666666666,0.19157088122605365,0.15434083601286172,0.18918918918918917
668,SP:926fd1d7c68b375aed2b47acf0d66517e0f6c55c,"This is a paper that focuses on causal discovery especially focusing on the causal relation among discrete latent variables.  The basic idea is that by mixture oracles, one can locate the latent variables and recovery their distribution. The main contribution is the model identification theory, including bipartite graph, latent distribution $P(H)$.","This paper considers learning discrete latent variables (the causes) and their dependencies, given observed variables. It is assumed that there is a DAG between the hidden nodes and a bipartite graph with edges from the hidden nodes to the obseved nodes, and no edges between the observed nodes. The idea is that a distribution of each subset of observed variables is described by a mixture model where each component corresponds to an assignment of values to  hidden variables which are the causes of these observed variables. The paper then presents proofs that from such mixture models (leaned in practice by k-means in the article), the distribution and the graphs related to the hidden nodes can be recovered.",The authors identify a set of assumptions under which it is possible to recover the joint distribution over the latent variables. My understanding is that they extend NMF to use tensor factorization and under suitable assumptions show that the tensor factorization solution is the unique solution - up to relabeling. I however have several issues parsing some of the results. I would appreciate feedback from the authors to help clarify the points below.  ,"This paper proposes a method for recovering a distribution over a set of discrete latent random variables, given observations from a set of observed children of these variables. The method additionally recovers the set of edges between latent and observed variables. The method is written in terms of a “mixture oracle”, i.e., a subroutine which returns mixture weights and components for the distribution over any subset of observed variables. Empirical results show that the method is effective at recovering the underlying causal graph.",0.3076923076923077,0.19230769230769232,0.28846153846153844,0.13559322033898305,0.22033898305084745,0.19444444444444445,0.13559322033898305,0.1388888888888889,0.17857142857142858,0.2222222222222222,0.30952380952380953,0.16666666666666666,0.18823529411764706,0.16129032258064516,0.22058823529411764,0.16842105263157897,0.25742574257425743,0.1794871794871795
669,SP:92e0350537f02ba65dec39adcbeb5f9e8ddf8349,"The paper presents a framework to evaluate the similarity of intermediate representations from two neural networks. Given two neural networks with the same architecture but trained with different weight initializations, the framework uses a stitching layer to search for a transformation between the intermediate representations of the two networks, such that the model performance on a given task is preserved. The paper further proposed two approaches to obtain such transformations. Under this framework, the author empirically observed that representations from a given layer of two convolutional networks can be matched.","The paper develops and applies a set of methods to determine the functional equivalence of architecturally-identical neural networks trained with different initializations. These methods involve taking the activations from a given layer l in one network, inputting them into a ""stitching layer"" (either a linear projection or a 1x1 convolutional layer), then taking the activations from the stitching layer and inputting them into layer l+1 in a second network. The stitching layer is optimized either to minimize the models' loss on a task, or to minimize the distance between its own outputs and the activations of the second network. The authors find that stitching layers can typically be learned that have minimal impact on model accuracy and loss. They also demonstrate some results that they claim undermine the utility of CKA, a popular representational similarity metric.","**EDIT: I have increased my score from 4 to 6 based on the authors' responses and their proposed edits**  The submission considers the representational similarity between two neural networks by fitting linear transforms between corresponding layers. The authors investigate ""direct mapping"", where the linear transformation is learned simply based on the layer activations, and ""task loss matching"", where the linear transformation is fit to optimize task performance of the ""Frankenstein"" network (layers 1–n from model 1, layers n–end from Model 2), when holding all other network parameters fixed. They observe that a single linear 'stitching layer' can successfully map between corresponding layers of two networks of identical architecture and training, differing only in their random initialization. The performance of the Frankenstein network depends on the architecture, the fitting method (direct fit vs task loss matching), and the layer at which the stitching is performed. When tested on Inception, there is no difference between layers or fitting method. The dependence on fitting method and layer are most pronounced for ResNet-20. They compare their results to Centered Kernel Alignment to highlight that low CKA similarity can be associated with high classification accuracy. Additional experiments attempt to characterize the stitching layers.","The paper proposes a new tool using a stitching layer to analyze the representational similarity between two deep neural networks (DNN). The idea behind the stitching layer is that it connects one frozen network to another via a parametrized layer that is optimized using the soft labels obtained from one of the frozen networks. The intuition is that if representations of two different networks are highly similar the final performance obtained using the stitching layer should be close enough to the original model's performance.     The key experiments demonstrate the following: 1. Using the stitching layer DNNs with the same architecture but different initialization can be matched almost perfectly 2. Standard similarity measures in the literature e.g. Centered Kernel Alignment(CKA) are not indicative of final task performance using stitching layer. Even with a low value of CKA, high performance can be obtained on the target task using stitching layers. ",0.3111111111111111,0.2777777777777778,0.32222222222222224,0.2246376811594203,0.21739130434782608,0.1890547263681592,0.2028985507246377,0.12437810945273632,0.19205298013245034,0.15422885572139303,0.1986754966887417,0.25165562913907286,0.24561403508771928,0.1718213058419244,0.24066390041493774,0.18289085545722716,0.20761245674740483,0.2159090909090909
670,SP:932cd5ede4f01e1eeea88226ae7eb1f274cc6f73,"This paper proposes a compression scheme for federated learning built upon previous relative entropy encoding works. It then proves that with some small modification (clipping the model updates), the algorithm is inherently differentially private. Empirical evaluation shows that the proposed method can achieve much more communication reduction at the cost of accuracy degradation.","This paper studies differentially private algorithms in federated learning, and proposes to take advantage of the randomness in Relative Entropy Coding (REC) to achieve good privacy-utility trade offs while significantly saving the communication costs. On four benchmark datasets (MNIST, FEMNIST, Shakespeare, Stack Overflow linear regression), under same privacy epsilon, the proposed method can save communication for ~(4000, 20, 800, 100)  times with accuracy loss ~(15, 6, 10, 1) compared to DP-FedAvg.      ======= after rebuttal ============ I appreciate all the clarification and improvement on the draft. This paper is a borderline to me for the remaining concerns.  I want to be more specific here on ""apples-to-apples"" comparison. There are three things considered: differential privacy, communication and accuracy. It would be useful to show how this method can help achieve a reasonably good metric by tuning the other two metrics. Only working in the low accuracy regime sounds like a big limitation, and did not compare with other stronger compression method further weaken the claim (as also mentioned by other reviewers).  The authors mention secure aggregation in their rebuttal. IIUC, secure aggregation is future work in this draft. It is not very convincing.  That being said, The idea seems to be interesting, and I will raise the score from borderline reject to borderline accept.  ",The paper proposes a differentially private and communication-efficient method to aggregate the client updates in federated learning. The method is based on a recently proposed compression technique relative entropy coding. The authors further modify this technique to satisfy differential privacy guarantees and perform various experiments to back their claims.,"This paper introduces a compression and privatization technique to federated learning based on Relative Entropy Coding (REC). For each round, client $s$ aims to privatize its local model update $w_s$ by releasing $\Delta_s = w_s + N(0, \sigma I)$. However, instead of directly adding Gaussian noise, the client first picks $K$ random vectors from a prior distribution (which is independent of $w_s$) with shared randomness and then performs importance sampling according to the law of $w_s + N(0, \sigma I)$. It can be shown that as long as $K$ large enough (i.e., when $\log_2 K$ is much larger than the Renyi divergence of the prior and the target distributions), the sample obtained from importance sampling has a law close to the target distribution (i.e. the Gaussian perturbation) and thus can potentially preserve privacy. Moreover, the communication needed is $\log_2 K$ bits per round per client. By accounting the privacy loss over $T$ rounds, the authors characterize the overall privacy guarantees, which have a clean form and connect to the communication budgets nicely.",0.3018867924528302,0.16981132075471697,0.3018867924528302,0.08878504672897196,0.13551401869158877,0.34,0.07476635514018691,0.18,0.08888888888888889,0.38,0.16111111111111112,0.09444444444444444,0.11985018726591758,0.17475728155339804,0.13733905579399142,0.14393939393939395,0.14720812182741116,0.14782608695652172
671,SP:935610278d4f64f29ebfd0b5c2859a1d01af0dec,"This paper focuses on the infinite-horizon average-reward setting and develops the non-asymptotic upper bound for TD($\lambda$) with linear function approximation and tabular Q-learning. Its upper bounds match the best-known results on the discounted-reward MDP for $\epsilon$-accuracy. Its technique used to deal with the non-uniqueness of the limit point seems new, and its construction of Lyapunov function from noticing the connection between the span seminorm and the infimal convolution is a non-trivial observation. ","This paper studies the average reward TD-learning and Q-learning. In detail, the authors analyze the TD-$\lambda$ learning with linear function approximation. A finite sample analysis is given for the constant step size and diminishing step size. They also provide a finite-sample analysis of Q-learning in a tabular setting. The main contribution in this paper is how to deal with the constant shift in Bellman equality. In detail, in the TD-learning part, their analysis does not need to assume the uniqueness of the limit point of TD-$\lambda$. In the Q-learning part, they build the contraction based on the distance between two function classes to consider the constant shift.","This paper focuses on the theoretical analysis (especially on the non-asymptotic sample efficiency) of two algorithms under the average-reward setting: (1) TD($\lambda$) with linear function approximation under Markovian observation noise (2) tabular Q-learning in the synchronous setting.  Previous works have obtained similar finite-sample guarantees under the discounted setting. However, this work focuses on the average-reward setting where the Bellman equation is known to have multiple fixed points. For the TD($\lambda$) analysis, the authors work in a subspace to get the unique solution of the projected Bellman equation. For the Q-Learning case, the Bellman operator is a contraction under the span seminorm.","The authors provide a finite sample analysis for average reward TD with linear function approximation. Importantly, the authors succeeded in eliminating a commonly used assumption. The authors also provide a J-step synchronous average reward Q-learning and a corresponding finite sample analysis. One central idea to the success of the paper is to work on equivalent classes directly to deal with the challenge that differential Bellman equation has multiple solutions.",0.36585365853658536,0.36585365853658536,0.1951219512195122,0.2782608695652174,0.21739130434782608,0.21100917431192662,0.2608695652173913,0.27522935779816515,0.22535211267605634,0.29357798165137616,0.352112676056338,0.323943661971831,0.30456852791878175,0.31413612565445026,0.20915032679738563,0.28571428571428575,0.26881720430107525,0.25555555555555554
672,SP:93ae0c577f705d2f15eb2114a6eac5555f55eeb3,"The authors propose a new method called ContraLord for the task of representation disentanglement for generative models. Their method extends LORD, a latent optimization method for improving content embeddings, and OverLORD which performs disentanglement on embeddings, by incorporating contrastive learning. Specifically, their method includes content-level and residual-level contrastive loss that leverage negative examples in a batch.  Their experimental results show promising results on standard datasets like CIFAR and ImageNet in terms of classification performance and disentanglement. The authors also included interesting ablations to showcase the efficacy of each loss term like the residual-level contrastive loss.","The paper proposes a new two-step approach for learning disentangled representation. In the first step, a generator is learned to map latent embeddings to reconstruct images. Similar to LORD, the embedding corresponding to each image is jointly optimized with the generator weights. The second step uses contrastive learning together with regression loss to learn an encoder that maps images to the latent space, which is the main contribution of the paper. Results show that the proposed approach achieves state-of-the-art performance on several datasets for disentangled representation learning.","The authors describe a method for learning disentangled representations (categorical and continuous). It is based on the generative latent optimization scheme for learning disentangled latent codes for a given dataset, followed by training an encoder network to predict these latent codes given an arbitrary image.  The main contribution of the authors is a proposition to include contrastive losses into the encoder training to prevent it from converging to a suboptimal state where it could predict similar embedding to dissimilar input images. They validate the resulting model using standard image generation and disentanglement benchmarks across multiple datasets.","This paper proposed to improve the generative disentangled representation learning of LORD and OverLORD with contrastive learning. The idea is simple: when learning the amortized encoders in LORD and OverLORD, regularize them using contrastive learning to improve the generalization. Despite the simplicity, the authors demonstrated that this leads to a significant improvement in the disentanglement of representation.",0.17346938775510204,0.1836734693877551,0.14285714285714285,0.23076923076923078,0.18681318681318682,0.14583333333333334,0.18681318681318682,0.1875,0.24561403508771928,0.21875,0.2982456140350877,0.24561403508771928,0.1798941798941799,0.18556701030927839,0.18064516129032257,0.22459893048128343,0.22972972972972971,0.1830065359477124
673,SP:93c2fa9ebd0782fc8a6349f5449053d956a4f789,"**Summary**: This paper presents a new retrieval system for multi-hop QA, i.e., answering the questions requires multiple relevant documents. On top of existing dense retrieval techniques, this paper proposes three independent techniques to tackle multi-hop retrieval problems, including a topk multi-vector scoring function, condensed fact selection for query reformulation and a latent method to recover the proper retrieval order during training. This system achieves significant improvements on two recent multi-hop datasets.  **Contribution**: 1. A strong multi-hop retrieval system that outperforms existing systems by a large margin. 2. This is the first work showing that dense retrieval can be generalized to more than 2 hops.  3. Thorough experiment results and ablations. ","This paper introduces a new multi-hop reasoning system called Baleen which conducts iterative retrieval and condensing. Baleen repeats the iterative process for pre-defined time steps T (e.g., T=2 for HotpotQA and T=4 for HoVer)  and at every time step, it takes the query from the previous time step and retriever top K passages, and create a new query for the current t step by extracting and condensing the facts (sentences) from the retrieved passages. When they finish retrieval (i.e., t=T), they feed the final query to the reader model to get the final predictions.  They conducted experiments on HotpotQA and HoVer. The experimental results on HotpotQA show its competitive passage retrieval performance and they show state-of-the-art results on HoVer, significantly outperforming the baselines.  ","This paper proposed Baleen a system that improves the accuracy of multi-hop retrieval while learning robustly from weak training signals in the many-hop setting. Baleen generally contains three modules -- Focused Late Interaction, Latent Hop Ordering, and Condenser. Focused Late Interaction is an improved version of ColBERT, in which they only consider $\text{top-}\hat{K}$ partial scores instead of all scores. Latent Hop Ordering proposed a way to supervise the retriever using weak supervision per-hop. The condenser summaries the sentence-level information retrieved per-hop so it could be better used for the next hop. Experimental results on two multi-hop-based datasets, namely HotpotQA and HoVer show that the proposed framework is quite effective for retrieving multi-hop evidence. ","The paper makes contributions to passage retrieval in the open-domain, multi-hop setting.  It builds on recent work which applied dense passage retrieval iteratively to work for multi-hop tasks (such as MDR and IRRR).  It proposes three modifications to existing work: condensed retrieval, which summarizes retrieved passages at each hop to reduce the search space; focused late interaction, which is a variation on Colbert to consider only top-k scores for each query embedding; and latent hop ordering, which is a learned strategy to order the passages which improves over existing heuristics.  Each of these modifications are relatively minor, but together lead to very strong results on the often used HotpotQA and the newly released HoVer benchmarks.",0.15517241379310345,0.1724137931034483,0.1896551724137931,0.18045112781954886,0.16541353383458646,0.22764227642276422,0.13533834586466165,0.16260162601626016,0.18487394957983194,0.1951219512195122,0.18487394957983194,0.23529411764705882,0.14457831325301207,0.1673640167364017,0.1872340425531915,0.1875,0.1746031746031746,0.23140495867768596
674,SP:93ecf12948e9dfb60f2d6afc79c8ad152186cb01,"The paper is focused on improving quantum-mechanical properties of molecules in a setting similar to semi-supervised learning, where X is known for samples without Y. In an iterative fashion, the current model is used to predict pseudo-labels for those samples, then pseudo-labels together with labeled samples are used to improve the model. The paper uses existing approaches to quantify the uncertainty in individual pseudo-labels and then uses the uncertainty to weight their contribution to the loss.","This paper starts from two real challenges in QM problems: OOD and low-data issues. Then it proposes a self-training methods called PseudSigma. PseudSigma is a simple, effective, and model-agnostic algorithm with robust empirical improvements.","Self-training, or pseudo-labeling, is very simple and popular approach in different domains like speech and image recognition.  For the first time, pseudo-labeling is designed for quantum mechanics (QM) calculations problem. This problem is challenging for pseudo-labeling as it cannot use data augmentation (""small perturbational noise in 3D molecular geometry could easily lead to a drastic energy difference"") or model noise (like dropout or layer drop due to design of models to work with 3D molecular geometry) which were found to be crucial for pseudo-labeling in other domains. Moreover, the main real case scenario for QM is small amount of supervised data available (low-resource) which makes pseudo-labeling is even harder. In this paper authors propose learning a single model with time-to-time regenerating pseudo-labels and incorporating these pseudo-labeled data according to the model uncertainty. The uncertainty is modeled in the following way: labels are assumed to be from normal distribution and model is designed to predict parameters of this normal distribution, thus model estimates expectation of the label and its dispersion. This uncertainty detects and prevents bad pseudo-label from affecting the model training by adaptively weighting the data according to inverse of uncertainty value. Pseudo-labeling results show that direction of improving learning strategy instead of improving physics-based representation has potentials. With experiments it is demonstrated that proposed pseudo-labeling algorithm, pseud$\sigma$, improves results over supervised baseline, works across different backbone models and applicable for low-resource regime. Moreover it demonstrates improved results for out of distribution data.","This paper proposes to use pseudo-labeling/self-training for the prediction of molecular properties which are traditionally computed with quantum mechanical chemistry methods such as density functional theory or coupled cluster methods. While several recent works have trained supervised models on the QM9 dataset with labels produced by density function theory, labeled data for more accurate methods such as coupled cluster methods are significantly more expensive to obtain. This paper proposes to approach this label scarcity issue with pseudo-labeling/self-training on unlabeled data. The quality of the pseudo-labels is taken into account as follows: datapoints for which the model produces a pseudolabel with large uncertainty are downweighted compared to datapoints for which the model has high confidence on the pseudolabel.   The proposed training strategy is used to train models on different versions of QM9 and PC9. The first case considers training on all the labeled data in the training set of QM9, and the data of PC9 is used as unlabeled data. The second case considers only the QM9 dataset, where only 1% of the labels are used (and the rest is used as unlabeled data), or 10% of the labels are used.    Empirically the authors observe that using additional unlabeled data with pseudolabels improves the performance of the baseline models, even when only a small fraction of ground truth labels are available.  The authors check their hypothesis on the test set of QM9 that the model's uncertainty is well calibrated, in the sense that MAE and the epistemic uncertainty are positively correlated.  ",0.07407407407407407,0.32098765432098764,0.2962962962962963,0.24324324324324326,0.2702702702702703,0.15384615384615385,0.16216216216216217,0.1,0.09302325581395349,0.03461538461538462,0.03875968992248062,0.15503875968992248,0.10169491525423728,0.1524926686217009,0.1415929203539823,0.06060606060606061,0.06779661016949151,0.15444015444015444
675,SP:940e6f29eecb6696c20de4e910127b80bd05a137,"This paper explores a more realistic situation for self-supervised representation learning of training with streaming data. In this paper, four different situations for training with streaming data are investigated. The experimental results show that the self-supervised pretraining less suffers from catastrophic forgetting compared to the supervised pretraining, when the model is trained sequentially using streaming data. The experimental results also show that existing continual learning techniques are also effective for self-supervised learning with streaming data.","The paper considers the setting where data for self supervised pre-training comes in a streaming fashion, where models are incrementally trained on new data. They test 4 different types of streaming data which have different distribution shift properties, on 12 classification datasets and an object detection dataset, using MoCov2 as the self-supervised algorithm. The paper finds that for certain types of streaming data, the downstream task performance of streaming SSL versus ""joint"" SSL (which is the standard pre-training scheme where all data is available at once) are somewhat similar, while there is a large gap in performance for streaming data types with a large amount of distribution shift. They find that this behavior is unlike the gap between streaming and non-streaming supervised learning, where the gaps are larger in all types of streaming data. Finally, they reduce some of the gaps with data replay and regularization.","The authors of this submission worked on an interesting and quite practical task, i.e., to study the self-supervised behavior under the continual learning setup. To this end, the authors studied four types of incremental-learning settings, i.e., Instance incremental sequence, Random class incremental sequence, Distant class incremental sequence, Domain incremental sequence. The four types are designed to reflect different incremental-learning scenarios with different semantics.The authors have also conducted extensive empirical validations, including pre-training >500 models on four categories of pre-training streaming data from ImageNet and DomainNet, and evaluating them on three types of downstream tasks and 12 different downstream datasets. ","This paper studies how models perform on downstream image classification tasks when they are trained via self-supervision using streaming data. Streaming data refers to a sequential training setting  (ST) where the entire training data is not available for training at the same time but can be only used in disjoint chunks. When training on one chunk of data, the previous chunks are unavailable for training during this sequential training. When the entire training data is available for training at the same time, it is called joint training (JT). The key insight of the work is that in the presence of little to moderate drift in the data distribution across chunks, the downstream task performance of self-supervised training in sequential setting (SSL-ST) is comparable to that of self-supervised training in joint setting (SSL-JT). While in the presence of severe drift in data distribution across chunks, SSL-ST performs worse than SSL-JT but through the application of continual learning approaches like data replay and MAS (Aljundi et al, 2018), this performance gap can be reduced to a large extent. Moreover, when training is done in a supervised manner (SL), sequential setting (SL-ST) performs much worse than joint setting (SL-JT)  The work experiments on four kinds of sequential settings with varying degrees of data distribution drift across consecutive chunks - (1) instance incremental sequence, where each chunk of unlabeled data is derived from the same set of classes having i.i.d. samples in each chunk, (2) random class incremental sequence, where data in each chunk belongs to a different set of randomly chosen classes, (3) distant class incremental sequence, where each chunk belongs to a different set of classes such that there is a larger semantic gap between the set of classes and (4) domain incremental sequence, where the chunks belong to different domains thereby exhibiting severe distribution shift. For the first three settings, Imagenet-1k is used which is split into 4 chunks as per the setting. For the fourth setting, DomainNet is used. MoCo-v2 is primarily used in the empirical analysis along with some experiments on BYOL are also conducted to show that the trends are observed in other SSL approaches as well.  The work also conducts an analysis on the amount of forgetting using the backward and forward transfer analysis which shows that compared to supervised setting where there is significant amount of catastrophic forgetting, self-supervised sequential setting has much lower forgetting. Further analysis via Centered Kernel Alignment (CKA) suggests that compared to supervised setting, SSL-ST has a higher feature similarity between two sequential models. Even the feature similarity of SSL-ST with SSL-JT is higher than the corresponding similarity of SL-ST with SL-JT. ",0.2948717948717949,0.16666666666666666,0.3717948717948718,0.13333333333333333,0.3,0.3644859813084112,0.15333333333333332,0.12149532710280374,0.06331877729257641,0.18691588785046728,0.0982532751091703,0.0851528384279476,0.20175438596491227,0.14054054054054055,0.10820895522388059,0.1556420233463035,0.14802631578947367,0.13805309734513274
676,SP:940e70110dee62dca6f8d3e47a259cea1252f8f0,"The domain is the optimization of deep and large neural networks. The author argues and presents information that supports (figure 1), a hypothesis that a large portion of neurons is pushed towards dormancy while optimizing a neural network and only a subset of the neurons are actively used. The authors, moreover, argue that these dormant weights are important during optimization as they can propose new solutions during training (figure 2). However, these dormant weights can also be an issue as they introduce gradient noise. The authors propose a new scheme to balance the role of the dormant weights in order to maximize new solutions while minimizing gradient noise. The authors do this by either setting them to zero at a certain iteration or regularizing their sizes. They only optimize a set amount of the weights at once (rewind). The authors show results on multiple benchmark datasets highlighting their performance and how new sparsity-oriented hardware works well with their algorithms. The dormitory vs active weights are picked by a threshold of their magnitude.","The authors proposed a new point of view to explain why deep learning models often require more weights are needed to train models rather than to run inference for tasks. One example is that sparse models usually perform well in inference, but do not perform well in training. The main content of this viewpoint is that more weights during training can expand the search space, create extra degrees of freedom for search, and form new optimization paths when model optimization falls into critical points, thereby facilitating model training. Based on this point of view, this paper proposes a series of steps to augment search spaces of sparse models during training to approximate the behavior of larger models. The experimental results show that this series of steps can effectively reduce the model error.","This paper seeks to explore and understand the improved performance of Neural Networks trained with additional parameters even though a majority of the weights can be pruned during inference. They analyze the magnitude, correlation, and movement of weights during training and propose a hypothesis that the reason is that the weights which can be pruned during inference provide degrees of freedom in the loss landscape which allows better optimization of the non-prunable weights. Based on this, they explore and recommend a methodology for sparse training as a method to efficiently approximate a higher dimensional weight space that maintains these good optimization properties. They ablate the effects of the different recommendations and explore the hyperparameter space to have a better understanding of the dynamics of the sparse training.","This paper studies the training of sparse neural networks and the effect of extra parameters (dense connectivity) during training. They hypothesize that pruned connection are useful, because they help optimization to take some random steps in those direction which help optimization escaping the local minima. Authors support their claims with experiments and finally share a very long list of experimental results on training sparse networks.  ",0.12138728323699421,0.1676300578034682,0.10404624277456648,0.21212121212121213,0.09848484848484848,0.15625,0.1590909090909091,0.2265625,0.27692307692307694,0.21875,0.2,0.3076923076923077,0.1377049180327869,0.1926910299003322,0.1512605042016807,0.21538461538461537,0.13197969543147212,0.2072538860103627
677,SP:941853219d2aae336365c6033666886fb4a3d381,"The focus of this paper is one linear and kernel classification in a streaming setting with limited working memory. I will focus on linear classification in this summary. Given a loss function l and a set of training samples {(x_t,y_t)}_{t=1}^T, the goal is to find the vector w minimizing $(1/T) \sum_{t=1}^T l(y_t w^T x_t) + \lambda\|w\|_2^2$. The twist is that this must be done using sublinear memory. Here sublinear is also in d - the number of features. Thus the optimal vector w cannot even be stored in memory.  What one does instead is to use techniques from streaming algorithms to extract the large coordinates of w, hoping that most of w is captured in a few coordinates of large magnitude. Formally, the setup is that training data arrives in a streaming fashion, with each item in the stream representing a coordinate of an x_t along with y_t. So we only see one single coordinate at a time.  In a standard streaming setting, one can use classic sketches such as the CountSketch to compute a small-memory representation of a given vector w, while allowing finding the heavy hitters (coordinates of magnitude at least $\varepsilon \|w\|_2$) efficiently. The tricky part in the problem considered in this paper, is to simultanously find the small coordinates AND figure out what the optimal solution w is.  A standard solution to linear classification with various loss functions, is to use online gradient descent. This however requires maintaining the current vector of parameters w. This is too expensive here, and the main idea in the work is to sketch w using one of the classic sketches for turnstile streaming heavy hitters / point queries. One can then ""simulate"" gradient descent via a gradient descent on the parameters in the sketch.  The results are also extended to tensor and kernel regression.","This paper tackles a very basic problem in machine learning, linear (and polynomial, and kernel) classification in the streaming model.  Such learned models can be represented as a coefficients vector w.  The task studied in this paper is to approximately identify the largest coefficients in the optimal coefficient vector w.    This model was introduced in a SIGMOD 2018 paper.  This paper:  - improves bounds under the l_1 model, including a deterministic option  - provides new poly(dimension) bounds in the l_2 model  - removes assumption that data is random ordered, it allows adversarial order  - extends to polynomial and kernel SVM classifiers","The submission is a follow up paper on prior works on performing online gradient descent in sketch space.   The premise here is that the dimension d is very large and therefore we do not want to maintain the parameters vector w* explicitly.   We may only want to know the heavy hitter entries of the vector or to be able to query for values at particular indices (with error of the order of a fraction of the norm).  The approach is to use sketches to ""update"" w*.  The submission improves over prior work using a “plug and play” approach of taking known results and methods from the streaming literature and applying them in the online gradient descent context. ","The current work deals with linear classification in the streaming, limited space model. The assumption is that the available space is both o(n) and o(d), where n = #data points and d = dimension. The goal is to be able to answer two different types of queries about the weight vector-- either to return a vector that is close to the original weight vector in l2 norm, or to return estimates of any coordinate of the weight vector. ",0.10559006211180125,0.11490683229813664,0.08695652173913043,0.16,0.15,0.18803418803418803,0.34,0.3162393162393162,0.358974358974359,0.13675213675213677,0.19230769230769232,0.28205128205128205,0.16113744075829384,0.16856492027334852,0.13999999999999999,0.14746543778801846,0.16853932584269662,0.22564102564102564
678,SP:943ba7babe0f5571676aa4d4831155b2b82af005,"The authors propose a mechanism to solve the problem of small data in federated learning. Rather than directly uploading models to the server, there is an indicator to re-distribute each local model to other local models. They also discuss the conventional privacy guarantee with their approach. Finally, they provide experimental results on multiple datasets. The main contribution is to discuss how to solve the small data research question in FL. This question itself is practical and interesting. Also, I would say the analysis of the privacy guarantee is convincing.","The paper presents a new training procedure for federated learning (FL) systems based on a daisy-chain network. Training the system has two phases, a daisy-chain phase in which models are transmitted from one client to another via a coordinator node, and the standard aggregation phase in which models are averaged according to FedAvg rule. The main motivation is reduced overfitting and improved generalization compared to the standard FedAvg training, especially with limited-sized local datasets. The paper presents PAC-like ($\epsilon, \delta$)-guarantees for their algorithm, and provides a discussion on privacy violation concerns of their algorithm. The method is demonstrated on several datasets and shows improved accuracy over the compared methods.","This paper considers the setting of federated learning where each client holds a small subset $D_k$ of the entire dataset $D$. The goal is to learn a hypothesis with small (generalization) error. The challenge is that each dataset might be too small to guarantee convergence and/or generalization.   1. The  ""obvious"" solution to this is to have multiple clients share data. But this is undesirable because of privacy considerations.  2. Instead this paper proposes having clients swap ""models"" periodically. The paper argues that from the point of view of the model, it sees a greater diversity of client data, and thus ought to be able to train with better generalization. The paper reports that this improves over existing state of the art work in federated learning in terms of accuracy. 3. Of course this incurs increased privacy risk. The paper proposes to tackle this through differential privacy. The results reported here in this regard are experimental. The permutation is superficially similar to the recently proposed shuffle model, but no connection is drawn in the paper.   ","This paper considers a federated learning setting in which the sample size at client is so inadequet that the local objectives greatly differ from the global one. This paper proposes a novel approach that intertwines model aggregations with permutations of local models. By doing so, local models are exposed to several clients' data which ultimately improves the model accuracy.",0.2,0.24444444444444444,0.13333333333333333,0.17543859649122806,0.14035087719298245,0.11363636363636363,0.15789473684210525,0.125,0.2033898305084746,0.11363636363636363,0.2711864406779661,0.3389830508474576,0.17647058823529413,0.16541353383458646,0.16107382550335572,0.1379310344827586,0.18497109826589594,0.1702127659574468
679,SP:945412e4de55e4a0f404940cf9e2978d8e7cd600,"The paper uses three techniques to ""sparsify"" a dense network over training. These techniques are: non-persistent pruning, which backpropagates the gradients and updates a copy of the dense weights of the network; soft thresholding, which replaces the hard threshold typically used while prunning neural networks with the soft threshold operator; and increasing the pruning ratio linearly while training. The paper shows that these three techniques can achieve good accuracy at a high sparsity levels on ImageNet and CIFAR-10, when compared to other approaches. ","In this paper, the authors proposed a training algorithm for sparse deep neural network. Their key idea is to decouple the forward and backward paths, that is, the weights used in the forward path are a threshold version of the weights maintained in the backward path. In this way, some pruned weights have possibility to be re-activated. The authors also conducted some experiments to evaluate the performance of their methods. ",The paper proposes a single training cycle method to train sparse networks. It adopts a soft-threshold pruning strategy and progressively increase the sparsity ratio of the network along the training iterations. Experiments show its advantage.,"The authors propose a method for unstructured model pruning that applies soft-thresholding in the forward pass, while using the straight-through estimator in the backward pass. The latter allows for accumulation of gradients and rewiring of earlier-pruned weights. The authors show SOTA performance for the high sparsity-regime on ResNet-50 for Imagenet, and include ablation experiments on CIFAR10 with ResNet-20.  ",0.15294117647058825,0.1411764705882353,0.18823529411764706,0.14084507042253522,0.23943661971830985,0.25,0.18309859154929578,0.3333333333333333,0.25,0.2777777777777778,0.265625,0.140625,0.16666666666666669,0.19834710743801653,0.21476510067114093,0.18691588785046728,0.2518518518518518,0.18
680,SP:94840d985f9f02ef322ce6d5f82758ad3215d05d,"The paper shows how to use the Conditional Entropy Bottleneck (CEB)[1] in a self-supervised fashion.  Thus resulting in a new method for training approximately minimal self-supervised  (SSL) representations, i.e., a new member of the SSL information bottleneck family (although extremely similar to [2]). Specifically, they show how to use CEB in conjunction with SimCLR and BYOL. Furthermore, they provide good arguments to suggest that their proposed objective will learn smoother encoders than standard self-supervised models, which can help robustness. Through extensive experiments,  they show significant performance gains of the proposed methods compared to SimCLR and BYOL.  - [1] Fischer, Ian. ""The conditional entropy bottleneck."" Entropy 22.9 (2020): 999. - [2] Federici, Marco, et al. ""Learning robust representations via multi-view information bottleneck."" arXiv preprint arXiv:2002.07017 (2020).","This work adds a conditional entropy bottleneck (CEB) objective to two existing frameworks for self-supervised learning, namely SimCLR and BYOL. The motivation for adding CEB is to encourage information compression in the representations learned from these models which can make the representations more robust. The authors measure the performance of SimCLR and BYOL with added CEB objective empirically on standard benchmarks such as linear evaluation protocol on ImageNet with different amounts of training data. They provide evidence that adding CEB outperforms other existing self-supervised learning models on these benchmarks, including vanilla SimCLR and BYOL. ","The paper posits that compressing the representation during learning improves the robustness of said representation. To demonstrate their argument, they derive an objective for SimCLR and BYOL that explicitly compresses the representation using the Conditional Entropy Bottleneck. The addition of this compression improves the performance for both BYOL and SimCLR on ImageNET. They also demonstrate that their method improves the robustness of the encoder and its representation to perturbation of the input data.","Contrastive learning approaches learn salient representations that will be effective for various downstream tasks, while not all learned information is relevant to the task target. Given this insight, the authors aim to learn compressed representations by adding information compression objectives to existing algorithms.   They applied conditional entropy bottleneck (CEB) objective to SimCLR and BYOL to compress away the irrelevant information from the learned representations. In addition, they discovered that enforcing the information compression entails the encoder with bounded Lipschitz constant which is closely related to robustness. Extensive experiments confirm their hypotheses that adding compression regularizer can improve the performance in terms of accuracy and robustness to domain shift.",0.16666666666666666,0.13636363636363635,0.14393939393939395,0.16666666666666666,0.21875,0.2602739726027397,0.22916666666666666,0.2465753424657534,0.17592592592592593,0.2191780821917808,0.19444444444444445,0.17592592592592593,0.19298245614035087,0.17560975609756097,0.15833333333333335,0.18934911242603553,0.2058823529411765,0.20994475138121546
681,SP:94a2707ba8e99d97bb9e884eb59cad2af3a95c90,"This paper introduces a novel latent state-space method for unsupervised reinforcement learning in dynamic partially-observed visual environments. The key insight of the paper is that minimizing the entropy of the belief over the latent state does not incentivize agents to construct a ""niche"", i.e. modify the environment such that it becomes more easily controlled. As a remedy, the authors propose to instead minimise the entropy of the belief over latent state visitation. The authors discuss a variety of reward incentivised constructed from the latent-space model and introduce a separate exploration policy. The authors evaluate on three separate single-agent environments, including one with continuous action spaces. It is found that the novel latent-visitation based reward incentives outperform existing methods on a variety of metrics.","This paper introduces a new intrinsic reward signal for partially observable environments with dynamic objects that can potentially be controlled by an agent. This reward seeks to minimize the entropy of the agent's state visitation as estimated by a latent-space model. To do so it makes use of a latent state estimation approach using a VAE and an ensemble of dynamics models, and specifies 4 reward functions that can be seen as optimizing the above objective through different means.  These proposed reward functions are evaluated on three domains with partial observability and dynamic objects that the agent's actions can somehow affect and potentially control. The experiments show that one or the other of the proposed reward functions lead to the agent controlling these dynamic objects, whereas previously proposed rewards fail to do so. Of note in the VizDoom domain, the paper shows that one of the proposed rewards leads to maximization of the extrinsic reward without learning to do so.","This paper presents a set of novel objectives for intrinsic motivation that aim to encourage an agent to learn to exercise control over its environment, despite only having partial observability. The key motivating idea is that the agent be encouraged to reduce ""Belief Entropy"" by limiting the ability of the environment to change, particularly when that part of the environment is not being observed. Four different objectives are presented that encourage the agent to learn policies that achieve this goal and results are demonstrated on a handful of toy environments meant to illustrate the central claims of the paper. Among the results is good performance on the DefendTheCenter VisDoom task, in which the agent learns to shoot enemies without specific instruction to do so, as this prevents the enemies from continuing to exist and ""surprising"" the agent in the future.","This paper studies the problem of designing intrinsic rewards that encourages active exploration in partially observable dynamic environments. The authors observe that by minimizing the entropy of the latent visitation frequency, the agent is able to achieve both the active information gathering and prevent the environment from changing. Specifically, the authors introduce Believer, a novel framework based on variational latent Bayes filters with discrete latent states, and designed several rewards. In the experiments, Believer has achieved good performance on 3 different tasks.",0.23255813953488372,0.21705426356589147,0.1937984496124031,0.20245398773006135,0.13496932515337423,0.15714285714285714,0.18404907975460122,0.2,0.3048780487804878,0.2357142857142857,0.2682926829268293,0.2682926829268293,0.2054794520547945,0.20817843866171004,0.23696682464454977,0.21782178217821782,0.1795918367346939,0.19819819819819823
682,SP:94d35644b6334d19d0fd42a7736aca906305efb3,"This paper proposed Recurrent Bayesian Classifier Chains (RBCC) for exact multi-label classification. The proposed method learned a Bayesian network of class dependencies to improve the Recurrent Classifier Chains (RCCs). The comparative experiments were performed with RCC ([15], 2017, Nam, etc.), TS-RCC ([15], 2017, Nam, etc.), OF-RCC ([4], 2018 , Chen, etc.), and BCC ([23], 2014, Sucar). And the results demonstrated the effectiveness of the RBCC method.","This paper presents a new method for optimizing the subset zero-one loss in multi-label classification. The new method is an adaptation of probabilistic classifier chains. The central idea is to replace the chain by a dependence structure learned using Bayesian networks. In theory the order of the labels in a probabilistic classifier chain does not matter, but in practice one can observe performance drops if the chosen order leads to propagation of errors. By replacing a random order with the dependency structure of a Bayesian neural network, the authors intend to solve the problem of choosing an appropriate order.   As underlying models to fit probabilities, recurrent neural networks are used. Experimental results are reported for three benchmark datasets. ","This paper proposes a new method, Recurrent Bayesian Classifier Chains (RBCC), for exact multi-label classification problems (maximizing subset accuracy). RBCC involves three steps: 1. Intra-class Dependency Network to learn a Bayes net to the classes; 2. Recurrent Conditional Dependency Model to summarize the information of features and the corresponding parent classes learned in the previous step; 3. Bayesian-Conditioning Classifier to predict. RBCC is shown to outperform several state-of-the-art (STOA) algorithms on some real-world multi-label data sets with respect to subset accuracy.","The authors propose Recurrent Bayesian Classifier Chains (RBCCs), a new method for exact multi-label classification. It combines the idea of ordering and conditioning labels based on a Bayesian network, utilized previously in Bayesian Classifier Chains (BCCs), and using a recurrent neural network as in Recurrent Classifier Chain (RCC). Usage of recurrent architecture allows modeling that condition on previously predicted classes instead of using many separate binary models originally used in the Classifiers Chain. The proposed approach uses an established approach to construct a Bayesian network. The novelty of the approach lies in the training and inference procedure. Instead of performing many to many classification, where labels are predicted one after another as in the case of standard RCCs, in RBCCs, many to one classification is performed for each class. Each class is conditioned only on features and labels it depends on in the obtained Bayesian network. The author validates the attractiveness of their method in empirical study on three popular benchmark datasets with a small number of labels. In all three cases, the proposed approach significantly improves in terms of subset accuracy over the baselines.",0.27941176470588236,0.3382352941176471,0.35294117647058826,0.16666666666666666,0.25833333333333336,0.29213483146067415,0.15833333333333333,0.25842696629213485,0.12903225806451613,0.2247191011235955,0.16666666666666666,0.13978494623655913,0.2021276595744681,0.2929936305732484,0.1889763779527559,0.19138755980861244,0.2026143790849673,0.1890909090909091
683,SP:955102d1234f76a0a87a23249f11c2d6981ee002,"In this paper, the authors introduce a new generalized additive model, called neural additive models, which use neural networks to learn a non-linear transformation of each input feature, independently. Along the way, they introduce a new activation function, called ExU, which enables a highly sensitive weights that can fit very large, seemingly noisy, changes of a given function. They demonstrate that NAM performs as well as other GAMs, such as EBMs, as well as black box models, such as DNNs and XGBoost. The major  benefit of NAM is that, similar to EBMs, they are intrinsically interpretable simply by visualizing the learned non-linear function for each input feature.  They compare the performance of these methods across several tasks. The paper is written very clearly with easy to follow examples. ",The authors propose the neural additive models which have an additive structure and which are more explainable than general neural networks. They apply this model for various tasks and applications and show their interpretability through visualizations. Its a good contribution and a reasonably well-written paper. ,"This paper propose Neural Additive Model that provides more interpretability with DNNs. It learns a linear combination of neural networks that each attend to a single input feature. Through experiments, NAM is shown to be more accurate than other intelligible models such as logistic regression and explainable boosting machine (EBM), while also provides explanability with respect to the input features. A multitask learning architecture along with the single task architecture is proposed.","This paper proposes Neural Additive Models (NAMs), a linear combination of neural networks that can be used as a highly accurate and interpretable model for tabular data. In order to account for the potentially detrimental bias of NNs to be smooth, the authors introduce an exp-centered hidden unit which enables the NN to better fit jagged shaped function, and compare this NAM to other tree-based and DNN based models. NAMs are able to achieve comparable accuracy to tree-based models with the added advantage of being easily adapted to the multitask setting, which can be helpful in increasing accuracy, discovering bias, constructing more helpful models for fields which need interpretability like healthcare. ",0.1,0.16153846153846155,0.17692307692307693,0.1956521739130435,0.21739130434782608,0.2777777777777778,0.2826086956521739,0.2916666666666667,0.20175438596491227,0.125,0.08771929824561403,0.17543859649122806,0.14772727272727273,0.2079207920792079,0.18852459016393444,0.15254237288135594,0.12499999999999997,0.2150537634408602
684,SP:956097bcab9530b503934383fe92037702201c91,"This paper proposes a sample efficient hypergraph grammar learning algorithm for generating molecules. The bottom-up search algorithm conducts iterative contraction according to a learned policy, where in each iteration, several hyperedges are sampled, contracted into a single node, and written into a production rule as part of the final grammar. The search policy is being trained using RL where the rewards are given by evaluating a set of molecules generated by the policy. The proposed method is evaluated on a small and a large monomers dataset and outperforms existing molecule generation algorithms in terms of the synthesizability and the membership rate.",This paper proposed an interesting grammar learning-based data-efficient molecule generation model.  The key idea of this method is to learn a set of production rules via a hyper-edge selection process that optimizes a set of evaluation metrics.  This method achieves impressive performance on extremely small datasets and achieves competitive performance compared with end-to-end models trained on a large dataset.  ,"The paper presents a data-efficient graph grammar-based approach to molecular generation (with a focus on polymer generation). The model requires that the molecule be represented as a hyper-graph. The productions of the grammar are basically rules for contracting connected nodes into a single node (a non-terminal symbol), which are later expanded during generation. Each rule is essentially derived from a random sample of the hyper-graph edges: the grammar is produced by iteratively contracting the nodes incident to the edges into non-terminal nodes until the hyper-graph consists of a single non-terminal node. This procedure is applied simultaneously to all the graphs of the training set. Once a grammar is obtained, it is optimized by maximizing some desired metrics (such as diversity). The optimization is non-differentiable since it requires generated graphs to evaluate the objective function, and thus the authors resort to approximate the gradient's expectation using MC sampling and the REINFORCE score function estimator. The experiments aim at showing that the proposed model is data-efficient (achieving strong performances using a small fraction of the molecules used by competitors), capable of extrapolation (since it can produce molecules outside of the training set), and explainable (since it is shown to identify functional groups which characterize the family of generated molecules).  ","This paper introduces a new grammar-based generative model for the molecule generation task. The graph grammar is defined as a set of production rules that operate on module graphs (i.e. the molecule graphs are not linearized as done in some previous work). The graph grammar is learned by iteratively contracting hyperedges (i.e.  edges that can connect multiple nodes, defined by simple chemistry-inspired rules) into non-terminal nodes and the submission proposes to learn how to sample the hyperedges by a REINFORCE algorithm that optimizes for several molecule generation metrics.   The evaluation is done both on small and large molecule generation datasets. Notably, the proposed method achieves strong performance while being very data-efficient.",0.17647058823529413,0.30392156862745096,0.23529411764705882,0.265625,0.34375,0.1651376146788991,0.28125,0.14220183486238533,0.20512820512820512,0.0779816513761468,0.18803418803418803,0.3076923076923077,0.21686746987951808,0.19375,0.2191780821917808,0.12056737588652484,0.24309392265193366,0.21492537313432836
685,SP:9587470c86fce30cc746dcffe55f9bc89d8fc497," This paper presents a technique for improving a pre-trained object detection model by learning an exploration policy in a 3D environment, automatically labeling the experience collected by this policy, and then using this data to finetune the object detection model. This improves the object detection performance of the pretrained model, and using this finetuned model in an agent leads to higher success in object navigation tasks.  The exploration policy operates on a 3D voxel-based map representation. Each voxel contains one number per object category (e.g., chair, bed) denoting the confidence that an object of that type is present in that voxel. This representation is built by using a pre-trained MaskRCNN and using depth information to map back pixels in 2D space to 3D voxels. This voxel-level object confidence scores are summed over time to create the 3D (voxel-based) semantic map of the environment. To learn the exploration policy, they use a curiosity signal named Gainful Curiosity. This essentially counts the number of voxels with a confident object prediction. The policy consists of 2 levels with the high level Global policy proposing a new goal every 25 timesteps, and a low level Local policy that uses a path finding algorithm to navigate to the proposed goal. The global policy is trained to maximize the reward obtained from the Gainful Curiosity signal.  The experience collected by the exploration policy is then used to finetune the object detection model. To do this, they introduce a label propagation technique called 3DLabelProp. This essentially looks at a set of connected voxels with the same label and constructs the 2D bounding boxes/masks by mapping back from 3D map to 2D observations. Then they train the object detection model on observations labeled in this way.  They evaluate their technique on a simulated house environment and show that they can improve object detection/segmentation performance of a pre-trained MaskRCNN. They also show that this improved object detection model can then be used to achieve higher success in object goal navigation. ","This paper proposes an approach to fine-tune and transfer an object detection and segmentation approach (Mask-RCNN) to a set of 3D textured scene models by label propagation and exploration. The object detector is pretrained on an annotated large-scale Internet image dataset (MS Coco). A ""perception"" component maps the class pixel labels of Mask R-CNN into the 3D scene through raycasting in the given map and classifying voxels in a volumetric 3D semantic map. A policy is trained to explore the environment for fine-tuning the Mask-RCNN detector.","The paper introduces “SEAL”, a framework for self-supervised learning of an embodied exploration policy while simultaneously refining the agent’s perception network. The method relies on building a semantic 3d map propagating confident predictions from one viewpoint to another for visual refinement. The method is validated on the Gibson dataset in terms of perceptual accuracy and on the downstream task of object goal navigation. ","The proposed method aims to improve the performance of a pretrained perception model in an active learning way. The whole training framework consists of the Action phase and Perception phase, which are trained with gainful curiosity reward and pseudo labels projected from that of the 3D Map, separately. The experiments conducted on detection, segmentation, and object goal navigation tasks illuminate that they can achieve a stronger perception which benefits the embodied robotic task (Object Goal Navigation) in static scenes.",0.08235294117647059,0.07647058823529412,0.09117647058823529,0.14130434782608695,0.14130434782608695,0.26153846153846155,0.30434782608695654,0.4,0.3924050632911392,0.2,0.16455696202531644,0.21518987341772153,0.12962962962962962,0.12839506172839507,0.14797136038186157,0.16560509554140126,0.15204678362573099,0.23611111111111116
686,SP:9587c20890e429070bc0ca9562befac98fbe4910,"This paper proposes a new variant of Transformer, called Shifted Chunk Transformer (SCT), for video understanding. Compared with ViT, SCT has three major contributions: 1) SCT uses a smaller patch size ($4\times 4$ instead of $16\times 16$) to extract more fine-grained features; 2) A combination of local chunk attention (i.e., self-attention within a local window) and global LSH attention is used to efficiently process the large number of tokens at each frame; 3) A frame-wise attention between adjacent frames and a clip encoder beyond the frame-level features are used for modeling temporal information. Experiments on multiple video benchmark datasets show the effectiveness and efficiency of the proposed SCT model.   Post rebuttal: I've read the author's response and reviews from other reviewers. I think most of our questions are well addressed in the rebuttal, and I will keep my original rating ""6: Marginally above the acceptance threshold"".","This paper proposes a new Transformer architecture for video modeling. There are following several components with their respective design motivation: - First, each frame is split into a large number of tiny patches (e.g. 4x4 pixels) which are then linearly projected into token vectors. Then, the authors propose Image Local Chunk Attention to enforce locality in cross attention to ensure attention is only computed among neighboring patches in local neighborhoods. The authors argue that it's necessary to have small patches for good accuracy. And the Image Local Chunk Attention is therefore there to reduce computation costs brought by small patches and also enforce locality.  - With the features computed from Image Local Chunk Attention, the authors then apply a locality sensitive hashing (LSH) attention to approximate the true cross attention to increase efficiency.  - Now tokens are mixed with attention for each frame, the authors then propose Shifted Multi-Head Self-Attention that computes cross-attention across frames to model object motion and spatial variances.  - Finally, a special CLS token is extracted from each frame and passed into another clip encoder to produce a clip-level summarization, which is used for final classification tasks. Overall, the authors proposed a set of methods to model both intra and inter frame relationships in videos using transformers and achieved SOTA results on several video classification tasks like Kinetics, UCF101, HMDB51 and Moment-in-Time. The authors also conducted thorough ablation to dissect the contribution of each components.  ","The paper proposes a new transformer-based architecture for video classification. The proposed architecture builds on the recently introduced vision transformer model (ViT), which first decomposes a given image/video frame into a set of non-overlapping patches. However, unlike in prior work,  instead of applying self-attention directly on patches, the authors group locally adjacent patches into local ""chunks"", and apply self-attention on top of these chunks. To ensure that the model can propagate local chunk information globally across the entire image/video frame, the authors introduce  shifted self-attention. Lastly, a separate clip encoder is used to incorporate temporal video information. The proposed approach achieves state-of-the-art on several action recognition benchmarks such as Kinetics-400, and Kinetics-600.","The paper proposes a novel transformer-based architecture for video processing. The input tokens are small patches extracted for each frame. The model consists of 4 modules: First, the tokens are processed locally, using a transformer with restricted, local connectivity (each image is split into multiple chunks and only the patches from the same chunk exchange messages using the self-attention layer). Second, a global processing is conducted using an efficient transformer (that use LSH attention approximation) to capture long-range interactions. In order to take into account the motion effect, a shifted self-attention is introduced, modifying the classical self-attention layer to use keys computed from the previous time step. Similar to language modelling, a special token ([CLS]) is used to extract frame-level representations that are fed into a final transformer block to compute the final video representation.",0.25806451612903225,0.2,0.16129032258064516,0.18518518518518517,0.1728395061728395,0.24193548387096775,0.1646090534979424,0.25,0.1773049645390071,0.3629032258064516,0.2978723404255319,0.2127659574468085,0.20100502512562812,0.22222222222222224,0.16891891891891891,0.24523160762942775,0.21875,0.22641509433962265
687,SP:95b78a410bca78903d89eb45950b45f93e61d312,"In this work, the authors propose an unsupervised, generative model for foreground-background segmentation of RGB images which is based on the previously proposed ""Latent Space Engergy-Based Prior Model"" (LEBM). Similar to a VAE, a latent vector sampled from a prior distribution is transformed into an image by a decoder network. Differently from VAEs however, the LEBM uses a more complex, learnable prior distribution and sampling from the prior and posterior distributions is realized by differentiable MCMC sampling using Langevin dynamics. An image is modelled using a spatial mixture of foreground and background images decoded by separate decoder networks. To foster the segregation into foreground and background, the backround model includes an additional, spatial resampling step targeted at the assumed background statistics. While the reconstructed images tend to be blurry, the reconstructed foreground-background masks have been shown to be more accurate than previous methods on several datasets.","This work proposes a novel unsupervised foreground segmentation technique based on Mixture of Experts energy based modelling with a generative neural network. They introduce an interesting inductive bias, pixel reassignment, which works as the main regularizer and the secret sauce that enables the fg/bg separation. They essentially reshuffle the pixels for the background generation based on the learned index matrix and ablations shows that's the most significant regularizer.  They compare their results on 6 datasets, including birds, dogs, and clevr6 to both object centric approaches such as slot attention and gan based methods. They consistently outperform the previous methods with a significant margin.","This paper proposes deep region competition to extract foreground objects from images in an unsupervised manner. The proposed method leverages energy-based prior with generative image modeling in the form of Mixture of Experts. In addition, a pixel re-assignment scheme is further learned to capture the regularities of background regions. Experiments demonstrate that the proposed method can achieve promising results on real-world images.","The paper tackles the problem of unsupervised foreground segmentation with a combination of energy-based and deep-learning-based models. It models the foreground and background as two components that can be selected by a latent variable. The background has an inductive bias that pixels can be re-assigned to a different location. The whole model is generative. During inference, EM is needed to figure out the foreground and background separation by maximizing probability to generate the image and set the right latent states.",0.1476510067114094,0.10738255033557047,0.1476510067114094,0.14285714285714285,0.17142857142857143,0.18461538461538463,0.20952380952380953,0.24615384615384617,0.2619047619047619,0.23076923076923078,0.21428571428571427,0.14285714285714285,0.1732283464566929,0.14953271028037382,0.18884120171673818,0.17647058823529413,0.19047619047619047,0.1610738255033557
688,SP:95c7755abeb1587e988164998ea14cfdfb42d53d,"This paper makes some analysis on the convergence behaviors of SGD with label smoothing in deep learning. Further, a two-stage label smoothing algorithm is proposed to improve the convergence. Experiments are conducted to verify its effectiveness.","This paper studies the power of Label smoothing regularization (LSR) in training deep neural networks by analyzing SGD with LSR in different non-convex optimization settings. The convergence results show that an appropriate LSR with reduced label variance can help speed up the convergence. Besides, this paper proposes a simple yet efficient two-stage label smooth (TSLA) whose basic idea is to switch the training from smoothed label to one-hot label. Integrating TSLA with SGD can produce the improved convergence result that TSLA benefits from LSR in the first stage and essentially converges faster in the second stage. Extensive experiments show that TSLA improves the generalization accuracy of deep models on several benchmark datasets.","This paper proved that when performing stochastic gradient descent for a non-convex objective function, an appropriate smooth parameter might speed up the convergence. The authors then proceed to introduce a two-stage TSLA algorithm which firstly learns with LSR. Then switch to learn with hard labels after certain epochs. Empirical evidence demonstrate the effectiveness of the proposed algorithm.","This work theoretically analyzes the convergence behaviors of stochastic gradient descent with label smoothing in deep learning, which implies that LSR may slow down the convergence at the end of optimization. The authors propose the two-stage label smoothing strategy to further improve the convergence. Experiments on several datasets also verify the effectiveness of TSLA.",0.4594594594594595,0.2972972972972973,0.5945945945945946,0.16521739130434782,0.1826086956521739,0.23728813559322035,0.14782608695652175,0.1864406779661017,0.4,0.3220338983050847,0.38181818181818183,0.2545454545454545,0.22368421052631582,0.22916666666666669,0.4782608695652174,0.21839080459770113,0.24705882352941175,0.24561403508771928
689,SP:95e06126f0a92dd208e8a598ce57c1463f604be0,"This paper takes a large number of neural networks that span range of different parameters (e.g. width), train them in a large number of ways (e.g. dataset, batch size), and studies several metrics that characterize the local (e.g. Hessian spectral norm) and global (weight vector pair-wise connectivity on potential non-linear paths). By doing a large scale study of this kind, the authors are able to identify several distinct phases of loss landscapes, dividing them phenomenologically into (globally {well,poorly}-connected) x (locally {flat, sharp}). The authors also develop a simple mathematical toy model that can model this taxonomy. ","The paper proposed an experiment framework for evaluating loss landscape of neural networks during training. The experimental setup systematically vary two types of hyperparameters: 1) temperature type, where at high temperatures, larger learning rate or smaller batch size will induce more noise to training, and 2) load type, including data quantity/quality and model width. Neural networks are trained with different hyperparameter combinations according to the load-temperature pairs. Then 3 metrics including mode connectivity, Hessian, and output CKA similarity evaluated on perturbed training data. Jointly consider the 3 metrics (local and global) evaluated on all loss landscapes, the authors classify the loss landscapes into different phases. One of the phases (IV with high connectivity, low Hessian and high CKA) corresponds to best test accuracy.  ","This paper explores factors that influence the loss landscape. In particular, it categorizes the loss landscape in different phases, each corresponding to different status of key factors such as Hessian spectrum and mode connectivity. It also tries to establish an ""effective loss landscape"" model that depends on key parameters.","The author(s) perform an empirical study of which (and how) properties of the landscape affect the test accuracy, obtaining results that are consistent over a large number of deep neural network models over several credible datasets. The study puts into practice this idea that the generalization is not uniquely determined by local properties of the landscape, but rather also by global properties. They argue that different kinds of acting on the system and on the dynamics can be grouped into two control parameters that they call 'load' and 'temperature', and show that according to the position in the (load, temperature) plane one has different phases. They observe that each phase seems related to a different test accuracy, and realize that to obtain the best test accuracy, in addition to being locally flat, the landscape should also be globally ""nice"". By ""nice"", they mean that the landscape is well-connected (no large barriers separating regions) and that different trained models have a large CKA similarity. ",0.1941747572815534,0.11650485436893204,0.1941747572815534,0.088,0.176,0.30612244897959184,0.16,0.24489795918367346,0.12121212121212122,0.22448979591836735,0.13333333333333333,0.09090909090909091,0.17543859649122806,0.15789473684210525,0.14925373134328357,0.12643678160919541,0.15172413793103445,0.14018691588785046
690,SP:95ed5e7893f8378f43fe04c911313b5526aa324c,"The paper considers the problem of stochastic convex optimization where the population loss is convex, while the empirical loss is not necessarily convex. Specifically, this work examines SGD,GD and RERM under this setting.    The first result demonstrates a separation between SGD and RERM, in contrast to the more common SCO problem. It is well known that SGD ,given $n$ samples, obtains an $O(1/\sqrt{n})$-optimal solution (in population risk) after one pass over the dataset. They provide a lower bound of $\Omega(1)$ for RERM w.r.t any sample independent regularizer and any regularization parameter(possibly sample dependent). The second result extends that of Amir et al. [2021] (in the slightly different setting of SCO), and they provide a lower bound in terms of sample complexity for GD of - $\tilde\Omega(1/n^{5/12})$. The authors also provide an instance of SCO problem in which $k$-multi-pass SGD achieves a $O(1/\sqrt{nk})$ excess population risk, while RERM attains a lower bound of $\Omega(1)$. Lastly, they provide a separation between RERM and SGD in the distribution free model, where SGD outperforms RERM (for any regularizer) for distributions where the population loss is convex - SGD upper bound of $O(1/\sqrt{n})$ vs RERM lower bound of $\Omega(1/n^{1/8})$. And also for distributions where the population loss is $\text{\emph{not}}$ convex - SGD upper bound of $O(1/n^{1/8})$ vs RERM lower bound of $\Omega(1/n^{1/8})$.","This paper considers the sample complexity comparison between one-pass SGD vs. ERM/RERM/GD/multi-pass SGD in the setting of SCO (with Lipschitz, finite $\ell_2$-norm solution, bounded gradient variance). It shows that, for any problem instances in the considered SCO class, one-pass SGD always achieves good, $1/\sqrt{n}$ rate; in contrast, ERM, RERM (that is distribution-agnostic but is allowed to tune a multiplicative hyperparameter), GD (with any number of passes) could be arbitrarily bad in the worse-case instance in the considered problem class. Moreover, when used with a hold-out validation set, multi-pass SGD can outperform (at least no worse than) one-pass SGD. In conclusion, the paper reveals an important message, that (one-pass) SGD does have its own implicit bias, but it cannot be explained by a simple explicit regularizer.","This paper studies the generalization error bound of multi-epoch, small-batch, SGD for learning over-parameterized models. In particular, the authors compare the generalization performance of SGD to GD and regularized empirical risk minimization (RERM). For RERM, the authors show that there is an SCO problem for which RERM will not give vanishing generalization error while SGD enjoys a $O(1/\sqrt{n})$ rate. For GD, the authors also show that for some SCO problems GD with any step size and number of iteration can only achieve a $\Omega(1/n^{5/12})$ rate, which is also worse than SGD. Lastly, the authors show that multi-epoch SGD can perform better than single-pass SGD. ","The paper discusses the current training approaches of SGD from a theoretical perspective. On the negative side, the paper denies a popular conjecture that SGD has an implicit regularization like regularized ERM. On the positive side, the paper shows that SGD does outperform GD in some SCO problems, and that multiple-pass SGD does help in some SCO problems.",0.12749003984063745,0.1394422310756972,0.07569721115537849,0.2127659574468085,0.11347517730496454,0.16379310344827586,0.22695035460992907,0.3017241379310345,0.3220338983050847,0.25862068965517243,0.2711864406779661,0.3220338983050847,0.16326530612244897,0.19073569482288827,0.12258064516129033,0.23346303501945523,0.16,0.21714285714285714
691,SP:95fb3f6e195c2f773c218a52ab00b2aac301f6d3,"This paper introduces a problem of modeling satiation effects in multi-armed bandits. For example, if we assume the arms are restaurants that an agent is going to over time. Said agent may have a preference for pizza hut, but eating there every night would lead to a lower reward than it would if you haven't been to pizza hut in a while. The authors prove that the greedy algorithm is optimal when satiation retention, base rewards, and satiation influence is known and is equivalent over all arms. The authors then provide an upper bound on regret when using their EEP (Explore, Estimate, Plan) policy. The authors also provide empirical results on a toy version of the problem.","The paper proposes a Multi-armed Bandits model with non-stationary reward affected by users' satiation level. The authors consider two cases of satiation dynamic: known deterministic dynamics and unknown stochastic dynamics. Then the authors show that under specific conditions a greedy algorithm is optimal. In the general setting (unknown stochastic satiation dynamics), they propose a two-step algorithm EEP with error estimation and analysis.","This paper introduced rebounding bandits where satiation dynamics are modeled as time-invariant linear dynamical systems. With consecutive exposures to the same item, the expected rewards for each arm decline monotonically, and whenever that arm is not pulled, the reward rebounds. It first studied the offline problem and then characterized the regret bound for the proposed online algorithm. The paper propose Explore-Estimate-Plan and proves that the greedy policy is optimal when the arms exhibit identical deterministic dynamics. It also claims that this is the first bandits paper to model evolving rewards through continuous-state linear stochastic dynamical systems with unknown parameters. ","This paper considers a class of Multi-Armed Bandits in which the rewards on arms decline monotonically with consecutive arm pulls and rebound to the base value when arms are not pulled. The paper distinguishes itself from previous similar approaches, which assume the reward to collapse to some “zero” after an arm pull and recharge with time — in this paper, the degree of satiation effect depends on the number of recent pulls (not just the last pull).  The paper first provides analysis for this problem setting with all arms sharing the same deterministic and known satiation dynamics and shows that the optimal policy in this case pulls arms cyclically. Further, the paper analyses the stochastic setting with unknown satiation dynamics, providing an algorithm to explore and plan in this situation and provides regret bounds for the same. Finally, the paper concludes with numerical evaluation to (1) evaluate the proposed algorithm and (2) run validation checks for the theoretical regret bounds.   ",0.13445378151260504,0.17647058823529413,0.21008403361344538,0.23076923076923078,0.36923076923076925,0.2621359223300971,0.24615384615384617,0.20388349514563106,0.15625,0.14563106796116504,0.15,0.16875,0.1739130434782609,0.1891891891891892,0.17921146953405018,0.17857142857142855,0.21333333333333332,0.20532319391634984
692,SP:965e1375dc44148561b24470c9209a422e0643d2,"The paper presents a machine learning approach to improve Guided Local Search (GLS) for finding high-quality solutions for the Travelling Salesman Problem (TSP). Specifically, the paper proposes to use an ML model to predict the global regret for an edge, i.e., the difference in the objective values of a locally optimal solution with that edge being included and a globally optimal solution. Then, the prediction is used to guide the GLS to escape local optimal solutions.  ","This paper proposes a novel deep learning + guided local search heuristic technique for approximately solving the Travelling Salesperson Problem (TSP). This research direction is **well motivated**, as inexpensive but approximate heuristics to graph combinatorial optimization problems such as TSP are promising for enabling real-time applications in vehicle routing and operations research.  From a methodological perspective, the paper’s major contribution is to develop and train a **Graph Neural Network (GNN) on line graphs** for approximating the ‘regret’ of including graph edges in TSP solutions. The use of regret per edge is novel; regret is a metric used in the metaheuristic **Guided Local Search (GLS)**. After the GNN is trained, the proposed technique starts from a greedy solution and uses a TSP-specific Local Search interleaved with the **‘learnt’ GLS metaheuristic** to arrive at the final solution.  From an empirical point of view, this work proposes a **new evaluation setup** to measure solution quality of TSP heuristics within a fixed computational time. Their GNN + GLS approach outperforms popular learnt baselines and handcrafted GLS in this setup for TSPs up to 100 nodes.","The submission is concerned with developing a heuristic for the traveling salesperson problem (TSP) using graph neural networks an dguided local search. The idea of the approach is to learn the regret incurred by including an edge into the solution and to use this to guide a local search out of local optima. As opposed to previous work, the regret is computed against the global optimum (which is approximated by learning). The submission is limited to the Euclidean TSP variant where we operate on a complete graph whose edge weights are given by Euclidean distances.","The paper presents a hybrid data-driven approach for solving the TSP based on Graph Neural Networks and Guided Local Search. The model predicts the regret of including each edge of the problem graph in the solution; GLS uses these predictions in conjunction with the original problem graph to find solutions. The experiments demonstrate that this approach converges to optimal solutions at a faster rate than state-of-the-art learning-based approaches and non-learning GLS algorithms for the TSP, finding optimal solutions to 96% of the 50-node problem set, 7% more than the next best benchmark, and to 20% of the 100-node problem set, 4.5× more than the next best benchmark. When generalizing from 20-node problems to the 100-node problem set, this approach finds solutions with an average optimality gap of 2.5%, a 10× improvement over the next best learning-based benchmark.",0.358974358974359,0.2564102564102564,0.3333333333333333,0.14835164835164835,0.16483516483516483,0.30526315789473685,0.15384615384615385,0.21052631578947367,0.17333333333333334,0.28421052631578947,0.2,0.19333333333333333,0.21538461538461542,0.2312138728323699,0.2280701754385965,0.19494584837545123,0.18072289156626506,0.23673469387755103
693,SP:966554cfc9c2b09b33e07015e4f195ad55801721,"This work studies the problem of improving the sampling speed of diffusion models, which is currently a timely and challenging topic. To tackle this problem, this work first proposes several new parameterizations of diffusion models for better stability along with a fast sampling, and then proposes a progressive distillation method that progressively distills two DDIM steps of teacher diffusion models to one DDIM step of student diffusion models. In the end, the method can reduce the number of model evaluations in sampling to as small as 4 or 8 steps while retaining high image quality. Experiments on CIFAR10, ImageNet (64x64), LSUN-bedrooms (128x128) and LSUN-Church (128x128) were conducted to show the effectiveness of the progrssive distillation in reducing the sampling time. ","Diffusion models have recently emerged as a promising class of generative models. Their main drawback is the slow, iterative sampling, which often requires hundreds or thousands of neural network calls. The paper proposes a method to progressively distill this slow iterative generation process into much faster synthesis relying on fewer denoising steps, while almost preserving generation quality as measured by metrics quantifying perceptual image quality. The method progressively reduces the number of sampling steps required by a factor of 2 in multiple rounds of distillation and relies on a teacher-student framework. The student model learns to model 2 steps of the teacher in a single step. The method leverages the deterministic synthesis scheme from Denoising Diffusion Implicit Models for that purpose. To get the method to work successfully, new score model parametrizations and score matching loss weightings are proposed. The quantitative results are promising, for example achieving FID scores $\leq3.0$ on CIFAR-10 in as few as 4 steps.","This work proposes to speed up diffusion models by progressively distilling a score network with deterministic dynamics. Authors report significant improvement of sample quality using very few iterations. Authors also provide a proof that DDIM is a special numerical integrator for the probability flow ODE, connecting two existing approaches. In addition, empirical studies on the weighting function and parameterization of score functions are also discussed.","The paper presents an approach to improve sampling speed of diffusion models. The idea is to iteratively learn integrators for the probability flow ODE which, given a diffusion model, maps noise to data samples. In each iteration, the integrator is trained to integrate steps of the ODE over twice the length than the previous model. Thus, the approach resembles an iterative distillation process, where a faster student integrator is learned from a slower teacher integrator. After the student has finished learning, it becomes the teacher for the next distillation iteration.  Enabling this distillation requires changes to the parameterization and loss weighting, for which different, intuitive options are presented. Experiments evaluate the different design choices and compare the resulting models to other recent works on the topic, which demonstrates significant gains of the proposed approach in the regime where a small number of evaluations are used.",0.20491803278688525,0.12295081967213115,0.21311475409836064,0.08074534161490683,0.16149068322981366,0.26153846153846155,0.15527950310559005,0.23076923076923078,0.1793103448275862,0.2,0.1793103448275862,0.11724137931034483,0.17667844522968199,0.16042780748663105,0.1947565543071161,0.11504424778761062,0.16993464052287582,0.16190476190476188
694,SP:9685ce4aa1b014f540de0797328b472cad0e42a5,"This paper introduces a learning paradigm to handle related MDPs (called Nested MDPs) that share the same structure and definition, varying only in their dynamics. Based on Fitted Q-Iteration (FQI), the authors propose an algorithm known as Nested FQI (NFQI) to learn from the shared structure of the Nested MDPs while also being able to adapt to the specific dynamics of the separate MDPs. NFQI is developed and analyzed empirically in the simplest setting where there are only two variants of dynamics are present, where there is a nominal imbalance between expected and out of distribution observations. NFQI is compared to standard FQI and standard transfer learning on an augmented Cartpole task as well as medical treatment task derived from retrospective EHR data. Extensive analysis on the learned policies is performed to establish the anticipated benefits of using NFQI in these settings.","This paper proposes nested policy fitted Q-iteration (NFQI), an adaptation of the well-established Fitted Q-Iteration (FQI) algorithm originally proposed by Ernst, Geurts, and Wehenkel in 2005 [1] to the setting where the agent is required to learn policies for two different MDPs while sharing information between them. At a high level, the authors' proposed modification involves learning a function $f$ which approximates the Q-function, $Q^{\pi}(s_t, a_t) = r_t + \gamma \mathbb{E}_{s'}[V^{\pi}(s')]$ by decomposing the $Q$-estimate into a shared component and a subgroup-specific component.   Concretely, they represent $f$ as $f(\mathbf{s}, \mathbf{a}, z) = g_s(\mathbf{s}, \mathbf{a}) + \mathbb{1}[z=1]g_f(\mathbf{s}, \mathbf{a})$ where $g_s$ models the $Q$-value using parameters shared across groups and $g_f$ models a group-specific modification to the estimated $Q$-value.  Through both simulated experiments (CartPole) and an observational dataset (electrolyte replenishment optimization on MIMIC) the authors demonstrate that their NFQI approach performs better than do alternative approaches which do not take known subject-specific group information into account.  [1] Ernst, Damien, Pierre Geurts, and Louis Wehenkel. ""Tree-based batch mode reinforcement learning."" Journal of Machine Learning Research 6 (2005): 503-556.","This paper considers decision making in nested MDPs where there are two distinct groups having partially shared (but unknown) state dynamics. A new algorithm called NFQI (nested fitted-Q iteration) is presented, and it’s based on a simple modification to FQI, a commonly used offline RL algorithm. Baseline comparisons and sensitivity analyses were done in a set of simulated experiments and using OpenAI gym CartPole. The proposed approach is then applied to a real-world RL task formulated from MIMIC-IV EHR data. ","The paper proposes an off-policy RL called NFQI (Nested Fitted Q-Iteration) as an extension of Fitted Q-iteration to estimate group-specific policies and account for group structure in the data having two pre-defined groups of observations (background and foreground). NFQI imposes a structure on the family of function to approximate the Q function with guarantees to converge to Bellman optimal Q-value function. NFQI is trained in two stages: The first stage the shared component is trained with all samples. In the second stage, the foreground component is trained using only the foreground samples. The NFQI method is validated on a nested cart-pole environment where the background is the original environment and foreground includes a constant force that pushes the cart to the left. NFQI is also validated using a real-world clinical data from MIMIC dataset. ",0.17482517482517482,0.14685314685314685,0.1958041958041958,0.09047619047619047,0.1619047619047619,0.25,0.11904761904761904,0.25,0.19718309859154928,0.2261904761904762,0.23943661971830985,0.14788732394366197,0.141643059490085,0.18502202643171806,0.19649122807017544,0.1292517006802721,0.19318181818181818,0.18584070796460175
695,SP:9709fc3d8c95ef345d9de145a9fc249a1da8fdfc,This paper is proposing smooth normalizing flows for modeling probability distribution in physical systems.  This is because the existing flow-based model approximates distributions and energies by computing forces and higher-order derivatives based on smoothed energies. This work aims at addressing this challenge by introducing a class of smooth mixture transformations working on both compact intervals and hypertori.  The proposed methods show advantages on two points: 1) enable training by force matching to simulation data and 2) used as potentials in MD simulations.  ,"The authors introduce a flow architecture, which they call smooth normalizing flows, that can operate on compact intervals and hypertori and has a C-infinity-smooth transformation. Furthermore, they propose to train normalizing flows via force matching, i.e. matching the gradients of the flow transformation which those of the energy function of the target distribution. They also show that a transformation with non-analytic inverse can be used to define a flow model which is trained by evaluating the inverse via black-box root finding and computing the gradients through the forward pass of each layer. On the experiment side, the authors apply their method to a toy example as well as the Boltzmann distribution of a complex molecule. They demonstrate that their flow architecture is expressive and the presented training procedure can be successfully used. When force matching is used, the gradients of the flow transformation resemble the respective gradients of the energy function of the target indeed more accurately and they can even be used as forces in molecular dynamics simulations.","The paper introduces a class of $C^k$-smooth ($k>1$) transformations working on compact intervals and hypertori that can be used to construct normalising flows. Such type of smooth transformations are important in physical systems as (1) they respect topological constraints and (2) higher-order derivatives have physical interpretations; however, existing transformations either don't respect the constraints or are limited to have only $C^1$-smoothness. Learning flows consisting mixtures of proposed transformations that requires black-box root-finders is made efficient through gradients derived by the inverse function theorem. The benefit of accessing to higher-order derivatives is demonstrated by training normalizing flows with force-matching regularisation and dynamics simulations of trained flows.","The paper considers a set of problems that come up when fitting normalizing flows to datasets of interest in physical sciences, and proposes a set of solutions to those problems.  To begin with, when it comes to datasets in physical sciences we might be interested in matching the *gradient* of the log density of the target distribution, as well as the log density itself. This is because this gradient can correspond to the *forces* acting on the physical objects, and as such the gradient is important to get right, in order to make the model useful in downstream applications, e.g. simulation. Furthermore, we often have access to the *true* forces at the points in the training sample, and would like to regularize the model to make its gradients match the available true gradients. Authors consider a straightforward way to implement such regularization: adding the MSE between the model's gradient and the true gradient to the training objective. However, doing this requires a normalizing flow model that is at least $C^2$-smooth, as the second-order gradients will be used during optimization.  Moreover, the data of interest in this context often has non-trivial topology, containing quantities such as angles, directions or axis. There's been prior work on building normalizing flows suitable for such data, but authors point out that there is no existing model that would be a) restricted to a manifold of interest (say, a circle or a torus)  b) $C^{\infty}$ (or at least $C^k$ for some $k > 1$) continuous c) *expressive*, i.e. able to represent complex densities on the manifold  Authors propose a way to construct a normalizing flow that *would* have the properties above. More specifically, authors parametrize a $C^k$-continuous univariate diffeomorphism on $[0,1]$, which could then be turned to a diffeomorphism on a circle, torus etc. using methods by Rezende et al. [34], and/or used as the elementwise transformation in coupling/autoregressive layers.   Unfortunately, to make the transformation above expressive, authors have to consider a mixture of simpler transformations, hence the transformation has no analytic inverse. This is not an uncommon limitation when it comes to normalizing flows, where inverting some of the flows (e.g. for sampling) requires performing numerical root-finding. This presents a problem in the context of this work: in literature it has been demonstrated that it is desirable to train the flow on a convex combination of a forward and an *inverse* KL divergences. The latter is the one that presents a problem as it requires sampling from the flow *and* evaluating the gradients of its inverse during training. The cost of doing this can be prohibitive when the inverse is not analytic. Authors derive a way to compute said gradients by *only* using samples from the model and the gradients of the forward transformation, which are typically readily available.  Finally, authors evaluate the proposed solutions on a battery of benchmarks, both synthetic and real, showing that the final model demonstrates desirable behaviour when compared to a strong baseline. ",0.25,0.2261904761904762,0.30952380952380953,0.16091954022988506,0.3045977011494253,0.28448275862068967,0.1206896551724138,0.16379310344827586,0.05108055009823183,0.2413793103448276,0.10412573673870335,0.06483300589390963,0.16279069767441862,0.18999999999999997,0.08768971332209106,0.19310344827586207,0.1551976573938507,0.10560000000000001
696,SP:97680345663884c24b887db7c8144926b605de98,"- This paper proposes F-MPNN, which extends the expressive power of MPNN whilst preserving their O(n) cost in each iteration. - This paper provides some theoretical guarantees for F-MPNNs.  -- It shows that F-MPNN can be at most as expressive as F-WL-test -- It compares the expressive power between F-WL-test and k-WL-test. - Certain improvements are shown in experiments. ","This paper proposes local graph parameter enabled GNNs as a framework for studying the local higher-order graph structural information. Local graph parameters can be added to any GNN architecture, and are cheap to compute. In terms of expressive power, their proposal lies in the middle of GNNs and their higher-order counterparts. Further, they propose several techniques to aide in choosing the right local graph parameters. The experimental evaluation shows that adding local graph parameters often has a positive effect for a variety of GNNs, datasets and grpah learning tasks.  ","This paper proposes a novel GNN framework, F-MPNN, to efficiently increase the expressive power of MPNNs. The proposed framework uses local higher-order graph parameters which are the homomorphism counts of small patterns (graph sets). The adoption of different patterns will result in different MPNNs.   From a theoretical perspective, the expressive power of F-MPNN is characterized based on the WL algorithm, and is compared to higher-order MPNNs. Since the performance of F-MPNN depends on the selection of patterns, how different patterns affect the expressive power is also investigated.  The theoretical findings are further validated via extensive experiments with various GNNs, datasets, and graph-related tasks. The results demonstrate the promising improvements achieved by using the proposed efficient local graph parameters. ","This paper provides a theoretical framework attempting to unify a variety of recent methods in the Graph Neural Network literature, which all follow a similar conceptual approach: they introduce extra structural features in the input of the Neural Network that cannot be computed by traditional message passing. The main formalism underlying the framework is that of homomorphism, which is an adjacency-preserving mapping between two graphs. Differently from subgraph isomorphism, homomorphisms are not necessarily bijective (think of walks vs paths).  The authors define $\mathcal{F}$-MPNNs as a message passing framework where node features are enhanced with the homomorphism counts of every pattern in a collection $\mathcal{F}$. The authors derive a deep characterization of the expressive power of $\mathcal{F}$-MPNNs, both in comparison to the k-WL hierarchy and between different instantiations thereof,  show how it can incorporate other approaches, and answer numerous research questions that have been left open by the most related paper, that of Bouritsas et al., arxiv’20. The method is also validated experimentally, by enhancing GNNs with homomorphism counts of cycles and cliques, on node, edge and graph level tasks, showing consistent improvement against various conventional MPNN counterparts.",0.1875,0.3125,0.25,0.2857142857142857,0.24175824175824176,0.28225806451612906,0.13186813186813187,0.16129032258064516,0.08205128205128205,0.20967741935483872,0.11282051282051282,0.1794871794871795,0.1548387096774194,0.2127659574468085,0.12355212355212356,0.24186046511627904,0.15384615384615385,0.21943573667711602
697,SP:976e004ab74ce6469c5cd8475af52483ac47b9c6,"This paper studies the limiting eigenvalue-distribution of the gram matrix $M$ associated with a random feature model, and extends current theory (Pennington and Worah 2017; Benigni and Péché 2019, etc.) to the case of multiple layers with additive layer-wise additive bias. Understanding the distribution of eigenvalues of the gram matrix useful for the understanding of the training dynamics and generalization in neural networks.  ","This paper studies the limiting singular value distribution of non-linear random matrix of the form Y=f(WX), where W and X both consist of iid entries and f is applied entrywise, in the proportional growth asymptotics. The limiting law is characterized in terms of its Stieltjes transform as the solution to a self-consistency equation. The same result was obtained before by Pennington and Worah [21] in the Gaussian case and later extended by Benigni and Péché [6] to the general case. The difference is that the method in the current paper is resolvent-based. However, it is unclear what the advantage of this approach is. For example, is the result stronger than that of [6] in the sense of the required assumptions? The authors should clarify the main contribution of this work and what is new.  ","This paper studies the spectral distribution of random matrix inspired by a one-hidden layer neural networks. In the first model, the matrix is obtained by applying a nonlinear function entrywise to the product of large rectangular matrices with zero mean IID entries. In the second model, a rank-one bias term is added before applying the non-linearity.  The main results  establish convergence of the emprical spectral distribution to a non-random limiting measure in the regime where the dimensions of the matrices scale linearly. This limiting measure is characterized in terms of the the second moments of the distributions on the matrices, the relative scaling of the dimensions, and certain properties of the non-linearity (described by Gaussian integrals).   While the result for the first model has been derived previously (using moment based methods), the result for the second model is novel. Furthermore, in comparison to the prior work, this paper uses the resolvent method, which requires an evaluation of the cumulants instead of the moments -- a potentially benefit of this approach is that is can be applied more broadly. There is some discussion about the implications of the bias term in the context of multilayer networks. Specially, a non-zero bias preclude the certain invariance properties seen in the setting without bias. ","The authors study the asymptotic eigenvalue distribution of the post-activation Gram matrix in a one-hidden-layer neural network, in a regime where all matrix dimensions are growing proportionally. For input data with i.i.d. centered entries and a weight matrix with i.i.d. centered weights, the authors provide a new proof (under weaker distributional assumptions) of a result first due to Pennington and Worah that the asymptotic eigenvalue distribution is characterized by a quartic equation in its Stieltjes transform. The authors then provide an extension of this result to a model with i.i.d. Gaussian biases in its hidden layer. A qualitative difference in the setting with biases is that---in contrast to the bias-free setting, where special choices of activation function can allow the eigenvalue distribution to remain the same as that of the input Gram matrix, and hence remain constant across multiple layers---the authors show that the eigenvalue distribution with biases must necessarily evolve over layers (cf. Remark 2.6 and the discussion on page 7).  The proof is based on deriving the fixed-point equation for the expected Stieltjes transform using the cumulant-expansion generalization of Gaussian integration-by-parts, and explicitly analyzing the asymptotic mixed cumulants of the entries of the post-activation matrix. A technical challenge is to translate the more easily-obtained understanding of mixed cumulants of the pre-activation entries to the post-activations, and the authors accomplish this using a Fourier-analytic technique.",0.4090909090909091,0.3787878787878788,0.4393939393939394,0.3357142857142857,0.2857142857142857,0.24186046511627907,0.19285714285714287,0.11627906976744186,0.11693548387096774,0.2186046511627907,0.16129032258064516,0.20967741935483872,0.2621359223300971,0.1779359430604982,0.18471337579617836,0.2647887323943662,0.20618556701030927,0.224622030237581
698,SP:97bb8206af2540958e50b00ea7611c1084ed9aa7,"The paper explores episodic memory and answers questions related to the impact of context and the influence of intrinsic image properties. They gathered data through an experiment with 47 elder subjects over 3 months, which led them to build a a novel dataset for event memorability. This allows them to build CEMNet, a network that predicts event memorability at the item level.    ","In service of studying event memorability, the authors construct a corpus of 10.6K images collected by 47 elderly study participants. The collection process involves the participants wearing cameras that take photographs at the rate of 2 frames per minute, and then regularly checking-in with researchers who gather memorability ratings from the participants for each image. The authors explore a variety of intrinsic (i.e., having to do with the image itself) and extrinsic (i.e., having to do with how the image relates to other images in the participant's log) feature sets for predicting the ratings. They find that extrinsic factors are the best predictors, and that adding additional CNN image features adds only a bit of extra prediction capacity.","EDIT AFTER AUTHOR RESPONSE: I read the other reviews, and I did not see the need to make any changes to my review.  This paper develops a novel experimental paradigm and introduces a dataset obtained from a lifelogging experiment where subjects rate how well they remember events corresponding to photos taken by first-person view photos.  It answers the question of whether intrinsic image features or extrinsic contextual factors are more predictive of memorability, and it further explores which factors are the most important for predicting memorability.","This paper makes 3 contributions. First, the paper proposes and builds an event memorability dataset, wherein life-logging and subsequent event recollection data are collected from 47 elder subjects over a period of 3 months (Section 3.1). Next, the authors investigate several potential predictors of event memorability, including intrinsic image cues and extrinsic factors (Section 3.2-4.1). Lastly, the authors consider baseline models, in the form of a contextual event memory network to predict event memorability (Section 4.2 and 5). ",0.20967741935483872,0.1774193548387097,0.27419354838709675,0.0975609756097561,0.13008130081300814,0.1724137931034483,0.10569105691056911,0.12643678160919541,0.20238095238095238,0.13793103448275862,0.19047619047619047,0.17857142857142858,0.14054054054054055,0.1476510067114094,0.2328767123287671,0.1142857142857143,0.15458937198067632,0.17543859649122806
699,SP:97e43fffcb1cfbcfa23372b4c3f060307de5de23,"This paper tackles the knowledge grounded dialogue generation task. The paper primarily explores two aspects of knowledge expression: (1) structure of the response, and (2) style of the content. Hence, the proposed model introduces two sequential latent variables to pre-trained encoder-decoder models like BART. The paper proposes a segmentation based generation model which first predicts if the current token to be generated belongs to the previous segment or not and if not then it predicts the type of the new segment. Segment types are knowledge-related (conditions on the knowledge) or knowledge-irrelevant (conditions on the dialogue context).  The paper proposes a novel model that explores the expression of knowledge in grounded conversations. The proposed model leverages pre-trained encoder-decoder models and extends their ability by adding two latent variables: one for representing structure and the other for style. Additionally, the proposed model has a variational segmentation-based decoder. ","The authors study the problem of knowledge-grounded conversation, and propose a segment generation process to better “express” knowledge in the conversation. Specifically, the authors identify that expressing a response can be broken down into two components: the structure of the response and the content style; the former marked by either “knowledge-related” or “knowledge-irrelevant” segments, while the latter is characterized by positive, negative, or other styles of formulating a response. The generation model is based on a normal encoder/decoder architecture, but the decoder is influenced by various introduced latent variables when determining how to generate the next token. A “boundary” indicator informs the decoder when it has reached the boundary of a segment (and must therefore transition), and a “module” indicator determines which structure/segment type to use when generating; these indicators inform the decoder which encoded representations to attend to when generating and are learned via weak supervision. The authors conduct experiments on two knowledge-grounded datasets and show that their model outperforms strong baselines in both zero-resource and low-resource settings, and additionally show that their method is more adaptable to training data (as it can learn more in a low-resource setting than the other models). The authors additionally show that the model is more controllable in a sentiment-control setting, such that sentiment classifiers indicate the controlled responses more effectively model a positive or negative sentiment depending on the setup.",This paper explores the knowledge expression in knowledge-grounded conversation on two benchmark datasets - Wizard of Wikipedia(Wizard) and CMU Document Grounded Conversation(CMU_DoG). The authors propose a novel variational segmentation-based model to control and capture the expression style by using a few examples. The proposed model outperforms the SOTA on both datasets.  ,"The focus of this research piece is on the introduction of a new model for knowledge-grounded open domain dialogue, that can handle different styles in generation. The authors identified that in NLG there are two aspects of generation, the knowledge-related, which is based on the prior knowledge and is factual, and the knowledge-irrelevant, which gives the style and the structure of the rest of the generation. Concretely the authors propose an encoder-decoder model where the three (or more) decoders are available, one based on context, other on prior knowledge and a third or more models which are trained on specific generation style. A module indicator is trained to choose which model needs to be used.  The model is compared against BART and ZRKGC on  Wizard of Wikipedia and CMU_DoG in terms of F1, Distinct unigrams and bigrams showing improvements over both models training with Reddit and Reddit + 10% of annotated data. Furthermore, the model is compared against DialoGPT and ECM in terms of sentiment control generation showing a significant difference compared to its competitors",0.2631578947368421,0.13815789473684212,0.24342105263157895,0.07983193277310924,0.18487394957983194,0.32727272727272727,0.16806722689075632,0.38181818181818183,0.20670391061452514,0.34545454545454546,0.24581005586592178,0.1005586592178771,0.20512820512820512,0.2028985507246377,0.22356495468277948,0.1296928327645051,0.21103117505995206,0.15384615384615383
700,SP:97ff8384cb07ccee86218641f2b9e66628a64bd2,"This paper proposes leveraging ""information gain"" with Bayesian Neural Networks to improve the robustness of Bayesian Neural Networks. Specifically, it maintains a set of particle models and uses SVGD to update this set of particle models. Furthermore, it uses the PGD-based attack to a specific particle model to craft adversarial examples and enforces the information gain of adversarial examples to be close to that of the corresponding benign examples. ","In this paper, the authors propose a conjunction of Bayesian learning and regularization via information gain as a means of adversarial defense. In particular, they start from the perspective that Bayesian learning of neural network parameters ought to be more robust to adversarial examples, and then state that by jointly minimizing the risk and regularizing adversarial and clean information gain that the resulting Bayesian posterior will be even more robust.   While this -- robustness and adversarial training of Bayesian neural networks -- is a worthwhile and important direction, the authors make several crucial mistakes and literature oversights which inhibit the potential impact of this work. ","In this paper, the authors propose to learn a multi-modal posterior of the Bayesian neural network to defense the adversarial attacks, which can prevent the collapse and encourage the diversity accordingly. The authors further investigate the information gain for adversarial Bayesian learning, which is used to guide the training a BNN with the information gain bound. Experimental results demonstrate tis superior performance for adversarial attacks. ","This paper explores adversarial training for Bayesian Neural Networks. In contrast to prior work which used Gaussian variational posteriors to estimate BNN parameters during adversarial training, this work uses Stein Variational Gradient Descent (SVGD) on a loss potential that includes an adversarial term similar to standard adversarial training and a term encouraging similarity between the Information Gain (IG) on natural and adversarial images. They evaluate PGD and adaptive adversarial attacks and find significant performance improvement over standard adversarial training and adversarial BNN. A theoretical justification is presented.",0.2714285714285714,0.22857142857142856,0.24285714285714285,0.22330097087378642,0.1650485436893204,0.22727272727272727,0.18446601941747573,0.24242424242424243,0.19540229885057472,0.3484848484848485,0.19540229885057472,0.1724137931034483,0.21965317919075147,0.23529411764705885,0.21656050955414013,0.27218934911242604,0.17894736842105266,0.19607843137254902
701,SP:987533ea1270a685a753c36ae73313414dbd03a6,This paper proposes a method to accelerate Stackelberg Actor-Critic by ignoring the terms that contain Hessian in the indirect gradient term and applying block-diagonal approximation technique to remaining inverse terms. They prove a faster convergence rate of the proposed method to a fixed point. Experiments are conducted to validate the fast convergence and stability. ,"This paper aims to establish a computational efficient Stackelberg training scheme for deep-learning-based actor-critic methods. The proposed approach,  Fast Stackelberg Deep Deterministic Policy Gradient (FSDDPG), considers the block diagonal approximation technique to reduce the training complexity while improving the convergence rate. This paper conducts both theoretical analysis and empirical evaluation to demonstrate the strength of the proposed approach.","This paper improves upon Stackelberg Deep Deterministic Policy Gradients by proposing a set of strategies on how to deal with the Hessian part of the gradient. This should overcome the major limitations of this class of methods, which are the great time complexity and the slow converging rate with respect to the standard actor-critic framework. The authors provide a formal justification on why removing parts of the Hessian and using a block-diagonal approximation is still achieving convergence. Time complexity is analyzed for many variations of the algorithm and the experimental session provides mixed results.","This work proposes a variant of the Stackelberg actor-critic algorithm, which treats actor-critic as a Stackelberg game with the actor being the leader and critic being the follower. Based on Stackelberg AC, this paper proposes to adopt deterministic policies in order to simplify the computation of the implicit gradient. Additionally, the authors propose approximation schemes including dropping small-order terms and matrix inversion via block diagonal matrix approximation. The computational complexity of gradient computation is established. Moreover, under relatively strong assumptions, the proposed method is shown to be convergent. Experiments on Mujoco are conducted to demonstrate the efficacy of the method.",0.32142857142857145,0.2857142857142857,0.42857142857142855,0.26229508196721313,0.3442622950819672,0.21875,0.29508196721311475,0.16666666666666666,0.23300970873786409,0.16666666666666666,0.20388349514563106,0.20388349514563106,0.3076923076923077,0.2105263157894737,0.30188679245283023,0.2038216560509554,0.25609756097560976,0.21105527638190955
702,SP:98c7dbef15de69c4c937a71655e727e0fda33218,The paper under review considers the generalized SDLG algorithms with exponential family noise introduced to the mini-batch gradient descent. The analysis of the error bound is based on the control of the expected stability. Experiments are provided to support the proposed EFLD SGD algorithms.,"This paper provides generalization guarantees for exponential family sgld which is a more general setup than standard gaussian noise langevin dynamics as it allows for noise settings from any member of the exponential family. The authors point out that the setup they consider is a strict generalization of the standard langevin dynamics, and give another explicit example of sign sgd based generalization.   The authors use and show gradient discrepancy as opposed to gradient norms as used by Li et al, and gradient incoherence as used by Negrea et al. but the latter have 1/\sqrt{n}. Overall, from useful empirical observations it seems that the bounds proposed by the authors are tighter than these other bounds. ","This paper studies the generalization of iterative noisy learning algorithms. First of all, the authors provide a new generalization bound based on the Hellinger distance. This bound is basically based on the Hellinger distance between the output of the algorithm when one of the points in the training set is replaced with a fresh sample. They they use this bound to provide a generalization error guarantee for a broad class of noisy iterative algorithm which is defined as W_t = W_{t-1} - eta_t, where eta_t conditioned on the past iterated follows an exponential family distribution which for instance include SGLD. For this broad class, they provide a unified generalization bound that includes SGLD and Noisy Sign-SGD. Finally, the authors provide numerical study of their bound for several benchmarks and show that their bound achieves better estimate of generalization error. ","This paper improves generalization bounds based on the notion of stability. The notion of stability measures the effect of resampling a data point to the output of the algorithm. The paper improves known analyses in two directions:  - It proposes to use the notion of expected stability rather than uniform stability, which should to be tighter. - It generalizes the analysis to noises in an exponential family rather than Gaussian noise. ",0.3111111111111111,0.3333333333333333,0.26666666666666666,0.1810344827586207,0.13793103448275862,0.14685314685314685,0.1206896551724138,0.1048951048951049,0.17391304347826086,0.14685314685314685,0.2318840579710145,0.30434782608695654,0.17391304347826086,0.15957446808510636,0.2105263157894737,0.16216216216216217,0.17297297297297298,0.1981132075471698
703,SP:990e089d51ed22f4a84b622c016430da89bd3595,"The paper proposes a an implicit representation of surface displacement fields that is well learnable by neural networks. The network is based on two SIREN architectures that separate low and high frequency information in the model. One returns an SDF of the rough shape while the second returns a displacement field at the surface of the SDF. In addition to this architecure (which existed before but for a different application), the paper introduces an inverse implicit mapping that speeds up the evaluation of the loss, and a feature based UV map that can be transferred between similar shapes. The paper compares to previous work on SketchFab, and specifically evaluates against a vanilla SIREN model where this method results in both a smaller model size as well as more detailed reconstruction. Additionally, there are experiments that show how details can be transferred across different (but similar) models. ","This paper proposes an implicit neural representation for 3D geometry, named implicit displacement fields. The method is consists of two SIREN networks as baseline models to approximate: a coarse base shape representation (low-frequency) and an implicit displacement field (for high-frequency details). Different from previous works, the displacement field is implicitly represented by a SIREN network. The coarse and fine networks design also enables applications such as shape manipulation.","This paper proposes an implicit representation for shapes, which utilizes two SIREN networks of different frequencies representing base surface and displacement respectively. In addition, the authors introduces transferable implicit displacement fields, which enables the transferability of displacements between aligned shapes. The method is a natural extension to the displacement mapping method and is theoretically grounded. The evaluation results show that this method could represent shapes with sharper details while using less storage compared to baseline methods.","The paper addresses the problem of obtaining a 3D surface reconstruction from point clouds. This has been one of the most studied topics in 3D vision, with different kinds of approaches. The authors tackle this problem using implicit representations, specifically using signal distance functions and query features. The main contributions are a consideration of implicit displacements for modeling different frequency levels and introducing a transferable implicit displacement field that replaces the common coordinates inputs with constructed transferable features. ",0.15753424657534246,0.1643835616438356,0.1232876712328767,0.2714285714285714,0.15714285714285714,0.18421052631578946,0.32857142857142857,0.3157894736842105,0.23076923076923078,0.25,0.14102564102564102,0.1794871794871795,0.21296296296296294,0.21621621621621623,0.1607142857142857,0.26027397260273977,0.14864864864864863,0.18181818181818182
704,SP:9941d8f249f2dda70f050e2f3b66db13fc3471fd,"This paper presents an online planning algorithm, called Adaptive Online Packing-guided Search (AdaOPS), for Partially Observable Markov Decision Processes. The article addresses the problem of improving the approximations of a belief using a particle filter. This improvement comes from two elements: adaptive particle filtering and belief packing.  Specifically, During the planning step, a particle resampling procedure is performed adaptively on new leaf nodes. This happens when the number of particles in the belief of the node is greater than twice the Effective Sample Size (ESS), as explained in (Kish [11], Liu [12]). AdaOPS use a KLD-sampling for this adaptive resampling (Fox [13], Li, Sun, Satter [15]), this guarantees a good approximation using a limited number of particles.  The article also presents a technique called Belief Packing, inspired by (Zhang, Hsu, Lee [17]). It is used to fuse similar beliefs, thus the planning procedure explores only beliefs that are significantly different. This improves the scalability of the online algorithm.  The paper shows that with enough planning time, this packing produces results that are $\epsilon$-optimal (i.e., the difference between the original belief and the belief that employs the packing is lower than a constant $\epsilon$ with a probability of at least $1 - \eta$). The article also presents an empirical evaluation of AdaOPS in four different domains (Laser Tag, Rocksampe, Roomba, and Light-Dark). The results show that AdaOPS can outperform state of the art planning algorithms (POMCPOW, ARDESPOT).","This paper proposes an online tree-search algorithm for partially observable Markov decision processes (POMDPs). Taking sampling-based tree search style (e.g., POMCP) approach as a basis, the paper proposes to employ an adaptive particle filter that tunes the number of samples in each branch of the tree using the KLD-sampling method (Fox, 2001). To prune the search tree, the paper proposes a new method of observation aggregation called belief packing: here, observation branches resulting in similar beliefs (and hence similar values) are aggregated together. The benefits of these ideas applied to online tree search planning are demonstrated empirically.","This paper proposes an online POMDP algorithm that performs belief tree search to find an action for current belief at each step. Building on the HSVI algorithm and more recent works using Monte Carlo belief tree approximations, the paper leverages on two ideas to achieve better performance: KLD sampling for more accurate belief representation, and the idea of belief packing to combine observation branches. The algorithm seems to have strong performance on several benchmark problems. A theoretical analysis is given.","This paper presents an online method for approximately solving POMDPs, called AdaOPS. The method combines several known techniques for belief-tree construction and sampling. Specifically, it maintains upper and lower bound and uses the highest upper bound for action selection and weighted excess uncertainty for observation selection. It uses weighted particles to represent beliefs and use the weights together with KLD for resampling to improve belief estimates. It then uses belief packing to aggregate values of nearby beliefs. The methods are tested on four benchmark problems and indicate better performance compared to DESPOT and POMCPOW. Convergence results that expand the work in [18] are also presented.  ",0.15416666666666667,0.09583333333333334,0.11666666666666667,0.2376237623762376,0.1782178217821782,0.25,0.36633663366336633,0.2875,0.2641509433962264,0.3,0.16981132075471697,0.18867924528301888,0.21700879765395895,0.14375000000000002,0.16184971098265896,0.2651933701657458,0.17391304347826086,0.21505376344086022
705,SP:9960a5f185e427a52ea1118ec13f0b8942b65ec2,"This paper proposed an algorithm named Drop-DTW, which enables DTW to drop outliers from the sequence-to-sequence matching. A differentiable Drop-DTW is trained for temporal step localization tasks, representation learning, audio-visual retrieval and localization. Drop-DTW gets good performance on these tasks. ","The paper presents a variant of DTW, denoted as Drop-DTW.  The author introduce drop costs, which is a criteria for deciding whether or not to ignore frames in times series. By ignoring some frames in input times series, the alignment performance between times series containing non-corresponding frames can be improved. They also propose a differentiable version using the softmin function for use as a loss function. Several experiments (e.g., video retrieval, weakly supervised action labeling, and representation learning) demonstrate the superiority of the proposed method over existing method.","I really enjoyed reading this paper. It was very well written and positioned against existing works, and the technical aspects explained clearly. I have a few concerns but overall it would make for an excellent inclusion into the NeurIPS program.  The paper addresses the problem of aligning two contiguous temporal sequences, e.g., an untrimmed video and a given activity to match. Unlike previous approaches that make use of dynamic time warping to align the sequences, this paper proposes to model and remove outliers from the matching process. This could be done as a two-stage process: first find and remove outlier elements and then, second, apply dynamic time warping to the remaining sub-sequences. Rather than doing this the paper formulates a joint objective that aligns inliers (enforcing the monotonicity constraint) while allowing outliers to not be matched (i.e., not incur a mismatch penalty).  The approach is differentiable and is used as the loss function of a deep learning model. Experiments show that the method outperforms previous approaches on some standard tasks. In particular it does better than the two-stage baseline of first removing outliers and then matching as two disjoint steps.","Paper proposes an approach for a sequence-to-sequence alignment specifically designed to handle sequences with outliers. The core of the approach is a simple extension to the DTW where ""no match"" is allowed and carries a certain cost. The standard DTW can then solve for the matching path, while selecting outliers (elements matched to ""no match"" token). This DTW variant is converted into a loss by following for mutation in [Hadji et al] and then shown on a number of tasks where it performs well.  ",0.2391304347826087,0.30434782608695654,0.2826086956521739,0.18681318681318682,0.16483516483516483,0.1076923076923077,0.12087912087912088,0.07179487179487179,0.1511627906976744,0.08717948717948718,0.1744186046511628,0.2441860465116279,0.16058394160583941,0.11618257261410787,0.19696969696969693,0.11888111888111887,0.1694915254237288,0.1494661921708185
706,SP:99ade8e1b171c98710635eea9251fc41e08c46aa,This paper presents rigorous analyses of self-supervised learning from different aspects. Here are their findings:  1. The pretraining dataset should match the target dataset in terms of being object- or scene-centric. 1. MoCo is robust to changes in the class distribution of the dataset (pre-trained on uniform ImageNet vs long-tailed ImageNet) 1. The model trained on data with discriminative features (ImageNet) performs equally well with the model trained on data without (BDD100K) in the fine-tuning setting for semantic segmentation tasks.  The authors also propose three strategies to improve MoCo: 1. Multi-scale constrained cropping data augmentation 1. Mixture of standard augmentation and AutoAugment 1. Including nearest neighbors as positive pairs,"- Current contrastive methods mainly focus on curated iconic datasets like ImageNet. - The paper studies the effect of dataset biases (like centered object, uniform data distribution) on performance of existing self-supervised learning methods. - The paper shows that MoCo works well across a wide range of datasets. The paper analyses these datasets by separating them into different categories. - In addition to the analysis, the paper proposes improvement by modifications to multi-scale cropping, stronger augmentations and a NN based methods. In particular, the authors observe that random cropping for instance discrimination tasks works well across scenarios.  - The paper shows that multi-crop transform leads to significant gains and gives rise to spatially structured representations which are useful for tasks like semantic segment retrieval and video instance segmentation. ","The paper provides a comprehensive study and a set of improvements for the self-supervised method MoCo. First, the authors challenge the common assumptions that the training dataset has to be object-centric, not long-tailed and the crops overlapping, which provides very useful insights. Then, the authors propose a bag of tricks to improve the performance of MoCo in practice.","The paper studies the current stream of instance discrimination-based self-supervised learning algorithm (MoCo) beyond the typical object-centric dataset, i.e., ImageNet. Experiments are conducted on scene-centric datasets, long-tail datasets, and general-domain datasets. Results show that MoCo works well on these datasets. Furthermore, the authors studied multi-crop and stronger data augmentation for MoCo and showed that these enhanced augmentation techniques improved performance.",0.1565217391304348,0.1565217391304348,0.16521739130434782,0.1111111111111111,0.1746031746031746,0.22950819672131148,0.14285714285714285,0.29508196721311475,0.27941176470588236,0.22950819672131148,0.3235294117647059,0.20588235294117646,0.14937759336099585,0.20454545454545456,0.20765027322404372,0.14973262032085563,0.2268041237113402,0.2170542635658915
707,SP:99bde7f0d191f44750afec97c605e688a719d951,"The paper proposes a method for action anticipation based on a graph representation of the temporal structure of video. Specifically a graph is built using semantic representations of video frames extracted with a 2D CNN backbone as vertices. Temporal reasoning is hence achieved modeling spatiotemporal information flow thorugh message passing across vertices. The approach is based on a message function, an unpdate function and a readout function, all based o a multi-head self-attention block. Experiments compare the proposed approach with respect to other approaches suing RGB inputs on EPIC-KITCHENS-55.","This paper addressed an interesting problem of action anticipation. To this end, the authors proposed a unified recurrence model that generates the graph representation of the video sequence. Extensive experimental results have shown the effectiveness of the proposed method.","The authors propose a framework for video action anticipation, where the task is to observe input frames and predict the action label after an anticipation period (not observed). The proposed architecture builds on top of message passing terminology (message, update, and readout functions) to create a recurrent transformer model that uses self-attention to capture spatial dependencies between frame patches (similar to ViT w/o [cls]) and recurrent module to capture the temporal dependencies across frames. Edge affinity matrix is learned and introduced into the MHSA block (Eqn. 12) as a prior to influence the attention matrix at every layer. Three different explicit edge learning approaches are proposed, in addition to implicit edge learning. The model is evaluated on the Epic Kitchen 55 dataset.","Paper proposes a novel architecture for video action anticipation task. The proposed method used a graph representation via message passing. The key contribution lies in the three explicitly edge learning strategies. These strategies to escape to the trivial estimation which is  purely based on the similarity in input tensors, by bringing the flexibility of edge representations.   1. Edge attention decouples the attention operator in message function into vertex and edge estimation separately. 2. Class token projection performs the outer-product of a trainable vector with supervision signal from class labels. 3. Template bank obtains the edge matrix by the linear combination of trainable templates, based on a selecting module conditions on inputs.  Experiment on the EK55 dataset outperforms the state-of-the-arts algorithms.",0.12903225806451613,0.24731182795698925,0.24731182795698925,0.3076923076923077,0.3333333333333333,0.18548387096774194,0.3076923076923077,0.18548387096774194,0.18548387096774194,0.0967741935483871,0.10483870967741936,0.18548387096774194,0.18181818181818182,0.21198156682027652,0.21198156682027652,0.14723926380368096,0.15950920245398773,0.18548387096774194
708,SP:99be0d0cf2653695c26b88fd511e25de8f31d7a8,"This paper proposed Long-Short Transformer (Transformer-LS), which aggregates sliding window attention (i.e. short-term attention) with linear attention via low-rank projection (i.e. long-term attention), by simply concatenating the two sets of keys. Due to the scale mis-match of the keys from the long and short term attentions, the authors further proposed the dual layernorm (DualLN) method.  Experiments were conducted on Long-range Arena (LRA), auto-regressive language modeling and ImageNet classification. Transformer-LS achieves improvements over previous efficient transformer models on both accuracy and efficiency.","This work presents Long-Short Transformer (Transformer-LS), which integrates a dynamic projection based long-range attention and a local window short-term attention to capture both global and local features. Specifically, 1) the long-range attention is implemented by projecting the Key and Value embeddings into shorter ones (in terms of number of tokens) based on the projection matrix generated from the original Key embeddings and 2) the short-term attention is implemented by dividing the input sequences into disjoint segments. A scale mismatch is identified from the long-range and short-term attention and solved by the proposed DualLN. The experiments are conducted in both NLP and CV tasks.","This paper addresses the quadratic complexity issue in Transformer, and proposes a long-short efficient Transformer model. It aggregates a long-range attention with dynamic projection to model distant correlations and a short-term attention to capture fine-grained local correlations. A dual normalization strategy is used to address the scale mismatch between the two attention mechanisms. The method is applied to both autoregressive and bidirectional models without additional complexity. Experiments on both vision and language benchmarks verify the efficacy of the method.","In this paper, the authors focus on reducing the quadratic computation complexity of the self-attention mechanism in the standard Transformer. They combine local and global context through sliding window attention and matrix projections. During aggregating long-range and short-term attention, they employ two sets of Layer Normalizations to make sure that the model will use both long and short range effectively. Experiments show some promising results on various tasks.",0.33695652173913043,0.2391304347826087,0.20652173913043478,0.23423423423423423,0.16216216216216217,0.24096385542168675,0.27927927927927926,0.26506024096385544,0.2676056338028169,0.3132530120481928,0.2535211267605634,0.28169014084507044,0.3054187192118226,0.25142857142857145,0.2331288343558282,0.26804123711340205,0.19780219780219782,0.25974025974025977
709,SP:99cd3f4611ea3b27f6241006527d6b42abc64c7a,"This paper presents an interesting idea on designing a novel graph neural network to capture the long-range dependency, which is the lack of ability of most existing works. A linear model that extends SGC with implicit infinite layers, EIGNN, is proposed to mitigate this issue. A closed form of the infinite sequences is used to define the layer-wise update rule and eigendecomposition is combined to achieve more efficient training. Experimental results on both synthetic and real-world datasets demonstrate the effectiveness of the proposed EIGNN for capturing the long-range dependency.",The paper proposes an infinite-depth GNN which captures long-range dependencies in the graph while avoiding iterative solvers by deriving a closed-form solution. The eigendecomposition method is introduced to further improve efficiency. Experiments on several datasets shows improved performance.  ,"This paper proposes an efficient method for infinite-depth GNN model that allows learnable transformations in each layer. A more efficient computation (eigen-decomposition) of the close-form solution is discussed and proved in the paper. In the experiments, EIGNN shows superior performance on both synthetic and real datasets which requires long-range dependencies capture. The robustness of the method is evaluated by perturbation of node features as well.","The authors propose a linear infinite-depth GNN, whose forward pass is a convergent series whose limit can be written in closed-form. Since the series involves the powers of the normalised adjacency matrix, the resulting model captures interactions between all pairs of connected nodes. At the same time, the existence of a closed-form expression makes the gradients easy to compute and provides an easy way to train the model. The authors show how the computational complexity of the model can be improved by diagonalising some of the matrices involved in the computation. The paper validates the hypothesis that the model can capture long-range interactions better than existent models in a synthetic setting and also on small real-world heterophilic graphs. ",0.15053763440860216,0.20430107526881722,0.20430107526881722,0.34146341463414637,0.2926829268292683,0.21739130434782608,0.34146341463414637,0.2753623188405797,0.15447154471544716,0.2028985507246377,0.0975609756097561,0.12195121951219512,0.208955223880597,0.2345679012345679,0.17592592592592596,0.2545454545454545,0.14634146341463414,0.15625
710,SP:9a0aa3fd8cd37d124a7547d465afae931dd1d1e7,"The paper presents a new soft-body physics simulator that is end-to-end differentiable, and shows how it supports efficient system identification and control.  Experiments show the value of the simulator for a range of complex realistic situations -- although only in simulation.  The experiments also report ablations of key model components (skeletons, collision contacts). ","This paper proposes a soft multi-body differentiable physics simulation framework based on projective dynamics. The authors propose a top-down matrix assembly algorithm for rigid body simulation algorithms and a new matrix splitting strategy for a generalized dry friction model for soft continuum. The experiments demonstrate that their designs make soft body simulation more stable and realistic compared to other frameworks, and the gradients help accelerate system identification and motion control of soft robots.","The proposed paper introduces a novel method for solving the simulation of soft articulated bodies based on a differentiable solver. The key contribution of the introduced method are a top-down matrix assembly approach for projective dynamics, a generalized friction model, analytic models for muscles, and a unified framework that enables computing gradients in a differentiable manner. The method is evaluated based on a number of meaningful experiments and ablation studies. ","This paper takes on the timely problem of devising differentiable simulators for soft bodies, with a focus on articulated bodies where the soft nature of the material comes with structure in the way different parts are connected together - hence requiring formulation of equations of motion in the sense of multi-body dynamics.  The core contribution is to formulate the simulation problem in a projective dynamics framework, and to exploit the implicit matrix structure in the equations of motion to achieve computational speedups.   This has been implemented in C++ libraries and demonstrated in a few different simulation settings, including a Baymax soft articulated body, a deformable ball falling on cloth and a bridge -  each representing a different form of structure. The main outcome is that this form of differentiable simulation enables faster system identification and control.",0.23636363636363636,0.23636363636363636,0.2545454545454545,0.26666666666666666,0.29333333333333333,0.29577464788732394,0.17333333333333334,0.18309859154929578,0.1037037037037037,0.28169014084507044,0.16296296296296298,0.15555555555555556,0.2,0.20634920634920634,0.1473684210526316,0.273972602739726,0.20952380952380953,0.20388349514563106
711,SP:9a6d0460c60d63f5459eece136ef256266a3b15d,"This paper presents a video prediction model named Clockwork VAE, which leverages hierarchical latent variables and temporal abstraction to learn long-term dependency. It also proposes the new Minecraft benchmark for long-term video prediction. The model is compared with existing video prediction methods on four datasets, and achieves remarkable long-term prediction results. ",The paper proposed a method for video prediction called Clockwork Variational Autoencoders. The method adapts Clockwork-RNNs to sequential VAEs and defines a hierarchy of latents operating at different temporal scales. This allows the model to generate long term videos.,"This paper proposes Clockwork-VAEs, a hierarchical latent dynamics video prediction model. CW-VAE uses different clock speeds at each level to model dependencies occurring at different frequencies. This allows it to perform well on long-term video prediction. The paper presents results on four diverse datasets, including using a new dataset (MineRL) for long term video prediction. ","In this paper, the authors tackle the problem of long-term video generation. To this end, they propose a hierarchical latent variational autoencoder: The Clockwork Variation Autoencoder. This model incorporates a hierarchy of latents separated at different frame rates. This frameworks allows the VAE to model dependencies in the video at different timescales. The authors evaluate their approach on a variety of datasets: a new benchmark from Minecraft, KTH, Moving MNIST, and GQN Mazes. The paper presents promising qualitative and quantitative results with the proposed approach.",0.2037037037037037,0.37037037037037035,0.24074074074074073,0.3,0.375,0.29310344827586204,0.275,0.3448275862068966,0.1511627906976744,0.20689655172413793,0.1744186046511628,0.19767441860465115,0.23404255319148937,0.35714285714285715,0.18571428571428572,0.24489795918367346,0.23809523809523814,0.2361111111111111
712,SP:9ab67536322151a701849fa68b13849f0c92a062,"The paper studies the difference between data-oblivious and data-aware adversaries in ML. In particular, the authors present a result about the effectiveness of such adversaries against feature selection with LASSO and show that the data-aware adversary is provably stronger in such a scenario. Experiments on synthetic and real data suggest that such a separation is also of practical importance.","This paper studies the ""data-oblivious poisoning attack,"" which presumes an adversary without the knowledge of the victim's training set. Once the authors define the threat model of an oblivious attack, they formulate the security game where an adversary crafts poison samples and a victim trains classifiers on the contaminated training set. Under this formulation, the authors compare the power of a full-knowledge attacker with that of the ""oblivious attack."" In there, the authors show that oblivious attackers are weaker than full-knowledge attackers. In evaluation, the authors demonstrated that the number of poison samples requires to cause the same deviation of a specific feature becomes larger in ""oblivious"" attacks. ","This paper studies the (information-theoretic) power of an adversary in poisoning attacks. The authors exhibit a learning model (feature selection with Lasso) in which an adversary that has knowledge of the training set is provably stronger than an adversary that has no knowledge of the training set. (In both cases the adversary also has access to the distribution of the data.) The authors complement their theoretical findings with experiments on both synthetic and real data.  To show their theoretical result, the authors define the notion of unstable and resilient distributions under addition of poisoned data to the original dataset. They show that Gaussian distributions with suitable parameters are both resilient and unstable. This means (acc. to Prop 3.5) that data-aware adversaries can profit from the instability of the distribution to create poisoned data that cause Lasso to incorrectly select certain features with high probability, while the distribution's resiliency ensures that data-oblivious adversaries have to create attacks at random.",This paper raises the need for separating the notions of data-oblivious poisoning attacks from data-aware poisoning attacks. Theoretical analysis on feature selection with LASSO show that data-aware adversaries are provably more devastating compared to data-oblivious adversaries. Empirical results on synthetic and real-world datasets also verify the arguments.,0.25806451612903225,0.4032258064516129,0.3225806451612903,0.32142857142857145,0.11607142857142858,0.1165644171779141,0.14285714285714285,0.15337423312883436,0.38461538461538464,0.22085889570552147,0.25,0.36538461538461536,0.1839080459770115,0.2222222222222222,0.3508771929824561,0.26181818181818184,0.15853658536585366,0.17674418604651163
713,SP:9ab8e7ebd4e31c23ee079f8333dece2a36eef63d,"The paper address a problem in cognitive science: identify the embedding of objects in the brain's semantic space based on the subjective judgments of objects' similarity (odd-one-out task). It proposes a new model (VICE) to address a few issues of the previous model (SPoSE). The new method uses a different form of prior (spike-and-slab) for the embedding, variational Bayes with Gaussian posterior, and appear to perform as well as or better than SPoSE in several metrics","The paper introduces a variational inference method for concept embedding in the odd-one-out task.  The objective is to learn representations that allow to predict the odd object from a triplet.  A variational inference problem is established to learn the representations through a Gaussian variational family with a mixture of two Gaussians as prior (spike-and-slab).  It is not clear how the selection of the variational family fits the prior.  Simultaneously, there is a dimensionality reduction procedure to improve the representations.  The experiments show improvement over the Sparse Positive object Similarity Embedding (SPoSE), and the method is also evaluated with random initialization and it shows to be stable.","The paper proposes a prediction model for the odd-one-out task [Zheng 2019], where the goal is to identify which pair people find to be the most similar within the triplet of pictures. The proposed approach learns a variational model to approximate the distribution over the triplet. The model is compared to SPoSE [Zheng 2019] and shown to outperform on small dataset sizes.","This paper presents a technique for learning image embeddings from similarity data provided as odd-one-out judgments over triplets of images (i.e., ball is more similar to apple than car). The authors build on an earlier technique called SPoSE that learns sparse, non-negative embeddings for images by maximizing the probability of choosing the right pair (where this similarity is calculated using the dot-product of image embeddings) with an L1 penalty on embeddings. In this paper, the authors argue that 1) a spike and slab prior is more suited for this setting and 2) a more principled approach to choosing the number of dimensions in learned embeddings is possible.  They present an improved version of SpoSE (called VICE) that uses variational inference to learn embeddings with uncertainties associated with them and place a spike-and-slab prior on the embeddings to encourage sparsity. They then present a way to prune the set of learned dimensions. This calculates the importance of each dimension first (using the learned uncertainties and false discovery rate), then clusters these, and finds the subset of clusters that give the best validation performance.  They argue that their technique is more principled than SpOSE and show that it performs similarly to it in terms of prediction accuracy. However, in small data regime, their model outperforms SpOSE.  ",0.2962962962962963,0.19753086419753085,0.2839506172839506,0.23636363636363636,0.3,0.359375,0.21818181818181817,0.25,0.10407239819004525,0.40625,0.1493212669683258,0.10407239819004525,0.25130890052356025,0.2206896551724138,0.152317880794702,0.2988505747126437,0.19939577039274928,0.16140350877192983
714,SP:9af2a9faf79a7e6a885e90d8cf0867075ea5ee9d,"This paper presents a new hierarchical transformer architecture with constant connection path length and linear time and space complexity for long-range time series modeling. The module at core is a pyramidal attention network that makes multi-resolution representations in a tree structure and perform attention operations on the tree. A stack of convolutions is used to initialize the pyramidal tree. Experiments show the proposed method is able to make more accurate predictions with significantly fewer attention operations and, as a result, less time and memory expenses.","The authors propose a new architecture to tackle the problem of long sequence temporal forecasting (LSTF) – which looks at capturing long-range dependencies in time-series data by providing more direct paths between the output and distant history.   The Pyraformer takes an interesting spin on the current state-of-the-art sparse transformers, consisting of 2 main components: 1.	A dilated CNN encoder to learn coarse-scale representations at multiple resolutions. 2.	A decoder with a pyramid structure that applies attentions masks to a limited subset of nearest neighbours (i.e. parents, adjacent nodes and children) – effectively sparsifying fully connected attention patterns by imposing a prior structure onto attention patterns. ","This paper proposes Pyraformer, a low-complexity pyramidal attention model for long-range time-series modelling and forecasting. The proposed architecture is build upon a pyramidal attention module (PAM) in which the inter-scale tree structure summarizes features at different resolutions and the intra-scale neighbouring connections model the temporal dependencies of different ranges. The proposed framework harnesses the benefit of both transformer and RNN. Evaluation on Electricity, wind, App Flow, and ETT dataset against baseline models including Informer., LogTrans, Longformer, Reformer, ETC shows superior performance.","The paper proposes a variant of the transformer architecture called pyramidal attention. In this architecture one forms a pyramid graph over the sequence at different resolutions and then applies attention among the neighbors of that graph. This leads to a network that can attend to long sequences of length L with O(L) computation but at the same time attention path between two sequence positions is O(1). This is especially relevant for time-series forecasting as it has potential to summarize the time-series at different scales like hourly, daily, monthly etc. Some experiments are performed on 4 datasets to show the advantage of this architecture over other transformer variants.",0.19540229885057472,0.27586206896551724,0.21839080459770116,0.11818181818181818,0.13636363636363635,0.22093023255813954,0.15454545454545454,0.27906976744186046,0.17117117117117117,0.1511627906976744,0.13513513513513514,0.17117117117117117,0.17258883248730966,0.27745664739884396,0.19191919191919193,0.13265306122448978,0.13574660633484162,0.19289340101522845
715,SP:9b38a23d1fc283a029eada552cf95f7362ac66a5,"To tackle the two limitations of the current robust training algorithms: 1). robust inaccuracy 2). the sacrifice of natural accuracy, this paper proposed a new training method that aims to maximize robust accuracy and minimize robust inaccuracy. Moreover, a robustness-based abstain mechanism is adopted to further boost overall robustness without sacrificing accuracy. Experiments show the effectiveness of the algorithm in terms of fewer robust and inaccurate samples and better robustness, with only marginally reduced natural accuracy.",The paper addresses the issue of trained models being inaccurate but robust for some samples. To address this issue a flexible fine-tuning mechanism is proposed. A robustness-based ensembling method is introduced as well. ,"This submission aims to reduce the prevalence of samples for which a neural network might predict the wrong answer, not just for the sample but also its nearby points in the input space. Authors call this metric robust inaccuracy.  Next, after reducing the robust inaccuracy metric, they take advantage of their models to improve conventional robustness, by taking advantage of their more reliable robustness metric to abstain from samples for which model prediction is not robust (perhaps better to call this consistent, since it can refer to both correct and incorrect predictions). ","This well presented work is motivated by the relevant issue that improving model robustness while maintaining high accuracy can result in robust inaccuracy, i.e. a non-negligible amount of samples get classified incorrectly, but the category remains consistent for their perturbations. Supporting this observation empirically, this work proposes an approach to address this issue.  There are two main technical contributions. The first complements the standard robustness objectives from the literature with a regularisation term penalising robust inaccuracy. The paper provides specific implementations of this term under two setups: empirical (adversarial) and certified robustness. The second contribution is an abstain model: a learned indicator function that decides which samples are to be processed by the robust (but less accurate) classifier, and which ones should default to the non-robust (but accurate) baseline. As a result, this two-stage approach achieves high robustness without compromising the classification accuracy. The experiments on standard benchmarks further confirm, that the robust model consistently decreases the fraction of robust inaccurate samples (albeit at the expense to robust accuracy).",0.11688311688311688,0.18181818181818182,0.33766233766233766,0.2,0.37142857142857144,0.20652173913043478,0.2571428571428571,0.15217391304347827,0.15028901734104047,0.07608695652173914,0.07514450867052024,0.10982658959537572,0.1607142857142857,0.16568047337278108,0.208,0.11023622047244096,0.125,0.14339622641509434
716,SP:9b5c404f8ff404e3eb5b2b515dc8eaddc4c3d70c,The authors of the paper have introduced a multilingual cardiac signal captioning framework. The authors have proposed a neural framework that generates captions for the cardiac signals in multiple languages simultaneously. The authors add an auxiliary task of identifying the language of some of the tokens to improve the performance of the decoder. The proposed framework achieves on par performance with state of the art pre-training methods. ,This paper proposed a new report generation system for cardiac signals. It applied replaced token language prediction (RTLP) settings to improve the report generation performance. Experiments show that the proposed RTLP framework can achieve comparable performance with SOTA models. Extensive analyses were conducted to examine the diversity and the performance in multilanguage settings. ,"The goal of this paper is to develop an approach for generating multilingual ECG reports. The authors propose a new multilingual pretraining method in which tokens are randomly replaced with those from a different language, and the model must learn to identify the language of all tokens. The method – RTLP – performs similarly to MLM, ELECTRA, and MARGE according to BLEU-1, METEOR, and ROUGE-L metrics on generation of ECG reports, and the authors qualitatively assess that the generated reports are clinically accurate. The paper compares monolingual and multilingual versions of the models and find that RTLP benefits from multilingual training.","This paper aims to build a multilingual cardiac signal captioning system to generate ECG reports, which describe the clinical findings in the input of electrocardiogram (ECG) signals. In particular, the proposed system can generate desirable and fluent reports in multiple languages, i.e., German, Greek, English, Spanish, French, Italian, and Portuguese. The experiments on a public dataset verify the effectiveness of the proposed approach, which performs on par with state-of-the-art language pre-training methods.",0.25,0.29411764705882354,0.39705882352941174,0.2641509433962264,0.20754716981132076,0.19801980198019803,0.32075471698113206,0.19801980198019803,0.35064935064935066,0.13861386138613863,0.14285714285714285,0.2597402597402597,0.2809917355371901,0.2366863905325444,0.3724137931034483,0.18181818181818182,0.16923076923076924,0.2247191011235955
717,SP:9bcf2deee9c3583e94dc734cfb3936b1f13ce50c,"This paper studies the role of overparameterization in meta representation learning.  Meta-learning studies how to learn from previous tasks to adapt quickly to new ones, while overparameterization studies the settings where the number of training points is smaller than the number of features. Recent works studied the generalization properties of meta-learning a linear representation but did not focus on the overparameterized setting, where the number of training samples per task is lower than the dimension of the meta-learned representation.  This work presents asymptotic results on the expected risk in the overparameterized regime. Furthermore they propose an algorithm to learn a linear representation together with sample complexity bounds. They also provide some empirical results validating their analysis. ","This paper studies overparametrized representation learning in meta-learning, i.e. representation dimensionality (R) is larger than number of samples ($n_2$) per task. It does so in the setting of linear representations, but goes beyond existing analyses that typically assume that the covariance of task regressors to be low rank and restrict representation dimensionality to be small. The paper characterizes the optimal representation for every dimensionality R (in an asymptotic regime), observes a double descent phenomenon w.r.t. R in the performance of the representation on test tasks, also concludes that the optimal dimensionality can be overparametrized in many cases. It then analyzes the sample complexity of learning such a representation with finite train tasks samples, and finds that choosing R adaptively with number of samples can strike a balance between bias and variance of the representation. An important phenomenon of alignment of feature covariance and task covariance is introduced and its effect on sample complexity and optimal dimensionality is studied. Experiments on simulated data verify some of findings and the presence of the alignment is observed on some real datasets (MNIST, CIFAR).","This paper considers the representation learning for multi-task linear regression problem in the setting with arbitrary feature covariance, arbitrary task covariance, and arbitrary representation dimension. In this setting, it aims to address two questions: (1) given a set of $T$ linear regression tasks with $N$ total samples, what is the optimal representation $\Lambda \in \mathbb{R}^{d\times R}$ for a downstream few-shot linear regression task? and (2) what is the sample complexity for finding this representation? To address these questions, the authors find a closed-form solution for the optimal representation when the fine-tuning phase involves an eigen-weighting procedure, which they argue introduces an inductive bias that enables few-shot success. They show that the key to estimating this optimal representation is estimating the ""canonical task covariance"", which they show necessitates a number of samples growing with the effective ranks (traces) of the task and feature covariances, rather than the ambient dimension $d$. The authors also provide an overall risk bound that shows it is preferable to have smaller $R$ for smaller $N$. ","This paper presents an analysis of the role of representation width when learning a shared feature extractor across multiple tasks. The authors argue that wider representations improve transfer, even when only a small number of labelled examples are accessible to fine-tune the last linear layer of a neural network. Their theoretical analysis focuses on linear regression with a linear feature extractor, with most experiments also on this setting.",0.2689075630252101,0.21008403361344538,0.15966386554621848,0.2,0.0918918918918919,0.10112359550561797,0.17297297297297298,0.1404494382022472,0.2753623188405797,0.20786516853932585,0.2463768115942029,0.2608695652173913,0.21052631578947367,0.16835016835016836,0.20212765957446807,0.20385674931129477,0.13385826771653542,0.14574898785425103
718,SP:9ccaa21cefb1e2b77c50759b88fa2d89e1884a19,"This paper proposes an ensemble-based video compression model to capture the predictive uncertainty of intermediate predictions. A loss is constructed to encourage diversity between ensemble members, and the paper investigates the benefit of incorporating adversarial training in video compression. The experimental result shows that the proposed model can save more than 20% compared to DVC Pro. ","This paper aims to capture the predictive uncertainty and proposes an ensemble-based video compression approach. In the proposed approach, an ensemble-aware loss is also introduced to seek diversity among ensemble members. Experimental results show its performance.","This paper presents an uncertainty-aware video compression framework with an ensemble of MV/residual decoders. To train the network, they use the ensemble-aware loss function which minimizes the loss function for the k best predictions. Moreover, this paper also analyzes underlying uncertainties in video compression, i.e., aleatoric and epistemic uncertainties, with visualization techniques for those uncertainties. Experimental results show that the proposed method can improve the compression performance by about 20% while not significantly increasing the computational complexity.","This paper works on end-to-end deep learning video compression. The authors study the inherent uncertainty and accordingly propose a so-called ensemble approach which is in effect multi-head decoder. A so-called ensemble-aware loss is proposed to encourage the diversity between ensemble members. Further, adversarial training is incorporated. Experiments show that the proposed model outperforms previous state-of-the-art models such as DVC Pro (Lu et al., 2020b) and Lu et al. (2020a), and the ablation study proves the effectiveness of each module. ",0.2807017543859649,0.3333333333333333,0.40350877192982454,0.3684210526315789,0.47368421052631576,0.19753086419753085,0.42105263157894735,0.2345679012345679,0.26136363636363635,0.1728395061728395,0.20454545454545456,0.18181818181818182,0.3368421052631579,0.2753623188405797,0.3172413793103448,0.23529411764705885,0.28571428571428575,0.1893491124260355
719,SP:9ccbad619517288ec7e0ec8041c2ed2d1d96bbd6,"The paper considers the problem of clustering time series data. The authors provide a coreset for this problem, under some assumptions. The coreset size depends polynomially on the number of clusters k, the dimension d, and 1/\varepsilon and the parameters related to the assumption on the data, and requires near-linear time to compute.  The authors validate their coreset empirically on synthetic data, as compared to a uniform random sample. They show that the coreset: (i) achieves better results can the random sample, and (i) can help accelerate the computational time, relative to the full data, by 1-2 orders of magnitude, with only a small compromise in the accuracy.","The authors propose a novel coreset construction framework for the problem of clustering multivariate time series. To my knowledge, this is the first construction of coresets in this setting. The framework, as well as the proof scheme, is close to the work in [45], in which Lucic et al. studied the problem of coreset constructions for the MLE task under the GMM model.   Here, the authors add to the GMM model a temporality and thus an additional autocorrelation in the underlying model. The proof's scheme is similar to that of [45] in the sense that they also reduce the GMM time series clustering problem to a $k$-means clustering problem (and then use more classical results for $k$-means from the state-of-the-art).",In this paper the authors provide the definition of a coreset for time series clustering for entities generated from a gaussian mixture model and then build a coreset for the same problem.  The coreset is independent of both the number of entities N and the the number of Time stamps T_i for each entity i. They demonstarte the accuracy and efficiency of their coreset by superior empirical perfromance over synthetic data as compared to uniform sampling.,"This paper uses the Feldman Langberg sensitivity framework to compute coresets for time series clustering under a Gaussian Mixture Model.  This imposes additional challenges since there is an instance domain as well as a time domain to approximate. The mixture model introduces covariances and to reflect the time dependence there is an additional autocorrelation introduced on the time domain.  Those covariance and correlation structures are being bounded in order to bound their impact on the sensitivities and after getting rid of all those structures the problem is reduced to a simpler clustering model for which sensitivity bounds are obtained more easily.   The resulting coresets are assessed empirically on synthetic data.  The writing is very good and the main body provides all information to understand even on a technical level, although the details are in the appendix.  In principle I would tend to accept this paper but knowing the previous literature on GMM as well as a recent coreset work on ""panel data"" my main impression is that the submission is just a dejavu of those two works.",0.24324324324324326,0.24324324324324326,0.26126126126126126,0.15873015873015872,0.24603174603174602,0.3116883116883117,0.21428571428571427,0.35064935064935066,0.1638418079096045,0.2597402597402597,0.1751412429378531,0.13559322033898305,0.2278481012658228,0.2872340425531915,0.2013888888888889,0.19704433497536947,0.20462046204620463,0.18897637795275593
720,SP:9d1f273bf578277c1d7a2247baddab97d040eec5,"This paper proposes the AL method which combines node selection and message passing of the graph models for minimizing labeling cost. The first technical contribution is to quantify the influence of labeling in GCN/LP and relate AL to the problem of finding seed nodes for maximizing the influence. In addition, this paper shows how to maximize the spread of influence by considering the noise and reliability of labels.","The authors propose a novel graph-based active learning approach, specifically with label propagation and graph convolutional networks in mind. They, in particular, tackle the problem of label noise in graph-based active learning, which is indeed very important and not well studied. They provide a conceptual and practical solution to this problem. For that, they introduce the notion of reliable influence maximization based on influence maximization in social networks, where they take not only the quantity of the influence into account but also a novel way of measuring the influence quality. On benchmark datasets, they achieve impressive performance in terms of predictive accuracy, runtime and label noise robustness compared to some baselines.","This paper studies graph active learning in the presence of labeling noise, which is an under-explored area with real-world impact. The key idea of this paper is to define Reliable-Influence-Quantity-Score Q as the product of label reliability and label influence. The active learning objective is then to maximize the number of activated node, whose Q score exceed a certain threshold. The authors proposed a greedy algorithm for selecting the nodes, and showed the greedy algorithm is close to optimal. Empirical results on real-world dataset shows this algorithm outperforms the baselines by large margins.","This proposes a novel graph-based active learning framework by reliable social influence maximization. The proposed model aims to select a subset of nodes to be labeled, so as to maximize the number of nodes influenced. Whether a node is influenced is determined by both the influence quantity and quality (assuming label noise). A greedy algorithm is adopted for optimization, whose approximation precision is guaranteed theoretically. SeveraleExperiments are conducted to validate the effectiveness and efficiency of RIM. ",0.2608695652173913,0.2753623188405797,0.2318840579710145,0.19469026548672566,0.168141592920354,0.2222222222222222,0.1592920353982301,0.1919191919191919,0.2077922077922078,0.2222222222222222,0.24675324675324675,0.2857142857142857,0.1978021978021978,0.22619047619047616,0.21917808219178087,0.20754716981132076,0.2,0.25
721,SP:9d42cd28faad99fdd25273cdf68d2acefb2675dd,"This paper works with the Dynamic Causal Global Optimization problem. For a temporal structural causal model, which is a sequence of structural causal models with causality in time, meaning there are dependencies on the values of variables from previous step. The goal is to optimize the outcome variable by manipulating some observable variables (those are chosen by the model).  An algorithm called dynamic causal bayesian optimization is proposed to solve such a problem. The objective function $f_{s,t}$ is decomposed into the sum of a function depending on the optimized observed variables and another function depending on others (variables in the current time). Thus the optimization can be done step by step in time.  A dynamic causal GP model is introduced for practically implement the DCBO algorithm.  For experiments, both Synthetic and Real data are used, in several different structural causal models, to compare the behavior of DCBO, to that of CBO, ABO and BO.","In this paper, the authors extend the notion of casual Bayesian optimisation from a static to a dynamic scenario which, indeed, has many relevant applications. The paper theoretically analyses the proposed approach and run the algorithm in a set of real-world and synthetic data. This work is really interesting but I have some questions related to scalability and applicability that I hope the authors could help me understand. ","This paper addresses the Dynamic Causal Global Optimization (DCGO) problem that aims to find both the optimal set of manipulative variables and the optimal intervention levels to the selected variables. This is a time-series optimization that determines the best way to intervene in the present based on the results of interventions made in the past, and is considered to be a formulation of a particularly important problem e.g. in the medical field. The authors make three assumptions about the DCGO problem: (1) the causal structure is invariant with respect to time,  (2) the function value of a variable Y_t is determined from the function value of its parent node, and the function value of the parent node can be expressed as the sum of the function value of the target node and that of the non-target node,  (3) the DAG does not contain any unobserved confounding factors.   Under these assumptions, it follows that the objective function of DCGO at time t can be expressed recursively using the results of interventions up to time t-1, and that the search space of DCGO is invariant with respect to time. Under the above problem setup, the proposed method DCBO is an EI-based Bayesian optimization algorithm based on the Dynamic Causal Gaussian Process model whose mean and covariance function are defined by the recursive representation of the objective function.    Experiments to validate the effectiveness of the proposed method have been conducted on both synthetic and real data. Experiments with synthetic data have been conducted in six situations: when DAGs and SCMs are stationary/non-stationary with respect to time, when the manipulative variables contain noise, when there are missing observations, when there is no causal relationship among manipulative variables,  and when there are multiple intervening variables. Two real data experiments were conducted: one on the problem of minimizing the unemployment rate in a closed economy (economics), and the other on the problem of finding an intervention that would reduce the phenomenon of a concentration of dead animals in a chemostat (biology).","The paper proposes a model, Dynamic Causal Bayesian Optimization (DCBO), which builds upon existing models to find a sequence of interventions , optimizing the target variable at each time step in a causal system. It combines Causal Bayesian Optimization, which builds around casual information and finds the optimal intervention in a DAG without temporal evolution, with Dynamic Bayesian Network, which is used in time-series modelling and carries dependence assumptions that don’t imply causation. DCBO represents associations between features and the causality between input and outputs, both of which can change over time. The modeling therefore offers a novel approach for decision making in dynamic systems ie. find optimal intervention at time t. The paper compares the model’s results against algorithms which either account for temporal evolution or optimize the sequence of interventions of a system (as no model exists which does both). The algorithm is run on both interventional and observational data. Finally, synthetic experiments are performed, such as creating noisy variables, incorporating missing observational data, and creating non-stationary settings. ",0.11538461538461539,0.33974358974358976,0.1987179487179487,0.3188405797101449,0.21739130434782608,0.12244897959183673,0.2608695652173913,0.15451895043731778,0.1791907514450867,0.0641399416909621,0.08670520231213873,0.24277456647398843,0.16,0.21242484969939882,0.1884498480243161,0.10679611650485436,0.12396694214876035,0.1627906976744186
722,SP:9d7eb65acaebcc40634cb4d11fe94b8e522977b9,The paper proposes a new representer decomposition based on the approximate optimal condition. This address issues for RPS-L2 by 1. this does not rely on regularization strength 2. this provides an explanation that varies much more than RPS-L2 across different testing points. They verify this in their experiments.,"The aim of this paper is to identify the contribution of the training data to the prediction. This work highlights the short comings of the existing Represented Point Selection (RPS-$l_2$) both empirically and by analyzing the derivation of RPS$-l_2$, and proposes an improved method called RPS-LJE. RPS-LJE is a clear improvement over RPS-$-l_2$ both qualitatively and quantitatively. Qualitatively, it is observed that the explanations produced by RPS-LJE isn't repetitive like RPS$-l_2$. Quantitatively, it performs comparably to, or better that the existing data-explanation approaches.",This paper addresses the problem of model explainability by looking at which training examples were responsible for the model’s predictions. The contributed model takes inspiration from RPS and changes its computation of the data importance factor such that it becomes less sensitive to the choice of hyper-parameters and leads to higher diversity in the explanations. The proposed RPS-LJE is compared against two baseline approaches from the state of the art.,"This paper addresses sample based explainable AI, where a model prediction at test time is explained by producing a ranked list of training set instances.  They build upon a method called Representer Point Selection, which develops a representer theorem wherein contribution to test loss is decomposed into terms involving the features of the last layer the trained network.  The authors address two shortcomings of the method by developing  a local Jacobean Taylor expansion that alleviates both.  They present experiments on data set cleaning (in line with the original RPS paper), as well as on three other tasks commonly evaluated with sample based explainability.",0.2,0.12,0.18,0.17708333333333334,0.16666666666666666,0.1917808219178082,0.10416666666666667,0.0821917808219178,0.08737864077669903,0.2328767123287671,0.1553398058252427,0.13592233009708737,0.136986301369863,0.0975609756097561,0.11764705882352941,0.20118343195266275,0.1608040201005025,0.1590909090909091
723,SP:9d8b6bc3f92325a4ee80edb6f81fad7522c573a5,"This work introduces a  novel PAC-Bayes bound for gradient-based meta-learning algorithms. This PAC-Bayes bound exploits a previous bound for uniformly stable learning algorithms.  In opposite to previously proposed bounds for meta-learning methods, the bound is non-vacuous in some data sets and much tighter. Authors also show that a heuristic method based on the minimization of the proposed bound leads to novel meta-learning algorithms. ","This submission is about generalization bounds for gradient-based meta-learning algorithms. It is motivated by the observations that meta-learning algorithms have demonstrated promising empirical results in a variety of tasks, while our theoretical understanding of these algorithms is lagging behind and in particular the existing generalization bounds for meta-learning are challenging to compute or give vacuous values even in simple settings.  I like how this work reasons about the challenges in generalization for meta-learning. The reasoning is by separately considering generalization at the base level and generalization at the meta level, and how the two levels must be coupled. The work proposes to rely on uniform stability to reason about generalization at the base level, and to use PAC-Bayes bounds to reason about generalization at the meta level. The claimed main contribution is that this approach leads to a novel generalization bound (Theorem 3) for gradient-based meta-learning, which is then leveraged to develop a learning method (PAC-BUS) by using the bound as optimization objective, and the method is evaluated in two meta-learning problems. It is not too clear to me how the two levels are coupled to learn the inductive biases that can be used for future tasks, but I suspect that the PAC-Bayes prior must play a role in connecting the two levels.  The argument behind the proof of the main result (Theorem 3) appears to consist of three blocks: (1) writing the distribution that generates data sets of a given size $m$ as a product distribution between the random tasks and the random samples for given tasks; (2) using the classical PAC-Bayes bound of McAllester for bounding the expected loss (expectation over data sets and over parameters) in terms of its empirical counterpart (that sums over the training tasks) plus a KL regularization term; and (3) using Uniform Stability for bounding the expected loss on a given task in terms of its empirical counterpart and stability. However, I missed an outline of the proof or indication of where the reader can see the full details of the proof.","The paper proposes a generalization bound for meta-learning based on the following PAC frameworks: Uniform Stability for assessing the generalization of the base (task-specific) models to unseen data, and PAC-Bayes for estimating the generalization of the meta-learner to unseen tasks. This implies the use of a stochastic meta-learner, that provides the distribution from which base models are sampled for initializing the task learner. The obtained bound is then tightened for the few-shot learning context, by accounting for additional validation data. The derived bound is finally used as objective function for optimizing the posterior model distribution by stochastic gradient descent, for the particular case of Gaussian posterior and prior distributions and using the reparameterization trick. ",The paper provides generalization bounds for meta-learning approaches. The result combines two frameworks: uniform stability of gradient descent based learning algorithms that cover guarantees at the base classifiers level and PAC-Bayes theory for obtaining results at the meta level. The objective is to have bounds that become tighter when the base learner adapts quickly. The framework proposed allows the authors to develop new algorithms and regularization scheme. An experimental evaluation is conducted on a toy problem and standard benchmarks.,0.4142857142857143,0.2571428571428571,0.24285714285714285,0.1168091168091168,0.09971509971509972,0.23333333333333334,0.08262108262108261,0.15,0.20987654320987653,0.3416666666666667,0.43209876543209874,0.345679012345679,0.1377672209026128,0.1894736842105263,0.2251655629139073,0.1740976645435244,0.16203703703703703,0.2786069651741293
724,SP:9da67c6284e5b32876b9d438a18f4f7c89087605,"In this paper, the authors proposed the spike-element-wise (SEW) ResNet to train deep SNNs with ResNet modules.  They achieved very excellent results compared to the SOTA. Although simple, the proposed methods turned out to be very effective in the practice. ","This paper wants to apply the residual block into deep spiking neural networks to achieve better classification accuracy. They proposed a spike element-wise (SEW) to realize residual learning. This top and idea are very important in training a robust deep spiking neural network and the residual block is approved useful in the deep learning field. The authors evaluated the SEW on several datasets such as ImageNet and DVS gesture datasets, the experimental results show the proposed method can achieve better performance.  ","This work proposes spike-element-wise ResNet, in an effort to achieve identity mapping. The original Spiking ResNet put the element-wise operation before the activation function. However, with the LIF activation function, the activation will be binarized to spikes and thus the identity mapping is not fulfilled. Thus, the authors put the element-wise operation after the LIF activation.   ","The present paper is concerned about a residual architecture for spiking neural networks. First, the authors point out two drawbacks of the existing Spiking ResNet. The first drawback is that Spiking ResNet cannot easily represent identity mapping for some neuronal dynamics. The second drawback is that even if a block of Spiking ResNet represents the identity mapping, the gradient of its output with respect to the input is not equal to 1, and therefore, a sequence of Spiking ResNet blocks suffers from vanishing or exploding gradients. These two drawbacks hinder us from training a deep Spiking ResNet. The authors present the spike-element-wise (SEW) residual block to address the drawbacks mentioned above. Since the drawbacks are caused by the architecture where the skip connection is fed into a spiking neuron, the idea is to skip the spiking neuron and directly add/multiply/IAND the residual mapping and the input spike sequence. The authors discussed that the SEW block can address the drawbacks. The authors present two sets of experiments, one uses ImageNet and the other uses DVS Gesture data set. The experimental results suggest that the proposed SEW ResNet successfully addresses the drawbacks and the deeper the network is the better the performance is. ",0.30952380952380953,0.2619047619047619,0.3333333333333333,0.17073170731707318,0.4146341463414634,0.3333333333333333,0.15853658536585366,0.18333333333333332,0.06829268292682927,0.23333333333333334,0.16585365853658537,0.0975609756097561,0.2096774193548387,0.2156862745098039,0.11336032388663969,0.1971830985915493,0.2369337979094077,0.15094339622641506
725,SP:9de1ef2688db0cc97769bb95b3ed2995b617db02,"This paper aims to create generalizable and controllable neural signed distance fields from monocular depth observations. The authors propose a hypernetwork to represent neural SDFs across different human shapes and cloth types. For fast fine-tuning, they additionally leverage meta-learning to learn an initialization of the hypernetwork. This method achieves impressive reconstruction results and demonstrates a cool application that reconstruct high-quality human shapes from 8 monocular depth frames in 2 minutes.","This paper presents a framework that can learn an animatable implicit avatar for clothed humans from very few observations. The key idea is using meta-learning to learn a prior (specifically speaking, a good initialization) to enable fast fine-tuning at inference time. To enable animations of various cloth types, this paper proposes to perform meta-learning on a hyper-network which controls the parameters of the neural implicit function conditioned on pose configurations. Results show that the proposed method is able to generate clothed human avatars from few shot observations.  ","This paper proposes a novel learning method to generate controllable neural SDF for non-rigid clothed human body from a few depth scans. The entire pipeline consists of three main parts, i.e., canonicalization, neural SDF, and hyper-net. The neural SDF takes canonicalized points as inputs and learns a static representation. The hyper-net is used to fine-tune the static neural SDF and generate dynamic SDF efficiently for forward LBS based animation. ","This paper looks at learning animatable avatars given a small number of depth images with associated known SMPL pose transformations. The proposed approach (similar to prior work e.g. SCANimate) learns: a) forward/inverse skinning weight predictors,  which given a query point x in canonical/posed space respectively predict the forward/inverse LBS weights, and b) a canonical space implicit SDF function, which conditioned on SMPL pose, predicts a pose-dependent shape s.t. its forward transformation looks correct.  While the paper uses a fixed network for a) above, the key contributions relate to how the pose-dependent canonical shape is modeled. This paper proposes to capture each object via a hyper-network which predicts the weights of the SDF function conditioned on SMPL pose parameters. It shows that such a hypernetwork can be meta-learned using training data with strong supervision, but can then be finetuned using only a small number of depth images in varying poses for a novel instance.  The experiments convincingly show the benefits of this approach across different datasets and in particular show how using only a small number of depth images suffice. ",0.2465753424657534,0.1780821917808219,0.2054794520547945,0.17582417582417584,0.27472527472527475,0.21621621621621623,0.1978021978021978,0.17567567567567569,0.0797872340425532,0.21621621621621623,0.13297872340425532,0.0851063829787234,0.21951219512195122,0.17687074829931973,0.11494252873563217,0.19393939393939394,0.17921146953405018,0.12213740458015267
726,SP:9de80b4578a13925b206c31c25e7674365de0296,"This paper introduces implicit quantile networks (IQN) into value-decomposition multi-agent reinforcement learning methods (QPLEX or weighted QMIX). The proposed method uses IQN for two components of QPLEX, for the joint action-value function and the transformed action-value function, respectively. The author expects IQN for the joint value function to account for environment-wise risk while the IQN for the transformed value function to account for agent-wise risk. ","The authors introduce DRIMA - a distributional CTDE multi-agent RL approach that separately learns to model return stochasticity arising from other agents vs the environment. They argue this is useful because for example it allows agent to be optimistic wrt teammates (who can prosocially adapt) but more risk-neutral wrt the environment (which does not adapt). They evaluate the effectiveness of their approach on the Starcraft MultiAgent Challenge (SMAC), and outperform several state-of-the-art baselines.","This paper proposes a novel multi-agent reinforcement learning algorithm that disentangles randomness/risk sources coming from (i) unobservable actions of cooperative agents and (ii) unobservable actions of enemy agents (environment stochasticity). The proposed method, DRIMA, uses Implicit Quantile Networks (IQN) to learn the joint action action-value distribution and controls the environment-wise risk level by changing its sampling distribution. The agent-wise risk level, on the other hand, is controlled through a hyperparameter in the loss function that softly ignores learning of the action-value function for non-optimal actions, and learns it accurately only for optimal actions in which agents are fully cooperative (similarly to previous approaches, e.g. Weighted QMIX). The proposed method shows improved performance compared to other state-of-the-art MARL algorithms in difficult StarCraft benchmark tasks.","This work introduces DRIMA, a Multi-Agent Reinforcement Learning algorithm that attempts to learn risk-specific behaviors, where risk is separately considered by its source: from other agents or from the environment. The authors explain their method as a novel combination of prior approaches and validate it empirically in the Starcraft Multi-Agent Challenge testbed. In addition, they provide some demonstration that non-disentangled risk sensitivity does not yield the same benefits.",0.11267605633802817,0.352112676056338,0.19718309859154928,0.24675324675324675,0.22077922077922077,0.16417910447761194,0.1038961038961039,0.1865671641791045,0.19444444444444445,0.1417910447761194,0.2361111111111111,0.3055555555555556,0.10810810810810813,0.24390243902439027,0.1958041958041958,0.18009478672985782,0.22818791946308725,0.21359223300970875
727,SP:9e22642d90a6d9873867dc09ee39b6feff14f9f7,The paper proposes texture-based and patch-based augmentations to generate negative samples from input images to improve the robustness and generalization ability of contrastive learning. The texture-based augmentation generates negative texture images based on two patches extracted from input images using texture synthesis tools. The and patch-based augmentations constructs non-semantic images by tiling randomly sampled patches of different sizes from the input image. Experiments on ImageNet and its variances demonstrate the effectiveness of the proposed method on improving generalization ability of existing contrastive learning methods. ,"This paper introduces non-semantic negative samples into self-supervised learning. These non-semantic samples force the network to reduce the reliance on texture-based features and focus on semantic ones, which are more general in different scenarios.  Experiments on several image classification datasets verify the robustness of the proposed method. In addition, texture-shape bias trade-off is also thoroughly analyzed.","The paper proposes to augment current contrastive self-supervised learning (SSL) methods such as BYOL and MoCo with additional negative samples that are augmented so that they remove semantics from the original image, thus forcing the learning of more robust representations that focus less on superficial image features. The authors propose to do so by generating negative samples based on textures and patches from an original image. The experiments suggest that models learned with the augmented negative samples generalize better.","This paper proposes new augmentation strategies for contrastive learning that produce negative samples designed to mimic the texture of points, but not have their shape data. The goal is to make models not rely on texture data. The authors report extensive experiments on the effectiveness of their method, and discuss how different image classes rely on different features in the shape-texture trade-off.",0.19101123595505617,0.21348314606741572,0.20224719101123595,0.20967741935483872,0.27419354838709675,0.1875,0.27419354838709675,0.2375,0.28125,0.1625,0.265625,0.234375,0.22516556291390724,0.22485207100591714,0.23529411764705882,0.18309859154929578,0.2698412698412698,0.20833333333333334
728,SP:9e3633c45729c542c7fb2b947646e3c8548e5429,"This paper investigates the problem of training a good computer vision model in the cross-device federated setting, focusing on classification. For deep networks, when the number of classes gets large ($\geq 10^3$), the number of parameters is dominated by the classification layers, hence scaling proportionally to the number of classes in the network. The authors propose to alleviate the resulting computation and communication burden by using sub-networks for each client, with a shared feature extractor but with only a smaller number of classes, re-using the sampled softmax proposed by Bengio & Senécal 2008. The resulting algorithm, Federated Sampled Softmax (fedSS), is benchmarked on image classification and image retrieval tasks along with baselines and variants. Experimental results demonstrate the validity of the approach.","This paper works on ""supervised"" representation learning in a federated learning setting. The main goal is to save the communication cost: by preventing sending the entire fully-connected layer between the server and the clients if the clients only have data from parts of the classes. The authors proposed federated sampled softmax, which is to compute the softmax only over the ""positive"" classes of which a client has data and a small portion of the other negative classes. The authors empirically show that, by doing so, even with a very small set of the negative classes (so small communication cost), the resulting feature network can achieve comparable performance to learning with the conventional softmax. ","In this paper, the authors proposed federated sampled softmax (FedSS) for resource-efficient federated image representation learning. When the number of classes is large, the final classification layer could take up a large part of the communication cost during training. FedSS allows subsampling the weights of negative classes to reduce data transfer and leads to similar accuracy compared to the full softmax.","This paper considers the federated learning problem, especially for the case that there are many classes and each client has a small set of classes. When there are many classes, the neural network has to have many parameters to define the last layer classifier, which makes a huge communication cost. To resolve this issue, the author proposes FedSS, by which each FL client samples a set of classes and defines the loss function only with the sampled classes.  Since unsampled classes are not involved in the client's training process, the client sends only the parameters corresponding to the sampled classes. The authors empirically show the proposed approach can reduce the communication cost without sacrificing performance.  ",0.20634920634920634,0.16666666666666666,0.21428571428571427,0.18421052631578946,0.2543859649122807,0.3064516129032258,0.22807017543859648,0.3387096774193548,0.23275862068965517,0.3387096774193548,0.25,0.16379310344827586,0.21666666666666667,0.2234042553191489,0.22314049586776857,0.23863636363636365,0.25217391304347825,0.21348314606741572
729,SP:9e9fdc790ae44306844c7ae6c82aaf2dc8e69e0e,"The authors propose a new quantization flow to train DNNs using only 8-bit fixed-point multiplications. They show that 8-bit fixed point can represent different exponent ranges based on fractional length, thus choosing the right fractional length is critical. They then empirically derive a formula to calculate the optimal fractional length for a tensor based on its standard deviation. The authors combine PACT (learnable clip threshold) with fixed-point quantization, and propose a two-pass method to handle batch norm. They can achieve a small accuracy improvement (<1%) when training from scratch or fine-tuning on ImageNet using a variety of small models (ResNet-18 and MobileNet variants), compared to other quantization-aware training methods. ","This paper describes a novel quantization framework that involving only fix-point 8-bit multiplication for DNN execution. The paper first highlights the advantages of the fixed-point numeric format. The paper then conducts some statistical study and derive an empirical formula to relate the fraction length of the fix-point representation with the standard deviation of the value distribution. After that, the paper introduces a novel approach to determine the right format for each layer during the forward propagation of the training. The proposed solution, F8Net, has been evaluation on ImageNet using multiple DNN structures (e.g., MobileNet, ResNet).  ","The paper proposes a low-precision DNN inference models with 8-bit fixed point. To realize the number of fraction bits, the author uses the variance of DNN parameters and combines it with PACT approach in QAT. The new approach is evaluated in various neural networks such as MobileNet V1/V2 and ResNet18/50 on ImageNet for image classification and the result are mostly par with the state of the art approaches. ",The paper analyzes the relationship between relative quantization errors and fixed-point formats for zero-centered normal distributions and finds a linear model which fits the best exponent length of the fixed-point data type given a standard deviation. These insights are then unified with parameterized clipping activation (PACT) to normalize incoming floating point data into the desired fixed point range. To handle the network with sole 8-bit multiplications a forward pass in floating precision is used to compute batch norm statistics for the main 8-bit forward/backward pass. Additional adjustments are made between successive layers and residual layers which rely on reusing some statistics of the previous layer. ,0.21367521367521367,0.11965811965811966,0.17094017094017094,0.18,0.22,0.2222222222222222,0.25,0.19444444444444445,0.18018018018018017,0.25,0.1981981981981982,0.14414414414414414,0.23041474654377878,0.14814814814814817,0.17543859649122806,0.20930232558139533,0.20853080568720378,0.1748633879781421
730,SP:9efc61e3dc186ecf18844948c2af696d7ae79d2f,"This paper presents solid experiments across many NLP benchmarks to show that the perplexity of the upstream language model can be a deceiving indicator of downstream quality. They also show that popular language model like T5-base and T5-large are relatively inefficient and scaling strategies differ at different compute regions, i.e., applying the same strategies at different compute regions has a different effect on model quality. Finally, they give a simple DeepNarrow strategy that can be applied to different model sizes and make them efficient while preserving their performance.   ",This paper presents scaling insights from pretraining and fine-tuning Transformers empirically. The main findings are as follows: (i) model shape matters for downstream tasks; (ii) scaling protocols operate differently at different compute regions; (iii) T5-base and T5-large are not Pareto-efficient. They will publicly release over 100 pretrained checkpoints for further research.,"This paper aims at providing insights from scaling Transformers for pre-training and finetuning. Based on extensive experiments involving pre-training and fine-tuning over different transformer configurations, the authors find that the model shape matters a lot besides model size when considering the downstream performance, and scaling strategies differ at different compute regions. With these new findings, the authors propose the DeepNarrow scaling strategy and verify it on additional experiments including tasks in different domains (language and vision).","This paper provides various ablation studies to show the effect of scaling various architecture parameters on the performance of a downstream task. The studies are performed on encoder-decoder transformer architectures that follow the T5 architecture as the backend. As a result of the ablation studies, the authors provide a heuristic on how to design architectures that lie on a better Pareto front compared to the previously proposed T5 architectures.",0.14285714285714285,0.23076923076923078,0.17582417582417584,0.32727272727272727,0.12727272727272726,0.1518987341772152,0.23636363636363636,0.26582278481012656,0.22857142857142856,0.22784810126582278,0.1,0.17142857142857143,0.1780821917808219,0.24705882352941178,0.1987577639751553,0.26865671641791045,0.11199999999999999,0.16107382550335572
731,SP:9f6362c6b6d6712590d102cb1233372f3b963db4,"This paper presents a theoretical framework for reasoning and analyzing domain-randomization techniques. Its approach models a simulator instance as a Markov Decision Process for which the parameters of the MDP correspond to tunable simulator parameters. In this designed setting, the work proves sharp bounds on the gap between an optimal policy's value in the domain-randomized and the real-world setting. The work also analyzes the conditions under which sim-to-real transfer can be successful in the considered theoretical setting.","This paper presents a theoretical framework for analyzing the sim-to-real gap in the context of domain randomization. The paper defines formally the sim-to-real gap and the domain randomization method. Next, it analyzes the upper bound of sim-to-real gap in three different scenarios: finite simulation classes with and without separation conditions and infinite simulation classes. The paper provides constructive arguments to prove the upper bounds.  In my opinion, this paper has made multiple contributions. I am most impressed by its theoretical framework that formally defines the sim-to-real problem and its proof of the upper bound of the sim-to-real gap based on the constructive arguments described in the paper. ","The goal of this paper is formalize and analyze the technique of domain randomization in robotics. The authors frame domain randomization as training over a set of ""plausible MDPs"", only one of which is the MDP that will be used at test time. The authors then analyze the best possible performance of models for this problem under 3 different settings: when the set of plausible MDPs is finite with and without a separation condition, and when the set of plausible MDPs is infinite. The authors find algorithms that achieve a performance gap that beats the worst case performance gap of $O(H)$ in all three settings ($O(log^3H)$ and $O(\sqrt{H})$).","The paper is a theoretical analysis of domain randomization in the context of latent MDPs. The paper provides bounds on a definition of the sim-to-real gap, defined as the difference between the optimal value function and a ""domain randomization oracle"" value function. In this context the oracle is defined as a history dependent policy which uses the first steps in the environment to improve its belief on the latent variable and then optimally behaves according to this belief.   Provided bounds can be applied in three different contexts: finite domain randomization (i.e. domain randomization on a finite number of domains) satisfying a separation conditions (i.e. a requirement on the existence of a state-action pair which leads to sufficiently different next states), finite domain randomization without the separation condition and infinite domain randomization.   Bounds are expressed in terms of D (i.e. a bound on number of time steps required to reach any other state in the state space) and of H (i.e. the number of steps in an episode). In the case of finite simulators with separation the domain gap is O(D log^3(H)). In the case of finite simulators without separation the domain gap is O(D H^(1/2) log^(1/2)(H)). In the case of infinite domain randomization the gap is approximatively O(D H^(1/2) log^(1/2)(H)).   Bounds are provided with assumptions and most of these assumptions are justified by proving that removing these assumptions opens the possibility to counterexamples which do not meet the provided bounds. ",0.3253012048192771,0.21686746987951808,0.3132530120481928,0.20512820512820512,0.3504273504273504,0.336283185840708,0.23076923076923078,0.1592920353982301,0.09923664122137404,0.21238938053097345,0.15648854961832062,0.1450381679389313,0.27,0.1836734693877551,0.15072463768115943,0.20869565217391303,0.21635883905013195,0.2026666666666667
732,SP:9f7ba0b3a8fd68126922a6b4b0ea31f9870a804a,"The paper proposes to add a component to the standard RL actor-critic architecture, that learns when to repeat (or not) the last action executed by the agent. This component is a simple binary classifier, conditioned on $s_t$, $a^-$ (the previous action) and $\hat{a}$ (what the actor wants to do now), that chooses between $a^-$ and $\hat{a}$. The authors theoretically motivate their algorithm, compare it to a wide range of related work, and provide extended experimental results that show that the proposed algorithm outperforms the Soft Actor-Critic, and its recent variations.",This work proposes a temporally abstract actor-critic (TAAC). They do so by incorporating a second-stage binary policy which chooses between the previous action and a new action - this act or repeat strategy hinges on the actually sampled action as opposed to the expected behavior. The key contributions are 1) a new compare-through Q operator for multi-step TD backup for policy evaluation and 2) computing the actor gradient by multiplying a scaling factor to the change in Q function wrt to the actions for policy improvement.  ," This paper studies a single option: repetitive actions that can be selected via a two-layer architecture. First layer is a policy selected conditioned on previous action; second layer is a binary decision classifier to select the action by the 1st layer or the actual output at the current step. It is a very simple idea, with some seemingly flaws (see below), but perform well in a wide range of experiments.  ","The authors introduce TAAC, which stands for Temporally Abstract Actor Critic, an extension of the actor-critic framework, in the continuous control setting, to enable the policy to choose between selecting a new action or repeating the previous one. TAAC acts as a middle ground between hierarchical RL and “flat” RL, allowing the policy to perform temporal abstraction while not having to deal with the computational or complex formulation burden of hierarchical RL methods. TAAC requires only to change slightly the architecture of the actor-critic agent, i.e. replacing the policy had with a two-stage policy head and adding the previous action as input, and to change the policy evaluation and improvement update. In this setting, using off-policy correction for multi-step TD learning such as RETRACE is not applicable. Thus, the authors also introduce a compare-through operator to stabilize learning in this setting. The authors provide novel expressions of the policy evaluation and policy improvements updates, in this setting, as well as convergence proofs. Then the authors compare TAAC to 6 baselines and 1 ablation on a set of 14 benchmarks, grouped in 5 categories. They show that on average TAAC outperforms its competitors and especially shows a significant improvement on benchmarks that exhibit exploration difficulties and require planning over long horizons. Finally, the authors propose an in-depth study of the properties of their agents and notably show that it better cover the state space compared to SAC and also exploits often the action repetition leading to high performance even in settings where we would not expect it to be useful.",0.21052631578947367,0.17894736842105263,0.3157894736842105,0.15730337078651685,0.3258426966292135,0.23943661971830985,0.2247191011235955,0.23943661971830985,0.11235955056179775,0.19718309859154928,0.10861423220973783,0.06367041198501873,0.21739130434782608,0.20481927710843373,0.16574585635359115,0.17499999999999996,0.16292134831460675,0.10059171597633135
733,SP:9f9813998b3d0cd257cd8603395c1c6ab6779111,"The paper proposed a simple Transformer-based neural network architecture for graph data, called GraphFormer. Inspired by the positional encoding in NLP, GraphFormer encodes the structural information of the graph into the Transformer architecture via 1) centrality encoding, which incorporates degree information in computing the node embeddings, 2) spatial and edge encoding, which utilizes the shortest path between nodes to determine the edge biases in self-attention. In addition, the author added a set of virtual nodes to GraphFormer that connects to all nodes via ""virtual"" edges. The author compared GraphFormer with other models in several molecule/chemistry graph classification tasks and show that GraphFormer significantly outperforms the other methods, including several SOTA Message-Passing Graph Neural Network (MPGNN) models. ",GraphFormer modifies the attention layer in a Transformer in a way to incorporate information about the structure of the graph. The modifications are intuitively motivated and show that GraphFormer is at least as powerful as GNNs. The experimental results are positive although they seem to come at the cost of many more parameters.,"This work proposes a transformer architecture suited for graph representation. More precisely, the structure of the graph is encoded via three different mechanisms focused on the degrees, the distance of the shortest path between two nodes, and the edges. The resulting model outperforms different GNNs on four molecular graph tasks.","This paper considers adapting a standard Transformer to work on general graphs, and applying it to a few recent graph representation learning benchmarks. They propose three ways of augmenting a standard Transformer architecture by adding new node features and additive attention biases, and show that their approach (GraphFormer) yields state-of-the-art performance on the considered benchmarks. They also prove some theoretical results stating that GraphFormer is more expressive than many standard graph neural network architectures, and conduct an ablation of the modifications they make to a standard Transformer, showing that each of them leads to better performance.  As the authors note, there has been a lot of work in adapting attention and other parts of transformers to a graph domain. In particular, there has been a lot of work on using attention mechanisms in graphs, and also more specifically in using edges to bias attention between nodes or using features based on shortest paths between nodes. Taken on their own, the modifications proposed in GraphFormer seem fairly incremental.  Nevertheless, most of the prior work I'm aware of focuses on individual modifications for specific tasks instead of a unified set of changes for standardized benchmark datasets, and often deviate from a standard Transformer backbone in a large number of ways. It does seem useful to demonstrate that the Transformer architecture can be applied to graph understanding tasks in a straightforward and general way, and that doing so leads to strong performance both theoretically and empirically. Transformers have shown strong results in many domains, and I think this paper could be a good starting point for future developments on using transformers for graph representation learning.",0.125,0.175,0.25,0.20754716981132076,0.3018867924528302,0.4,0.2830188679245283,0.42,0.10869565217391304,0.22,0.057971014492753624,0.07246376811594203,0.17341040462427745,0.24705882352941178,0.15151515151515152,0.21359223300970873,0.09726443768996962,0.1226993865030675
734,SP:a03cb97d6455c6b87e30a824df9fffbd01d7aeb4,"The authors discuss that existing methods on including classifiers in a cGAN biases the generator in generating easy to classify images. Therefore, they propose a way to include classifiers in a cGAN to improve its performance in a principled manner. To do so, they decompose the joint probability distribution by the Bayes rule that results in linking classifiers to conditional discriminators. The proposed formulation shows that a joint generator model can be trained from two directions: a conditional discriminator and an un-conditional discriminator with a classifier. They combine the formulation of these two routes and propose a new method called Energy-based cGAN (ECGAN). ECGAN shows how to use a classifier for cGANs, and it explains other variants of cGANs such as ContraGAN, ACGAN, and ProjGAN. They empirically show that ECGAN outperforms existing cGANs by achieving higher FID (and similar ones) score on two sets of datasets (CIFAR10, Tiny ImageNet). ","This paper introduced a general framework for conditional image generation. And existing cGAN methods such as ACGAN, ProjGAN and ContraGAN are inclued in the proposed algorithm. Experimental results in Cifar and Tiny ImageNet verified the effectiveness of different components of the proposed method.","The authors of the paper propose a unified view of conditional generative adversarial networks (cGANs). To this end, they analyze the log of joint probability distribution p(x, y) through two different perspective of views; via a conditional discriminator using Eq.(2), and via an unconditional discriminator and classifier using Eq.(3). Inspired by a multitask learning, they suggest Energy-based Conditional Generative Adversarial Networks (ECGAN) where two approaches are combined together to approximate the joint probability distribution. Under this framework, previous cGANs such as ProjGAN, ACGAN and ContraGAN can be closely explained using the variants of ECGAN with the right choice of hyperparameters, namely, ECGAN-0, ECGAN-C and ECGAN-E, respectively.  Their experiments on CIFAR-10 and Tiny ImageNet show that these ECGAN variants outperform ProjGAN, ACGAN and ContraGAN in most cases of experiments. Furthermore, ECGAN-UC outperforms these variants of ECGAN as well as previous cGANs with various backbone structures such as DCGAN, ResGAN, and BigGAN. Through this result, the authors conclude that adding a classifier helps improving cGANs with the help of the unconditional discriminator as opposed to the previous findings in Shu et al. [42] and ProjGAN [34].	","This submission proposes to analyze the most popular variations of conditional GANs (ACGAN, ProjGAN, ContraGAN) under a unified, energy-based, formulation (ECGAN).  Specifically, ECGAN is composed of terms derived from two different decompositions of the joint probability p(x,y). The paper then links each term to one of the popular cGAN approaches.  ECGAN is evaluated against the traditional models and an extensive ablation study tests the impact of each component of the proposed model. ",0.0728476821192053,0.23841059602649006,0.08609271523178808,0.3953488372093023,0.27906976744186046,0.10880829015544041,0.2558139534883721,0.18652849740932642,0.17333333333333334,0.08808290155440414,0.16,0.28,0.1134020618556701,0.20930232558139533,0.11504424778761062,0.14406779661016947,0.2033898305084746,0.15671641791044777
735,SP:a04638e57d23c2c3f95345d865b7a511e01da5d0,This paper introduces the problem of active offline policy selection which consists of selecting the best policy from a set of candidates given a small budget to interact with the environment. The paper proposes a solution (A-OPS) based on Bayesian optimization using a kernel function to relate candidate policies and generalize between them. The proposed solution is thoroughly evaluated on simulated robotic control environments using benchmark datasets from the off-policy evaluation literature and several ablations are included to rationalize the choices of the proposed method.,"This paper considers a novel problem setting of ""Offline Policy Selection"", whereby there is access to an initial set of policies trained offline, and an algorithm must choose a policy to deploy in the online simulated environment. I am giving this paper the score of weak reject because despite introducing an important problem setting, I am not convinced the method is actually particularly effective. Hopefully this can be clarified during the rebuttal phase. As such, the emphasis of this review is on areas for improvement.","This paper works on the policy selection problem in a setting characterized by features: available logged data and allowing a limited number of online interactions with the environment. To do so, this work proposes to use Bayesian optimization to combine existing OPE techniques and online interactions. For the setting with large number of policies, a kernel function is introduced to take the correlation between policies, which indicate the performances of different policies. ","This paper introduces a problem of active offline policy selection, where the goal is to select the best-performing policy among the fixed K policies using an abundant offline dataset and a limited additional interaction with the environment. This work proposes to solve this problem by Bayesian optimization (BO) with a Gaussian process (GP), where the input and output of the objective function are a policy and its expected return respectively, and function queries render noisy observations. To facilitate GP in this problem, a new GP kernel is introduced, which takes two policies as input, and its output is computed based on the similarities of policy distribution for each state in the offline dataset. Also, to make BO more data-efficient, off-policy evaluation (OPE) estimates through the offline dataset are used for a warm start of BO. Finally, UCB acquisition function is used for active policy selection. Experimental results show that the proposed BO with OPE-based warm start significantly outperforms the baselines. Also, ablation studies confirm that UCB selection is better than uniform selection, GP is better than methods that do not consider policy correlation, and using OPE is better than not using it. ",0.21839080459770116,0.22988505747126436,0.39080459770114945,0.17647058823529413,0.3058823529411765,0.3472222222222222,0.2235294117647059,0.2777777777777778,0.17346938775510204,0.20833333333333334,0.1326530612244898,0.12755102040816327,0.22093023255813954,0.25157232704402516,0.24028268551236748,0.19108280254777069,0.18505338078291816,0.1865671641791045
736,SP:a075bcc590a5a7cb3378a4686062e977249069a8,"This paper presents ACH, a neural network policy-gradient method for approximating a Nash equilibrium in two-player zero-sum games. ACH is used to compute a strong agent for a two player variant of Mahjong, competitive with the strongest human players. It has a thorough experimental analysis, both in Mahjong and in smaller test environments. ","This paper proposes a method somewhat similar to Deep CFR, NeuRD and RPG, where a policy is trained to predict a weighted counterfactual regret. In small openspiel experiments the method seems to outperform NeuRD and RPG, but no comparisons are made to Deep CFR/DREAM/ARMAC. Impressive results against a top Mahjong player demonstrate that this method can scale to large games.  ","The authors propose a new, non-tabular method for large, two player zero-sum games. The method is evaluated on some standard benchmark games (e.g. Leduc poker) as well as on large game of 1v1 Mahjong.","This paper presents Actor-Critic Hedge (ACH): an actor-critic method for approximating Nash equilibrium strategies in large extensive-form games. ACH is an extension of the CFR family of algorithms that uses deep learning, and is able to learn model-free by training on trajectories and not full game traversals or subgames, which is common in much of the related CFR literature. The technique is demonstrated in toy poker domains commonly used in the literature (Kuhn and Leduc poker), Liar’s Dice, and in 1-on-1 Mahjong. In Mahjong, the ACH agent is shown to defeat several human players, including a Mahjong champion. ",0.19642857142857142,0.19642857142857142,0.375,0.08064516129032258,0.20967741935483872,0.2972972972972973,0.1774193548387097,0.2972972972972973,0.2,0.13513513513513514,0.12380952380952381,0.10476190476190476,0.1864406779661017,0.23655913978494622,0.26086956521739135,0.10101010101010102,0.15568862275449102,0.15492957746478872
737,SP:a14f773c7aa9b8da9afcd577b95e49dc74170d1a,"The paper addresses image-goal navigation, where an agent is placed in a novel environment, and is tasked with navigating to an (unknown) goal position that is specified using an image taken from that position. The method builds a topological map and uses three learned networks for planning navigation. The first one, G_D, is a GNN which takes the current topological graph and goal image, and predicts distances to goal for all nodes. The second one predicts visual features for unexplored nodes in the graph. The third one, G_T, is a termination predictor which estimates probability of goal within sight, and its relative pose. Finally, the system uses non-learned components: shortest distance planner, a local heuristic navigation policy G_LP for reaching subgoals, a heuristic module G_EA for expanding graph with unexplored nodes. The experiments are conducted in 2 environments: Gibson and MP3D. The method shows improvement with respect to 2 BC and 1 RL baseline.","This work details a method for planning and navigation on the image-driven navigiation task in a photo-realistic simulator. The novelty of this approach is that the method is trained with sequences of (RGBD, pose) sampled from the environment, without the associated actions or rewards. The sequences are used to learn a predictive model of a distance function, which the authors claim can learn priors about the indoor layouts of indoor buildings, which enables the method to be transferred to buildings that were not observed during training. ","The paper introduces a new method for learning a navigation policy for indoor environments without RL and without simulation (which is, admittedly, fitting given the title). The method uses SLAM on passive video recordings to construct navigation graphs that are then used to train several networks including a goal-detection network that can be used in goal-based navigation.  I personally like the idea in this paper a lot and clearly, a lot of work went into the experiments and into the polish. That is why I'm upset that I have to give this paper a lower score than it deserves. The main thing that's hindering me from wholeheartedly recommending this is the lack of code and reproducibility. If code is not included, then I have to put the methods section under more scrutiny and there aren't enough details to reproduce this work. I'm happy to change my score if the authors address the issues listed below.","The paper looks into offline data as the sole source of training actor policies for navigational agents. The proposed approach achieves this by a mix of learnable modules/networks and rule-based, heuristic ones. Moreover, the overarching goal is to reduce (to zero) the number of interactions in the simulator during training. The approach is compared with behavior cloning and end-to-end RL flat policy. The task chosen by the authors is image-goal navigation i.e. navigating to a specified image on data that they generate on the Habitat simulator.",0.125,0.14375,0.11875,0.2159090909090909,0.17045454545454544,0.12422360248447205,0.22727272727272727,0.14285714285714285,0.20652173913043478,0.11801242236024845,0.16304347826086957,0.21739130434782608,0.16129032258064516,0.14330218068535824,0.1507936507936508,0.15261044176706826,0.16666666666666666,0.15810276679841898
738,SP:a1cc4345c112038b3949be9ec0f0752d169aa518,"The paper combines the results of two previous works: (Liu, 2017) and (Liu et al., 2019). I'll first describe the contribution of the previous works and then will move to the contribution of the current paper.   (Liu, 2017) provides the theoretical analysis of the Stein Variational Gradient Descent (SVGD). It shows that the evolution of the density under SVGD is described by the Vlasov equation (non-linear analog of the Fokker-Planck equation); moreover, in the space of distributions, SVGD defines the gradient descent minimizing the KL divergence between the current point and the target density (similarly, Langevin Dynamics {LD} defines the gradient descent in the Wasserstein space). Importantly, to derive this result (Liu, 2017) introduces a novel metric in the distribution space (H-Wasserstein distance) based on the Stein operator.   In a similar fashion, (Liu et al., 2019) provides the analysis of different MCMC dynamics (Langevin, HMC, RMHMC, SGHMC) by considering the time evolution of the densities in the space of distributions. They first show how any MCMC dynamics can be reformulated as a completely deterministic procedure by deriving the corresponding vector field in the state space, which induces a vector field in the Wasserstein space. Further, they derive a kind of a Hodge decomposition for the vector field in the Wasserstein space, which provides many insights into existing MCMC dynamics. To describe SGHMC and SG Nose-Hoover thermostat algorithms, the authors introduce the Fiber-Riemannian manifold, which is a Fiber Bundle, but with the Riemannian structure on each fiber. Unlike (Liu, 2017), this paper works in the Wasserstein space (distributions with a finite second momentum equipped with the 2-Wasserstein distance).  The current paper goes the same way as (Liu et al., 2019), but uses the H-Wasserstein distance (introduced in {Liu, 2017}) for the space of distributions. To be more precise, it starts with the same family of MCMC dynamics (introduced in (Ma et al., 2015)) and then uses Theorem 5 from (Liu et al., 2019) changing the projection operator onto the tangent space, which changes because of the different metric. Substituting the different metric into Theorem 5 from (Liu et al., 2019) the authors extend the previous results by additionally describing the SGRHMC algorithm.   - (Liu, 2017) Qiang Liu. Stein variational gradient descent as gradient flow. In Advances in Neural Information Processing Systems, volume 30, pages 3115–3123, 2017. - (Liu et al., 2019) Chang Liu, Jingwei Zhuo, and Jun Zhu. Understanding MCMC dynamics as flows on the Wasserstein space. In International Conference on Machine Learning, pages 4093–4103, 2019. - (Ma et al., 2015) Yi-An Ma, Tianqi Chen, and Emily B. Fox. A complete recipe for stochastic gradient MCMC. In NIPS’15 Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2, volume 28, pages 2917–2925, 2015.","In this paper the authors generalise the Stein Variational Gradient Descent (SVGD) algorithm by considering an alternative flow, essentially equipping the Stein Geometry with a Riemmanian Poisson structure.   This enables a particle evolution scheme which is no longer a gradient flow of the KL divergence in the Stein geometry.    The resulting flow differs from the original SVGD flow through the introduction of an ""irreversible term"" characterised by a Skew-Symmetric matrix $C$.    Adopting this approach, the authors are able to consider augmented variable strategies to improve the performance of SVGD by promoting superior exploration, using analogues of underdamped Langevin and Nose-Hoover type dynamics.   This is demonstrated on some toy examples and on a Bayesian Neural Network model. ","This paper proposes a generalization of the Stein variational gradient descent (SVGD) algorithm via a discretization of the fiber-gradient Hamiltonian flow. The approach that the authors take to extend the framework of particle-based variational inference is similar to the way in which MCMC dynamics are extended through the ""complete recipe"" framework in Ma et al. (2015). The authors provide background material to establish the connection between MCMC dynamics, the Fokker-Planck equation and gradient flow methods. The GSVGD then follows by applying the fiber-gradient Hamiltonian flow ideas from MCMC dynamics to the Stein-Wasserstein metric, which is then discretized to produce the GSVGD algorithm. The authors test the efficacy of their algorithm on a toy data model and a Bayesian neural network.  ","The authors propose a generalization of Stein Variational Gradient Descent (SVGD) that hopes to produce more reliable surrogate measures of an invariant measure pi. SVGD can be viewed as a procedure that is minimizing the KL divergence by taking (functional) gradient steps in an RKHS; it can similarly be viewed as an approximation to the gradient flow induced by Langevin dynamics by accommodating H^d as its function space. This paper studies the generalization of SVGD that occurs by replacing Langevin dynamics by the dynamics induced from an Ito diffusion with invariant measure pi. Using Ito diffusions in lieu of the overdamped Langevin diffusion produces deterministic analogs of SGRLD, SGHMC and SGRHMC. This also permits the formulation of GSVGD, a variant of SVGD that incorporates auxiliary variables in a flavor similar to HMC. The authors demonstrate the efficacy of this PARVI method on a toy 3-component GMM and on Bayesian neural nets for a suite of datasets.",0.07741935483870968,0.0967741935483871,0.09247311827956989,0.2796610169491525,0.2796610169491525,0.288,0.3050847457627119,0.36,0.2721518987341772,0.264,0.2088607594936709,0.22784810126582278,0.1234991423670669,0.15254237288135591,0.13804173354735153,0.271604938271605,0.2391304347826087,0.2544169611307421
739,SP:a2466ade9d3817e7ee57c3fae14b447c9f314c50,"The paper proposes a model for classification (on ImageNet) that involves having a very small network act as a selector and early predictor, and a set of experts that specialise on a subset of the training data. The selector chooses one of them and thus keeps a constant cost irrespective of the number of experts. This is then combined with a number of existing techniques to raise the overall performance. Namely, the authors use a PWRL non-linearity instead of ReLU, they use logit-matching knowledge distillation, and conditional convolution (CondConv). The outcome is a tiny model that yields excellent performance on ImageNet, with performance envelopes superior to existing methods.","In this paper, the author considers the model collaboration problem, specifically ensemble learning. The author identifies two issues of ensemble learning: significant runtime cost and high memory access cost. To alleviate these issues, the authors propose a model collaboration framework named Collaboration of Experts (CoE) along with a training algorithm designed for the framework. The training algorithm contains three components: weight generation module (WGM), label generation module (LGM), and selection reweighting module (SRM). Experiments are conducted on ImageNet to demonstrate the effectiveness of the proposed method.","This paper proposes the Collaboration of Experts (CoE) framework to both eliminate the need for multiple forward passes and keep hardware-friendly. A training method including weight generation module (WGM), label generation module (LGM) and selection reweighting module (SRM) is proposed to improve the expert selection. Results on ImageNet shows its hardware-friendly performance. ","The paper proposes an efficient mechanism for  image classification. It has a light-weight delegator to yield coarse prediction, where early termination is possible. Once the coarse prediction is not confident enough, it actuates an down-stream expert with higher computational power for refined prediction. Since the expert selection is done across models, the method can take advantages of heavily-tuned efficient neural networks. The so-called CoE framework achieves very competitive results on the ImageNet dataset with low FLOPs and low CPU latency.",0.15454545454545454,0.11818181818181818,0.15454545454545454,0.29069767441860467,0.11627906976744186,0.2222222222222222,0.19767441860465115,0.24074074074074073,0.20238095238095238,0.46296296296296297,0.11904761904761904,0.14285714285714285,0.173469387755102,0.15853658536585366,0.1752577319587629,0.3571428571428571,0.1176470588235294,0.17391304347826086
740,SP:a2945bdcf140b466ffdc3b5da35b67ab7f9f70f6,"The paper studies the problem of linear contextual bandits where the rewards of the actions are not always stochastic but rather they are corrupted in C of the T rounds. This model of adversarial corruptions is well understood in the multi-armed bandit setting. The paper combines the OFUL algorithm that works for linear contextual bandits without corruptions with a multi-level algorithm that is robust to corruptions for the multi-armed bandit setting. Unlike multi-armed bandits, in linear contextual bandits, the action space is not finite and therefore techniques that are based on eliminating actions after selecting them enough times are not as direct. Moreover, the variance of the estimator can be changing over time which is dealt with via the use of a weighted ridge regression estimator. The resulting guarantee of Theorem 5.1 matches the regret bound of OFUL when C is constant and adapts to the corruption level C. ","The authors study  a linear contextual bandit problem in the presence of adversarial corruptions that are bounded by an unknown C.  Specifically, they propose an algorithm that uses a parallel-learning idea from Lykouris et. al, 2019 using multi-level partition of the observed data.  To deal with varying variance in the observations, they use a ridge regression estimator similar to the earlier work of Kirschner and Krause, 2018; and Zhou et al., 2020.   Finally, they show their algorithm produces a regret that is quadratic in C and the dimensionality of the context vectors. ",This paper proposes an algorithm that achieves the first corruption-robust result for contextual linear stochastic bandits with infinite arms using the idea of multi-level sampling rates. The regret bound has multiplicative dependence on C^2 and is variance-aware. The algorithm recovers the optimal regret in terms of T and gap when no corruptions are presented.,"This paper studies the linear contextual bandit problem with adversarial noises in the reward function. The authors assume that the decision set can change over time; that reward can be corrupted up to a corruption level (similar to previous work); and that an upper-bound on the true variance of the reward is revealed after each instance. An algorithm, which is based on the linear bandit algorithm OFUL with two notable modifications, is proposed for the problem. First, it considers multiple levels of possible corruption noise to deal with unknown value of C and second it considers a weighted version of the ridge regression estimator to deal with varying variances (this modification was already present in previous work). The authors prove a regret upper-bound for their algorithm and compare it to the literature. They also provide some experiments in the supplementary material.",0.18181818181818182,0.11688311688311688,0.24025974025974026,0.13829787234042554,0.2872340425531915,0.25862068965517243,0.2978723404255319,0.3103448275862069,0.25874125874125875,0.22413793103448276,0.1888111888111888,0.1048951048951049,0.22580645161290325,0.16981132075471697,0.24915824915824916,0.17105263157894735,0.22784810126582278,0.1492537313432836
741,SP:a335dd125084de792fd6c8d2b34bbd3f28d1c9bc,"This paper proposes an auxiliary loss function and a model architecture to facilitate RL agent’s semantic understanding. Specifically, a separate discriminator that partially shares weights with the policy is trained to select the correct goal given the input instruction, while another attention module is trained to better extract the goal-relevant features. Experiments show that the proposed method improves both the sample efficiency and generalization dramatically on instruction-following tasks in visual navigation and robot manipulation. ","The paper addresses problem of sample efficiency and generalization in the multi-target reinforcement learning. The author proposes goal-aware cross-entropy (GACE) loss for auto-labeling goal states. The author also develops goal-discriminative attention network (GDAN) for multi-target reinforcement learning. They evaluate the proposed method on visual navigation robot arm manipulation with multi-target environment.  The proposed method in the paper compares with A3C, SAC.","This paper presents a method for training multi-target policies using reinforcement learning. It introduces an auxiliary classification-type loss for a feature encoder on which RL is performed, and additionally, uses an attention-based architecture to compute the policy and value function. In combination, these two elements seek to learn features which can distinguish between different goals in the environment, and then have the RL algorithm directly use these features to compute goal-relevant policy outputs. The authors show that adding these two components to existing RL algorithms improves performance and sample efficiency on simulated visual navigation and robotic manipulation environments compared to the base versions of these algorithms.","This paper introduces a novel approach for improving sample efficiency in what they call ""multi-target reinforcement learning"". This is essentially goal-conditioned reinforcement learning where the tasks are of the form ""bring me a spoon"" or ""go to the kitchen"". Sample efficiency is improved in two ways. Firstly, they introduce a new auxiliary task of training a goal-discriminator that takes as input a state embedding and predicts which goal this state achieves. Second, they introduce a new architecture that uses attention and information from the goal discriminator to better extract task-relevant features for the policy.  Several experiments are provided to test the sample efficiency of agents trained using the method in both visual navigation tasks and robotic manipulation tasks. In both settings, their method outperforms on-policy baselines. Additional analysis is done, showing that: (1) while explicitly optimizing the state representation to improve the goal discriminator results in the fastest improvement in accuracy in the discriminator, the RL loss itself still causes the representations to contain information about the goal, just slower. This shows that the representation learning loss is helping the model learn faster what it would have to learn anyways. And finally, (2) saliency maps are provided which show that when using attention, the policy attends to more interpretable regions of the state space.",0.19480519480519481,0.33766233766233766,0.2987012987012987,0.22058823529411764,0.3235294117647059,0.2727272727272727,0.22058823529411764,0.23636363636363636,0.1050228310502283,0.13636363636363635,0.1004566210045662,0.136986301369863,0.20689655172413793,0.27807486631016043,0.1554054054054054,0.16853932584269662,0.15331010452961674,0.182370820668693
742,SP:a36e0701024df06aba7bb99dce1089713d5ccde3,"The submission investigates the role of response stochasticity, a feature of biological neurons, in robustness to adversarial attacks in deep neural networks.  They use manifold analysis to compare the neural population geometry of standard-trained, adversarial-trained and stochastic networks and find geometrical signatures of the different conditions. They show that the robustness achieved through adversarial training is qualitatively different than that achieved by the stochastic network. They characterize how increasing noise affects the resulting manifolds. They observe a tradeoff between adversarial robustness and classification performance and can explain this tradeoff in terms of geometric features of the representations. These results (on images) are replicated in the auditory domain.","The paper compares networks that are adversarially trained with networks that have biologically inspired stochasticity in the first layer, including previous visual object-recognition networks and a new auditory network. The analysis is based on manifolds of classes and exemplars in representation space, and considers measures of manifold size and overlap. Robustness to adversarial attacks is also considered. The analysis highlights different responses of the stochastic and adversarially trained networks to adversarial attacks.","It is well known that a deep learning model is fragile to adversarial perturbation, whereas a biological system is not. This study investigate the geometry of internal representations (especially, in the penultimate later) of a DL model, that trained using adversarial examples, and that with biologically-inspired component. The basic approach is based on what the authors call MFTMA. ","The authors note that stochasticity is an important component of recent attempts to improve network adversarial robustness by emulating known biological mechanisms. They also demonstrate their own similar result for a network trained on auditory signals. Motivated by this, they analyze the representations formed by a set of example networks to understand the relationship between representation geometry and robustness. Their analysis reveals differences among networks that underwent standard training, adversarial training, and those with stochasticity added. They claim that this analysis improves our understanding of mechanisms of robust perception as well as our understanding of the role stochasticity plays in biological computation.",0.14678899082568808,0.12844036697247707,0.1834862385321101,0.1506849315068493,0.1917808219178082,0.23728813559322035,0.2191780821917808,0.23728813559322035,0.19607843137254902,0.1864406779661017,0.13725490196078433,0.13725490196078433,0.17582417582417584,0.16666666666666666,0.18957345971563985,0.16666666666666666,0.16,0.1739130434782609
743,SP:a3c776bbd0d7e4bcfb80c0f0990c07c8f3891da4,"This paper studies the problem of finite horizon multi-agent general-sum Markov games.  The proposed algorithm CE-V-Learning achieves $\epsilon$-coarse correlated equilibirum (CCE) using $\tilde{O}(H^5S\max_{1\leq i \leq m}A_i/\epsilon^2)$  episodes, and $\epsilon$-correlated equilibrium (CE) using $\tilde{O}(H^6S\max_{1\leq i \leq m}A_i/\epsilon^2)$  episodes. The sample complexity is polynomial in $\max_{1\leq i\leq m}A_i$, while previous results had an exponential dependence on $m$ ($\Pi_{1\leq i\leq m}A_i$).","This paper studied the sample complexity for learning the coarse correlated equilibrium (CCE) and correlated equilibrium (CE) of m-player general-sum Markov games (MG), as well as learning the Nash equilibrium (NE) of Markov potential games (MPG). For MG, the authors proposed an algorithm with sample complexity that grows polynomially in $\max_{i\leq m} A_i$ to achieve $\epsilon$-approximate CCE and CE. For MPG, the authors proposed an algorithm with sample complexity that grows polynomially in $\sum_{i\leq m} A_i$ to achieve $\epsilon$-approximate NE.","The paper proposes algorithm for learning coarse correlated equilibrium (CCE) and correlated equilibrium (CE) of multi-player general-Sum Markov Games, the propose algorithm has polynomial dependence on the number of state and the horizons. The proposed algorithm builds upon the Nash V-learning of (Bai et al. 2020), and incorporate several new ideas.","The paper studies general sum episodic Markov Games. They provide an algorithm which is effectively V-learning with Follow the regularized leader subroutine (FTRL is hedge algorithm in their case). They prove convergence of their algorithm to $\epsilon$- CCE (coarse correlated equilibria) and $\epsilon$-CE equilibrium policies in $1/\epsilon^2$ with dependence on \max_{i} A_{i} (cardinality of action space of an individual rather than the whole action space). This happens due to ``independent updates"" of the agents. Moreover for the case of potential markov games, they manage to show $1/\epsilon^3$ convergence to $\epsilon$-Nash pure Nash policies, improving Leonardos et al paper and solving an open question about convergence to deterministic policies.  However, the agents do not update simultaneously (i.e., the updates are not concurrent!). They also provide lower bounds.",0.30851063829787234,0.14893617021276595,0.26595744680851063,0.25555555555555554,0.2222222222222222,0.3333333333333333,0.32222222222222224,0.25925925925925924,0.18382352941176472,0.42592592592592593,0.14705882352941177,0.1323529411764706,0.31521739130434784,0.18918918918918917,0.2173913043478261,0.3194444444444444,0.17699115044247787,0.1894736842105263
744,SP:a3e994db0b780f0a0a932ed90af03b0b8a6aaa9d,The paper proposes a method for grounding sentences in videos and images without explicit localization supervision.  The approach rely on a cross-attention architecture that is used in a way to make contrastive learning possible (since it prevents the final feature from containing information from the other modalities while still enabling information to be shared across modalities earlier in the network). Localization can be obtained from the cross-attention patterns between the language and the visual signal.  The authors apply their method on a subset of the HowTo100M dataset for videos and evaluate on the YouCookBB object for objects and a newly introduced YouCookInteraction where they show improvements over baselines. The method is also applied on the Flickr30K dataset on images.,"This paper concerns a new problem of grounding narrated interactions in video. It is similar to existing work on (object) phrase localization/grounding, but with a twist on grounding more complex narrations that could involve both entities and predicates (e.g., put tomatoes into the baking tray). The model training is performed in a weakly supervised fashion on a subset of HowTo100M. Training loss is the standard InfoNCE loss at the sentence level (vs. word-level as in existing grounding work). Model architecture is designed to capture inter-modal correspondence through cross-modal attention. Evaluation is conducted on YouCook2-BB, Flickr30k-Entities, and a newly collected dataset for narration grounding called YouCook2-Interaction.","The paper presents an approach for self-supervised spatial localization of interactions in videos by leveraging a large corpus of video clips and narrations. An alternating attention mechanism is presented that combines cross-modal and self (within modality) attention schemes. Pre-trained models on the HowTo100M dataset are evaluated by using pointing hand accuracy on a subset of the YouCook2 dataset, interaction annotations are also created in this work. The approach is also applied to grounding in Flickr30k images. ","- This paper considers the task of spatially grounding the language description in videos, specifically, instructional videos are used to train the model, where the language description and visual signals are likely to be synchronised.  - The proposed architecture consists of stacks of intra- and inter attention modules, which allows the video and language to attend each other at early stage, and effectively highlighting or suppressing the corresponding features.  - An evaluation dataset is proposed with bounding box annotations for interactions described by natural language sentences, and proposed approach shows good performance on it. In addition, authors have also evaluated on Flickr30K under the setting of weakly supervised language grounding, showing state-of-the-art performance.",0.17355371900826447,0.19008264462809918,0.19008264462809918,0.168141592920354,0.1592920353982301,0.189873417721519,0.18584070796460178,0.2911392405063291,0.20353982300884957,0.24050632911392406,0.1592920353982301,0.13274336283185842,0.17948717948717946,0.22999999999999998,0.19658119658119658,0.19791666666666669,0.1592920353982301,0.15625000000000003
745,SP:a4659fb9e7fe2b9e6c08ea8246a3c9a57b15e002,The focus of this paper is to develop a learning framework to discover spatiotemporal PDEs from scarce and noisy data.  The authors suggest devise a deep convolutional-recurrent network which encodes prior physics knowledge followed by  sparse regression with reconstructed data to identify the analytical form of the governing PDEs. The framework is validated  on three high-dimensional PDE systems where superiority over baselines is demonstrated.,"The paper focuses on discovering the underlying physics dynamics, in the form of partial differential equation (PDE) from low-resolution (LR) measurements. It leverages sparse regression-based recovery to identify the constituents of the unknown PDE. The main contribution is a module that constructs high-resolution (HR) observations from LR data, for sparse regression to perform well.","The authors propose a physics-encoded discrete learning framework to improve the robustness of the PDE-FIND algorithm. The physics knowledge such as known terms and initial conditions can be used to reconstruct higher-resolution data. Then, sparse regression is performed with fine-tuning of coefficients to finally determine the discovered PDE.","Problem: data-driven PDE discovery methods are not robus to low-quality measurement data.  Solution: A novel architecture that can encode prior knowledge (known terms, PDE structure, boundary conditions), and sparse regression procedure that is hypothesized to be more robust to scarce and noisy data scenarios.  Other Contributions: * proof that proposed Pi-block is a universal polynomial approximator. This is why I scored a ""4"" on technical contributions.",0.21212121212121213,0.21212121212121213,0.15151515151515152,0.22807017543859648,0.12280701754385964,0.17307692307692307,0.24561403508771928,0.2692307692307692,0.14705882352941177,0.25,0.10294117647058823,0.1323529411764706,0.2276422764227642,0.23728813559322037,0.14925373134328357,0.2385321100917431,0.112,0.14999999999999997
746,SP:a488244c00248f5276b488911a7550cefeda6093,"This paper proposes a new randomized smoothing technique to improve the robustness of models. Specifically, in the original randomized smoothing approach, we use a fixed variance parameter $\sigma$ for the Gaussian smoothing. The authors claim that this is in general suboptimal as some data samples far away from the decision boundary can be further smoothed. Motivated by this, the authors propose to optimize the smoothing parameter $\sigma$ for every data sample, by solving an optimization problem using Monte Carlo approximation and SGD. The paper also provides a simple procedure for the certification of the data dependent smooth classifier. Through extensive experiments, the paper shows the effectiveness of the proposed approach.",The paper focuses on training robust deep neural networks. The author(s) developed a data-dependent randomized smoothing method to certify the DNN classifier. Their proposed methods are evaluated on benchmark datasets and are shown to outperform other baseline approaches. ,"This papers proposes to optimize the certified bound of randomized smoothing to get data dependent perturbations for different data. It also proposes a simple memory-based approach to certifying the resultant smooth classifier due to the lack of guarantee on the overlapping of the perturbation regions. The technique is incorporated into 3 randomized smoothing approaches, which shows improvement over existing methods in experiments.","The paper's core contribution is to extend randomized smoothing to a setting where the smoothing depends on the input data. That is, the variance of the smoothing distribution is not constant anymore but depends on the input. To ensure a proper certificate, the authors propose some memory-based technique (which essentially changes the final classifier based on the observed input data so far).",0.07272727272727272,0.18181818181818182,0.17272727272727273,0.2,0.25,0.20634920634920634,0.2,0.31746031746031744,0.296875,0.12698412698412698,0.15625,0.203125,0.10666666666666667,0.23121387283236994,0.21839080459770113,0.1553398058252427,0.19230769230769232,0.2047244094488189
747,SP:a4aaf6e291cec402fabf38bcba5a3c96c7d82c1b,"This paper studies the use of adversarial attacks and optimal transport (OT) in the context of semi-supervised domain adaptation. The authors show that adversarial attacks satisfy the cycle monotonicity property. They then propose an algorithm that generates adversarial examples for the labeled target samples using the source classifier, and maps the vanilla target samples to this new domain via OT. ","This paper proposes using the method of adversarial attacks for the task of semi-supervised domain adaptation. The adversarial perturbations are constrained to satisfy cyclical monotonicity [1].   [1] Cédric Villani, Topics in Optimal Transportation.",This paper proposes a technique for domain adaptation in the semi-supervised setting (we have access to some labels of the target domain). The idea developed in the paper is to train a classifier (source classifier) on the source domain in order to use the latent representations of this classifier for two things:  1. “anti” Adversarial examples are computed on the labeled target data in order to have them classified correctly by the source classifier. (I call them anti adversarial examples since the goal is to change the labeled target example to improve the accuracy of the source classifier). Such a modified set is called $\Omega_f$ 2. An Optimal transport algorithm (OT) is used to transport the latent representations of $\Omega_t$ to the latent representations of the target domain $\Omega_f$ (with a consistency constraint between labeled examples of $\Omega_t$ and $\Omega_f$)  The authors eventually try their algorithm experimentally.  ,"The paper proposes an algorithmic trick (related to adversarial examples) to enhance existing semi-supervised domain adaptation techniques. The enhanced method does require at least a few labeled samples in the target domain. The paper first shows that if you perturb target samples by a small enough epsilon, you can maintain cycle monotonicity (and thus it is strongly related to the optimal mapping between two empirical distributions by construction). The paper proposes to create a ""source fiction"" dataset where each labeled target point is perturbed by a bounded epsilon amount (as in adversarial examples) but *unlike* adversarial examples, you are trying to move the point to be **correctly** classified. Then, all the target data is mapped to this ""source fiction"" dataset and the classifier is applied to this mapped target data. The paper shows that this idea can possibly boost the performance of existing OT-based domain adaptation methods. ",0.26229508196721313,0.3442622950819672,0.32786885245901637,0.37142857142857144,0.3142857142857143,0.21568627450980393,0.45714285714285713,0.13725490196078433,0.1342281879194631,0.08496732026143791,0.0738255033557047,0.2214765100671141,0.3333333333333333,0.19626168224299065,0.1904761904761905,0.1382978723404255,0.11956521739130437,0.21854304635761593
748,SP:a4ac21e2bbf31f0fabc379a83b401ff85d1ce16c,"This paper proposes an offline imitation learning framework that incorporates both optimal and suboptimal datasets to learn decision-making tasks, without requiring any reward annotations. To leverage high reward transitions from the suboptimal dataset, the authors formulate a discriminator that optimizes a positive-unlabeled learning objective, where positive samples come from the optimal dataset and unlabeled samples come from the suboptimal dataset. This discriminator is trained in an adversarial fashion along with the policy, resulting in a behavior cloning objective where samples from the optimal and suboptimal datasets are weighted differently according to the discriminator’s predictions. Experiments demonstrate that on a set of simulated locomotion domains, the proposed algorithm can leverage the suboptimal dataset to learn more performant policies compared to vanilla behavior cloning objectives and prior offline IL/RL baselines.","The paper deals with the following setup: offline imitation learning in the presence of both an expert dataset and a non-expert dataset. More precisely, the goal is to learn a policy as close as possible to the one(s) that generated the samples in a dataset $D_e$, while making the most of samples in a non-expert dataset $D_o$. The “reward” information is not present/used in the dataset. The authors draw inspiration from the positive-unlabeled classification as well as the adversarial imitation learning literature to propose a new algorithm to tackle this problem. They interleave the training of a discriminator and a policy. The discriminator is trained to discriminate between expert and non-expert dataset (using a positive/unlabeled loss) and takes as input the state, the action and the logit of the policy $\pi(a | s)$. The policy is trained to imitate the expert on $D_e$ and to “fool” the discriminator.   The authors present results on four environments from the Gym Mujoco suite with datasets extracted from the D4RL datasets. ","The paper proposes a new offline imitation learning algorithm, DWBC, for datasets that combine both optimal and suboptimal demonstrations. The approach is based on a modified behavioral cloning loss that weighs expert and non-expert data based on a learned discriminator. DWBC is compared against prior methods in OpenAI Gym tasks, and it is shown to yield better policies compared to the prior work. As a by-product, the method learns a discriminator that can be used to estimate the relative performance of any policies without rolling them out in the environment.","The authors consider the problem of offline imitation learning in the presence of suboptimal datasets. In the presence of suboptimal data, classical baselines like behavior cloning suffer performance hits, the drop in performance often correlates positively with increase in number of suboptimal trajectories. In this work, the authors propose a novel learning objective inspired by the min-max formulation in GANs. Particularly the agent learns a discriminator to distinguish between samples from the expert and the suboptimal demonstrator. Building on prior work in cost-sensitive learning, this discriminator is used to reweight loss per sample in the offline buffer.   The proposed algorithm is evaluated on standard offline RL benchmarks, across multiple environments. In many environments, e.g Hopper-v2 style environments, the policy improves against strong imitation learning benchmarks. The discriminator (which takes action probabilities as input) is also evaluated in the context of offline policy evaluation. On Hopper-v2 datasets, the discriminator output is compared with true reward accumulated by multiple policies.",0.25757575757575757,0.1893939393939394,0.22727272727272727,0.14124293785310735,0.21468926553672316,0.22826086956521738,0.192090395480226,0.2717391304347826,0.18404907975460122,0.2717391304347826,0.2331288343558282,0.12883435582822086,0.22006472491909385,0.22321428571428573,0.20338983050847456,0.18587360594795535,0.22352941176470584,0.16470588235294117
749,SP:a50c11a7dbeb81a5f36d83082105092e6888214f,This paper introduces the Gradient based Memory Editing (GMED) framework for task-free online continual learning that edits stored examples using gradient updates. GMED can be easily applied with other memory-based continual learning algorithms to improve their performance. The paper conducts several experiments to validate their method.,"In this paper, the authors propose a technique for replay-based continual learning that is applicable to both task-aware and task-free settings. Instead of replaying the same memory over and over, each sample from the memory is updated to maximize the forgetting (the increase of loss) through gradient ascent. Its effectiveness is backed up by extensive experiments.","This paper proposes a replay-based method (GMED) for tackling the challenging and important problem of task-free continual learning (CL), which is concerned with mitigating catastrophic forgetting when learning from a non-i.i.d. stream of data without knowledge of task boundaries. The key idea of the method is to *edit* samples from the replay buffer during training in a way that maximises the interference that would be caused to the performance on these examples by a gradient update on the minibatch from the current task. The method is motivated by previous work that has shown that prioritising replay of samples that would be most interfered with mitigates catastrophic forgetting (e.g.[1]).  Experiments are run demonstrating that GMED yields a statistically significant performance increase on a number of standard image recognition CL tasks (with discrete task boundaries and also with fuzzy task boundaries, which is more challenging and less common) when combined with a handful of existing replay-based methods ((namely experience replay (ER), ER with augmentations and Maximally Interfered Retrieval (MIR)). Further experiments are run to show that the method is robust to different memory sizes and to a range of values for the hyperparameters that the method uses to balance terms in the editing update. Additionally, experiments are also run to show that the type of editing used by GMED results in a better performance than random edits or adversarial edits to replay samples.  [1] Aljundi, Rahaf, et al. ""Online Continual Learning with Maximal Interfered Retrieval."" Advances in Neural Information Processing Systems 32 (2019): 11849-11860.","In this paper, the authors propose a method called Gradient based Memory EDiting (GMED) for task-free continual learning. The motivation is to address one drawback of existing experience replay methods for continual learning, that is, the utility of the stored examples may diminish over time during training. The idea is to update the example in the memory to maximize its interference. The proposed method can be combined with existing continual learning methods to further boost the performance. The authors conduct extensive experiments and ablation studies to show the benefits of the approach. ",0.22916666666666666,0.3958333333333333,0.5416666666666666,0.3728813559322034,0.4067796610169492,0.1417624521072797,0.1864406779661017,0.07279693486590039,0.27956989247311825,0.0842911877394636,0.25806451612903225,0.3978494623655914,0.205607476635514,0.12297734627831715,0.36879432624113473,0.1375,0.31578947368421056,0.2090395480225989
750,SP:a5481f8f1144fa8751a9c173e1ebdba4aef731c1,In this papers the author proposed a new Gaussian covariate teacher-student model that is capable of accurately estimating the training and generalization errors for its learned linear coefficients of its student part with an exponentially decaying concentrate bound. Their framework covers a wide range of learning problems that either are either directly in teacher-student style or can be easily adapted into such setup. Their empirical study demonstrated the effectiveness of their framework for a variety of problems even when letting loose some of the theoretical assumptions required.   ,The paper introduces a teacher student method that does not require the input data to follow the i.i.d. assumption. Authors provide a closed form expression for the asymptotic training and generalization error and provide strengths and limitations of the proposed method under empirical settings. The paper covers the related work and the theoretical arguments are sound. The method is tested on synthetic and real world data. ,"This paper investigates the performance of a class of teacher-student models in the context of supervised learning, and theoretically derives learning curves in the high-dimensional limit. An important contribution of this paper is to have unified a number of related previous works by introducing a new model called Gaussian covariate model. In this model, two covariates, u and v, are assumed to be generated from a common input data in different ways; u and v are given to the teacher and student, respectively, and are assumed to be mutually correlated Gaussian characterized by generic (though some regularity is assumed) covariance. The introduction of the conversion step from the input to the covariates makes it possible to handle more realistic datasets in the teacher-student framework, while keeping the model analytically tractable by the Gaussian assumption of the covariates. Numerical experiments well support the theoretical prediction. Especially, the experiment using GAN, in which the GAN generated data exhibits a good consistency with the theory while the original one used for training the GAN shows an inconsistency, is interesting since it simultaneously shows the possibilities and limitations of the proposed theory.  ","This paper introduces a Gaussian covariate teacher-student model, allowing for the teacher and student to work on different spaces introduced by different feature maps. The authors demonstrate close-form expressions for the training and generalization error in an asymptotic setting. These predicted results are compared with actual performances for kernel methods with ridge regression. The predicted quantities are shown to fit well to observed learning curves, even in the ridge regression case with real data. However, it is also observed that the prediction is limited in general for real datasets. ",0.15730337078651685,0.23595505617977527,0.19101123595505617,0.29411764705882354,0.3088235294117647,0.12041884816753927,0.20588235294117646,0.1099476439790576,0.18681318681318682,0.10471204188481675,0.23076923076923078,0.25274725274725274,0.17834394904458598,0.15000000000000002,0.18888888888888886,0.15444015444015444,0.2641509433962264,0.1631205673758865
751,SP:a565f0db02d4490304651dc62bc773212ff14768,"The paper calculates the forward pass at initialization of resnets in the joint proportional limit of infinite width and depth. In particular, the paper shows that this limit exhibits log-Gaussian behaviors, extending what was previously known for feedforward networks. From here, the paper makes several interesting observations regarding interlayer correlation, enlarged output variance and how to alleviate them. The proof crucially exploits homogeneity of the ReLU activation function, together with an assumed conjecture on the pre-activation vector’s direction being almost uniformly distributed.","This paper aims to deal with the problem that the gaussian process approximation of Neural network at initialization will become worse while the network become deeper due to the error accumulation. This paper proposed a central limit theorem in depth to have a log-gaussian limit of the infinite width and depth limit.  The subject of paper is pretty cool, but the assumption in the paper and conclusion doesn't convinced me the new characterization is right and essential.  ","The authors study signal propagation in randomly-initialized feedforward residual neural networks with gaussian weights, with architectures consisting of one fully-connected layer per each residual block in the network. Conditional on a certain conjecture about distributions of normalized preactivations in the residual network, they prove theorems that precisely articulate the distribution of the norm of the network output in a limit where the network depth $d$ and width $n$ simultaneously tend to infinity and are directly related, and as a byproduct articulate precise dependences of the mean and variance of this output, as well as the inter-layer correlations of the network, as functions of the depth and width. The authors claim interesting consequences of this conditional analysis relative to predictions one would get by considering a limit where just the width $n$ is taken to be infinite (and then possibly the depth) -- for example, they show that standard ResNets have less than half of their neurons in each layer activated, which can lead to certain unstable output behaviors, and that networks in this limit (in particular when $d > n$ as the limit is being taken) may have exponentially-large output variances, which may be challenging to mitigate (Section 3). The authors justify the conjecture underlying their theoretical results with Monte Carlo simulations and a heuristic argument. ","The papers studies the infinite width/depth limit of ReLU ResNets, to contrast it to the more usual infinite width/fixed depth limit. They show (under the assumption that a certain conjecture is true) that in the infinite width/depth limit, the limiting distribution of the outputs of the network at initialization (for i.i.d. Gaussian entries) is not Gaussian but log-Gaussian. While this was known for fully-connected networks, proving it for ResNets is more difficult due to dependence between the layers. This is related to the fact that less than half the neurons are active and that there exists correlations between layers of the network (which does not happen for fully-connected networks). These two properties of ResNets however lead to very large variances at initialization which can make training difficult, the authors therefore propose a balanced ResNets (where the activation at each neuron is chosen randomly between the ReLU and the ""negative"" ReLU) which ensures that half the neurons are active and removes the correlations, however the variance at initialization (though smaller than that of ResNets) can still be large (much larger than what is predicted by the infinite width limit). Finally the results are checked empirically, showing that even for small depth/width ratios (say 0.1) the combined with/depth limit describes the statistics of DNNs more accurately than the fixed depth, infinite with limit.",0.2,0.23529411764705882,0.29411764705882354,0.31645569620253167,0.3037974683544304,0.2018348623853211,0.21518987341772153,0.09174311926605505,0.10775862068965517,0.11467889908256881,0.10344827586206896,0.1896551724137931,0.20731707317073175,0.132013201320132,0.15772870662460567,0.1683501683501684,0.15434083601286175,0.19555555555555557
752,SP:a5c4ceccc273e2ed55280578cf3b9dc30ee7e1e0,"This paper proposes a method based on generative vision transformer with latent variables  following an informative energy-based prior for salient object detection. Both the vision transformer network and the energy based prior model are jointly trained  via Markov chain Monte Carlo (MCMC)-based maximum likelihood estimation.  Extensive experiments reveal both high performance model and meaningful confidence map scores, as well as estimation of uncertainty maps.",The author construct a generative model for salient object detection in the form of top-down conditional latent variable model. A generative vision transformer is used by adding latent variables into the traditional deterministic transformer. Experimental results on RGB and RGB-D image salient object detection show that the generative framework equipped with the EBM prior and the transformer-based non-linear mapping is powerful in representing the conditional distribution of object saliency given an image.,"This paper introduces a novel* supervised generative model for detecting salient image regions, where saliency is defined as containing object. This is a latent-variable model with an energy-based prior and a transformer decoder that maps the latent variable to a saliency map. The prior and posterior inference is done via SGLD. The model is well-evaluated on several datasets, with several metrics, and compared across a range of state-of-the-art baselines. It not only outperforms every single baseline (setting new SOTA), but also has the added benefit of providing high-quality uncertainty estimates. Detailed ablation studies show the relative importance of various model components.  \* I think it is novel, but difficult to say as I am not expert in the field and there is very information in the paper about prior art.  # Update I'm happy with the author response and am increasing my score to 7. I still have some doubts about the motivation of this work, but the theory is sound and the the results are very good.","The submission proposes a method for saliency detection based on a generative adversary learning framework, which uses a transformer architecture to replace the backbone of the framework and is jointly trained with a Markov chain Monte Carlo-based maximum likelihood estimation to improve the performance. The performance of the proposed method is evaluated on both RGBD- and RGB-based saliency prediction datasets. The paper is generally written well and easy to follow.  ",0.21212121212121213,0.25757575757575757,0.3333333333333333,0.3026315789473684,0.19736842105263158,0.12643678160919541,0.18421052631578946,0.09770114942528736,0.3055555555555556,0.13218390804597702,0.20833333333333334,0.3055555555555556,0.1971830985915493,0.1416666666666667,0.31884057971014496,0.184,0.20270270270270271,0.1788617886178862
753,SP:a5ffed3e726340976bb175091746fa86734f1a60,"This paper studies the convergence of Feedback Alignment, and proves that in the over-parameterized setting the error converges to zero exponentially fast. This is made possible by taking inspirations from recent work on the NTK, albeit with specific challenges as some quantities are not obviously positive semi-definite. Furthermore, it makes the somewhat surprising finding that regularization helps alignment, proving this in linear networks, and confirming this finding in simulations on Gaussian data and MNIST.","This paper builds upon the work of Lillicrap et al 2016 of attempting to find biologically plausible methods for implementing backpropagation. They thoroughly describe the issues with non-local information and why there are limits to using backprop-based learning to understand more about biological neural systems.  They describe the algorithm of feedback alignment using random back-propagated weights.  The authors go on to confirm some of the results of the Lillicrap paper, and find a surprising and novel result that regularization is required to produce convergence in over-parameterized settings. The experimental section of the paper shows some work on a more standard ML benchmark task of MNIST classification.  They show the results that regularization is required to get good performance on the task. ","This paper gives a theoretical analysis of feedback alignment (FA), an algorithm to train neural networks by approximating the gradient of the loss function using random matrices. More precisely, the weights of the network that is being trained are replaced with random matrices in the back-propagation step.  The authors study two-layer neural networks in the over-parametrised regime, where the number of samples n is much larger than the number of neurons in the hidden layer, n, and provide two results:  1. A proof of linear convergence of the loss to 0. This proof follows the proofs    of Du et al. (2018), Gao & Lafferty (2020) for standard backprop. The added    difficulty of analysing DFA in this setting is that the effective kernel of    the network is not a priori positive semi-definite.  2. An analysis of the alignment between the second-layer weights of the network    and the feedback matrix used in the feedback alignment algorithm, which    highlights the important role of regularisation and finds that (the role of)     alignment is more complex than previously thought. ","The paper provides a theoretical analysis of the feedback alignment algorithm (FA), a biologically plausible approximation to backpropagation. The authors consider a two-layered network, and derive training error bounds and error alignment for a variety of cases, using the neural tangent kernel method. The results show that FA-trained network can achieve zero training error for infinitely wide networks, and have some (strictly positive) degree of alignment between backprop and FA error vectors when proper regularization is used.",0.23684210526315788,0.3026315789473684,0.15789473684210525,0.232,0.136,0.1404494382022472,0.144,0.12921348314606743,0.1518987341772152,0.16292134831460675,0.21518987341772153,0.31645569620253167,0.17910447761194032,0.1811023622047244,0.15483870967741936,0.19141914191419143,0.16666666666666666,0.1945525291828794
754,SP:a60a3f931e1d579583c922aeb4607f885e3e13d0,"This paper considers the Gaussian process optimization problem where the objective function f lives in a reproducing kernel Hilbert space (RKHS). So far there is a significant gap between the lower and upper bound on the simple regret performance. The authors propose a full exploration algorithm, called Maximum Variance Reduction (MVR) which achieves a simple regret bound $\mathcal O^*(\frac{\sqrt{\gamma_T}}{T})$, where {\gamma_T} is the maximum information gain. This bound significantly improves the existing simple regret bounds. Further, on specific Square Exponential and Mat\’ern kernels, their bound matches the lower bound up to logarithmic factors. Further, the authors extend their results to the more general class of light-tail distributions, thus broadening the applicability of the results. Finally, they validate their theoretical results on several synthetic functions and show that their algorithm is better than other algorithms such as GP-PI, GP-EI, IGP-UCB. ","The paper derives improved upper bounds for the best-arm identification problem in GP bandits. Specifically, they improve the previously best-known upper bound on simple regret from O(\gamma_N/\sqrt{N}) to O(\sqrt{\gamma_N/N}), where \gamma_N is the maximal information gain and N is the number of function evaluations.   Removing the extra \sqrt{\gamma_N} factor is important as \gamma_N can grow faster than \sqrt{N}. In particular, for the special cases of Matern and SE kernels, the authors show that the new bound (unlike old ones) goes to 0 as N-->\infty and is only logarithmically far from previously established lower bounds.   The improvement in regret analysis boils down to a new confidence ellipsoid bound that is available for both subgaussian and light-tailed noise distributions. Unlike previous works, that focused on cumulative regret, the new confidence ellipsoid bound requires that query points x_1,...,x_n are independent of noise terms e_1,...,e_n for each n. This additional restriction gives improved bounds and is not restrictive for best-arm identification using a maximum variance reduction algorithm.","The paper introduces a new approach for simple regret minimization in Bayesian optimization. This is a useful testbed as an intermediate setting harder than pure black-box optimization due to the presence of noise, but avoiding some of the worst case pitfalls of a cumulative regret analysis.  Note that a GP with noisy observation will use regularization in its posterior, and that standard analysis is to split the prediction error in a part depending on the regularized posterior as if it was trained on the noiseless feedback and an error term as if it was trained only on the noise. From this template the paper derives its two main novel approaches: - Prop. 1, a new equality that connects the maximum (worst-case) prediction error of a regularized posterior trained on (unavailable) noiseless observation to the posterior variance of the regularized GP plus a regularization term depending on the regularization (and therefore on the noise level)  - Thm. 1, where the main difference is that using Prop. 1 the authors construct a point-by-point concentration inequality rather than the more commonly martingale concentrations used in the literature that apply to the whole input space. The upside is that this allows for confidence intervals whose radius does not depend on the information gain $\gamma_N$, unlike most methods. The downside is that it requires the noise to be uncorrelated with the observations, which in optimization means that the candidates for evaluation are chosen without looking at the feedback at all. The authors also provide a variant (Thm. 2) for light-tailed noise.  With these two tools the regret analysis is pretty straightforward, relying on an ideal discretization of the input space to extend the for-any concentration to a for-all bound, and then union bound the success probability. The final tool required is a unsupervised strategy to select candidates, and the authors resort to greedy information gain maximization, playing a sort of optimism role in the inner optimization loop. Finally all the candidates are evaluated and the best candidate found is returned.  Compared to other simple regret minimization approaches, this shaves a $\sqrt{\gamma_N}$ factor off, taking it closer to its min-max lower bound.   Experimentally, the proposed approach does not seem to outperform existing methods, slightly weakening the argument that the tighter confidence interval improves exploration/exploitation trade-offs in practice. ","This paper studies the problem of pure exploration in Gaussian processes. In particular, given a function sampled from a GP, the paper gives bounds on the simple regret of the classifier: the gap between the predicted optimum at time n and the true optimum. The authors assume an additive noise model where the noise is either assumed to be subgaussian or more generally “light tailed” according to a definition provided therein. In these settings, the authors develop novel confidence widths for GPs with additive noise. These are not the first bounds for GPs. Chowdhury and Gopalan for instance give tight confidence widths. What is novel is that in contrast to past works, the authors do not assume that the additive noise is Gaussian- a fact that makes computing past posterior distributions easier. These bounds allow the authors to derive simple regret bounds that scale like O(\sqrt(\gamma_N / N)) matching known lower bounds up to logarithmic factors and shaving a factor of \sqrt(\gamma_N) off of prior art. As the authors note, without this improvement, there are settings where past algorithms are not guaranteed to converge while theirs is. It is worth noting however, that the lower bounds given in 28 and 29 appear to be minimax in nature which would imply that the algorithms presented in this paper are minimax optimal, but that an instance dependent algorithm may be possible. For instance, a very recent work (recent enough that is does not impact my score since that would be unfair) https://arxiv.org/abs/2105.05806 studies pure exploration in RKHSs and gives a instance dependent result. They specialize their result to the GP case, and it may be worth comparing against them?",0.22,0.24,0.23333333333333334,0.1925133689839572,0.20855614973262032,0.13520408163265307,0.17647058823529413,0.09183673469387756,0.12237762237762238,0.09183673469387756,0.13636363636363635,0.1853146853146853,0.19584569732937687,0.13284132841328414,0.16055045871559634,0.12435233160621763,0.1649048625792812,0.15634218289085544
755,SP:a6142ebff7f84f8026cc48edd9edf6ddfe6de78a,"This paper studies the change point detection problems in the multivariate nonparametric regression under the constraint of local differential privacy. Specifically, the goal is to detect changes in the regression function as soon as the change occurs. To achieve LDP, the proposed method first uses a locally private mechanism to transform the data point (x,y) to another random pair and then uses a classical change point detection algorithm with newly defined CUSUM statistics to detect the change. Theoretical false alarm probability analysis and the minimax detection delay properties are given. To quantify the cost of privacy, the paper also provides the optimal rate in the non-private setting. ","The paper analyzes the online change point detection problem subject to local DP.  They claim to be the first to study this problem, which seems like a natural problem to solve, although the central DP setting has been studied before, as they point out.  The contributions of this work are not limited to privacy considerations by also giving minimax rate of detection delay.  ","This paper aims to detect the change in the regression function for multivariate sequential data, under the constraint of local differential privacy. Theoretical analysis on the information-theoretic lower bound to the detection delay is proved. And the detection statistic used in this paper is proved to be near-optimal since it matches the lower bound.","The paper studies change point detection for regression under the constraint of local differential privacy (LDP). Its main contribution is a non-interactive (in the sense that all randomizers are chosen ahead of time) algorithm and upper bound on the error of the identified change point, with a sequentially interactive lower bound matching up to logarithmic factors in the failure probability. The paper also provides a non-private benchmark, which is apparently also original.",0.1743119266055046,0.22018348623853212,0.27522935779816515,0.15873015873015872,0.19047619047619047,0.3392857142857143,0.30158730158730157,0.42857142857142855,0.40540540540540543,0.17857142857142858,0.16216216216216217,0.25675675675675674,0.22093023255813954,0.2909090909090909,0.3278688524590164,0.1680672268907563,0.17518248175182483,0.2923076923076924
756,SP:a63f86fd0f4d050455513e52efb38b2df4183024,"This paper proposes a joint prediction of the entire scene graph that fully captures the dependency among different objects and relations using a unified conditional random field.  The proposed model can be summarized as starting from joint modeling of the object and relation component label and visual features. Joint modeling uses CRF to model comprehensive dependency of the object using unnormalized likelihood and partition function. In addition, the potential function used for object and relation components computed affinity through a distance-based learnable prototype. Finally, the knowledge graph embedding techniques for projection of different embedding to the same embedding space (e.g. context space to relation space, object space to relation space) have been used.  All the learnable parameters are trained with maximum log-likelihood function and the partition function uses a mean-field variational inference for efficient sampling. Mean-field inference initialized with independent relation labels for triplet component factor update.  Authors have conducted extensive experiments on the Visual Genome dataset and a brief ablation study that provide a good insight into the method. ","This paper presents a conditional random field (CRF) based joint modeling of objects and relations for the task of scene graph generation. Unlike previous relation discovery methods, this paper employs embedding-based relation feature representation, and thus enables an unified modeling of the unitary and clique potential functions. Using MCMC sampler and mean-field variational inference, the proposed JM-SGG can effectively update the relation triplets utilizing the contextual label dependency. The experiments show that JM-SGG has good performances on the relationship retrieval task, even under the zero-shot setting.","* The paper proposes a method for generating scene graphs of images by modeling the objects and relationships with a Conditional Random Field. The paper claims that other existing methods do not model the dependency between all the objects and relations in an image. The proposed method is meant for modeling all the relations jointly which could lead to better performance in generating scene graphs. * The potential function for object components is defined using the representation obtained from an object detector network. The potential function for relationship components is defined using the knowledge graph embedding method which represents both the objects and relationships in the same embedding space. * A mean-field approximation of the probability distribution of the CRF is obtained by treating the labels for objects and relations as independent of each other. A message passing algorithm is used to iteratively update the factors and perform inference with the mean-field approximation. This is also helpful for sampling the scene graphs during the learning phase. * The experiments are performed on the VisualGenome dataset and results are shown for a few different tasks like Relationship Predicate Classification, Scene Graph Classification and Generation. The paper also includes results on the task of zero shot relationship retrieval that evaluates the model’s ability to identify head-relation-tail combinations not observed during training.  ","The paper proposes a new method for supervised scene graph generation from images. The new method models dependencies between objects and their relations as a conditional random field (CRF), conditioned on the output of an object detector. To facilitate inference in the GRF, the deterministic prediction by the object detector is used to initialize a mean field approximation to the posterior. This is then updated iteratively via message passing. The method is shown to yield improved performance in relationship retrieval on the Visual Genome dataset.",0.15428571428571428,0.28,0.17714285714285713,0.34065934065934067,0.1978021978021978,0.17272727272727273,0.2967032967032967,0.22272727272727272,0.36470588235294116,0.1409090909090909,0.21176470588235294,0.4470588235294118,0.20300751879699244,0.24810126582278483,0.23846153846153845,0.19935691318327975,0.20454545454545453,0.24918032786885244
757,SP:a6e664e1aa7f0d10727b33c37e26b0d089c09008,"The paper tackles the problem of label cleaning, where labels for incoming data points might be flawed. The authors propose an interactive learning approach, where the system builds counter examples for data points, which consist of pairs of data points which the model finds incompatible. The human expert is then asked to relabel either or both of these presented data points.  The empirical evaluation applies the method to tabular- and image data, comparing the proposed approach with skeptical learning and a variation of the proposed approach without counter examples. The results show that the approach outperforms its competitors in terms of F1 score. The authors also validate that using the fisher information matrix supports finding good counter examples.","The paper introduces a new data cleaning technique (CINCER), which selects potentially mislabeled samples to present to an expert for relabelling. CINCER works by identifying so-called `suspicious' samples and selecting a counter-example for each of them. The suspicious sample and the counter-example are both provided to the human expert for re-annotation. Samples are selected by maximizing the margin for a probabilistic classifier, while counter-example selection is performed via an approximation of the influence function which uses the Fisher information matrix. The use of the approximation is theoretically justified (mostly by pointing to relevant work), and experimentally shown to be a better choice than the alternatives from either the perspective of computational cost or performance (data cleaning / F1 score of resulting model).","The paper introduces an approach called CINCER which tackles label noise in a sequential learning task by querying a human annotator whenever it finds a suspicious example. In support of its suspicion, the model also presents the user a maximally-incompatible counter-example from the already accumulated training set. The primary contribution of the paper lies in choosing an informative counter-example using Fischer information matrix as an approximation of influence functions. CINCER enables the user to correct both incoming and old examples.","The article introduces a novel approach to interactively cleaning the erroneous labels for an ML task. It works by identifying highly-influential, mutually-incompatible examples. The authors show that, over time, correcting the newly acquired example (or the incompatible old one, or both), does improve the accuracy of the learned model. ",0.19491525423728814,0.16101694915254236,0.11864406779661017,0.19047619047619047,0.12698412698412698,0.14457831325301204,0.18253968253968253,0.2289156626506024,0.27450980392156865,0.2891566265060241,0.3137254901960784,0.23529411764705882,0.1885245901639344,0.1890547263681592,0.16568047337278108,0.2296650717703349,0.18079096045197737,0.17910447761194032
758,SP:a715516d21881edf60ca35709a8d81c256121785,"This work presents a method for estimating the viewing directions of projection images in single-particle cryo-electron microscopy (cryo-EM). The method consists of two steps, first the distance between the viewing angles are estimated using a neural network, then these distance estimates are combined to yield a viewing angle estimate for each projection image. The method is trained and evaluated on synthetic projection images, where it is shown to perform well in producing reconstruction distance estimates are combined to yield a viewing angle estimate for each projection image. The method is trained and evaluated on synthetic projection images, where it is shown to perform well in producing reconstructions of moderate resolution. ","The authors tackle the problem of image orientation estimation in single particle cryo-electron microscopy. This imaging technique generates a dataset of ~10^{5-7} 2D projection images of a 3D protein with unknown pose (elements of SO(3)xR2). Once poses are inferred, the 3D structure may be reconstructed with standard tomographic projection techniques. To estimate the distances, the authors propose a Siamese convolutional network architecture, which takes as input a pair of images, and attempts to predict the distance between their orientations (parameterized as quaternions). Once this distance function is learned (from a training set of posed images on a single protein), gradient-based optimization is used to infer each image’s pose. The authors show results on two small, synthetic datasets of 5000 images.","This paper uses deep learning to recover the orientations of molecules measured in the context of single particle cyroEM. The proposed method first estimates the pairwise distances between the angles of molecules based on the their projections. This is accomplished by embedding the projections into a feature space that was trained such that the cosine similarity between two features is equal to the distance between the orientations of the two molecules. Next, the authors solved an optimization problem (3) to recover the orientations, up to global ambiguities, from the pairwise distances. ","In this paper, “Learning to recover orientations from projections in single-particle cryo-EM,” the authors present an approach to recovering unknown projection orientations based on learning to predict the distance between projection angles given observed projections. Given these predicted projection angle distances, it is possible to recover the orientations themselves by solving for the orientations with distances that best match the predicted distances. The distances themselves are predicted using a Siamese neural network that is trained on projection pairs with known orientation distances. The authors demonstrate this approach on two synthetic datasets.",0.22123893805309736,0.19469026548672566,0.20353982300884957,0.14960629921259844,0.1732283464566929,0.24175824175824176,0.1968503937007874,0.24175824175824176,0.24731182795698925,0.2087912087912088,0.23655913978494625,0.23655913978494625,0.20833333333333337,0.2156862745098039,0.22330097087378645,0.1743119266055046,0.19999999999999998,0.2391304347826087
759,SP:a73b4d661910a2d2f7b254b769267054c0901cda,"A technique is developed to reduce the number of unique weights used in a network. The ultimate goal is to exploit this to reduce data movement in hardware. The unique weights are discovered multiple stages of training and clustering. An additional goal is to produce compressible weights. Results demonstrate that good compression ratios are possible, comparable to SoA quantization techniques, but with far fewer numbers of unique parameters.   ","The paper proposes a method called Weight Fixing Networks (WFN), which is designed to minimize the data movement in deep learning inference, thus minimizing the energy cost. WFN aims to maintain very few unique weights, low-entropy weight encodings, unique weight values which are amenable to energy-saving versions of hardware multiplication, while keeping the same performance. A new clustering algorithm for weight generation is shown and analyzed, and experimental evaluation shows the merits of the new scheme.","This paper focuses on reducing the data movement costs by reducing the number of unique weights in a network, i.e., resuing weights. Based on this consideration, a new compression pipeline that can successfully compress whole neural networks has been proposed, called WFN. WFN is based on traditional quantisation manner but aims to cluster the unique weights and reduce the weight-space entropy of the whole network. By doing so, the empirical studies show that the proposed WFN is efficient in terms of reducing parameter counts.","This work presents WFN, an ensemble of techniques to reduce data movement costs by reducing the number of unique weights in a network.  Overall the authors find some distinct advantages in their approach: They emphasise a low entropy encoding with a regularisation term, achieving entropies smaller than even those seen using 3-bit quantisation approaches. They require no additional layerwise scaling; the unique weights are shared across all layers. They substantially reduces the number of unique parameters in a network when compared to existing SOTA quantisation approaches. ",0.22058823529411764,0.23529411764705882,0.27941176470588236,0.19230769230769232,0.16666666666666666,0.2558139534883721,0.19230769230769232,0.18604651162790697,0.21839080459770116,0.1744186046511628,0.14942528735632185,0.25287356321839083,0.20547945205479454,0.2077922077922078,0.24516129032258066,0.1829268292682927,0.1575757575757576,0.254335260115607
760,SP:a7d0d472c2d21661f9fddc0b15758bc55a146abb,"The paper applies well known methods in topological data analysis for quantitative assessment of neural net unit performance. Specifically, a scheme is proposed wherein spatial patterns of activation in the network units are determined as function of a sequence of activation thresholds, patterns transformed to a graph representations (i.e. successive filtrations) and their corresponding kth betti number, (i.e. number of kth dimensional holes), evolution calculated. In this paper k is set to 1.  For units that the authors term, effective units for a given class and a given input image class population, the analysis is expected to yield to similar birth time of the holes. Thus, a feature entropy is defined on the first birth time in the betti curves computed on a given instance and aggregating the estimated birth times over a population of instances in a given class.  The main results include the illustration of invariance of the calculated feature entropy when one does reweighting / rescaling transformations in the inputs and the illustration of correlation between the choice of hyperparameters, perturbations in data sets, and the feature entropy measure.   ","This paper proposes a novel method to tell whether a unit (feature map) is effective in CNNs quantitatively, via ""feature entropy"". The proposed method builds a weighted graph based on a feature map and calculates the Betti curve. Then it defines the ""birth point"" that implies the appearance of regularized spatial pattern of notable components in the feature map. By computing the ""birth point"" based on a sample of images from a specified class, it can get a birth distribution, upon which the ""feature entropy"" is computed.","In this paper, the authors propose a technique to measure the state of each neural network unit. The features of the neural network units are given by a method that applies TDA to a graph representation of the activity degree of each unit. The authors also experimentally measure the degree of learning and show its effectiveness against other methods in terms of scaling and randomness.","The paper proposes a topology-based approach to measure the effectiveness of a unit in a neural network for a set of inputs (i.e. images from a specific class). A graph is constructed based on a given image and the units representation and the birth point is computed. The importance of a unit is then measured as the entropy over the birth point distribution, which is estimated by sampling a set of images. Experimental results highlight the methods advantage over three baseline approaches and illustrate its ability to discriminate models with different generalisation capability.",0.13114754098360656,0.12021857923497267,0.15300546448087432,0.1724137931034483,0.3103448275862069,0.3230769230769231,0.27586206896551724,0.3384615384615385,0.29473684210526313,0.23076923076923078,0.28421052631578947,0.22105263157894736,0.17777777777777778,0.1774193548387097,0.2014388489208633,0.19736842105263158,0.2967032967032967,0.2625
761,SP:a811878eb3cad4b46e738782622c32cb42dff046,"This paper proposes two methods for estimating the density of the outcome variable for the compliers in the potential outcomes framework. The first is a kernel-smoothing based approach, while the second is a model-based approach, and in both cases, the double machine learning approach based on data splitting is employed. The nature of the contribution seems mostly theoretical, with a lot of effort into proving the convergence rates. ","The most common problem in causal effect estimation is that of estimating average effects over populations. However, to get to individual level causal effects, one has to estimate the full *distribution* or densities in the continuous outcome case. The authors are estimating such a density for compliers in the IV setting. Two density estimation strategies are provided i) kernel-based, ii) model-based. Essentially the core contribution is that in the identifiable setting (under monotonicity), authors have extended density estimation methods to the IV setting for estimating density of causal effects for compliers. The key methodology used is double machine learning which has extensive precedent for designing robust estimators for causal inference problems. I think the contribution is valuable to the community and toward making causality realistic and using it for real-world applications. ",This paper deals with the problem of the estimation of the local treatment effect (LTE) which is measure of the causal impact of a treatment. To this extent the authors develop kernel-smoothing and model-based methodologies to recover this quantity from the data.  The theoretical properties of the machinery are developed and the authors show that the estimator of the LTE is consistent. Two applications of the proposed methodology showcase the technique.,"The authors propose kernel-smoothing-based and model-based approaches for estimating the LTE density in the presence of instruments. They derive Neyman orthogonal scores and construct the corresponding DML estimators (KLTE and MLTE), that exhibit debiasedness property. They experiment on synthetic and real datasets.",0.2857142857142857,0.2714285714285714,0.14285714285714285,0.15671641791044777,0.11194029850746269,0.1780821917808219,0.14925373134328357,0.2602739726027397,0.2222222222222222,0.2876712328767123,0.3333333333333333,0.28888888888888886,0.196078431372549,0.26573426573426573,0.17391304347826086,0.2028985507246377,0.16759776536312848,0.22033898305084745
762,SP:a86cbda9bc66680a017eb84c836390815acf834e,"This work is motivated by the performance dropoff of RL algorithms when the time scale $\delta$ decreases to 0. Although previous work has studied such an issue in Q-learning,  this paper rather focuses on policy-gradient (PG). Observing that the variance of PG explodes as $\delta\rightarrow 0$, the authors introduce Safe Action Repetition (SAR), which ensures bounded gradient regardless of the time scale $\delta$. Inspired by action repetition, the proposed technique is additionally able to handle unexpected events by interrupting repetition when the agent reaches an unsafe/uncertain region in the state-space. ","Real world tasks are often in continuous time, whereas typical RL algorithms operate in discrete time. In such situations the algorithms are often sensitive to the chosen discretization time step.  Small discretization scales can lead to various problems such as increases in the gradient variance, high computational costs, and difficulties with exploration.  This paper proposes safe action repetition (SAR) to tackle these issues. SAR augments the action space with an additional variable that determines for how long the chosen action will be repeated. In particular, this additional variable will determine the size of an L1 ball around the current state, such that when the agent leaves this ball, a new action will be chosen, i.e. it determines the size of a ""safe region"" during which the action will be repeated.  One previous related work that tackled this problem was FiGAR that used a similar augmentated action, but which determined the number of action repeats, not the size of a safe region. A downside of this approach is that it can not react when something goes wrong during the action repeat period.  They perform experiments on MuJoCo continuous control tasks combining their algorithm with PPO, TRPO and A2C and show invariance to variance time discretization lengths and improved performance. They compared to FiGAR as well as DAU, another algorithm for discretization invariance (they compared to the official implementation). Their method usually considerably outperformed the other algorithms.  They also proved the explosion of the policy gradient variance when the discretization goes to 0 under the assumption that the variance of the return stays lower bounded.  ------------------------------------------------------------------------ Update: See the comments for more discussion. The score remained unchanged.","The paper proposes an alternative to durative actions where instead of outputting an action and how long the action is to persist, it instead outputs an action and a radius. This radius defines a ""safe region"" where the action is to be repeated for as long as an agent is within it. Having the action duration depend on state makes it invariant to the environment's time discretization. They evaluate their proposal on the MuJoCo suite and demonstrate improved performance over an agent which bases its durative action on time alone.","This paper studies the effect of discretization of the time on the policy space methods in RL. Since the underlying physics of the most of the RL systems is in continuous time, one needs to perform a discretization of the time horizon to employ policy gradient to find the optimal policy. However, as shown in the paper, policy space methods might suffer from high variance when the discretization parameter is small. In addition, a large discretization parameter is not also optimal, because it will result in a suboptimal policy, and it is not robust to stochastic behavior of the system. To deal with this problem, the authors propose SAR (safe action repetition). The idea is to let the algorithm repeat the same set of actions for a window of time, until the system exists some ""safe region"". To find this safe region, the policy outputs the radius of the safe region, and the algorithm needs to find the optimal policy which maximizes the reward while finding the optimal safe region. The authors compare their algorithm with the previous work experimentally, and they show that their algorithms outperforms FiGAR-C.",0.25263157894736843,0.14736842105263157,0.25263157894736843,0.10545454545454545,0.15272727272727274,0.21978021978021978,0.08727272727272728,0.15384615384615385,0.12698412698412698,0.31868131868131866,0.2222222222222222,0.10582010582010581,0.12972972972972974,0.15053763440860213,0.16901408450704225,0.15846994535519127,0.18103448275862072,0.14285714285714285
763,SP:a86fd478de532b16d4e13b468cab27ff5311443b,"The paper at hand shows relations from self-supervised-learning (e.g., contrastive learning) and meta-learning (e.g., fine-tuning). Whereas the shown relationship is interesting it's not that novel and I would even argue that several teams have implemented similar ideas already. However it has not made that explicit, especially the gradient accumulation for contrastive learning which is quite relevant for many  learning in real world applications.",This paper formalizes a connection between the training procedures of (few-shot) meta-learning and contrastive learning. It shows that meta-learning algorithms can be used to pretrain on image data and outperform standard contrastive learning on downstream tasks. The authors further use the connection to develop data augmentation procedures for contrastive learning.,"This paper proposed a framework to integrate contrastive/self-supervised learning into meta-learning literature. The authors demonstrate that contrastive learning principles implemented in meta-learning methods such as R2-D2 can achieve comparable results on various computer vision tasks. Next, the authors proposed two tricks in meta-learning literature to improve SSL models, including (1) rotation prediction (2) batch gradient accumulation. Both methods demonstrate incremental improvement over the standard SimCLR baseline.","The paper first shows that the current popular contrastive learning for self-supervised visual representation learning shares some similarity with a meta-learning framework for few-shot learning. This inspires the authors to propose a meta-learning framework for self-supervised learning. In addition, the paper also shows that tools (data augmentation and gradient accumulation) developed in meta-learning can help enhance contrastive learners.   Experimental results show that the proposed meta-learning framework for self-supervised learning outperforms SimCLR, a state-of-the-art contrastive learning method, on multiple downstream tasks in a semi-supervised learning framework, though it performs not as well as SimCLR under the linear evaluation protocol on ImageNet.  Other results show tools developed in meta-learning can help enhance contrastive learners under the linear evaluation protocol.",0.15714285714285714,0.17142857142857143,0.2714285714285714,0.2830188679245283,0.3584905660377358,0.3055555555555556,0.20754716981132076,0.16666666666666666,0.14615384615384616,0.20833333333333334,0.14615384615384616,0.16923076923076924,0.1788617886178862,0.16901408450704225,0.19000000000000003,0.24000000000000002,0.20765027322404375,0.21782178217821782
764,SP:a8c4d99d9ef7b9f24288086ae0d1fee3995941cc,"This paper proposes a new method to tackle the problem of generalization to unseen dynamics for model-based RL. Previous methods learn to predict a vector $Z$ that characterizes a particular environment dynamics from past transitions. However, as the environment id or label is not available, this vector inevitably contains redundant information, which might hurt the generalization of the model. The paper therefore proposes an interventional approach to estimate the probability that two vectors $\hat{z}_i$ and $\hat{z}_j$ belong to the same environment, and then uses a relational head to force similarity between them. As a result, the redundancy in $Z$ is reduced which leads to improved generalization.","This paper considers the unsupervised dynamics generalization problem. In this problem there are a set of train MDPs and a set of test MDPs, all with the same state and action spaces, but with different dynamics functions. The authors build an approaches that use past transition segments to estimate an environment-specific vector $Z$, which is then intended to act as a contextual input to the dynamics function. The problem the authors identify is that a naive application of inferring $Z$ from past transition segments undermines the ability for Z to be similar in the same environment and dissimilar in different environments. The authors claim that generalization ability is hurt by the inability to preserve this property. The authors propose a loss (equation 6) that pull $Z$'s from the same trajectory together and push away $Z$'s from different trajectories together. However, to avoid not pushing away different trajectories from the same environment, the authors propose to estimate the controlled direct effect (CDE) of the $Z$ on the next state $S_{t+1}$: if $z^i$ and $z^j$ from trajectories $i$ and $j$ have a similar CDE, then the authors declare $i$ and $j$ to be likely from the same environment. This likelihood is used as a weighting coefficient on the loss: the authors use this weighting coefficient as a ""soft"" indicator variable to distinguish examples from the same environment (positive examples) or examples from different environment (negative examples). Figure 1 shows that a PCA visualization of the learned context vectors using the proposed method separate the context vectors into distinct clusters. Empirical evaluation on standard benchmarks shows that the proposed method does slightly better than the next best baseline, TMCL from (https://arxiv.org/abs/2010.13303).","This work tackles the problem of unsupervised dynamics generalisation in model-based reinforcement learning, improving approach introduced by additional of inferring a latent context which conditions the dynamics model and captures the variation in dynamics between environments. They introduce a set of auxiliary losses based on relational intervention and causal reasoning to encourage the inferred context to be the same in trajectories from the same environment (even when the environment identification is unknown) through inferring which environments are the same and weighting the context-similarity loss by this environment similarity. The method shows improved prediction error and test reward on a range of continuous control tasks compared to SOTA baselines from MBRL, Meta-RL and dynamics generalisation MBRL specifically. Visualisations qualitatively show that their method clusters transition segments from the same trajectory together better than previous methods, which could explain their improved performance.","The paper studies multi-task generalization in model-based reinforcement learning (MBRL). One of the standard ways to approach the problem is given by inferring a latent variable Z encoding each task and then conditioning the dynamics model on it. The authors propose to encode segments of state-action trajectories into Z vectors and maximize similarity between Zs from the same trajectory. Using the mechanism, the paper then studies the ability of the agents to generalize to unseen variations of the training environments. For example, if the agent was trained on MuJoCo HalfCheetah with body mass 1, it is asked to generalize to body masses 0.5 and 1.5. The paper compares the method against the baselines across 3 axes: in terms of dynamics prediction error, in terms of returns on testing environments, and in terms of separability of the inferred Z for different environments.",0.32432432432432434,0.21621621621621623,0.18018018018018017,0.13793103448275862,0.1310344827586207,0.23076923076923078,0.12413793103448276,0.16783216783216784,0.136986301369863,0.27972027972027974,0.2602739726027397,0.22602739726027396,0.17955112219451372,0.18897637795275593,0.1556420233463035,0.18475750577367206,0.17431192660550462,0.22837370242214536
765,SP:a8f537f6ff00b380a66a642d75439862217293c0,"In this paper, the authors introduce a novel method for the class inversion problem: given a trained classifier and a specific class/label, generate an example image of that class.  This method is called Plug-In Inversion (PII) and it works as follows: a gradient descent is performed to find the image x such that the output of the classifier is close to the target label under a suitable loss function.  In PII, each successive gradient update is computed using a data-augmented version of the current iterate of the image x.  The authors introduce new data augmentations and combinations thereof.  The benefits of this inversion method is that it works for any classification architecture (which is treated only as a black box), and the same hyper parameter values (e.g. amount of color shift) work across a range of significantly different classifier architectures.  The authors present the output of PII across 12 different networks (including Convolutional, Transformer, and MLP nets), and they demonstrate several examples where their method is more interpretable than DeepInversion, a state-of-the-art method for class inversion.   ","This paper attempts to solve the problem of image inversion by introducing three augmentation-based techniques. It specifically tries to solve the problem of class inversion, which generates an interpretable image from a neural network by sending the pre-initialized image to the network. Then optimization is wrt the input image instead of the weights of the network. It introduced new sets of augmentation-based techniques like centering, zooming, color shift augmentation, ensembling that have not been tried before, and finally, they combined them with VIT and MLP based vision models. They validated all the augmentations using Vision transformer and MLP based vision models and compared them against the exiting method Deep Inversion(Yin et al., 2020). The paper is well written but lack quantitative evaluation which is a very significant shortcoming. ","This paper proposes a method to perform class inversion on image data, called Plug-In Inversion. This method consists of a sequence of augmentations on image data and is designed to be applicable to a variety of architectures. This method is evaluated on ImageNet trained models and is compared with other techniques used for the same goal.","This paper proposes Plug-In Inversion (PII), which can invert a trained image classifier so that it generates a class-conditional image. In addition to CNNs that previous work has considered, PII can handle Vision Transformers and Vision MLPs as well. PII starts generating from the center of a random initialization with lower resolution. Then, the method gradually broadens the generating area (centering) and increases the resolution (zooming). During generation, colors are randomly shifted (ColorShift) and the averaged cross-entropy to $e$ color-shifted images are used.  ",0.16393442622950818,0.13114754098360656,0.13114754098360656,0.10606060606060606,0.12878787878787878,0.22807017543859648,0.22727272727272727,0.42105263157894735,0.27586206896551724,0.24561403508771928,0.19540229885057472,0.14942528735632185,0.1904761904761905,0.2,0.17777777777777778,0.14814814814814814,0.15525114155251143,0.18055555555555558
766,SP:a90c10defd5a443cb28eeb948ab81680f2b130c1,"This paper studies the learning of the optimal generalized Tikhonov regularizer for linear inverse problems. Optimality is investigated with regards to the mean-squared error.  The authors show that the optimal parameters are independent of the forward operator and additive noise. Next they look into how the optimal parameters can be learned from finite data for the supervised and unsupervised setting. They bound the excess risk for both settings. Both these bounds turn out to be similar. Lastly, the authors verify the theoretical bounds through numerical simulations. ","The paper considers solving linear inverse problems of the form $y=Ax+\epsilon$, where $A:X\to X$ denotes a bounded linear operator. The setup is a Bayesian setup with known distribution of source and noise. To solve this problem the paper focuses on using generalized Tikhonov regularization and solving the following optimization: $\min_x  d_Y(Ax,y)+\|\|B^{-1}(x-h)\|\|^2$. Let  $R_{h,B}(x)$  denote the unique solution of the mentioned optimization and define $L(h,B)=E_{x,y}\|\|R_{h,B}(x)-x\|\|^2$. For a given problem, the question is what is the optimal choice of $B$ and $h$ in terms of minimizing $L(h,B)$. The main result of the paper is to provide a full characterization the minimizers of $L(h,B)$ and show that $(\mu,\Sigma_x^{1/2})$ is a global minimizer of $L(h,B)$. (Here, $\mu$ and $\Sigma_x$ denote the mean and covariance of the input, respectively.) The paper also studies the case where instead of the actual source and noise distributions we have access to training data. In that case it studies two different methods for learning $(h,B)$ from data. ","The paper studies the linear MMSE in infinite-dimensional systems. Specifically, given an observation $Ax +\varepsilon$ where $A$ is a known linear operator and $\varepsilon$ is some form of noise, one wants to estimate $x$, which minimizes the mean squared error.  Focusing on regularization, the paper asks what is the optimal regularizer with respect to the mean squared error. It is shown that, when constrained to Tikhonov regularization, the optimal regularizer has a rather simple form which depends only on the statistical properties of the system and, in particular, is independent from the operator $A$.   The authors also address the problem of approximating the regularizer, based on a finite sample. They show two approximations schemes: A ""supervised-learning"" scheme, based on minimizing the empirical risk and an ""unsupervised-learning"" scheme, which is based on the particular form the optimal regularizer was found to have.  For both schemes, the authors show that as the sample size $m$ tend to infinity, the approximation error decays at the expected rate $\frac{1}{\sqrt{m}}$.","This paper considers solving the inverse problem with a quadratic regularizer. It shows that the optimal parameters for the regularizer, in terms of achieving the smallest reconstruction error, are given by the mean and covariance of the true signal distribution. The paper also provides upper bounds for excess risk (expected error) for the estimated parameters learned from a training data set in either a supervised or unsupervised manner.",0.22988505747126436,0.3448275862068966,0.21839080459770116,0.19289340101522842,0.12690355329949238,0.11695906432748537,0.10152284263959391,0.17543859649122806,0.27941176470588236,0.2222222222222222,0.36764705882352944,0.29411764705882354,0.1408450704225352,0.23255813953488372,0.24516129032258066,0.20652173913043476,0.18867924528301888,0.16736401673640167
767,SP:a9246fdc83f65ae557a7dfb1fff7f465be9e44fb,"This contribution adapts to sequential data the paradigm of continuous convolution kernels in convolutional neural networks (CNNs). While usual kernels are an extensive list of weights (one for each position), the introduced CKConv frames these kernels as continuous 1D functions, parameterized by a small multilayer perceptron. This paradigms allows for wider kernels with fewer parameters, making it possible to deal with long-range dependencies without recurrence. Several experiments are provided and show that the resulting CNN achieves state of the art performances on a variety of tasks, and in particular is able to deal with irregularly sampled data. ","The paper proposes continuous convolutional kernels parametrized in the form of an MLP. The MLP gets the relative time as input and outputs the column of the convolutional kernel at the given relative time. The authors show that Sine-based (SIREN) non-linearity is best suited for the kernel generating MLP, and an experimental evaluation is performed.","The paper considers convolutional networks for sequence processing and suggests to parameterize convolutional kernels in this setting by small neural networks. These neural networks are similar to implicit networks in the sense that its input represents a time difference to the currently evaluated time point in a sequence. For each time difference, the associated weights of the convolutional kernel are computed.  This allows to handle arbitrarily large convolutional kernels (albeit the desired size needs to be determined somewhat deterministically apriori), to handle irregularly sampled data and to handle data at different resolutions. Good results on a set of experiments show the practical validity of the idea. The paper does a larger study into the importance of the type of activation functions.   ","The paper introduces convolutions with continuously parametrized kernels for sequential data. Continuous kernel convolutions, CKConv for short, parametrize the kernel associated to a convolutional layer as a continuous mapping, $\psi: \mathbb{R}^+ \rightarrow \mathbb{R}^{N_{out} \times N_{in}}$, from the relative time offset $\Delta\tau \in \mathbb{R}^+$ to a weights matrix $\mathbf{W} \in \mathbb{R}^{N_{out} \times N_{in}}$, i.e. $\psi(\Delta \tau) = \mathbf{W}$. This is in contrast to traditional CNN approaches, which represent the convolution kernel as an explicitly learnable set of weights over an a-priori fixed receptive field.  The corresponding CKConv operation can be applied 1) over arbitrarily long receptive fields, including a global one over the history of the input sequence, without the heavy memory burden incurred by representing the convolution kernel explicitly as a set of weights along the horizon; 2) to sequences that are sampled irregularly or asynchronously, since the admissible set of relative positions are not fixed in advance.  A relation is given, which shows approximately how the CKConv changes under resampling the input sequence (the output gets rescaled by the ratio of the sampling rates), which is suggested by the authors to support well-behavedness under varying input resolutions during training and/or between training and testing. Afterwards, a discussion is given about the parametrization of the kernel. Specifically, various MLP models are considered with different activation functions, such as, ReLU, LeakyReLU and Swish, and it is demonstrated that these MLPs are unable to learn functions with high-frequency oscillations, while a recently introduced Sine activation is able to.   The experimental evaluation includes 1) classic stress tests for RNNs, 2) discrete sequence tasks (sMNIST, pMNIST, sCIFAR10), 3) time series modelling (CharacterTrajectories, Speech Commands) , where additionally robustness to missing/irregularly-sampled data and resolution changes are also investigated. The proposed model overall performs favourably to common alternatives; and it seems robust with respect to missing data, and moderate resolution changes.",0.1326530612244898,0.20408163265306123,0.2653061224489796,0.2982456140350877,0.43859649122807015,0.256198347107438,0.22807017543859648,0.1652892561983471,0.08024691358024691,0.14049586776859505,0.07716049382716049,0.09567901234567901,0.16774193548387098,0.18264840182648404,0.12322274881516589,0.19101123595505617,0.13123359580052493,0.1393258426966292
768,SP:a957f57177578f21b59e7704b9a164b5c490b839,"This paper presents Graph Deconvolutional Networks (GDN), a kind of graph neural network designed to invert the low-pass filtering operation implemented by Graph Convolutional Networks (GCN, Kipf & Welling, 2016).  GDN uses a high-pass filter to invert the effect of GCN and a learnable thresholding operation in the wavelet domain to remove any high-frequency noise. The method relies on the Maclaurin series to approximate the high-pass filter (which in principle should be the exact inversion of GCN, although this is expensive) and the wavelet basis (which relies on the eigendecomposition of the graph Laplacian).   The authors conclude the paper with two main kinds of experiments:   1. Graph feature imputation, in which only a fraction of the nodes in a graph is observed and the remaining must be reconstructed. 2. Graph generation using graph variational autoencoders (Kipf & Welling, 2017) and the Graphite technique (Grover et al., 2019).","This paper proposed GDN, the opposite of GCN that recovers graph signals from smoothed representations. The introduced GDN uses spectral graph convolutions with a high pass filter to obtain inversed signals and then de-noises the inversed signals in wavelet domain. Effectiveness of GDN is validated on graph feature imputation and graph structure generation tasks.","In this paper the authors introduce a Graph Deconvolutional Network (GDN) as a combination of inverse GCN filters (spectral domain) and denoising layers (wavelet domain). They motivate the design of their GDN by observing that a simple inverse GCN filter likely results in noise amplification. The proposed architecture also has computational and scalability advantages over the previous ones in the literature.  The authors test the capabilities of the GDN on two tasks: 1. feature imputation (reconstructing graph *features*) 2. graph generation (reconstructing graph *structure*), where GDN is used to enhance the performance of two graph-generation techniques  They compare their proposed architecture with various baseline and state-of-the-art approaches and achieve the best performance in all of the tasks.","This paper derives an inverse operator, which is said to be better than other methods in ref [50] and GALA. The work also points out the inverse operation results in a high pass filter and may amplify the noise. Motivated by this observation, a de-noising layer is designed and introduced into the proposed network. The graph deconvolution uses the wavelet transforms on the graph, which is implemented based on heat kernel and the series in terms of symmetric graph Laplacian. Theoretical analysis was provided for the derivation of the deconvolutional layer.",0.174496644295302,0.2214765100671141,0.18120805369127516,0.38181818181818183,0.2909090909090909,0.18181818181818182,0.4727272727272727,0.2727272727272727,0.29347826086956524,0.17355371900826447,0.17391304347826086,0.2391304347826087,0.2549019607843137,0.24444444444444444,0.22406639004149376,0.23863636363636367,0.217687074829932,0.20657276995305165
769,SP:a963beee141456d27ba7662048c78d660e1905c2,"This paper examines the control of linear time varying systems in which the controller is given predictions of the system matrices, cost functions, and disturbances for a horizon in the future. This setup is motivated by problems, such as power systems, in which reasonably accurate predictions of future dynamics and disturbances are available at least over short horizons. The paper gives two algorithms, one based on model predictive control (MPC), which replans at each time step, and another that only replans once over fixed windows. They bound the dynamic regret and competitive ratios of these algorithms, respectively. For the MPC method, the key technique bounding how the solutions to the optimization sub-problems change when the initial condition, the disturbances, and any possible terminal constraints vary. They use these bounds to analyze stability and dynamic regret in a unified manner. For competitive ratio bounds, they show how the particular algorithm can be viewed as solving a smoothed online convex optimization problem. Then they use the previously derived methods to bound the deviation of the solution they obtained from the optimal solution. ","The paper considers the control of a linear time-varying system with predictions. The predictions must be accurate and describe both the transition parameters, the noise, and the cost functions. The length of the prediction window must be larger than the system's sequential strong controllability index, i.e., such that it gives the controller the ability to transition perfectly between any two given states in a fixed number of steps without exerting too much control.  The main results are that for, prediction length $k$ large enough, we get dynamic regret $O(\lambda^k T)$ and competitive ratio $1 + O(\lambda^k)$.  To achieve their results, the authors formulate the control as an optimization problem and carry out a sensitivity analysis with respect to both the start and end state as well as the perturbations. They then use this to reduce the problem to smoothed online convex optimization, which then yields the dynamic regret. They also use the sensitivity analysis together with what they describe as standard methods to achieve the competitive ratio result.","This paper establishes both dynamic regret and competitive ratio bounds for online control with time-varying linear dynamics and adversarial perturbative noise, thereby extending past work for time-invariant noise-free dynamics. The analysis is facilitated by a general-purpose sensitive analysis for smoothed OCO.   Update: The reviewers adequately addressed my concerns, and I am ammending my review to an 8. ","This article studies finite-time horizon   predictive control for linear systems with time-varying and strongly convex costs. The authors assume that each time points,  the system coefficients and  the random disturbance of the state dynamics at the next $k$ time steps can be observed exactly, based on which the controller choose the current action. By reformulating the control problem as an equivalent optimisation problem,  the authors establish a perturbation bound of the algorithm,  and further analyze the dynamic regret and competitive ratio. ",0.23756906077348067,0.08287292817679558,0.15469613259668508,0.08045977011494253,0.16091954022988506,0.21311475409836064,0.2471264367816092,0.2459016393442623,0.3373493975903614,0.22950819672131148,0.3373493975903614,0.1566265060240964,0.24225352112676055,0.12396694214876032,0.2121212121212121,0.11914893617021277,0.21789883268482488,0.18055555555555555
770,SP:a97e63d289b17ddc5c9b520f2f2df40fa34393b6,"The paper considers a method of certifying decision trees for pointwise robustness to data bias. More specifically, the paper introduces a language to encapsulates different types of data bias and then introduces a criterion for robustness for the CART algorithm.  Main Contributions:  - Sec 3: A language to characterise data bias.  - Sec 4: The transformed components of the decision tree algorithm.  - Experimental evaluation of the proposed certification.","This paper introduces and explores a framework for certifying that classification models produced by a machine learning algorithm are making predictions on examples that are robust to bias in the training data.  Certification is per example: a certification for an example means that the same prediction would have been made on that example if the training data used to train that model had been perturbed in ways that correspond to potentially eliminating a specified type of bias in the dataset.   The paper presents an algorithm aimed at certifying robustness for a standard decision tree learner and proves that it is sound.  It explores the algorithm in a series of experiments on standard datasets, with different bias definitions and across different demographic groups.  ","The paper focuses on robustness against bias in training data. In particular, the main goal of the paper is to develop a method that can certify models produced are pointwise-robust to potential dataset biases. Authors focus on decision-tree learning due to the interpretable nature of the models. Authors propose using a novel symbolic technique to evaluate a decision-tree learner on a large number of datasets, certifying that each and every dataset produces the same prediction for a specific test point. ","The paper proposes to certify that models are pointwise-robust to potential dataset biases using decision trees. The approach programmatically specifies bias models), i.e. models that specify suspected bias. Robustness is certified using a symbolic technique to evaluate a decision tree on a large number of datasets and certifying that all datasets provide the same decision for a test point.",0.2727272727272727,0.24242424242424243,0.19696969696969696,0.1721311475409836,0.13934426229508196,0.4578313253012048,0.14754098360655737,0.1927710843373494,0.21311475409836064,0.25301204819277107,0.2786885245901639,0.6229508196721312,0.19148936170212766,0.21476510067114093,0.20472440944881892,0.20487804878048776,0.18579234972677594,0.5277777777777778
771,SP:aa1556c49793512f8a17884d966c2ce62558b442,"The paper presents a new solution to dimensionality reduction and data visualization. They define an explicit extension of the pairwise distances in the lower dimensional space, the equivalent extended distance (EED), which is proportional to the reduction in volume when going from a high dimensional space to a lower-dimensional space, This helps to avoid the crowding problem known from T-SNE, UMAP, etc. and could provide a better understanding of dimensionality reduction methods in general. In addition, they define a distortion framework that allows more efficient implementation of the EED. Finally, they introduce a hierarchical scheme for manifold approximation, where instead of splitting the problem into ambient space and data manifold, they add a third layer, the near field. This is similar to UMAP, but they use a Gaussian function instead of a uniform for the near field. Experiments are thorough and the visualizations behave as expected, with the data being more spread out compared to t-sne and UMAP. A quantitative analysis using recently proposed metrics is also provided.","This paper suggests what it calls a novel dimensionality reduction and data visualization method (SpaceMAP), but perhaps it would be more precise to say that they suggest two modifications to UMAP. One modification is theoretically motivated low-dimensional similarity kernel (that depends on the estimate of the input dimensionality). Another modification is multi-scale high-dimensional similarity kernel. The authors argue that their method performs better than existing methods on a variety of different datasets.  The first part about theoretically grounded low-dimensional similarity kernel (which seems to be the main focus of the paper) I found interesting. However, I feel that combining two very different UMAP modifications together made the paper weaker because it is not clear what effect do they have on their own. I'd much rather see more experiments that really elucidate the ""space expansion"" idea and effect. Also, the paper is missing a lot of relevant context/citations. Overall I feel that the work has potential but the paper is not really ready yet. I am giving a lower-borderline score.","This paper proposes Space-based Manifold Approximation and Projection (SpaceMAP) method to solve two existing issues in dimensionality reduction task: (1) capacity matching between high-dimensional data space and low-dimensional visualization space. (2) complexity in various manifold properties. To address these issues, SpaceMAP proposes two ideas: (1) space expansion (Section 3.2): inferring equivalent extended distance (EED) (Definition 3.2) for low-dimensional space based on volume matching and introducing function distortion (FD) (Section 3.2.2) to transform the similarity functions in high-dimensional and low-dimensional space; (2) hierarchical manifold approximation (Section 3.3): making distinctions between the near field, middle field, and far field of data distribution in a data-specific hierarchical manner. SpaceMAP achieves competitive performance on a diversity of datasets.","This work studies dimensionality reduction and visualization of high-dimensional data. The idea is to construct two graphs in the original high-dimensional space and the low-dimensional space, respectively, based on space expansion and hierarchical manifold approximation. Space expansion is to map the distance in high-dimensional space to an equivalent extended distance (EED) in low-dimensional space such that the volumes of the two (hyper-)spheres in the two spaces are equal. Hierarchical manifold approximation divides the points into near field, middle field, and far field, and assigns EED based similarity for points in the near field. Then a problem minimizing the sum of a KL divergence and a repulsive force term is solved by SGD to determine the projected coordinates. In numerical experiments, the effectiveness of the proposed method is demonstrated by comparing with some baseline methods on synthetic and real-world datasets.",0.18128654970760233,0.1695906432748538,0.25146198830409355,0.11931818181818182,0.1590909090909091,0.31746031746031744,0.17613636363636365,0.23015873015873015,0.2945205479452055,0.16666666666666666,0.1917808219178082,0.273972602739726,0.1786743515850144,0.19528619528619529,0.27129337539432175,0.1390728476821192,0.17391304347826086,0.29411764705882354
772,SP:aa43ab857fbd99d8ab86519143253da8ca21d46d,"This paper studies a new to DP noise addition mechanism, based on the Skellem distribution.  The benefits of the Skellem distribution include closure under addition, a property lost on discrete Gaussians, and its discrete nature which makes it able to be exactly sampled from with finite machines, unlike continuous Gaussian noise.  Furthermore, this work bounds the increase in the scale of the noise compared with Gaussian noise for the same level of privacy.  Lastly, they provide experimental results of their approach. ","This paper introduced and analyzed the multi-dimensional Skellam mechanism which is a discrete differential privacy mechanism based on the difference of two independent Poisson random variables to the federated learning setting. This paper also studies the experiment performance of such mechanism compared with existing continuous gaussian mechanism.  Contributions are, 1) analyzed the mechanism differentially private behavior in multi-dimensional federated learning setting. 2) experimented its performance and analyzed why this mechanism is suitable for real federated learning practice. ","This paper proposed a new discrete differential privacy mechanism based on the difference between two independent poisson random variables, which may be a new attractive alternative of discrete Gaussian mechanism. The authors also carefully proves a sharp privacy guarantee via privacy loss distribution, and renyi divergences. Extensive experiments results on centralized, federated learning, demonstrate that the effectiveness of proposed methods. ","The paper introduces the multi-dimensional Skellam (difference of two Poisson random variables) mechanism for differential privacy. They show that the Skellam Mechanism is Renyi DP (RDP) with slightly worse guarantee than than Gaussian Mechanism. They also compare the privacy loss over several rounds against Gaussian and Discrete Gaussian using Renyi DP, Advanced Composition and the Privacy Loss Distribution. Results show that Skellam performs similarly to Discrete Gaussian and Gaussian when using RDP and PLD bounds. As an application they consider using the Skellam Mechanism for distributed mean estimation and two Federated learning tasks (EMNIST and SO-NWP). Experiments show that they can match the performance of continuous Gaussian.  *** After rebuttal. I've read all the reviews and replies. Raising my score appropriately.",0.19753086419753085,0.19753086419753085,0.18518518518518517,0.2911392405063291,0.27848101265822783,0.3333333333333333,0.20253164556962025,0.26666666666666666,0.12195121951219512,0.38333333333333336,0.17886178861788618,0.16260162601626016,0.2,0.22695035460992907,0.14705882352941174,0.33093525179856115,0.21782178217821782,0.2185792349726776
773,SP:aa7d28cfd8e4b5f33ac213361edd4312eacfa35c,"The authors approach the solution to the problem of meta-learning as simply running a recurrent network through various tasks and back-propagating to train the RNN to adapt to new tasks (as in some previous works). The main advance of this work is proposing an architecture - a Deep Convolutional LSTM (with few architectural details) and showing that it works very well. The simplicity of this advance is a plus. The authors should change initial writing and clarify what they mean by distributed memory (see below in more detail) but otherwise the paper is easy to read. A good number of experiments is conducted, though it would be good if it was tested in harder tasks.",In this paper the authors propose a distributed memory system to tackle the online few-shot setting. The system is based on layers of conv-LSTMs that take as input an image and the labels at previous steps to generate activations and store information.   Overall I think that the technical contribution is marginal. Both the system structure and the problem formulation make the method impractical and difficult to scale to more complex settings. Experiments are limited to small datasets and do not show the potential of the proposed solution. Limitations are not appropriately discussed and a better framing w.r.t. more recent online few-shot learning work is missing. In the current form the paper is not ready to be accepted. I am open to change my mind and increase my score if the authors provide a strong rebuttal and a satisfactory answer to the concerns I have outlined below.,"In this paper, the authors propose a novel network architecture for online few-shot learning. They propose to use an LTSM framework and store the previous information in the hidden states of different layers. They also evaluate their methods on other tasks, such as online continual learning, online few-shot semantic segmentation, and standard supervised learning. ",The paper proposes to adapt Meta RNNs (memory-based meta-learning) by instantiating multiple layers of LSTMs where some of them are convolutional. The authors refer to this as distributed memory due to each layer having its own LSTM-based memory. They demonstrate good performance in few-shot learning and continual learning.,0.19827586206896552,0.10344827586206896,0.1206896551724138,0.1390728476821192,0.08609271523178808,0.16071428571428573,0.152317880794702,0.21428571428571427,0.2692307692307692,0.375,0.25,0.17307692307692307,0.17228464419475656,0.1395348837209302,0.16666666666666666,0.20289855072463764,0.12807881773399013,0.16666666666666666
774,SP:aaa9e7af8e21c4c30df1fa05d2c09eb1314e0998,"The paper studies bidding in the first pice auction with budget constraints. In this problem, there are n bidders/buyers and T rounds of auctions. In each round t, an item arrives and the bidders observe their value for the item and need to submit a bid for it. The paper studies if one can design a bidding policy that results in an online market equilibrium (OME). The authors present a pacing algorithm in which the submitted bid of a buyer is equal to the buyer's pacing factor and his/her bid. The pacing factor is then get updated over time. It is shown that under the pacing algorithm, the pacing factors, which are the dual variable of the budget constraints, converge. In addition,  the buyers' time-averaged utility converges to the optimal equilibrium utility and the buyers' time-averaged expenditure converges to the target budget range. ","This paper investigates the problem of finding market equilibrium in an online Fisher market. Different from the traditional static Fisher market, in the online setting the items arrive one by one following an unknown distribution. The algorithm needs to allocate the items in the same online fashion with the goal that the time-averaged allocation and utilities asymptotically converge to an online market equilibrium. The main result of this paper is a simple PACE algorithm that achieves this goal. The algorithm maintains a pacing variable for each buyer and at each step assigns the item to the buyer with the highest paced value, after which the pacing variables are updated accordingly. In the essence, this PACE algorithm is a version of the dual averaging algorithm applied to the dual of the EG convex program, and therefore enjoys many nice converging properties. The authors also discuss the implication of this PACE algorithm to the online fair division problem and extend it to the Fisher market with a continuum of items. Finally, experiments are conducted to show its quick convergence in practice.","=Paper summary= The authors study an online Fisher market. Unlike classic Fisher markets, here the items arrive one by one in an online fashion and must be immediately and irrevocably allocated to a set of static agents. The online market equilibrium notion introduced is a pair of sequences of (step-wise) allocations and prices, such that the market clears and the agents have no regret in hindsight. The authors present an algorithm for allocating the items, named PACE dynamics, with the property that the prices and the utilities converge to those of an equilibrium as the number of time steps goes to infinity. PACE is essentially the dual averaging algorithm on a regularized convex program that is the reformulation of the dual of the generalized Eisenberg-Gale convex program of Gao and Kroer [24]. Experiments on real datasets are also presented, showcasing that PACE converges fast in practice. ","The submission considers a setting of static buyers and online arriving items from an i.i.d. distribution. Buyers have linear utilities for items and budget constraints that only have to be satisfied in expectation.   The authors propose running a centralized first-price auction, together with a simple distributed algorithm for adaptively adjusting a pacing multiplier for the buyers’ bids. They show that after a sufficiently large number of rounds (polynomial in the number of buyers), their procedure converges to the static equilibrium outcome with all its nice properties. ",0.2702702702702703,0.20945945945945946,0.16216216216216217,0.2777777777777778,0.15,0.14189189189189189,0.2222222222222222,0.20945945945945946,0.2696629213483146,0.33783783783783783,0.30337078651685395,0.23595505617977527,0.24390243902439024,0.20945945945945946,0.20253164556962025,0.30487804878048785,0.20074349442379183,0.1772151898734177
775,SP:ab10e1914acc7e7c7f1d6223279cd04e27a0fe3d,"In this paper, the authors consider the partial unsupervised domain adaptation (PDA) problem. Different from the existing methods that assign lower weights to the source-specific classes and adapt one-to-one shared class mapping, the authors explore the relationship among different classes and expect to extract useful/similar information from other classes in a finer granularity for further boosting the positive transfer. The authors design a class2vec module to extract the implicit semantic topics from the visual features. Semantic responses of source and target data are aligned to retain the relevant information contained in multiple categories by weighting the features, instead of samples or classes. Extensive experiments on three benchmark datasets demonstrate the effectiveness of the proposed implicit semantic discovery and alignment.","This paper studies the partial domain adaptation problem. The authors propose an implicit semantic response alignment method where the source domain has more categories than the target domain. The motivation is to further utilize the extra source categories to boost the adaptation performance. To achieve this, anew module class2vec is proposed to extract the implicit semantics and explore the relationship among different categories, and align the semantic response of source and target data. Experiments on three benchmark datasets demonstrate the effectiveness of the proposed methods on different backbone networks. ","This paper introduces a new partial domain adaptation (PDA) method. Different from previous methods that ignore the irrelevant source classes, this work argues that source data coming from extra classes also contributes to the adaptation. Motivated by this, the proposed method first identifies implicit semantic, then performs semantic alignment. The strength of this work is proved on common benchmarks.","This paper considers the problem of partial domain adaptation. To address this problem, the authors propose the Implicit Semantic Response Alignment (ISRA) method, which attempts to align the source and target domains on the semantic topic level instead of class level. Experiments on several datasets show the benefit of the proposed ISRA.",0.3170731707317073,0.18699186991869918,0.21138211382113822,0.21348314606741572,0.3258426966292135,0.23728813559322035,0.43820224719101125,0.3898305084745763,0.5,0.3220338983050847,0.5576923076923077,0.2692307692307692,0.3679245283018868,0.25274725274725274,0.29714285714285715,0.25675675675675674,0.4113475177304965,0.2522522522522523
776,SP:abf467cba4ed26415c5cf2fae03729b2819eed4b,The paper approaches the problem of nearest neighbor search in hyperbolic spaces with focus on graph methods. The authors claim to obtain an acceleration in hyperbolic spaces over Euclidean ones using graph based NNS method (e.g. Prokhorenkova 2020). The provide bounds on the dimension and Radius of search in the dense  NN domain (ie d<<logn). Experimental validation is presented which demonstrate the advantage of graph methods and also tradeoffs on efficiency in different parameters regimes to validate the theoretical analysis.,"This paper looks into graph-based algorithms for nearest neighbor search (NNS) problem in the hyperbolic space.  Theoretically, the authors derived the time and space complexity of graph-based NNS under some assumptions (data uniformed distributed in the ball, low dimensional dense), and in some cases, graph-based NNS has lower time complexity in the hyperbolic space.   Practically, the authors apply the methodology to Poincar{/'e} Glove word embeddings, where it outperforms some existing baselines. ",This paper proposes to use the graph-based method for similarity search in hyperbolic space. Extensive analysis shows that graph-based method achieves sublinear time complexity for hyperbolic space and its performance is even better than for the Euclidean space under some mild conditions. Empirical experiments are also conducted to validate the theoretical analysis. ,"The paper considers the setting of nearest neighbor search (NNS) and devises a graph-based algorithm for this task in Hyperbolic space. The problem is pertinent, as hyperbolic space has become prevalent for representing hierarchical data in various domains, and nearest neighbor search is a very fundamental problem. The paper proposes a graph-based nearest neighbor search closely based on existing literature, and analyzes time and space complexity assuming a uniformly distributed dataset of $n$ elements over a $d$-dimensional ball of radius $R$; the dataset is assumed to be dense in the underlying space ($\log n \gg d$). It is shown that under some assumptions on $d$ and $R$, graph-based NNS has lower time complexity in hyperbolic space when compared with its Euclidean counterpart. Practically, the paper focuses on the sparse data setting. It is shown that graph-based NNS is more efficient for Poincaré GloVe word embeddings than for Euclidean GloVe word embeddings, assuming an HNSW graph construction. Moreover, it is shown that the proposed graph-based NNS method outperforms other existing baselines on Poincaré GloVe embeddings.",0.2073170731707317,0.2073170731707317,0.2804878048780488,0.22666666666666666,0.5333333333333333,0.3888888888888889,0.22666666666666666,0.3148148148148148,0.12777777777777777,0.3148148148148148,0.2222222222222222,0.11666666666666667,0.21656050955414013,0.25,0.17557251908396945,0.2635658914728682,0.3137254901960784,0.17948717948717952
777,SP:ac6c8b597168ce6223b92ab1e6f59d67c39edf80,"This paper studies zero-order optimization for strongly convex functions. The authors propose a novel 2d point zero-order gradient estimator, combing with distributed projected gradient descent, they provide upper bounds that are robust to adversary noises and have better dependence on (1 - \rho). In addition, they provide an improved analysis for 2-Holder functions using 2 point gradient estimator in that the dimension dependence is improved from d^1.5 to d. ","This paper considers distributed optimization with zero-order information and under rather general setting of noise in the evaluation of function values. The authors propose a zero-order projected gradient descent algorithm, in which a novel zero-order gradient estimation scheme is used. An important contribution of this paper is to provide explicit upper bounds for the average cumulative regret and optimization error of the proposed algorithm. The obtained bound is clearly presented and show the advantage of the proposed algorithm over existing ones. ","This paper studies a distributed optimization problem where the nodes conform to a given adjacency graph. The prevailing formulation of this problem in the literature is based on first order (gradient based) algorithms. This paper however, following [11, 27, 28, 29, 35, 37] considers a zeroth order optimization problem where only direct observations from the function value are accessible (instead of gradient). The average objective function is assumed to be strongly convex and the local objectives are assumed to be Hölder continuous. A distributed optimization algorithm based on a novel gradient estimator is presented. It is shown that the proposed algorithm is favorable over the existing ones in both convergence rate and computational complexity.  ","This paper considers the problem of distributed zeroth-order optimization for a class of strongly convex functions. The authors propose a distributed zeroth-order optimization algorithm that only requires only functional evaluations of the objective functions, subject to a general noise model. The error bounds of the algorithm have also been established and improve upon the previous works. Lower bounds have also been discussed and they are nearly optimal. ",0.2602739726027397,0.2328767123287671,0.2465753424657534,0.2976190476190476,0.25,0.1826086956521739,0.2261904761904762,0.14782608695652175,0.2608695652173913,0.21739130434782608,0.30434782608695654,0.30434782608695654,0.24203821656050953,0.18085106382978725,0.25352112676056343,0.25125628140703515,0.27450980392156865,0.2282608695652174
778,SP:ace27bdd52316b4095deadc1735a98b7b4ed1a39,This paper proposed an automatic curriculum learning design method for goal-orientated Reinforcement Learning. The idea is to learn a reachability trace function to measure the temporal difference between a state and the final goal state. Then a curriculum is learnt based on the reachability trace function. The algorithm is evaluated on tabular and continuous robotic control environments.,"The paper introduces a new method of doing curriculum learning for goal-directed RL. It uses a notion of reachability traces to model ""temporal closeness"" of states to a goal state. The reachability trace function is then used to define a sequence of sub-goals for which policies are learned iteratively, and then the final policy is learned  using advice from the decomposed sub-policies.   Experiments are run on a few gridworld like experiments showing favorable performance vs other CL methods, it is also shown that the method may be more robust when reward function is poorly misspecified.","This paper considers a sparse reward goal-based RL problem and introduces a notion of reachability trace for reinforcement learning, which intuitively approximates how reachable (or how far) the goal state is from the current state. The proposed idea is to divide the whole learning problem into three phases: 1) learning a reachability function w.r.t. the main goal state, 2) finding a sequence of subgoals that are gradually closer to the main goal, 3) learning a main policy while using the sequence subgoals (corresponding subpolicies) as an exploratory policy in epsilon-greedy action sampling. The empirical results on grid worlds and several maze environments show that the proposed method outperforms DDPG, RIS, PER, and EBU. ","The paper tackles the problem of curriculum design for single-task/single-goal reinforcement learning problems with sparse rewards. The primary contribution of the work is a method to generate a subgoal curriculum using reachability traces — a learned metric that captures the distance to goal using under a pre-determined behavioral policy. The recipe can be summarized as follows: (i) Use an _available_ behavioral policy $\pi$, or demonstrations, to obtain interactions from the environment, some of which reach the goal; learn a reachability function $\phi(s)$ that creates a connectivity map, (ii) generate a sequence subgoals using this connectivity map, eventually leading up to the final goal G, (iii) use the subgoal curriculum to learn to reach goals. ",0.3448275862068966,0.3793103448275862,0.3448275862068966,0.25510204081632654,0.20408163265306123,0.21367521367521367,0.20408163265306123,0.18803418803418803,0.1694915254237288,0.21367521367521367,0.1694915254237288,0.211864406779661,0.25641025641025644,0.25142857142857145,0.22727272727272727,0.23255813953488375,0.18518518518518517,0.2127659574468085
779,SP:ad0a9cdbdacb3c3097bd47e5be878dee1b395c05,"The paper presents an approach for skill learning and fine-tuning. The main idea of the manuscript is to combine offline skill learning with few online demonstrations to provide efficient learning experience in out of distribution (OOD) tasks. There are two main novelties in the paper:  - Learning of an inverse skill dynamics model that infers which skills should be used given the current state and a future state - Learning a distance function between states that can be used to select states from the demos as ""target/future"" states for better generalization  Apart from the technical novelties, the paper presents ablation studies with intuitions on which of the parts of the new approach (FIST) are important and why.","This paper proposes a novel semi-parametric way of few-shot imitation learning. It assumes access to a large task-agnostic dataset to extract fixed-length skills $\pi(a|s,z)$ and an inverse skill dynamics model $p(z|s_t, s_{t+H})$, which infers required skills given the current and future states. With the inverse skill dynamics model, an agent can sample a skill to reach a given goal state. When tackling a new downstream task with a few demonstrations, this method seeks for the closest state $s^*_{t}$ in these demonstrations and sets $s^*_{t+H}$ as a new goal. Since we assume $s_t$ and $s^*_{t}$ are similar, reaching $s^*_{t+H}$ should be plausible with the learned skills. The experiments show that this approach successfully solves unseen tasks with a few demonstrations and outperforms the behavioral cloning and skill prior RL baselines. ","This paper tackles the problem of learning generalizable long-horizon tasks from offline human demonstrations with the below two insights:  - (1) The long-horizon task learning is enabled by extracting behaviour priors Z as basic skills from offline data, which has been seen in previous publications  (Pertsch et al. 2020). - (2) The generalization is enabled by matching the new states from adapted envs with the states from demonstrations using a pre-trained distance function, which is common in few-shot imitation learning.  Experiments on a simulated navigation task (Pointmass and Ant) and a robotic manipulation task (Kitchen) were performed to prove the effectiveness of the proposed approach.   ======================================================  Clarifications:  Since task generalization benchmark is not well established in our research community currently, I feel it is worthy to state clearly what is the claimed generalization here. Specifically, the demonstrated generalization performance is under the few-shot learning setting. For example,   - (1) In Pointmass and Ant env, some regions are blocked in offline human demonstrations.      - When testing generalization, the authors provide a few more human demonstrations covering the blocked regions. The proposed method will thus generalize. - (2) In the Kitchen task, some objects are removed from offline human demonstrations.     - When testing generalization, the authors provide a few more human demonstrations covering the removed objects. ","The paper presents an approach for skill extraction from an offline dataset of demos/interaction data, for use in few shot imitation learning of previously unseen long horizon tasks. The method trains a skill encoder mapping trajectories to a latent skill space, as well as a decoder which outputs actions conditioned on a state and skill (similar to SPiRL, Pertsch et al). The method also learns a skill prior which maps a current state and goal state (H steps ahead of current state) to the latent skill space. The main different to SPiRLis that the skill prior takes as input both a current and goal state, while in SPiRL it took in only takes in the current state. The skill prior and skill encoder/decoder are jointly trained on the demos.   Then during evaluation of a new long horizon task given a few demos, the agent takes in as input the current state, queries a nearby state in the provided demos, recover the state H steps ahead, and apply the skill prior given the s, and s_{t+H}. The querying nearby states is using a learned distance function trained to capture temporal distance. All components of the method are finetuned on the few demos provided for the new task.  Experiments suggest this method outperforms SPiRL and naive BC + finetuning on unseen multi step tasks in a simulated Franka kitchen environment and on ant maze.",0.2222222222222222,0.18803418803418803,0.2905982905982906,0.17567567567567569,0.2905405405405405,0.17452830188679244,0.17567567567567569,0.10377358490566038,0.14468085106382977,0.12264150943396226,0.1829787234042553,0.1574468085106383,0.19622641509433963,0.13373860182370823,0.19318181818181818,0.14444444444444443,0.22454308093994776,0.16554809843400445
780,SP:ad981a193c7e86965daba313931b9c7c6bfcbd8f,This paper proposed a novel random search method that can converge to a second-order stationary point at a faster rate for high-dimensional problems with only zeroth-order oracle under standard Lipschitz gradient Lipschitz Hessian assumptions. The number of function evaluations is only linear in the problem dimension. Experiments verify the theoretical results. ,"This work studies the problem of optimizing a non-convex function in a deterministic setting, where the gradient of the function is not accessible. The authors provide analysis of convergence to a second order stationary point, where the gradient of the function is zero and the Hessian matrix is positive definite.     Assumptions: Let $f$ to be the objective function:  1. $f$ is lower bounded. 2. The gradient and Hessian of $f$ are Lipschitz 3. The smallest eigenvalue of the Hessian matrix has a uniform lower bound.  First the authors proposed a decent algorithm (Algorithm 1, page 4), which is a variant of vanilla random search method. Their analysis for this algorithm is divided into two parts: The case when the gradient is large (Lemma 2, page 4), and the case when the gradient is small but the smallest eigenvalue of Hessian is negative (Lemma 4, page 5). The latter is the case where they are close to a strict saddle point. Aggregating Lemmas 4 and 5 yields one of their main results, Theorem 5 (page 5). This theorem outlines the fact that convergence to a second order stationary point grows exponentially as a function of $d$ (dimension). To overcome this issue, they proposed Algorithms 2 and 3. The main idea of these algorithms is that instead of a random direction search (which causes an exponential dependency on $d$), they estimate negative curvature directions for escaping a saddle point. This new approach requires $4d$ additional function evaluations at each iteration. As they initially claimed, they could overcome the curse of dimensionality and obtain a linear dependency with respect to $d$. This result is marked out in Theorem 10 (page 7). ","In this submission, the authors analyse a variant of stochastic three points method which is a derivative free optimisation algorithm using a random search routine. They prove that the method converges to a second-order stationary point but suffers a complexity that increases exponentially with respect to the dimension of the problem. They propose a modification to the algorithm so that the complexity in terms of the number of function evaluations is only linear in the problem dimension. They provide experiments showing that the new method outperforms other random search methods in escaping saddle points for large dimensional problems.","This paper is related to random-search methods when optimizing non-convex objective functions without having access to derivatives. The authors provided convergence guarantees to a second-order stationary with only access to the function evaluations. Further, they showed some numerical results for highlighting their theoretical results. ",0.37037037037037035,0.5,0.2962962962962963,0.1111111111111111,0.06810035842293907,0.1717171717171717,0.07168458781362007,0.2727272727272727,0.3404255319148936,0.31313131313131315,0.40425531914893614,0.3617021276595745,0.12012012012012012,0.3529411764705882,0.3168316831683168,0.164021164021164,0.1165644171779141,0.23287671232876714
781,SP:ad99d1ed7d4019db957f38ca6b6bbd88f9572254,"The paper proposes a theoretical framework for unsupervised representation disentanglement (in the sense of Higgins et al.) based on three constraints (group structure, data and model). It further describes a learning method for satisfying these constraints, and experimentally shows some performance gain on traditional disentanglement metrics.   My intuitive understanding of the constraints is a follows: - the model constraint is satisfied by construction of the agent group, whose structure is transferred from the latent space by the model. - the group structure constraint ensures that this transfer is an isomorphism, and thus that the agent group is actually a group. - the data constraint ensures that the agent group actually matches the ground truth generative factors.   Please clarify if one of my statements is incorrect.","This paper presents an unsupervised approach to achieve group-based disentangled representation learning, as opposed to existing environment-based approaches. A theoretical framework for the group-based VAE was proposed, and implementations of the group and isomorphism reported. Experiments were performed on four common benchmark datasets, with comparisons to several representative VAE-based disentanglement framework (beta-vae, annealVAE, factorVAE, beta-TCVAE), demonstrating the groupified VAE achieves better disentanglement in many metrics. Qualitative evaluations demonstrate the cyclic representation space learned by the groupified VAE.","This paper proposed a new theoretical framework for achieving unsupervised representation disentanglement based on $n$-th dihedral group. Further, it proved three sufficient conditions on the model, group structure, and data respectively. Evaluations on multiple benchmark datasets demonstrate the proposed framework achieves better mean performance with smaller variances compared to some VAE models.",This paper provides a theoretical framework on the learning of group-based disentanglement representations. It proposes a method to learn a cyclic group representation with the Abel loss and Order loss based on VAEs. Experiments validate the effectiveness of the proposed method on improving existing disentanglement VAE models.,0.13114754098360656,0.16393442622950818,0.13114754098360656,0.20481927710843373,0.1686746987951807,0.2830188679245283,0.1927710843373494,0.37735849056603776,0.3333333333333333,0.32075471698113206,0.2916666666666667,0.3125,0.15609756097560976,0.2285714285714286,0.18823529411764708,0.24999999999999994,0.21374045801526717,0.297029702970297
782,SP:adfc8035d815dfbda5a609c364ea00e8b13b7131,"In this paper, the authors proposed a new variant of GW distance with linear computational complexity. The proposed LinearGW-LR leverages the low-rank property of the distance matrices and the low-rank assumption imposed on the target optimal transport matrix, which combines the advantage of the quadratic entropic GW and that of the low-rank Sinkhorn factorization in [32]. In the experiments, the authors show the feasibility of the proposed method in various tasks, especially in the cases of low-dimensional data.","The paper proposes a new approximating algorithm for Gromov-Wasserstein distance with $L_2$-norm loss that runs in a linear time regarding the number of samples. This algorithm is rooted in the combination of two low-rank techniques: one is the speed-up on cost matrix computation using low-rank factorization, and the other comes from the additional low-rank constraints on the set of feasible couplings. They also provide a guarantee on the convergence to a stationary point of the algorithm. Furthermore, they experimentally show that the proposed algorithm can accurately compute GW distance compared to previous techniques while being significantly faster.","In general, computing GW distance exactly is NP-hard. To compute nonconvex approximations of it generally take time $O(n^3)$-$O(n^4)$, depending on if the loss is separable. This work follows a recent trend in works that use low-rank approximations to cost matrices and couplings that have been used in the optimal transport literature to develop a method that has linear scaling in n, the number of points. Experiments demonstrate the efficiency and accuracy of the proposed method on some baseline experiments. ",This paper presents mainly two contributions around the computability/approximation of the Gromov-Wasserstein distance (NP-hard in general). The first contribution is to consider cost matrices that admit a low-rank approximation. This allows roughly to go from a cubic problem in complexity to a quadratic problem. The second contribution is to propose to optimize the couplings of the Gromov problem under low-rank constraints. Putting end to end these contributions allow to compute an approximation of GW in $O(n)$ (with respect to the number of samples). ,0.2891566265060241,0.24096385542168675,0.2289156626506024,0.16346153846153846,0.18269230769230768,0.19767441860465115,0.23076923076923078,0.23255813953488372,0.21348314606741572,0.19767441860465115,0.21348314606741572,0.19101123595505617,0.25668449197860965,0.2366863905325444,0.2209302325581395,0.1789473684210526,0.19689119170984454,0.19428571428571428
783,SP:ae0fd8a52410b31dd6cbe7a50ed538711e6b7533,"The manuscript introduces LAMBDA a Bayesian model-based policy optimization algorithm that adheres to supplied safety constraints. The approach relies on the model to simulate trajectories and therefore improve efficiency of learning and effectiveness of safety. Experiments on SG6 compare the proposed algorithm to previous approaches and ablations of LAMDA. Based on experimental results, authors conclude that LAMDA is more efficient and effective for learning and safety.  ","This paper is about solving CMDPs in an uncertainty-informed model-based fashion, without going through the usual LP route. The blueprints for this paper is the following:   1. Take a standard CMDP formulation 2. Express the objective and constraints in a UCRL (Auer, 2009) fashion (joint maximization over the set of possible dynamics), 3. Represent and learn the dynamics as a  Recurrent State Space Model from Hafner et al. (2019) 4. Estimate the objective and constraints themselves  (which involve a max operator) by sampling, and where the Bayesian posterior over the ""world model"" parameters is computed by SWAG (Maddox et al. 2019), where you average out the iterates generated over a trajectory of stochastic gradient descent  5. Using 3 and 4, compute a step of Augmented Lagrangian (a variant of) to solve the CMDP in 2.  The authors provide motivation as to why the UCRL aspect is important, and how the underlying optimism principle gives rise to persimism in the constraints, which is important from a ""safety"" perspective. This gives rise to an interesting tension (perhaps more ""balance"") between optimism in maximizing the objective while remaining cautions in abiding to the constraints.  ","The authors present LAMBDA, a novel algorithm for learning in CMDPs. The approach seeks to improve on the sample efficiency of previous algorithms by learning a probabilistic model of the environment. This model is then used to train a policy to maximise the return and respect the constraints by backpropagating through imaginary trajectories. The uncertainty of the model can further be used to steer the agent towards for optimistic or pessimistic behavior. The method is benchmarked against one online planning method and previous model-free approaches on the SG6 set of tasks of the Safety Gym environment. The results show that the proposed method compares favourably to competing algorithms, particularly in terms of constraint satisfaction and sample efficiency.","This paper proposed a method called LAMBDA for optimizing an agent's policy in constrained Markov decision processes. LAMBDA is a model-based approach leveraging Bayesian world models, which deals with reward optimistically and safety pessimistically. The authors demonstrate the effectiveness of SG6 tasks in Safety-Gym in terms of sample efficiency and satisfaction of safety constraint(s).",0.22388059701492538,0.31343283582089554,0.19402985074626866,0.12953367875647667,0.07772020725388601,0.15254237288135594,0.07772020725388601,0.17796610169491525,0.22413793103448276,0.211864406779661,0.25862068965517243,0.3103448275862069,0.11538461538461538,0.22702702702702704,0.20800000000000002,0.16077170418006428,0.11952191235059761,0.20454545454545459
784,SP:ae5db3dc47b2c6682437e9f0523566ec54349c9c,"This paper provides a theoretical analysis for sequential training based on related NTK results.  It shows the similarity between the target functions is a key factor for forward and backward transfer, and when the target functions are the same the samples size of each task is a key factor to forward and backward transfer. It also shows even a slight difference between the target functions can cause catastrophic forgetting and the forgetting might still happen for the same target function when the sample size of a later task increases. ","The paper is a theoretical paper about continual learning, that studies 2 tasks settings in the NTK regime.  The paper study in particular transfer from tasks with the same target functions and introduce the concept of ""self-knowledge transfer"", i.e. transfer from two tasks with the same target function.","This paper studies the generalization/transfer behavior of the continual learning/sequential learning under the neural tangent kernel (NTK) regime. It gives the formulation of the generalization error between two tasks in forwarding and backward ways and analyzes the influences of target similarity and sample sizes. Then the self-knowledge transfer case with the same target function is studied, which shows the universality of the catastrophic forgetting and the influence of the sample size. The results are then generalized to many tasks. ",The paper presents a study of continual learning of neural networks in the neural tangent kernel (NTK) regime. The authors extend recent work which estimates the generalization error of such networks using a statistical mechanics technique known as the replica trick. The theory makes a number of predictions regarding backward and forward transfer learning which describe well the behavior of finite neural networks trained with gradient descent.,0.1797752808988764,0.2696629213483146,0.14606741573033707,0.4,0.3,0.25609756097560976,0.32,0.2926829268292683,0.19402985074626866,0.24390243902439024,0.22388059701492538,0.31343283582089554,0.2302158273381295,0.2807017543859649,0.16666666666666666,0.30303030303030304,0.2564102564102564,0.2818791946308725
785,SP:ae6746b5684b8ec2d24c7be99fa85299a484b314,"Paper studies the routing problem in sparsely gated mixture of experts model for text. The central contribution of the paper is experimenting the efficacy of deterministic routing strategies that are not learned over the course of training but fixated at the beginning of training by utilizing hash functions. Various hash functions are experimented on four different tasks, and compared against prior art methods (BASE layers and Switch Transformer).  - Overall well written paper with a focused contribution - Conclusion has a more compelling and striking story which would be great to hear early on: learning free approaches for routing are quite competitive hinting that the learned routing algorithms may not be doing what we expect them to be doing.  - Paper suggests the proposed algorithm should be considered as a baseline to other methods, and given the results I cannot disagree with the proposal. ","The paper studies a simple hash-based approach to route input tokens in sparsely-gated mixture-of-experts models. The proposed approach for routing is a static hash-based approach, input tokens are hashed to 1 of the several experts in the MoE layer without the need for any learnable routing network parameters  which are typically used in MoE layers. This approach is compared against Switch Transformers and BASE layers [1, 2] on a generative dialogue task on the Pushshift.io dataset, language modeling on Roberta+cc100 and Wikitext-103 datasets and on fine-tuning on the BST dialogue tasks. On all these tasks hash-based routing is either competitive with or shows slight improvements over the previous (more complex) routing approaches.  In addition to the comparison against Switch and BASE layers, the paper also conducts some analysis on: 1. Several approaches for hash-based routing, and finds that simple balanced or random hashing performs better than approaches that take into account the preceding tokens or predicted future tokens for routing.  2. The effect of the number of experts (or the amount of sparsity) on the performance of Switch layers and Hash layers, and demonstrate that learnt routing layers are more competitive at lower levels of sparsity.  References:  [1] Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity, Fedus et al.  [2] BASE Layers: Simplifying Training of Large, Sparse Models, Lewis et al.","**Summary And Contributions:**  This paper is interested in routing strategy in MoE Transformers in NLP where each tokens of the sequence are “routed” to a specific FFN (also called expert). The authors argue that the difficulty of learning parametrized routing strategies (especially as the number of experts grows) calls for simpler baselines. The experiments show that a simple hashing baseline (pre-defining the expert routing for each token based on some token-frequency rules) is an easy-to-implement, robust and strong baseline that is either on par or outperforming learned routing strategies. Most of the comparisons are done with the perplexity on a variety of datasets and the authors also analyze a series of different hashing strategies.  ","The authors propose using hashing layer to replace the routing mechanism in the methods of mixture-of-expert such as Switch Transformers and BASE Layers. The main benefit of the proposed method is that no routing parameters need to learn and achieve quite good performance on the language modeling tasks. The authors also have a wide exploration of different hashing mechanisms, such as random hash, balanced hash, bigram hash, clustered hash, etc. ",0.22695035460992907,0.15602836879432624,0.11347517730496454,0.11440677966101695,0.09322033898305085,0.1864406779661017,0.13559322033898305,0.1864406779661017,0.2222222222222222,0.2288135593220339,0.3055555555555556,0.3055555555555556,0.16976127320954906,0.1698841698841699,0.15023474178403756,0.15254237288135594,0.14285714285714288,0.23157894736842105
786,SP:ae6937931d25fbacb47ee40f39d35ff523adc662,"This paper proposes LSP, an approach to set-to-set prediction that doesn’t require handcrafting a distance metric. This distance metric is often needed to perform bijective matching (e.g., the Hungarian algorithm) between the predicted set and the ground-truth set. The key idea is to perform the matching in the latent space instead, which means that encoding each element of the ground-truth set to this latent space has to be performed. Further, the authors adds two “tricks” to help with convergence: asymmetric latent loss (Eq. (2) with hyperparameter beta and gamma)) and gradient cloning with and without rejection (GC & GCR, with hyperparamter d to control the degree of rejection.)  The paper also provides the convergence analysis and experimental validation on 1 synthetic and 3 real datasets. ","Previous set prediction networks have typically been learned by using a matching step (e.g. Hungarian algorithm, chamfer distance) post-output in order to match the predicted set with the target set before computing the loss and propagating the gradient. Because the sets are represented as vectors (one per set element) stacked up in matrices, the predicted and target sets have an arbitrary order that might not be compatible with each other to directly compare corresponding elements (that is there might be a permutation that must be applied to one of the sets in order to align the elements with the other).   This paper proposes to move the matching step inside the network during training, such that a learned representation of each target element (in the order provided by the target) is produced that can be matched with the latent representation of each predicted set element. As such, when the latent vectors corresponding to the input are decoded they are in an order that is compatible with the ordering of the target set.   The inherent problem that arises is that there are discontinuities when assignments between the target and predicted sets change, so this work proposes a neat workaround that allows the target encodings to move towards and 'chase' their matched predicted latent.  ","A major component of the set prediction task has to do with the computation of a matching loss between the predicted set and the original set.  In this paper,  the authors propose to instead compute this matching loss in a latent space. Computing the matching loss in latent space introduces some computational problems such as computational stability. Additionally, computing the matching loss in a latent space allows teacher forcing in the set prediction task. To tackle this, the authors outline sufficient conditions for necessary for computational stability as well as an efficient algorithm that improves model convergence in the    set prediction task. The authors perform experiments on two set prediction tasks including image captioning and object detection.   ","This paper proposes a novel learning framework for set predictions. Due to its permutation-invariant nature, learning to predict sets usually requires costly matching between ground-truth sets and generated sets. Most of the previous methods do this in the actual data space where the sets reside; based on a pre-specified distance metric, optimal assignments between ground-truth sets and generated sets are computed via Hungarian algorithm, and then the errors are backpropagated with those assignments. As the authors pointed out, it is often hard to hand-specify such distance metrics that can be informative enough to get training signals, especially for high-dimensional data. This paper proposes to conduct the matching between sets in latent space in order to alleviate this problem; Of course, if possible, matching in latent space would make the problem much easier, but now the problem is that matching in latent space requires matching two moving targets in the latent space (representations of ground-truth sets and representations of generated sets). This actually causes the ""switching"" problem that can hinder the training. The authors an optimization algorithm where the representations of ground-truths and generated sets are kept close via additional loss function and a special mechanism called gradient cloning (with rejection). The proposed algorithm is shown to guarantee convergence.",0.2692307692307692,0.23076923076923078,0.2692307692307692,0.14084507042253522,0.18309859154929578,0.29914529914529914,0.1643192488262911,0.2564102564102564,0.16203703703703703,0.2564102564102564,0.18055555555555555,0.16203703703703703,0.20408163265306123,0.242914979757085,0.2023121387283237,0.1818181818181818,0.1818181818181818,0.2102102102102102
787,SP:ae70788404df28dd3f2e3089960d4f4e4d965bda,"This paper introduce a universal module to improve vision transformers. They show that vision transformers suffer feature collapse, which mean that features in deep layers will become similar. To address, this paper presents an augmented shortcut module to  be flexibly embedded into various transformer models. Extensive experiments are conducted on multiple tasks (image classification, object detection, and transfer learning), which show that augmented shortcuts can bring obvious performance improvements. That is a good paper to me because of the new and effective module which is a new investigation to the field.","This work proposes to boost the performance of vision transformers from shortcuts. As analyzed, the feature diversity of tokens declines as the layers go deeper while the shortcut connection provide a fix towards to potential ``feature collapse'' issue. To this end, additional branches of augmented shortcut is proposed to further improve the feature diversity as well as the final recognition power.","This paper observes the problem of feature collapse in transformer, where the tokens in deep layers collapse to the same representation. It then proposes to add additional augmented shortcuts in parallel to the original shortcut connection in the multihead self-attention modules and MLP modules, based on the observation that simple identity shortcut alleviates the problem of feature collapse. In the experiments, the proposed method brings 1.1% performance gain in the ImageNet classification task compared to the baseline model. Performance gains can also be seen from COCO detection task and other downstream finegrained classification tasks, as compared to the baseline models.","This paper alleviates feature collapse in vision transformers by introducing an efficient module, named augmented shortcut. As shown in the theoretical and empirical results, the feature diversity is enhanced and obvious performance improvement is achieved. The authors conduct experiments with various SOTA vision transformer models and validate the effectiveness of the proposed augmented shortcuts. ",0.15384615384615385,0.1978021978021978,0.16483516483516483,0.26229508196721313,0.18032786885245902,0.1568627450980392,0.22950819672131148,0.17647058823529413,0.2777777777777778,0.1568627450980392,0.2037037037037037,0.2962962962962963,0.18421052631578946,0.18652849740932642,0.20689655172413796,0.19631901840490798,0.19130434782608696,0.20512820512820515
788,SP:aeae9cb5ac2176b94e841dfe48bdf160f814abf2,"This work extends HER to meta-RL setting where the task parameters are not provided to the agent. The authors propose two different ways to relabel transitions and demonstrate their effectiveness through experiments. On goal reaching tasks, the performance of HTR on sparse reward setting is comaparable to baseline under dense reward setting.","This paper describes an extension of Hindsight Experience Replay (HER) to a meta-RL setting with task inference. Like HER, trajectories are relabelled with rewards as if they had achieved a goal. Unlike HER, the goal cannot be explicitly relabelled. Instead, a context is provided from which the agent attempts to learn a task embedding: this task inference is also trained using the reward-relabelled data.","This paper extends the idea of hindsight experience replay (HER) to a meta reinforcement learning setup. The primary focus is on sparse reward settings, where it is difficult for HER to generalize and even harder to conceptualize and optimize in the meta RL setting.   This paper provides a simple learning algorithm to relabel experiences in an off policy learning settings and provides experimental validation on continuous control tasks.  Key idea: the task is unknown in the meta RL setting. An arbitrary experience trajectory collected during policy evaluation (unknown task) can be reused for an artifically induced task (therefore known). This lets the algorithm leverage past experiences under unknown task distributions to meta learn strategies for an unknown task with sparse rewards.  ","This paper proposes an algorithm. Hindsight Task Relabeling, for meta-reinforcement learning in sparse reward environments. It can be viewed as applying the hindsight relabeling in a meta-RL setting, instead of in the goal-conditioned setting of the original HER. With the Meta-RL setting, the task is not revealed to the agent during test time, and thus needs to be inferred as well (compared to the goal-conditioned setting). Specifically, they proposed two relabeling strategies, Single Episode Relabeling (SER) and Episode Clustering (EC). The key difference from naive HER style relabeling is that SER samples an entire episode, subsamples transitions, and then relabels them to the same hindsight task. With EC, similar trajectories are clustered into the same hindsight buffers (e.g. by discretizing the state space into bins) with stratified sampling during model training. The major hyperparameter introduced is K, which determines the probability of sampling original vs hindsight experience when training the model.   The experiments were applied on goal-reaching tasks (2D navigation, wheeled locomotion, and ant locomotion). They used PEARL as the underlying off-policy meta-RL algorithm. The results indicate that HTR is crucial for sparse reward tasks, and can learn as well (sometimes better) than with shaped dense reward. Unlike HER where higher hindsight ratio helped more, in HTR only a K=0.1 (10% hindsight) is best in their environments. Finally, despite EC being more complicated to implement, it performed similarly to the much simpler SER strategy.  ",0.2641509433962264,0.3018867924528302,0.49056603773584906,0.2878787878787879,0.3333333333333333,0.2231404958677686,0.21212121212121213,0.1322314049586777,0.10612244897959183,0.15702479338842976,0.08979591836734693,0.11020408163265306,0.23529411764705882,0.18390804597701152,0.17449664429530198,0.20320855614973263,0.14147909967845657,0.14754098360655737
789,SP:aebd763164f00bed383d6f8dd87c3726803e8400,"This paper studies a new learning method that makes a model be invariant under a data augmentation (implicitly, to achieve high test performance when the test data distribution is different from the training data distribution). The authors propose a new objective function, called DAIR, characterized by a regularized function where a regularizer maintains the performance of the model on data augmentations. Some numerical experiments by varying regularizers and datasets are provided (in Sections 2 and 3), and some theoretical results are presented. The authors claim that DAIR consistently outperforms the baseline methods including ERM and DA-ERM.","The authors propose a conceptually simple regularizer which forces consistency of loss between the true samples and their corresponding augmentations, termed DAIR-SQ. The authors demonstrate the qualitative characteristics of DAIR-SQ with a clean toy example, and a supporting theoretical result of asymptotic consistency on the same. For real-world datasets, the authors demonstrate the robustness induced by DAIR-SQ under domain shift, label noise, and adversarial attacks.","In this paper, the authors proposed a regularizer (DAIR) that penalize the difference between the model performance on original data and augmented data. The authors argued that the new regularizer imposes invariance and effectively prevented the model from overfitting to spurious features. By applying the regularizer on various types of machine learning tasks, the authors analyzed the experiments results and explained the empirical novelty.","< Summary >   Many machine learning models depend on empirical risk minimization (ERM), vicinity risk minimization (VRM), and ERM with data augmentation. Original ERM and VRM show a good performance in many tasks, but they fail to generalize the out-of-distribution dataset. This paper proposes a new regularization method, data augmented invariant regularization (DAIR) methods. DAIR is a kind of regularizer that penalizes loss inconsistency. This paper validates the superiority of DAIR on drug discovery, visual question answering, and CIFAR-10 ",0.14432989690721648,0.1958762886597938,0.15463917525773196,0.2318840579710145,0.13043478260869565,0.15625,0.2028985507246377,0.296875,0.1875,0.25,0.1125,0.125,0.1686746987951807,0.23602484472049692,0.1694915254237288,0.24060150375939848,0.12080536912751677,0.1388888888888889
790,SP:aee275530f29aa91d5689e7f1dcc5362dee6436b,"The paper introduces a series of Bayesian neural networks that predict the performance across different, discrete fidelities. The models for higher fidelities use the prediction on lower ones as inputs, which makes this model auto-regressive. the authors use this flexible model and combine it with a new acquisition function that aims to minimize the mutual information of a batch of potential query points and the current belief over the optimum. By designing a heuristic to optimize the acquisition function, the authors are able to apply their model to the problem of Batch BO.  In the experiments, the proposed method is compared to other state-of-the-art methods for regular BO, batched BO and multifidelity BO.","The paper suggests a methodology for performing multi-fidelity Bayesian Optimziation (BO) using a particular (deep auto-regressive) Neural Network model as the surrogate. The authors formulate a batch acquisition strategy based on max-value entropy search, and propose an approximation using posterior sampling and cyclic optimization that renders the optimization of the acquisition function tractable. The method is evaluated on both synthetic and real experiments, where it shows improved performance over existing approaches.","In multi-fidelity black-box optimization, f() can be queried at points x at different levels of precision. Higher levels of precision have higher costs. Prior work on Bayesopt for mfbbo used GP models for multi-fidelity observations that make simplistic assumptions about correlations across fidelities. In response, the authors propose instead using a flexible autoregressive model for modeling the sequence of fidelity scores for a given x. Bayesian inference on the model is performed using HMC.  The paper also introduces a batched form of max-value entropy search and an algorithm for optimizing this complex acquisition function that works well in practice.  The experimental results are impressive. The proposed method outperforms a large number of popular methods from the literature.","Multi-fidelity methods are prevalent in problem settings where high-quality (but difficult to acquire) observations are enhanced with lower-quality (but more easily obtained) samples of a function. In this paper, the authors focus on the application of multi-fidelity modelling to hyper parameter tuning, whereby different model settings are verified with varying fidelity constraints (such as epochs in a training loop) in order to achieve the best accuracy-cost trade-off. The model introduced here is based on chained neural networks, whereby the input at each fidelity layer combines the input in the original domain for the function to be estimated, along with the outputs from the preceding fidelities. In order to carry out BO, the authors develop a batch acquisition technique that encourages diversity across the samples being drawn. This is in contrast to other techniques where new samples are drawn one a time, thus incurring the risk of there not being sufficient diversity across successive iterations. In order to achieve this goal, the authors also propose an alternating optimisation scheme for iteratively selecting samples across different fidelities.",0.17094017094017094,0.19658119658119658,0.20512820512820512,0.25675675675675674,0.20270270270270271,0.18181818181818182,0.2702702702702703,0.19008264462809918,0.13259668508287292,0.15702479338842976,0.08287292817679558,0.12154696132596685,0.2094240837696335,0.19327731092436976,0.1610738255033557,0.19487179487179487,0.11764705882352941,0.1456953642384106
791,SP:af2133a56b0920beb9986d19217a8b5aef94811c,"This work has two main contributions: (1) reformulation of the zero shot action recognition task for the multi-label case, and (2) a new model motivated by the authors idea that nearest neighbor is sub-optimal on a static semantic space. A pairwise scoring function is proposed in this work which first maps both visual and semantic embeddings into a joint space to predict match/no match probabilities of a 3D-CNN based visual representation and a Word2Vec/Sent2Vec based text representation. Their approach is evaluated not only on multi-label action recognition (where the zero shot performance is evaluated for the first time) dataset but also on single-label action recognition dataset, achieving state-of-the-art results. Evaluation protocols for single-label zero-shot action recognition datasets, UCF-101, HMDB, RareAct is proposed and another two evaluation protocols for multi-label zero-shot action recognition based on MEVA and EVA datasets are proposed in this work. ","In this work, authors reformulate zero-shot action recognition such that does not rely on nearest neighbor classification, but rather consists of a pairwise scoring function. Given a video and a set of action classes, the method predicts a set of probabilities for each class, allowing for semantically distinct classes to be predicted with high confidence. The proposed method improves on previous state- of-the-art zero-shot action detection models and is tested on UCF101, HMDB, AVA datasets among others. ","This paper proposes a strategy for zero-shot action recognition. One of the contribution is to design the model such as it directly outputs a score given an unseen class name. This is different from past work that instead rely on nearest neighbor in a joint text-video space. The authors explore various ways to combine the visual and semantic class name in the architecture. They report results on multiple datasets UCF-101, HMDB, RareAct, AVA and MEVA where they show improvements over previous work trained on the same dataset [4].","This work investigates zero-shot action recognition from a multi-label perspective. Rather than a nearest neighbour action selection, this work opts for a binary cross-entropy optimization. Experiments on common datasets as well as AVA and MEVA show the effectiveness of the approach.",0.1962025316455696,0.17088607594936708,0.10759493670886076,0.24691358024691357,0.16049382716049382,0.17582417582417584,0.38271604938271603,0.2967032967032967,0.38636363636363635,0.21978021978021978,0.29545454545454547,0.36363636363636365,0.2594142259414226,0.2168674698795181,0.1683168316831683,0.2325581395348837,0.208,0.23703703703703702
792,SP:af22742091277b726f67e7155b412dd35f29e804,"This paper proposes a novel contextual bandit algorithm, Neural-LinUCB, that transforms the raw feature vector using the last hidden layer of a deep ReLU neural network (deep representation learning), and uses an upper confidence bound (UCB) approach to explore in the last linear layer (shallow exploration). The algorithm is shown to enjoy the best of two worlds: strong expressiveness due to the deep representation and computational efficiency due to the shallow exploration. In theory, a $\sqrt{T}$ regret bound is established. Numerical experiments further demonstrated the importance of decoupling representation learning and uncertainty estimation.","The paper studies the contextual bandit without knowing the specific reward generating function. The authors propose to use a deep neural network to model the reward function where the hidden layers of the deep neural network are used to represent the features and the exploration is only performed in the last layer of the neural network. In this way, the reward generating function can be expressed as the inner product between a deeply represented feature vector and an exploration weight parameter. As the author explained, the main advantage of this reward model is to efficiently reduce computation because we only need to explore the last layer of the deep neural network instead of exploring the whole network like existing neural bandit algorithms.  The authors show that the proposed algorithm for this problem obtains a near-optimal regret which matches the regret bound of linear contextual bandits. They also provided experiments based on real-world datasets to show the performance and computational efficiency. ","The paper studies a contextual bandit problem for which the authors propose a neural network-based policy that takes a raw feature vector as an input without knowledge of the specific reward generating function. The proposed policy transforms the raw feature vector using the last hidden layer of a deep ReLU neural network and uses the upper confidence bound (UCB) approach to explore in the last linear layer which the authors call ""shallow exploration"" instead of using the entire network to explore. The paper provides regret analysis which shows O(\sqrt{T}) regret. The proposed method enjoys computational advantages over existing neural contextual bandit policies.","Authors provide a new regret bound for a recent approach to solving contextual bandits. Previous approaches have introduced deep neural networks to learn context, leading to a paradigm where one must explore over the entire network parameter space, which is inefficient for learning. The authors address this by taking an existing approach that decouples the deep neural network feature representation learning from most of the exploration of the network parameters by only exploring over the final layer of the network. Although this has been done previously in the context of Thompson sampling, there has not been a regret bound given. The authors analyze a UCB version of this approach, then build from techniques from both deep neural contextual bandits and linear contextual bandits to prove an O(\sqrt(T)) regret bound. Finally, they show experimentally that their algorithm is better than neural-only or linear-only contextual bandits.",0.3263157894736842,0.4842105263157895,0.25263157894736843,0.24691358024691357,0.2037037037037037,0.2571428571428571,0.19135802469135801,0.4380952380952381,0.16216216216216217,0.38095238095238093,0.22297297297297297,0.18243243243243243,0.24124513618677043,0.46,0.19753086419753088,0.299625468164794,0.21290322580645163,0.21343873517786557
793,SP:af2f100dea724cfccbb1a5942e689275c11c9103,This is an interesting paper in which the authors introduce a new downsampling method for generating coarse elevation maps. A new framework was also built to incorporate the governing PDE (and PDE solver) in the training process. The authors have shown the effectiveness of this proposed method on a dataset with simulated water height at fine resolution. ," The paper proposes a method for physics-informed downsampling of the terrain map for scalable inundation modelling.   Main paper contributions are stated to be as following:  - new method for modelling physical processes with deep learning. Specifically, authors combine a deep neural network (DNN) with the partial differential equations (PDEs) describing the flow of water. The DNN downsamples the elevation map, and the downsampled elevation map is fed into the PDEs, where each time step is a recurrent unit, to calculate the water height solution) -creating of a dataset for a particular problem ","The paper presents a deep learning-based method for coarse-grain discretization in solving the PDEs of flood modeling. A ResNet is used to downsample the input terrain, and the coarse-grain output is then fed into a numerical solver. A fine-grain numerical solver generates the ground truth for end-to-end training of the downsampling neural network. The work also builds a dataset of elevation maps and their flood simulation results.  The experiment shows that the proposed method surpasses the baseline that uses the average pooling downsampling. ","The paper investigates the problem of downsampling high-resolution elevation maps for more computationally feasible flood modeling simulations. For this purpose, a new pipeline is introduced, that features a downsampling network for the elevation maps, and a differentiable PDE solver for the 2D shallow water equation. The authors also provide their simulated large-scale dataset of flood modeling simulations. Finally, the proposed method is evaluated on 1) an example case that highlights the ability to backpropagate gradients through a large number of simulation steps, 2) an experiment with different boundary conditions, and 3) a generalization experiment to new elevation maps.",0.24561403508771928,0.2982456140350877,0.3508771929824561,0.20652173913043478,0.18478260869565216,0.2696629213483146,0.15217391304347827,0.19101123595505617,0.2,0.21348314606741572,0.17,0.24,0.1879194630872483,0.23287671232876714,0.25477707006369427,0.20994475138121546,0.17708333333333334,0.25396825396825395
794,SP:af823e1f87bd747cb72e6a2ef7f24820a8222752,"This paper proposes SPANN, which uses the inverted index to conduct large-scale nearest neighbor search on SSD. The centroids are kept in memory and indexed by a neighborhood graph for fast access. The lists of vectors associated with the centroids are stored on disk. Optimizations include (1) hierarchical balanced clustering to balance and limit the sizes of the lists; (2) assigning vectors on cluster boundaries to multiple clusters; (3) adjusting the number of lists to check according to each query. Experiment results show that SPANN significantly outperforms DiskANN, a state-of-the-art SSD-based solution, and is effective for distributed search.   ","SPANN is a hybrid disk/RAM vector indexing method.   SPANN uses a hierarchical clustering (with some tricks) to split the dataset into buckets that are stored uncompressed on disk. At search time, the buckets corresponding to the nearest centroids to the query (again with some tricks) are visited to retrieve the nearest neighbors.   The indexing-time tricks are intended to balance the bucket sizes and reduce assignment ambiguity with some level of multiple storage.   At search time, the number of buckets to visit is tuned based on the distance of the query to the centroids.   There is also a section about a distributed version of the method. ",The paper claims the following contributions. (1) It proposes a new disk-based algorithm (called SPANN) for billion-scale approximate nearest neighbor search on a single machine. (2) SPANN adopts two techniques. One is hierarchical balanced clustering for index building and the other is query-aware schema for search. (3) SPANN is two times faster than a state-of-the-art solution (called DiskANN). ,"In this paper, the authors study the approximate nearest neighbor search (ANNS) problem and develop an inverted index based algorithm using both memory and disk in the searching to reduce the required amount of memory. The proposed algorithm first partitions the vectors into clusters and then stores in the memory the centroid of each cluster and the disk address of the vectors belonging to the cluster, or called posting lists. To limit the length of posting lists (the number of vectors in a cluster), the algorithm performs the clustering with an additional constraint of balancing the length of each posting list, as suggested in [27]. But the algorithm performs the clustering in a hierarchical way so that the computational cost can be improved. It further replaces a centroid by the closest vector and another index [10] to speed up the search. It also assigns boundary vectors, those close to multiple clusters, to the multi-clusters to reduce the number of posting lists accessed. An RNG rule [37] that assigns a vector to clusters at different directions is employed to avoid generation of similar clusters. Finally, during the query, only posting lists that are sufficiently close to the query compared to the closest posting list will be searched. Experimental evaluations have been conducted on two datasets of one billion to demonstrate the superior performance.",0.23300970873786409,0.21359223300970873,0.3300970873786408,0.1308411214953271,0.2616822429906542,0.234375,0.22429906542056074,0.34375,0.15246636771300448,0.21875,0.12556053811659193,0.06726457399103139,0.22857142857142856,0.26347305389221554,0.2085889570552147,0.1637426900584795,0.1696969696969697,0.10452961672473869
795,SP:afe9fdf31524e29d7695fa05ddf374ca8c324a3c,"The paper introduced a variant of PCA in the presence of noise, i.e., a variant of robust PCA. The ""noise"" in this case is permutations of the column entries of the data matrix. Using concept from Algebraic geometry, the authors showed that they can get the solution upto permutation of the rows. Experimental results have shown that the proposed method can recover the data matrix under such corruption.","The paper aims to recover a low-rank matrix in which the entries of the columns are permuted. The paper first provided theoretical analysis to show the feasibility of the recovery problem and then proposed a two-step method with a modified unlabeled sensing algorithm. The numerical results on synthetic data, face images, and other two datasets showed that the proposed two-step method is quite effective and the modified unlabeled sensing algorithm can outperform existing algorithms in most cases.","- The authors describe and tackle the problem of unlabelled principal component analysis, a variant of PCA where the columns of the data matrix might have been permuted arbitrarily with different permutations for each column. The problem shares some of its motivation with the easier problem of unlabelled sensing, where the principal subspace is known. - The authors divide the problem of UPCA into two natural steps: i) Identifying a rank-r subspace S* where all data points lie if permitted correctly, and; ii) mappings the permuted data points onto the identified subspace. These two steps can be naturally addressed by existing lines of work: robust PCA for former and unlabelled sensing for the latter. - The authors use a algebraic framework to formulate and solve the problem. The main theoretical result is a powerful one: under genericity of the true solution, then any solution of the rank minimization formulation of the problem recovers the ground truth up to a permutation. - A combinatorial solution is prohibitively expensive. One could consider a polynomial system solver, but the authors correctly point out that scalability and robustness to noise would pose separate challenges and leave that to future work.  - Instead, the authors focus on specific subclasses of the UPCA problem where only a fraction of the entries are permuted. Under this assumption it is indeed possible to have efficient solutions of the problem, and the proposed two-stage workflow seems well suited. - Finally, the authors provide extensive experiments on their methodology on synthetic and real datasets. ","This paper introduces unlabeled principal component analysis (UPCA), which refers to the reconstruction of the ground-truth data matrix X^* from its unlabeled version \tilde{X}, whose each column is generated from a permutation of the coordinates of the corresponding column of X^*. The authors first establish that UPCA is well-posed by showing that if X^* is sufficiently ""generic"", then the solution to the UPCA presented in Eq. (2) applied to \tilde{X} is X^* (up to a global permutation matrix). The authors then establish that X^* (up to a global permutation matrix) is the unique solution of a polynomial system of equations parametrized by \tilde{X}. Based on these theoretical results, the authors propose a two-stage algorithmic pipeline for UPCA for a practically relevant case where only a fraction of the data has been permuted. The first stage uses robust PCA methods to estimate \hat{S} of the ground-truth column space S^* from \tilde{X}. The second stage uses unlabeled sensing methods to estimate \hat{X} of the ground-truth data matrix X^* from \hat{S} and \tilde{X}. The authors also introduce a novel algorithm for unlabeled sensing based on least-squares with recursive filtration (LSRF).",0.2608695652173913,0.4057971014492754,0.3333333333333333,0.275,0.2875,0.204,0.225,0.112,0.11442786069651742,0.088,0.11442786069651742,0.2537313432835821,0.24161073825503354,0.17554858934169282,0.1703703703703704,0.13333333333333333,0.16370106761565834,0.22616407982261638
796,SP:aff55e093254dea3d98c56ba58ac7f8ec721b9e8,"This paper proposes a product of EBM and FLOW and samples in latent space to improve mixing. Such a model can be learned using both MLE or NCE. Empirically, the paper shows that a single MCMC chain of such a model can traverse through different modes. ","I find that this paper is a re-submission from NeurIPS2020, for which I acted as one of the reviewers. The content almost remains the same.  This paper studies the learning of a special class of EBMs, which is a correction or an exponential tilting of a flow-based model. An interesting observation that the resulting EBM in the latent space is of a simple form that is much more friendly to MCMC mixing. It is said that HMC sampling of the EBM in the latent space, which is a simple special case of neural transport HMC, mixes well and traverses modes in the data space. Regarding this main claim, the authors raise a number of scientific questions, which are validated by empirical evaluations.","Inspired by NueTra [1], in this paper the authors propose a new approach to train deep energy-based models (EBMs). The motivation here is that EBMs are typically trained via contrastive divergence which requires MCMC sampling in a high-dimensional space and from  a mutli-modal distribution. As a result, most MCMC sampling methods do not mix well which results in a biased estimation of MLE. To address this problem, the authors propose to train an energy-based model with a (pre-trained) backbone flow; the EBM can be interpreted as a correction step to the flow model. The flow $q_\alpha(z)$ defines a latent space $z$ with a base distribution $q_0(z)$. As pointed out in [1], we can use the flow network to additionally yield the following distribution on $z$-space: $p(z) = p(x)\frac{\delta x}{\delta z}$. Similar to NeuTra [1], the authors propose to run HMC to sample from $p(z)$ which geometrically is a simpler target, and then feed $z$ to the flow to get $x$ again. The authors perform some experiments on both toy and some of the common vision datasets to show that unlike SGLD, this approach mixes way better and is able to traverse to different local models.   ","The authors propose to learn energy-based models with a flow-based model as ""backbone"". This is to utilize Neural Transport and ultimately make the Markov Chains mix well in data space. This is an important problem.",0.30434782608695654,0.32608695652173914,0.10869565217391304,0.23387096774193547,0.11290322580645161,0.08571428571428572,0.11290322580645161,0.07142857142857142,0.13513513513513514,0.1380952380952381,0.3783783783783784,0.4864864864864865,0.16470588235294115,0.1171875,0.12048192771084337,0.17365269461077845,0.17391304347826086,0.145748987854251
797,SP:b01c336b2949622f30a08fa997f48c9f94f1d0f5,"This paper proposes a new information-theoretic exploration approach, called OVD-Explorer, for deep reinforcement learning with continuous action space. The OVD-Explorer is capable of exploring epistemic uncertainty while avoiding aleatoric uncertainty. The paper also contains extensive experiments to evaluate the performance of OVD-Explorer.","This paper introduces OVD-Explorer, an efficient exploration algorithm that is able to detect and avoid areas with high aleatoric uncertainty. Actually, the proposed algorithm explores state-action pairs that have not been visited frequently (high epistemic uncertainty) and at the same time avoids areas with high aleatoric uncertainty. In order to achieve this behavior, OVD-Explorer maximises the mutual information between policy and corresponding upper bounds. Theoretical results show that the over-exploration issue can be tackled through the maximisation of such mutual information. Empirical analysis has been conducted on a toy task and at five MuJoCo environments including their stochastic variants. ","The paper is concerned with the problem of over-exploration in RL. The authors propose to capture the aleatoric uncertainty during exploration and propose a new exploration method called Optimistic Value Distribution Explorer (OVD-Explorer). OVD-Explorer is designed to explore optimistically while avoiding the areas with high aleatoric uncertainty. The authors propose a new measure for policy's exploration ability which aims at maximizing the mutual information between policy and policy return upper bounds. OVD-Explorer achieves good empirical performance, outperforming other methods.",The paper proposes the so-called Optimistic Value Distribution Explorer (OVD-Explorer) to deal with over-exploration issue in reinforcement learning (RL). It is claimed that the proposed algorithm is tractable for continuous action space and can avoid exploring  the areas with high aleatoric uncertainty. The paper tests the performances of OVD-Explorer and some of its competitors on some common environments.,0.2826086956521739,0.391304347826087,0.41304347826086957,0.20388349514563106,0.21359223300970873,0.25,0.1262135922330097,0.21428571428571427,0.3064516129032258,0.25,0.3548387096774194,0.3387096774193548,0.174496644295302,0.27692307692307694,0.35185185185185186,0.2245989304812834,0.26666666666666666,0.28767123287671237
798,SP:b0dcaa82600fc7db2ba9012dff7df695584a1fd8,"This paper focuses on adaptive label smoothing that could reflect the change in probability distribution mapped by a model over the course of training. To deal with this issue, this paper proposes a label soothing scheme that brings dynamic nature into the smoothing parameter and the prior label distribution from the distilled knowledge. Specifically, the smoothing parameter is computed with the entropic level of model probability distribution per sample on the fly during the forward propagation in training.  Besides, the prior label distribution is selected from the self-knowledge distillation. Experiments on various datasets demonstrate that the proposed adaptive label smoothing achieves state-of-the-art performance.","The paper proposes a simple yet effective way of smoothing the labels for each data point. The smoothing parameter alpha is dynamically adapted for each data point based on the (normalized) entropy of the predicted label-distribution for this data point. The distribution that is used for label-smoothing originates from the same but at earlier epoch of the training, i.e., a teacher-student learning framework like in knowledge distillation is employed, where the teacher is a model learned in an earlier epoch. From among these earlier model-candidates, the one is chosen that obtains the best evaluation metric g (Eq. 5). Function g does not have to be the training loss, but can be the (possibly different) evaluation metric. The validity of this approach is theoretically supported by a gradient analysis, and experimentally corroborated by improved evaluation metrics, improved calibration, and an ablation study. ",This paper proposes a new method that is based on label smoothing. The original label smoothing uses the same level of smoothness among samples and among time steps.  This paper tries to extend this to a dynamic nature with different smoothness between samples and also throughout training. The main idea is to use a normalized version of the entropic level of model prediction for a certain input data point for the smoothness parameter. This is combined with self-knowledge which is used as the distribution of label smoothing.,"Label smoothing is a popular approach to regularize modern neural networks. However, the amount of smoothing is the same across all samples in the dataset, which can be sub-optimal. In this paper, the authors of the paper proposed a novel adaptive label smoothing method so that each sample gets a different amount of smoothing. The key insight of the paper involves using the entropy of predictions from past timestamps as a way to quantify the amount of label smoothing applied to samples. The authors of the paper demonstrate empirically the effectiveness of the proposed method.  ",0.21495327102803738,0.2616822429906542,0.205607476635514,0.1506849315068493,0.1506849315068493,0.22727272727272727,0.15753424657534246,0.3181818181818182,0.22916666666666666,0.25,0.22916666666666666,0.20833333333333334,0.18181818181818182,0.28717948717948716,0.21674876847290642,0.18803418803418803,0.1818181818181818,0.21739130434782608
799,SP:b10ca107b60a35f68a63fca3105c9c6b30eea094,"The paper proposes a pre-trained technique based on correcting tokens and contrasting sequences of text. The correcting objective is based on ELECTRA with a modification in the formulation of multi-task setup where the one task is to identify the corrupted tokens and the other task is to correct it. The contrasting objective aims to maximize the similarity between similar sentences while pushing dissimilar sentences away in cosine space. With the two modifications, COCO-LM is trained much faster than ELECTRA. The paper also presents a comprehensive evaluation on GLUE and SQUAD 2.0 benchmarks and ablation study of their method.",Authors propose a pretrained language model that extends Electra. They operate within the generator/discriminator proposed by Electra. The key contributions in the paper are a couple of novel loss functions that allow the discriminator to predict the tokens rather than binary labels and a contrastive loss for the sequence embedding. With a detailed analysis they show that the resulting token level and sentence embedding are much better at discriminating random input. They also demonstrate that this improves the quality on the GLUE and SQUAD datasets. They further demonstrate that their approach results in faster training and improved parameter efficiency.,"This paper tackles two challenges in current self-supervised learning frameworks, including the pre-training efficiency and the anisotropy of text representations. To address these two challenges, the authors propose the COCO-LM framework that pre-trains language models by correcting and contrasting corrupted text sequences. COCO-LM follows ELECTRA-style pre-training such that an auxiliary language model is employed to corrupt text sequences, upon which two new tasks are constructed to pre-train the main model. The first task is Corrective Language Modeling (CLM) to detect and correct tokens replaced by the auxiliary model, in order to better capture token-level semantics. The second task is Sequence Contrastive Learning (SCL) to align text sequences originated from the same source input while ensuring uniformity in the representation space. Experimental results on GLEU and SQuAD demonstrate that COCO-LM can improve the performance of pre-trained models and also the efficiency of pre-training.","This paper introduces COCO-LM, a pretraining method. The pretraining has two components: *COrrecting*, and *COntrasting*. The COrrecting objective shares most of the efficiency advantages of ELECTRA, while still allowing some language modelling ability. The COntrasting objective ensures that sentences do not all have high cosine similarity, something that occurs in ELECTRA.   * The pretraining method achieves higher accuracy more efficiently than counterparts across various tasks (GLUE, SQuAD 2.0) and settings (base, base with more data and large). * There is a comprehensive ablation study and analysis section that supports the design choices",0.20588235294117646,0.2647058823529412,0.23529411764705882,0.21,0.14,0.11038961038961038,0.21,0.17532467532467533,0.2608695652173913,0.13636363636363635,0.15217391304347827,0.18478260869565216,0.20792079207920788,0.2109375,0.24742268041237112,0.16535433070866143,0.14583333333333334,0.13821138211382114
800,SP:b11f233444b96303f509d617c5fdc953d992089c,This paper proposes a variational multi-task learning method that uses Gumbel-Softmax priors for both the weight (w) of each task and the latent representation (z) of each data point. The Gumbel-Softmax trick is used for learning the relatedness between tasks. Amortized inference is also used for faster inference of the posteriors of the weight vectors and also for the latent representations. Experiments on different benchmark datasets show improvement over baselines especially in the low-data regime.,"In this work, the authors propose to use Gumbel-Softmax as the prior of a task for multi-task learning under the variational Bayesian inference setup. Instead of exploiting task relatedness for representations only, the proposed method jointly infers posteriors for both representations and classifiers. State-of-the-art results are achieved in the multi-input multi-output setting with a small training data regime. ","This paper proposes a unified few-shot multi-task learning framework which learns latent representations of data points and latent classifier weights through an amortized variational inference. Specifically, the proposed method regularizes the inference networks by minimizing KL divergence between the target variational distribution and the mixture of the non-target variational distribution. The mixing probability of the non-target variational distribution is sampled through Gumbel-Softmax distribution with learnable parameters. The intuition behind this regularization is that a model is able to learn from related tasks to improve the inference of latent representations and classifier weights (especially useful for the few-shot multi-task learning setting). The extensive experiments demonstrates that the proposed method is more effective than other methods for the few-shot multi-task learning.","This work proposes a probabilistic method (variational inference) to exploit task relatedness and ignore harmful interference in multi-task learning paradigms. In particular, the authors focus on a unique MTL setting (training data is extremely limited; tasks have different input spaces; tasks have the same target space) to highlight the utility of their method. Experimental results on 5 datasets and compared to 6 related work is highly compelling to establishing the efficacy of the proposed method.",0.24050632911392406,0.3037974683544304,0.20253164556962025,0.26153846153846155,0.2,0.1484375,0.2923076923076923,0.1875,0.21052631578947367,0.1328125,0.17105263157894737,0.25,0.2638888888888889,0.23188405797101447,0.2064516129032258,0.17616580310880828,0.18439716312056736,0.18627450980392157
801,SP:b129e8c1d8878e074b1e8cf081ecae301447f611,The paper proposed a novel idea of formalizing the generality-forgetting trade-off in continual learning via a two-player sequential game. Theoretical and empirical results are represented to support the authors' claims. The main contribution of the paper is to formally discuss the forgetting and generalization trade-off theoretically.,"This paper considers the stability-plasticity dilemma in continual learning. The authors develop an algorithm which addresses catastrophic forgetting while trying to improve generalisation on new tasks.  The paper introduces a new cost function for continual learning, which takes into account the model's performance on previous, current and future tasks. An approximation, H, to this cost function, which involves only the data that has been encountered so far, is optimised. The optimisation is phrased as a 2-player game, where player 1 tries to maximise H by creating worst-case dissimilar inputs for future tasks. Player 2 minimises the H by optimising the model's parameters. The authors show that the two players would eventially reach a single solution (an equilibrium). Catastrophic forgetting is prevented by maintaining a subset of previously encountered datasets and using it when optimising H.  Contributions: - A novel formulation of the cost function of continual learning (CL) - A theoretically justified approach to CL. - An implementation of their approach - Experiments which show that the resulting method outperforms the many baselines. ","The paper theoretically investigates the trade-off between generalization and forgetting in continual learning (CL). The authors prove the existance of a saddle point in the generalization/forgetting space; they show that it is stable under certain conditions; and propose a practical algorithm called balanced continual learning (BCL) for finding it. Finally, the authors compare their approach experimentally against the state of the art in CL.","The paper attempts to present a theoretical framework for modelling the trade-off between generalization (adaptation to future tasks, plasticity) and forgetting (stability on previous tasks). Towards this, the paper proposes an objective function that minimizes the weighted loss on all the past (implemented using episodic memory), present (implemented using the dataset of the current task) and future tasks (approximated using preturbing the datasets of the past and current tasks). The perturbation to this loss is then approximated in the local neighbourhood of the current task. The optimization of the loss is posed as a minimax game between the two players, where player 1 maximizes the generalization by maximizing the discrepancy between the successive tasks, and player 2 minimizes the forgetting by adapting the model parameters against that discrepancy. The authors show that if player 1 starts first, the equilibrium solution of this game exists in the local neighbourhood that is defined using the current task. The proposed approach achieves strong results against several baselines on MNIST and CIFAR-100 variants used for continual learning.  ",0.32,0.3,0.38,0.1206896551724138,0.27011494252873564,0.3484848484848485,0.09195402298850575,0.22727272727272727,0.10857142857142857,0.3181818181818182,0.26857142857142857,0.13142857142857142,0.14285714285714285,0.25862068965517243,0.1688888888888889,0.175,0.2693409742120344,0.1908713692946058
802,SP:b17565be9a2ed41bdc188bb8085eac2a56390c22,"The authors propose a new approach to the class of scalable, multimodal VAEs. They propose to use a surrogate joint posterior which, using an additional sum of KL-divergence terms, helps keeping the posterior approximation of subsets of modalities similar to the joint posterior approximation. The proposed method is evaluated on two different datasets, MNIST-SVHN-Text and bimodal CelebA where the proposed method shows good results.","This paper addresses the problem of formulating a well-justified ELBO for multi-modal VAEs.  The paper gives a thorough recounting of previous attempts at combining and training the per-mode posterior (approximations).  Previous workarounds involve, for instance, subsampling subsets.  This paper proposes approximating the joint posterior with a variational model and then adding the KLD to this variational posterior as an extra term on the ELBO, analogous with but an approximation to the KLD between the approximate and true posteriors that controls the gap in the ELBO.  The experiments report classification performance on MNIST + SVHN + Text and bi-modal CelebA.","SMVAE aims to solve the challenge of combining multiple modalities into a coherent latent space using VAE framework. SMVAE is based on MoPoE-VAE model, the main architectural change is replacing the MoE rule with minimising the KL-divergence between the latent space distribution generated by the PoE combined experts and the joint encoder.    (The reviewer is sorry for the slight delay for submission as he is still on vacation and marked november third as deadline for submission. In the interest of time,the following is just a short summary of my notes)","In this paper, the authors propose a model for learning joint representations for multimodal data that allows inference when a few of the modalities are missing. Previous approaches addressed the missing modality problem by treating the inference network for each modality as an expert and combining the information across these experts. The proposed model claims to be useful in the following scenarios: 1) When cross-generation is difficult (and hence, MoPoE-VAE fails). 2) When the number of modalities is large.  The authors achieve this by learning a surrogate posterior conditioned on all the modalities while simultaneously optimizing the KL-Divergence between the surrogate posterior and a subset-conditioned posterior. The surrogate posterior is learnt by optimizing a variational lower bound to the log-likelihood objective. The authors evaluate the representations learnt by the model on secondary tasks and report improved performance. The generated modalities also perform well on secondary tasks. ",0.2835820895522388,0.22388059701492538,0.29850746268656714,0.16831683168316833,0.27722772277227725,0.23655913978494625,0.18811881188118812,0.16129032258064516,0.13245033112582782,0.1827956989247312,0.18543046357615894,0.1456953642384106,0.22619047619047616,0.1875,0.1834862385321101,0.1752577319587629,0.22222222222222224,0.18032786885245905
803,SP:b1f09dd0de4eb69803991b9f7856ef46e3fa6816,"This paper studies the generalization error of a class of interpolator in linear regression and identifies conditions for optimality. They consider a setting where there is a true linear model generating the data, whose parameters are drawn from a Gaussian distribution with zero-mean and covariance Phi. In this setting they consider a natural class of estimators that are a function of the data, the population covariance matrix, the prior covariance matrix Phi and the signal to noise ratio. Among this class of estimators they calculate a closed-form expression for the optimal interpolator, with respect to the expected excess risk and show that there are certain settings where the expected excess risk of this estimator can be much lower than the risk of the OLS solution. They also provide an extension of their results to the random features model.","The paper under reviews studies the problem of optimal linear interpolation. More precisely the authors derive an explicit formula for the *best* linear interpolator that can be written as a linear function of the outputs and as a function of desired quantities. From this, they show how to recover theoretically this estimator from a optimization algorithm. Finally they expose cases where the designed estimator show some good generalization properties while the miminum norm interpolator does not.  The link with kernel methods is also made.","This paper is concerned with inducing a certain type of best possible interpolator in linear regression. It is based on the notion of best possible interpolator, defined in Muthukumar et al., 2019., which is the best estimator (i.e., with minimum risk) among the class of interpolators. Based on that, the authors define a new theoretical estimator, the best estimator among interpolators that are also linear in the response variable.  They manage to induce a closed form of this estimator and prove that it is the limit of preconditional gradient descent under a specific initialization. In addition, using approximations for quantities that are usually unknown in practice (e.g.  population covariance matrix, signal to noise ratio), they turn this theoretical estimator into a fully empirical one and compare its performance to other frequently used estimators, showing that this estimator performs arbitrarily better in certain cases. Finally, they extend this notion of best possible theoretical estimator to the random features setting, where they again induce a closed form for this estimator and prove that it is the limit of preconditioned gradient descent under a specific initialization. ","This paper considers the problem of optimal interpolation in linear regression. It defines the class of response-linear achievable interpolators, and provides a closed-form expression for the optimal interpolator in this class. The paper is about an important topic in machine learning and statistics. However, it is unclear to me whether the ""achievable"" interpolators are really achievable, and as a result if the investigation is meaningful.",0.15714285714285714,0.2642857142857143,0.17142857142857143,0.2619047619047619,0.19047619047619047,0.12432432432432433,0.2619047619047619,0.2,0.3582089552238806,0.11891891891891893,0.23880597014925373,0.34328358208955223,0.19642857142857142,0.2276923076923077,0.23188405797101447,0.16356877323420074,0.2119205298013245,0.18253968253968256
804,SP:b24d755d341dd210a4c3406d756a18e61923b96c,This work proposes an RNN with a gating mechanism that encourages its latent state to have sparse updates by incorporating stochastic gating units trained using an approximation of L0 regularization of the state updates. The authors propose to use this RNN in partially observable problems in which the latent dynamics are known to change slowly and long-term memory is important. The authors show in a number of partially observed tasks that their proposed RNN and training objective outpeforms well known RNN architectures when predicting the environment dynamics.,"The manuscript presents a novel recurrent neural network architecture with an inductive bias for learning sparsely changing hidden state representations. This seems to be achieved by the proposed gating scheme using the novel rectified tanh activation function for the update gate values. This activation function yields true zero activations producing an exact copy of the corresponding value from the previous hidden state. By applying L0 regularization to the state-updates, the network is incentivized to minimize the number of modified dimensions in the state.","This paper proposes a new recurrent neural network architecture (RNN) called GateL0RD, designed to learn sparsely changing latent variables of sequential data. The authors hypothesize that latent variables should be updated sparingly only if significant changes happen in the considered environment. To enforce this hypothesis in RNN, GateL0RD uses a new gating function of Rectified Tanh to output exactly zero values. For actual implementation, the considered prediction task is regularized to minimize the number of positive elements in the sampled gate inputs. Ablation study shows that GateL0RD improved prediction accuracy and planning performance compared with LSTM or GRU in several POMDP environments.","This paper presents a sequence prediction method that incorporates a particular inductive bias -- infrequency of latent state updates -- as measured by an L_0 term on latent state differences. This term is approximated and made more easily optimizable using a method similar to [16], which examined L_0 regularization in non-sequential domains. The paper conducts experiments on several low-dimensional POMDP tasks and then draws the following conclusions: the method generalizes better to out-of-distribution inputs in several partially observable domains than other commonly used RNNs, exhibits long term memory in a domain, and exhibits explainability (in its latent states), sometimes at the cost of reduced prediction performance.",0.2159090909090909,0.19318181818181818,0.25,0.2619047619047619,0.17857142857142858,0.14705882352941177,0.2261904761904762,0.16666666666666666,0.2,0.21568627450980393,0.13636363636363635,0.13636363636363635,0.22093023255813954,0.17894736842105263,0.22222222222222224,0.23655913978494625,0.15463917525773194,0.14150943396226415
805,SP:b252424b14d73328379f794c0150789a3435cc64,"The authors propose a new Multimodal Bottleneck Transformer (MBT) for audio-visual learning. Taking image and spectrogram patches as inputs, MBT uses self and cross-attention operations to model both unimodal and cross-modal information at multiple layers in the network. Notably, it adopts a small set of ‘bottleneck’ latent units to force the model to collate and condense the most relevant information in each modality and only share what is necessary. Extensive ablation studies and experiments are conducted on multiple audio-visual classification benchmarks including Audioset, Epic-Kitchens, and VGGSound.  ***Post-Rebuttal***  I've read comments from fellow reviewers and the rebuttal from the authors.  Thank the authors for considering my comments and responding to my questions. The rebuttal successfully addressed my major concerns. Thus, I would like to keep my initial positive rating.   The authors should add the missing details, discussion, and new results in the rebuttal to the paper. ","For efficient multimodal fusion, the authors proposed the attention bottlenecks for audio-visual classification tasks. Compared to previous works, they emphasize their bottleneck architecture ""to collate and condense the most relevant information in each modality."" They showed that it also has the advantage of a lower computational cost. They evaluate the proposed method using three competitive methods, vanilla self-attention, vanilla cross-attention, and their attention bottlenecks. They noted that the position of modality fusion was a critical factor among early, mid and late. ","The authors propose a new transformer architecture MBT. The MBT uses additional learnable tokens to summarize multimodal information between different modalities and an attention modification to process the constructed multimodal sequences efficiently. They perform ablations on fusion strategies and comparison with related models on audio-visual datasets (such as AudioSet, Epic Kitchen 100, VGGSound). They show improved results on most benchmarks. ","This paper conducts an experimental study of different fusion methods for multimodal audiovisual transformers, and proposes a new fusion mechanism. They mix ordinary transformer layers with new fusion attention layers. These new layers perform cross-attention, in which one modality is used and the other is used as the key/values. This takes places using special fusion tokens, which have a function similar to the classification tokens in ordinary transformers. They find that (1) mid-fusion can obtain better results than early/late fusion in both their model and ""vanilla"" fusion, (2) attention bottlenecks improve performance, (3) they outperform the very recent, state-of-the-art Perceiver model on AudioSet classification. They also conduct additional experiments on VGG-Sound and Epic Kitchens, in which they compare with other state-of-art models. The supplement contains several other datasets.",0.1513157894736842,0.13815789473684212,0.125,0.11904761904761904,0.20238095238095238,0.2459016393442623,0.27380952380952384,0.3442622950819672,0.13768115942028986,0.16393442622950818,0.12318840579710146,0.10869565217391304,0.19491525423728814,0.1971830985915493,0.1310344827586207,0.13793103448275862,0.15315315315315317,0.15075376884422112
806,SP:b3badb3a0ad6a4ac365e17dc28a81a4027240add,"To determine the importance of filters in DNNs, this paper proposes a new concept, channel independence, which measures the correlation of multiple feature maps. Also, this paper systematically investigates and analyzes the quantification metric, measuring scheme, and sensitiveness & reliability of channel independence in the context of filter pruning. As a result, the proposed scheme has achieved outstanding performance compared to existing methods. Although this paper is well-written and presents a solution for network pruning that has steadily gained a lot of attention, there are some minor comments as a result of my review of this paper. (Please refer to my main review)","The authors propose to explore and leverage the cross-channel feature information for filter pruning. Specifically, the authors propose a cross-channel correlation-based metric to measure the importance of filters. In this case, the low-independence feature maps are considered redundant. As a result, the corresponding filters that output the low-independence feature maps are viewed as uninformative and can be safely removed. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of the proposed method. However, the experiments are not sufficient. Please see my detailed comments below.","This paper presents a metric to evaluate pre-trained dense networks to identify the feature maps that need to be dropped in an optimal manner, such that the performance of the pruned network is maximized. Authors present a simple, yet effective approach to identify the feature maps that need to be removed from the network such that it does not compromise the performance. Authors demonstrate the improvement on ImageNet dataset, which clearly demonstrates the efficacy of the proposed metric.",This paper proposes a channel pruning method named CHIP to improve the existing channel pruning frameworks. The proposed method measures the importance of channels through channel independence. Extensive experiments show the effectiveness of trunk pruning.,0.1941747572815534,0.14563106796116504,0.1262135922330097,0.21348314606741572,0.14606741573033707,0.11392405063291139,0.2247191011235955,0.189873417721519,0.37142857142857144,0.24050632911392406,0.37142857142857144,0.2571428571428571,0.20833333333333331,0.16483516483516483,0.18840579710144928,0.22619047619047616,0.2096774193548387,0.15789473684210525
807,SP:b3c78951cdeab8f71677629f323bb5b994255123,"This paper proposes DNN quantization with Attention (DQA), which uses a learnable linear combination of high, medium, and low-bit quantization at the beginning. It gradually converges to a single low-bit quantization at the end of training. Experiments show that DQA outperforms the naive quantization and the Binary-Relax method consistently across three datasets and two networks.","The papers addresses the problem of compression of neural networks. The paper builds upon binary-relax prior work. The precision is adapted during training with a mixture-based quantization method through temperature cooling and a set of an ``attention’’ vector a. The idea of the method, called DQA, is to progressively moves from a mixture of quantization functions (mixing high with low precision, for instance 32 bit with 2 bit training), to a single one (low precision) towards the end of the training. The paper states that the method can be used with several types of quantization methods. The evaluation is carried on computer vision architectures (ResNet18, MobileNet) for image recognition tasks (Cifar-10, Cifar-100, Imagenet ILSVRC 2012).  ","This work presents a training method for low-bit network quantization. While training, it employs a multi-bitwidth paradigm in order to alleviate the nonsmooth optimization landscape with lower bitwidth. It uses a temperature parameter and a penalty term to force the network to gradually converge to the target low bit. Experiments are conducted on CIFAR10, CIFAR100, ImageNet Classification with ResNet 18 and MobilenetV2.","This paper attempt to address a challenging quantization problem, i.e., low-bit quantization. This work utilizes a learnable linear combination of high, medium, and low-bit quantization at the beginning while converging to a single low-bit quantization at the end of the training. In the quantization procedure, multiple quantizers and the corresponding attention matrices are adopted to fuse the quantized weights or activations.",0.3620689655172414,0.22413793103448276,0.5517241379310345,0.1092436974789916,0.16806722689075632,0.21875,0.17647058823529413,0.203125,0.49230769230769234,0.203125,0.3076923076923077,0.2153846153846154,0.23728813559322037,0.21311475409836064,0.5203252032520326,0.14207650273224046,0.21739130434782608,0.21705426356589147
808,SP:b3fabee204f6f0761b849f93dbf9e071f6256382,"The paper introduces a variant of dropout called R-drop. First, the data goes through the forward pass once, and we obtain the output distribution. Next, the data goes through the forward pass (with the same dropout rate but different dropout neurons) another time, and we obtain the output distribution. The goal is to minimize the loss, which is the standard negative log-likelihood (adding two NLL losses from two different passes together) together with KL divergence (KL(p1 || p2) + KL(p2 || p1)). There is large performance gain on machine translation, GLUE tasks, as well as language modeling. ",The paper proposes the dropout-based regularization method R-Drop. The proposed R-Drop further regularizes the output predictions of sub-models from dropout.  The theoretical analysis was performed to show that the proposed method reduces the freedom of model parameters. The empirical results on multiple datasets were performed to show that the proposed method improves the performance (accuracy) for four NLP tasks and one CV task. ,This paper proposes a simple yet effective regularization method for deep learning models. The model performs two forward passes with different dropout masks for the same input and minimizes the difference between the two output distributions by the bidirectional KL divergence. The proposed method has been sufficiently validated on both CV and NLP benchmarks. ,"This paper proposed to forward an input to neural networks twice with two different dropout masks, and apply a consistent loss as regularization, named R-Drop, between the two outputs to penalize the discrepancy. KL divergence of the two output probabilities is used as the consistent loss. Experiments on different NLP tasks demonstrate the effectiveness of R-Drop.",0.17346938775510204,0.14285714285714285,0.14285714285714285,0.208955223880597,0.19402985074626866,0.3148148148148148,0.2537313432835821,0.25925925925925924,0.2413793103448276,0.25925925925925924,0.22413793103448276,0.29310344827586204,0.20606060606060606,0.18421052631578946,0.1794871794871795,0.23140495867768596,0.20800000000000002,0.30357142857142855
809,SP:b3fee1a115ab61623d575dfe93992fd635312f0c,"In this paper, the authors studied a particular metric for M-class multipartite ranking, with the particular motivation of FPR-sensitive applications in mind. Given that the combinatorial objective is hard to solve exactly, a series of relaxations are introduced / assumptions brought in for the algorithm to be amenable to mini-batch gradient updates. The high-level approach taken is to turn the problem into M-1 bipartite ranking sub-problems, after which a probability-based differentiable surrogate loss function are used and a cache module are leveraged to efficiently approximate the ranking of examples. Numerical experiments are also conducted on a series of real-world datasets.","In many practical applications such as medical diagnosis, the machine learning models are often required to has a low FPR. Motivated by this, the paper proposes a novel framework to optimize the true positive rate at a fixed false positive rate (TPR@FPR) for multipartite ranking. In their framework, the author proposes a surrogate optimization problem and a rank estimation technique to directly optimize TPR@FPR.","The authors propose to optimize an interesting metric named TPR@FPR in multipartite ranking. This work approximates the original constraint on FPR to a probability-based loss function and speeds up the ranking estimation with embedding caches. With the above techniques, it is available to optimize TPR@FPR for deep models in an end-to-end manner.","In this paper, the authors aim to optimize the True Positive Rate (TPR) at a fixed False Positive Rate (FPR) for multipartite ranking, denoted as TPR@FPR. They considered a new evaluation metric TPR@FPR for multipartite ranking in FPR sensitive scenarios. This metric focuses on model performance with a low FPR, which is consistent with practical requirements. To optimize this metric, they propose cross-bnatch approximation for multipartite ranking. ",0.14018691588785046,0.14953271028037382,0.14953271028037382,0.22727272727272727,0.36363636363636365,0.2807017543859649,0.22727272727272727,0.2807017543859649,0.22857142857142856,0.2631578947368421,0.34285714285714286,0.22857142857142856,0.17341040462427743,0.19512195121951217,0.18079096045197737,0.24390243902439024,0.3529411764705882,0.25196850393700787
810,SP:b4253f6bb60f924e16994613d646207791a0a755,"This article proposes a new architecture to solve node and graph classification tasks. The main idea is that an initial transformation is used to map nodes to K different communities, after which community-independent node features are computed and used together to classify the nodes/graph. The training process undergoes an initial unsupervised pre-training stage, followed by a supervised finetuning stage that, for some reason not well understood, alternates the optimization of two supervised objectives (one of which is included in the other), by fixing different parts of the end-to-end architecture at each step. ","This paper proposed a edge partition based graph neural network model. It samples a K sets of edges based on learnable node affinity matrixes. A composer layer is used to combine node representations from each partitions. To improve training stability, the paper trains the edge partition part first with unsupervised then finetune it on supervised learning task. Built upon mature techniques, the authors suggest this model has superior performance with experiments on both node classification and graph  classification data.","This paper develops Edge-Partition Modulated Graph Convolutional Networks, a GNN architecture that explicitly models latent relations between nodes in a graph. The key idea is that nodes may be organized as overlapping ""communities"", where each community represents a particular type of relation. For example, in a citation network of papers, where the task is to predict the category of each paper, all papers from a research group could form one latent community. If one could infer this latent community information, then this could help predict unknown labels.  To accomplish this task, the model learns a community affiliation matrix whose entries express how strongly each node is affiliated to a particular community. Once we have this matrix, the adjacency matrix is partitioned as a sum of K adjacency matrices, where K is the number of communities. The model then runs K different GCN models using the same node features but K different adjacency matrices, and the outputs of all these GCN models are concatenated. Finally, we apply a GCN model using these composite node features and the __original__ adjacency matrix, to get a final classification for each node. These GCN models together comprise the ""generative network"".  The different weights are learned using an intricate training procedure. First, the model learns a good initial community affiliation matrix, by maximizing the posterior probability of the community matrix given the adjacency matrix. This stage is entirely unsupervised, because it uses only the observed edges and not the edge labels.  In the next stage, the model iteratively runs inference (where the community affiliation matrix is learned by optimizing the ELBO using label supervision) and learning (where the community matrix is held fixed but the generative GCN models are trained). Inference and learning are repeated till convergence.  Finally, a community matrix is sampled from the learned posterior and this is used along with the generative network to predict unknown labels.  The authors evaluate their model on a variety of node classification and graph classification benchmarks. They also visualize some embeddings for better interpretability, and perform some ablation studies. ",This paper investigates how an edge is formed by different latent inter-node relations and extends the community-based edge formulation mechanism to graph neural networks. A variational inference framework has been proposed to jointly learn how to partition the edges into different communities and combine relation-specific GCNs for the end classification tasks. Experiments on several real-world graph datasets demonstrate the effectiveness of the proposed method in both node-level and graph-level representation learning problems.,0.13402061855670103,0.31958762886597936,0.14432989690721648,0.3670886075949367,0.21518987341772153,0.06997084548104957,0.16455696202531644,0.09037900874635568,0.1794871794871795,0.08454810495626822,0.21794871794871795,0.3076923076923077,0.14772727272727273,0.1409090909090909,0.16,0.13744075829383887,0.21656050955414013,0.11401425178147268
811,SP:b44066e83f5db650528e5dc93965e053f57385c4,"This paper proposes a method to utilize partially labeled data for the CTC loss. In order to handle untrascribed lables, it introduces a wild-card for the beginning of the sequence and uses unconstrained endpoints for the ending of the sequence. It also discusses how to summarize the paths at the ending. The proposed algorithm was evaluated with simulated masked data and it was shown that the proposed algorithm can handle incomplete labels more effectively than the naive CTC loss while the naive CTC loss is slightly better when the label is complete.  Dynamic time warping (or DP matching) with unconstrained endpoints itself is not a new idea and a classical topic for speech recognition (e.g. word spotting). The contribution of the paper is the formal introduction of the approach to CTC and to give experimental results to confirm the effectiveness.","Mapping between two sequences of different length is frequently done via Connectionist Temporal Classification (CTC) loss when alignment is not available. CTC performs the full alignment between input and label sequences. However, there are applications when label is incomplete and only part of it is given. In the current paper authors consider the case when begin and end of label sequence are missed and extend CTC applicability to these partial labels. Based on the dynamic time warping SPRING algorithm for incomplete labels authors propose W-CTC: they similarly prepend and append label with ""*"" token which model missed begin and end of label sequence; then they aggregate all valid paths with the latter modification. W-CTC is empirically proved to significantly improve performance over CTC even if up to 40-70% label sequence is missed (overall performance similar to the complete label case) across different tasks, like speech, optical character, and continuous sign language recognition. W-CTC is simple and efficient.","This paper proposes an extension of CTC by considering the wild-card to adjust the label missing issues during training, which tends to happen in the onset/offset edges of the utterance. The paper elegantly formulates it as an extension of the CTC framework and applies it to two tasks (ASR and OCR). The method shows robust training behaviors compared with the original CTC training.  Other comments: - Can we use it for RNN transducer? - I'm expecting that this method may work even if missing labels happen in the middle if we use the self-attention-based network. The self-attention network can handle some re-ordering (move the hidden vectors corresponding missing part to the edge). - Section 1.1, the definition parts. These parts are well written, but the discussion about <blank> is missing. - I mentioned below, but this problem can be applied to the missing audio (missing X) case in the long recording scenario by just using a longer segment to cover all labels. So, for my major applications of this method, I don't think that this becomes a limitation. - does the wild-card symbol contain the <blank> symbol? It may not be a matter, but I'm curious. - It would be better if the paper has more discussions of why 'max-prob' does not work and 'weighted-sum"" works the best. - Is it possible to correctly find the wild card region with the Viterbi algorithm? I think this is a very good option. I also want to verify that the wild card region is correctly identified (I think Figure 4 shows it to some extend, but I want to know more analysis/examples).   ","This paper presents a modification of the CTC training loss to cope with incomplete transcripts in the training set, where the actual transcription of the beginning or of the end is missing. The authors propose to minimize the loss over all possible sub-segments of the input to automatically align the one that matches the available transcript. The main contribution is that it allows to directly train the neural network with CTC without having to clean or align the data first when some transcript are only partial.",0.176056338028169,0.2535211267605634,0.176056338028169,0.15,0.11875,0.10830324909747292,0.15625,0.1299638989169675,0.28735632183908044,0.08664259927797834,0.21839080459770116,0.3448275862068966,0.16556291390728475,0.1718377088305489,0.21834061135371177,0.10983981693363845,0.15384615384615385,0.16483516483516486
812,SP:b4cbe16b1ecbb5e6391eb2951fc72d802be1e0d0,"This paper describes an improvement to previous methods for discovering PDEs from data. They show that even for clean data, the design matrices for discovering PDEs using sparse regression can violate irrepresentability condition (IRC), which motivates their approach. They introduce randomised adaptive Lasso (rAdaLasso) and incorporate stability selection and error control. They demonstrate this approach for discovering PDEs from data that was numerically differentiated, comparing to two previous methods on two tricky PDE test cases. They also incorporate rAdaLasso with stability selection and error control into DeepMod (an existing model discovery framework that uses deep learning and avoids numerical differentiation) and show with four PDEs that this is an improvement over the original DeepMod method. Then they compare their version of DeepMod to six previous papers on two PDEs, showing the ability to tolerate more noise and larger libraries. ",This manuscript studies the variable selection consistency in model discovery of PDEs. A randomised adaptive Lasso (rAdaLasso) with stability selection and error control algorithm is proposed to recover the true underlying PDE in the presence of design matrices that are highly correlated and violate the IRC. Better model recovery performance is obtained by integrating rAdaLasso within the deep learning model discovery framework DeepMod. ,"This paper deals with the problem of model discovery based on sparse regression. Authors propose a new randomised adaptive lasso for that selects the terms to be used in the PDEs selection that is used in conjunction with Deep learning framework. Authors then experimentally verify on 4 PDEs that their method perform better for model selection and at least as well when looking at MSE loss.   For a detailed summary, authors first verify the violation of IRC which is defined as in eq (1) for Lasso on an analytic PDE (Korteweg-de-Vries (KdV) equation). As their first contribution, authors develop a diagnostic metric that indicates the violation of Lasso when one know the true support of the design matrix. In proposition 1, authors show that under certain assumptions on weights, the adaptive Lasso, where one reweighs the design matrix has better diagnostic score than Lasso.  Since the design matrix is typically approximated, if we incur a deterministic error term, authors empirically show that adaptive Lasso is also susceptible to the violation of the IRC. In lieu of this experiment, authors propose randomised adaptive Lasso, where each column of the design matrix of adaptive Lasso is randomly scaled with $\beta(1,2)$. Author integrate it in deep learning model discovery framework and empirically verify the performance of of rAdaLasso for 1) MSE loss and 2) model selection error.   ","The paper discusses the unidentifiability of lasso-based differential equation discovery, and proposes to use a more stable lasso variant (randomized adaptive lasso), which can more accurately identify PDE coefficients. The method is well-motivated and adresses a major bottleneck in differential discovery. The method is correctly presented, and the initial results are excellent. ",0.1366906474820144,0.17985611510791366,0.07913669064748201,0.4126984126984127,0.1746031746031746,0.07456140350877193,0.30158730158730157,0.10964912280701754,0.2037037037037037,0.11403508771929824,0.2037037037037037,0.3148148148148148,0.18811881188118815,0.13623978201634876,0.11398963730569947,0.17869415807560138,0.18803418803418803,0.12056737588652482
813,SP:b4f3670f5da7385cb5cb2f6e4dbe89899110d8f8,"The paper studies differential privacy for pre-trained certifiers that offer certified robustness through input perturbation. The key insight is to analyze differential privacy afforded by the input perturbation by noise transformation (computing corresponding gradient noise), and augmenting with gradient perturbation when needed for differential privacy guarantees. This improves upon past work by reducing the differential privacy budget needed by showing that some differential privacy is obtained from the input perturbation. In addition to new analysis techniques for providing the privacy bounds, experiments show gains over prior methods, e.g. better differential privacy under adversarial attack.     ","This paper focuses on providing both differential privacy and certified adversarial robustness to machine learning models.  The authors propose an algorithm called TransDenoiser to achieve such a goal. TransDenoiser consists a denoiser through both input and gradient perturbation for achieving DP and certified robustness, and following by a pre-trained classifier for classification. The privacy guarantee is carefully analyzed. Extensive experiments demonstrate the effectiveness of proposed method from model utility and adversarial robustness.","In this paper, authors studied the problem of achieving both the overall differential privacy and certified robustness simultaneously for pre-trained models. They proposed a framework called TransDenoiser based on an existing framework (Salman et al, 2020) [1] by adding additional and transformed gradient perturbations for the overall DP. Authors analyzed DP guarantee provided by these perturbations and empirically evaluate their methods on MNIST and CIFAR-10, and shown that TransDenoiser is effective against FGSM and PGD attacks with guaranteed DP. ","This paper studies the problem of integrating differential privacy and robustness to adversarial examples for pre-trained machine learning models. Specifically, this work aims at designing methods that guarantee both privacy and robustness without having to re-train the model at hand. To achieve this goal, the authors build upon an existing technique in the adversarial example literature that involves placing a denoising auto-encoder in front of a pre-trained model before applying a noise injection scheme known as ""randomized smoothing"" [1]. While this technique is known to provide state-of-the-art ""certified accuracy"" against adversarial examples, its privacy guarantees remained to be studied. This work proposes to do just that by adapting the algorithm to guarantee differential privacy for the dataset used to train the auto-encoder. The authors claim three main contributions:   1. Exploiting the intrinsic train-time input perturbation that existed in the previous implementation of the algorithm and composing it with an explicit gradient perturbation to satisfy differential privacy. The authors claim that their treatment of this input perturbation allows a finer analysis of the algorithm's privacy, which ultimately leads to better accuracy, for the same privacy guarantees. 2. Introducing two new analytical tools, namely MGM and MMGA,  for analyzing the privacy guarantees of multivariate Gaussian noise injection.  3. Conducting extensive experiments on several benchmark datasets to demonstrate that their algorithm, called « TransDenoiser »,  provides better privacy guarantees and achieves similar level of certified robustness compared to previous works.   [1] Provably Robust Deep Learning via Adversarially Trained Smoothed Classifiers Hadi Salman, Greg Yang, Jerry Li, Pengchuan Zhang, Huan Zhang, Ilya Razenshteyn, Sebastien Bubeck ",0.1875,0.14583333333333334,0.3645833333333333,0.2602739726027397,0.3972602739726027,0.32098765432098764,0.2465753424657534,0.1728395061728395,0.13011152416356878,0.2345679012345679,0.10780669144981413,0.09665427509293681,0.21301775147928995,0.1581920903954802,0.1917808219178082,0.24675324675324675,0.16959064327485382,0.14857142857142858
814,SP:b54a69d523175f772408cc49a1b4eeefba9f9793,"This paper proposes a correlation test to detect if heteroscedasticity exists between X and Y.   The method makes use of the neighboring variables and take the pearson correlation between $\Delta$ Y and $\Delta$ Y"".   This test does not require the computation of the regression residuals that requires well-specified model assumptions.  The authors claim that this test was able to successfully detected the heteroscedasticity of the data.  ","This paper proposed a statistical measure named the absolute neighbour difference based neighbour correlation coefficient, to detect the associations between variables through examining the heteroscedasticity of the unpredictable variation of dependent variables. The method can somehow measure nonfunctional relationships. The method is simple and easy to implement that does not rely on explicitly estimating the regression residuals or the dependencies between variables so that it is not restrict to any kind of model assumption. ","This paper proposed a new statistical measure, called absolute neighbour difference based correlation test, to detect the existence of heteroscedastic noise in the residual of a functional relationship.   Theorem 1 shows the proposed measure nCor_Delta is only affected by the dispersion function of the residual g(x) and not affected by the functional relationship f(x). As a result, compared to those existing statistical test of heteroscedastic noise, the major advantage of the proposed measured is to avoid fitting the regression functions. The estimator of the measure on real-world datasets is also presented in Definition 2. The statistical test of heteroscedastic noise vs. homoscedastic noise is presented in Theorem 2.   In the experimental results, the proposed nCor_Delta is firstly compared against two existing tests of heteroscedastic noise, including Parker test and White test. It shows the nCor_Delta can correctly test for heteroscedastic noise while its competing alternatives either fail on some datasets or uses over-complicated regression functions.  The proposed nCor_Delta is also evaluated against a few existing non-linear dependency measures that are not designed to test heteroscedastic noise, including dCor, RDS, MIC, MI and nCor. These measures cannot detect heteroscedastic noise as expected.",This paper proposes a new test for heteroscedasticity.  Heteroscedasticity is an important issue in real-world applications that in my opinion is under-considered and under-addressed.  The paper presents theoretical results on type I and type II error and provides significant empirical results for the benefits of the method over others such as the Park test and the White test.  It discusses limitations such as decreased relative power in high-dimensional settings.,0.3283582089552239,0.34328358208955223,0.23880597014925373,0.3918918918918919,0.16216216216216217,0.1,0.2972972972972973,0.115,0.2191780821917808,0.145,0.1643835616438356,0.273972602739726,0.3120567375886525,0.17228464419475656,0.22857142857142856,0.2116788321167883,0.163265306122449,0.14652014652014653
815,SP:b56ddad72ca98cd253e0fc4998b6c9793020e58e,"The paper proposes an unsupervised approach for instance segmentation that leverages pretrained GANs. The approach is divided into two steps: first, the authors discover a latent direction in the generator that allows them to segment generated images. Then, the resulting image-label pairs are used to train a segmentation network. This network can finally be used to perform inference on unseen images. The approach is evaluated on a variety of GAN architectures. The main contribution claimed by the authors is that their method can be applied to off-the-shelf pretrained GANs, whereas competing approaches require architectural variations and re-training.","This paper presents a new unsupervised algorithm for image segmentation via deep generative models. Specifically, the authors combined edge preserving and brightness enhancement loss functions to search the latent manipulation directions which only adjusting the foreground object brightness. As a result, the directions could be utilized to synthesize image segmentation training samples. Experimental results on saliency detection and image segmentation demonstrate the effecteness of the proposed method.   ","The paper tackles the problem of image segmentation. It focuses on finding foreground-background image separation cues from a pre-trained GAN and create a synthetic dataset. Once the dataset is created, it trains a segmentation models and test the approach on several benchmarks. The paper is centred around how to extract and detect this foreground-background separation in an unsupervised way. The method works with any GAN architecture and in the end achieves competitive results while being unsupervised.","This paper describes a simple procedure that allows to train an unsupervised image segmentation model only using a generative model as input. By tricking the generator to predict proxy segmentation masks along with a generated image, the method allows to construct a possibly infinite dataset of images associated with a F/B mask. A segmentation model can then be trained on this dataset to predict foreground. The proposed approach is evaluated across a wide range of small-scale segmentation datasets, and compared with various approaches to the problem (both supervised, handcrafted and unsupervised). An adaptation of this approach to semantic segmentation is proposed and evaluated on Pascal VOC.",0.15841584158415842,0.1485148514851485,0.24752475247524752,0.16417910447761194,0.26865671641791045,0.20253164556962025,0.23880597014925373,0.189873417721519,0.23148148148148148,0.13924050632911392,0.16666666666666666,0.14814814814814814,0.19047619047619047,0.16666666666666666,0.23923444976076552,0.1506849315068493,0.2057142857142857,0.17112299465240643
816,SP:b5c44519971f3f075f5c920846346c2c838d8563,"This paper proposes a novel technique to generate counterfactual explanations for Markov decision processes (MDPs). Technically, it first remodels the state transition of MDPs with a Gumbel-Max structural causal model. Then, it formalizes the counterfactual explanation generation as an optimization problem and proposes an algorithm to generate the explanations. With the experiments on both a synthetic and a real-world dataset, the paper demonstrates the effectiveness of the proposed technique in finetuning a given policy to collect more rewards, especially when the corresponding environments have uncertainties.  ","This paper is about counterfactual explanations for Markov decision processes. Instead of the usual counterfactual explanations as ""alternative traces which can be used to explain that the chosen (optimal) choices are in fact optimal"" as in explainable AI and reinforcement learning (RL), this paper uses them in offline settings, where data is available of the chosen paths, and where potentially better alternatives (counterfactual) can be computed. The main idea is to employ structural causal models to capture the model of the MDP such that better alternatives (here: action sequences that differ at most in k actions) can be computed in that model. The paper features experiments in two domains; one artifical and one in a small (in terms of the model) medical treatment domain. The experiments clearly show the benefits and characteristics of the approach.","The paper proposes a method using Markov decision process and dynamic programming to build alternative sequences with better reward/ outcome. The algorithms used can find optimal counterfactual explanations in polynomial time, testing in both synthetic and real dataset. With different uncertainty (defined manually), optimal counterfactual sequence outperformed the observed one considering the outcome in most cases. Thus the paper validated the contribution in finding the optimal counterfactual sequence in multiple, dependent sequence under discrete and low-dimensional situation.",The paper proposed a novel method to generate counterfactual explanations. The authors view the problem as sequential decision-making processes using Markov decision processes and the Gumbel-Max structural causal model. They discuss an algorithm based on dynamic programming to find optimal CFEs. The model was validated on synthetic and cognitive behavioural therapy datasets. ,0.26436781609195403,0.21839080459770116,0.26436781609195403,0.13333333333333333,0.1037037037037037,0.20512820512820512,0.17037037037037037,0.24358974358974358,0.42592592592592593,0.23076923076923078,0.25925925925925924,0.2962962962962963,0.2072072072072072,0.23030303030303031,0.3262411347517731,0.16901408450704225,0.14814814814814814,0.24242424242424246
817,SP:b5f1303850323da3ce093a47a669f8a696569964,"The paper outlines four problems and proposes a solution for each problem:  (i) Generally, score-based generative models (for example [1]) do not compute the loss on the entire time interval $[0,1]$ but rather only on $[\varepsilon, 1]$, for some $\varepsilon > 0$. The paper claims that choosing $\varepsilon$ too large results in poor details for image generation (see Figure 4), for example, the details (pores) of a human nose.  (ii) The paper shows that the the squared Euclidean norm of the data score (for an unspecified data distribution) empirically diverges as $t \to 0$.  The paper then presents a lemma that states that no neural network can learn an unbounded function as long as it is conditioned on time $t$ or variance $\sigma$. The paper then proposes to condition on $\eta(t)$ where $\eta$ is constructed such that either $\eta(t) \to \infty$ or $\eta(t) \to -\infty$ as $t \to 0$.  (iii) The paper shows that Monte Carlo samples of the likelihood loss have a large variety of scales: values are very large for small times $t$ (or equivalently small noise levels) and very small for large times $t$ (or equivalently large noise levels). Due to the large variety of scales, the authors claim that ""the diffusion loss is dominated by the end diffusion time with the front range in practice, and the loss at the end diffusion time is barely counted in the gradient signal. This imbalanced loss brings the immatured score estimation at the end diffusion time, which ruins the overall sample shape."" (I am not entirely clear what this quote means.) The paper then proposes a so-called ""soft-truncation"" (ST) trick. The high-level intuition of the ST trick is described as follows: instead of sampling time points $t$ uniformly over the time range $[\varepsilon, 1]$ in each batch; in each batch it samples a $\tau \sim \mathbb{P}(\tau)$ and then samples $t$ uniformly over the time range $[\tau, 1]$. The authors claim that this has the effect that some batches focus on large time steps (providing a good gradient signal for large noise levels) and some batches focus on small time steps (providing a good gradient signal for small noise levels).  The original setup with fixed $\varepsilon$ can be recovered by setting $\mathbb{P}(\tau)= \delta(\tau = \varepsilon)$. The authors then propose a particular one-parameter ($k$) distribution $\mathbb{P}$ and show different instantiations of $k$. The family is designed in such a way that $\mathbb{P}(\tau) \to \delta(\tau = \varepsilon)$ as $k \to \infty$.  (iv)  The authors show that the continuous VESDE [1],  proposed  as a continuous extension of  [2],  is not actually “geometric” as apparently desired by [2]. The authors then propose the ""Reciprocal Variance Exploding"" SDE (RVESDE) which can be shown to be ""geometric"". The authors then claim that importance sampling is simple for the RVESDE.  The paper tests the proposed solutions on CIFAR10 (32x32), CelebA (64x64), CelebA-HQ (256-256), and STL-10 (48x48).  References:  [1] Song et al. Denoising Diffusion Implicit Models. ICLR 2021.  [2] Song & Ermon. Improved Techniques for Training Score-Based Generative Models. NeurIPS 2020.","This paper considers two problems associated with optimizing diffusion models, namely, training instability due to unbounded denoising score functions when the level of noise is very small (at the later stages of the reverse diffusion process), and problems at the earlier stages of the reverse diffusion process where coarse-scale artifacts are sometimes introduced as the corresponding terms of the objective have relatively little influence on the training loss. These problems are addressed through a new parameterization that is better suited to extremely small noise levels, and a soft truncation (ST) trick that better enables the optimization with sufficient influence given to the entire range of noise levels.  The paper has strong technical contributions and very good experimental work, showing improved quantitative measures of performance, namely NLL, FID, and IS.","Diffusion models have achieved great performance in many applications. However, learning a good diffusion model can be challenging since the data score can go to infinity when $t$ goes to zero. To address this issue, this paper proposes an alternative parameterization for unbounded data scores and provides a practical trick (ST-trick) to handle the variation of the scales of score values. It proposes Reciprocal Variance Exploding Stochastic Differential Equation to sample from the model. The proposed method can be applied to existing NCSN and DDPM models. Empirically, it achieves strong performance on high-resolution image generation.","This paper studies the behaviour of training SDE-based score-matching generative models for time t close to 0. The paper identifies two issues with existing forward SDEs for training such models: 1) the score of the marginal distribution could be large, and 2) the expected squared loss could be large, draining the loss contribution at other times. The paper introduces several remedies to fix this issue.    ",0.07293666026871401,0.061420345489443376,0.04798464491362764,0.11538461538461539,0.1076923076923077,0.12371134020618557,0.2923076923076923,0.32989690721649484,0.373134328358209,0.15463917525773196,0.208955223880597,0.1791044776119403,0.11674347158218126,0.10355987055016182,0.08503401360544217,0.13215859030837004,0.14213197969543148,0.14634146341463417
818,SP:b61313e7184586404f96f93ceab02c2abe7121bf,"This work proposes meta-learning of loss functions for regression tasks. This approach has powerful applications for cases when the performance metrics are non-differentiable. Such cases often arise in tasks where the cost of misprediction is asymmetric and ripe with conditional clauses. The authors have proposed a simple yet powerful two-step process, wherein two neural networks are involved such that (a) a loss-learning NN learns an approximate differentiable loss function that mimics the non-differentiable performance metric, and (b) predictor NN that aims to perform well on the loss function defined by this loss-learning NN. The authors specify two crucial approaches to make this proposal work - (a) loss exploration, and (b) co-training of two neural networks. Their proposal is well supported by the experiments on time-series prediction tasks. ","This paper proposes a loss learning framework for the timeseries forecasting regression problem. They introduce two learning blocks, one for producing the next step prediction and the other for learning the loss function. The experimental results show that the proposed method slightly increase the performance compared to MAE.","The study proposes MetaLoss, a loss meta-learning approach that is capable of autonomously optimising the properties of traditional loss functions (e.g., MAE, MSE) to a target task. The proposed framework has improved the regression accuracy in multiple time series datasets, outperforming classical fixed learning-based models, which do not employ any learning procedure to train a target loss function. The motivation of this work is clear and sound; nevertheless, I have multiple concerns about the experimental setup of this work.","This paper focuses on meta-learning of loss functions for time-series forecasting, where the loss function to be used to train the actual model is learned. For this problem, they propose MetaLoss, which performs joint co-training of a regressor network and of the loss-learning networks. They show under one specific set of hyperparameters and for a single simple baseline method, that it can work well. They also apply it for a few different applications. The idea seems interesting. However, the actual contribution of this work, and effectiveness of the approach is unclear due to the issues discussed in detail below.",0.12686567164179105,0.13432835820895522,0.1865671641791045,0.2708333333333333,0.3125,0.2073170731707317,0.3541666666666667,0.21951219512195122,0.24271844660194175,0.15853658536585366,0.14563106796116504,0.1650485436893204,0.18681318681318684,0.16666666666666666,0.21097046413502113,0.19999999999999998,0.1986754966887417,0.1837837837837838
819,SP:b635a9fb4530e686cf751013f74c90d588aa434a,"The problem of computing equilibria in CGs is consider an easy task whenever the strategy space has a compact representation, since it turns out that the users minimize a convex function. When having to decide on some parameters of the CG in order to optimize the performance at equilibrium we get a bi-level optimization problem, where in the upper level the designer sets the parameters and in the lower level the users optimize their function reaching an equilibrium. Solving this bi-level problem is regarded to be far more difficult. This work deals with the latter problem and combines optimization techniques to give an algorithm that performs well in practice.","This paper focus on the Stackelberg model of combinatorial congestion games (CCGs), which is important for the case involved the leader and multiple self-interested followers. The main contribution of this paper is proposing a fully differentiable framework which can take the advantage of automatic differentiation.  My main concern is the contribution of this paper. There are two key techiniques in the proposed algorithms, softmin and Zero-suppressed binary decision diagrams (ZDD), where softmin is a widely used technique to make argmax operation to be differetiable and the ZDD technique is proposed in the 2020 AAAI paper, Practical Frank–Wolfe Method with Decision Diagramsfor Computing Wardrop Equilibrium of Combinatorial Congestion Games. I would like to ask authors to clearly state the difference of the techniques in this paper and the one in the 2020 AAAI paper.  Minor issues and typos:  ""practical utility"" in line 60, ""utility"" -> usage or application? ""Our method introduces benefits by improving the designs of social infrastructures"" is strange.","The authors of this paper aim to find a combinatorial congestion game that optimizes a given objective over some parameterized class of games, assuming that the players in the game attain a Wardrop equilibrium. The problem is formulated using a framework of Stackelberg bi-level optimization models, where the designer (leader) chooses the game's parameters, and the followers then play the induced game. The authors propose to apply gradient-based methods to approximate the solution. To this end, it remains critical to be able to compute the followers' equilibrium and estimate its gradient, i.e., the changes in the equilibrium when the parameters of the game shift. For this purpose, the authors introduce a differentiable variant of the Frank–Wolfe algorithm. Instead of the strict minimization included in the original version, the authors suggest using softmin. Computing the softmin may be computationally demanding because it sums over all possible strategies, which is (in general) a set exponential in the problem size. More efficient computation can be achieved using the so-called zero-suppressed binary decision diagrams (ZDDs), a data structure enabling calculation of the softmin in time linear in its size. Because the size of the ZDDs is often many orders of magnitude smaller than the cardinality of the set of strategies, ZDDs offer a significant speedup in practice. In the last part of the manuscript, the authors present empirical results achieved with their algorithm and its variants on two classes of games per each strategy representation (Hamiltonian cycles and Steiner trees). The results show that the algorithm can converge to Wardrop equilibria faster than baselines, and the gradient descent running on top of it consequently designs better congestion games than the heuristic baselines. ","This paper studies the problem of designing parameters of combinatorial congestion games, where selfish non-atomic agents choose an optimal route/network to optimize their own objectives. The combinatorial congestion games are a special type of potential games. The equilibrium of a congestion game with given parameters can be solved by optimizing the potential function. The problem of optimizing the game parameters is therefore a bilevel optimization problem with an equilibrium (potential function optimization problem) involved in the inner problem. Benefited from the recent development of differentiable optimization, the authors propose to solve the bilevel optimization problem by backpropagating the gradient from the argmax of the inner problem to the outer objective to run gradient descent. This end-to-end gradient descent approach can avoid solving the bilevel optimization problem with complex constraints by directly running end-to-end gradient descent.  The challenge involved in this end-to-end differentiable optimization approach is how to efficiently compute the gradient of a complex optimization problem (equilibrium computation/potential function optimization) with high-dimensional constraints. The authors leverage the Frank-Wolfe algorithm to solve the optimization problem and to get the corresponding gradient of the optimal solution. The use of the Frank-Wolfe algorithm decomposes the optimization problem into an iterative algorithm composed of multiple minimization problems, where each of them is relaxed to a softmin in order to make them differentiable. To solve the softmin problem with complex constraints, they use zero-suppressed binary decision diagrams (ZDD) to compactly represent the decision variables, which can speed up the optimization step while keeping the differentiability. The authors also utilize Nesterov's acceleration to improve the convergence of the Frank-Wolfe algorithm to $O(1/T^2)$, with the differentiability maintained.",0.18018018018018017,0.27927927927927926,0.2882882882882883,0.25308641975308643,0.22839506172839505,0.23508771929824562,0.12345679012345678,0.10877192982456141,0.1111111111111111,0.14385964912280702,0.1284722222222222,0.2326388888888889,0.1465201465201465,0.15656565656565657,0.16040100250626566,0.18344519015659957,0.16444444444444442,0.23385689354275743
820,SP:b6855e4030ad7decd9919becb609c8b903ef4d11,"The paper focuses on generalizing graph neural networks to model higher-order structures. It proposes a new message passing scheme operating on regular cell complexes. The induced models, called CW Networks, generalize and subsume the message passing simplicial networks (MPSNs) which operate on simplicial complexes, and are proved more powerful than the WL test and in some cases not less powerful than the 3-WL test.    ","Recently, there have been several attempts to put higher-order structures on graphs and perform message passing over these structures. One example is the Message Passing Simplicial Networks proposed in e.g, ref[4]. The present paper extends the higher-order structures from simplicial complexes (SCs) to regular cell complexes (also known as CW complexes), or more precisely, generalizes the Simplicial WL of ref[4] to a Cellular WL (CWL). Both theoretical understanding, and experimental studies of CWL are presented. Specific contributions include:   (1) To obtain a CW complex from a graph, the paper introduces a ``cellular lifting map"", and shows a skeleton-preserving lifting map gives rise to a CWL that is at least as powerful as standard WL (Weisfeiler-Lehman) in graph isomorphism testing. It also gives examples of specific natural lifting maps (which includes the previous SWL in [4] as a special case) that is at least as powerful as the so-called 3-WL.   (2) It describes and implements a CW network using a lifting map that attaches 2-cells to all induced cycles in the input graphs. This architecture (CWN) makes sense especially for molecular graphs applications (in capturing both the covalent bonds and chemical rings).   (3) It presents various experimental results on synthetic and real datasets, which show that a specific instantiation of CWN, called CIN, can both differentiate certain hard cases for graph isomorphism testing (e.g, 4-regular graphs), comparable or better performance on TUDatasets, and better performance in molecular benchmark datasets.  ","This paper presents an extension of the message passing simplicial network based introduced in [4] on cell complex. The authors first introduce background on regular cell complex. They present an extension of the Weisfeiler Lehman test that generalises the simplicial WL from [4] to Cellular WL and give theoretical results about the power of this new test. Then they introduce CW Networks which is a natural extension of message passing GNN to message passing on cells and show that their architecture is permutation equivariant. Finally, they show on synthetic benchmarks and real-world graphs that their architecture achieves better performances.","In this work, the authors propose a new member of graph neural networks, called CWN, that considers the higher-order structure of graphs. The proposed model leverages the topological properties of cell complexes and establishes a novel hierarchical message passing procedure. The authors demonstrate that the proposed CWN is more powerful than the WL test and can be comparable to the 3-WL test. The proposed model achieves encouraging performance on graphs with complicated structures, e.g., molecules, while it requires a preprocessing step with the tolerable time cost.",0.3333333333333333,0.21212121212121213,0.3333333333333333,0.12,0.096,0.15,0.088,0.14,0.24719101123595505,0.3,0.2696629213483146,0.16853932584269662,0.13924050632911392,0.1686746987951807,0.2838709677419355,0.17142857142857143,0.1415929203539823,0.15873015873015872
821,SP:b6b262f39c856b4450c4208a399319af63b367f2,"This paper is concerned with speeding up Bayesian optimization by using evaluation data from previous, related tasks defined over the same configuration space. The authors propose to model the data from each experiment (or ""task"") by independent Gaussian processes, which all share the same mean and covariance function. This surrogate model can be learned from past data.  The paper also presents experiments on a fairly simple search space of 4 optimizer parameters. This is done for a bunch of datasets and NN models. And there is a pretty simple extension of theoretical results from (Wang, 2018b).","This paper suggests a meta Bayesian optimization strategy that optimizes free parameters of GP including a prior function and noise variance, where multiple sets of historical observations are given. In particular, the proposed method chooses a free parameters using one of three approaches: (i) optimizing a marginal likelihood, (ii) measuring KL divergence, (iii) considering both marginal likelihood and KL divergence. The authors finally show the theoretical analyses on regret bounds and the numerical results on hyperparameter optimization.","This paper presents a Bayesian optimization method based on meta-BO. The motivation is tasks can share the same parameter structure and this shared information, e.g. correlation between tasks, can be transferred to new and similar tasks. An example is to optimize the the hyper-parameters of a same optimizer across different architectures and different datasets. This problem is a very important one in the community of Bayesian optimization and a reasonable method can lead to a potentially dramatic decrease in the required computation, especially when the objective function is very expensive. This work tries to overcome limitations of existing methods. For example, the method proposed in this work does not need to evaluate all objective functions associated with all tasks on the same parameters."," HyperBO assumes the tasks are independent given the hyperparameters, unlike typical metalearning approaches which assume tasks are related. This allows for an efficient Kronecker decomposition of the kernel and thus linear, rather than cubic scaling, across tasks.   Using this model, HyperBO performs BO as usual; maximize the acquisition function to obtain the next point to evaluate. HyperBO also makes the critical assumption of an offline pre-training of hyperparameters on a representative set of completed tasks; during optimization itself the hyperparameters are fixed.  ",0.125,0.22916666666666666,0.13541666666666666,0.22077922077922077,0.12987012987012986,0.12698412698412698,0.15584415584415584,0.1746031746031746,0.1566265060240964,0.1349206349206349,0.12048192771084337,0.1927710843373494,0.13872832369942195,0.19819819819819817,0.14525139664804468,0.16748768472906403,0.125,0.15311004784688995
822,SP:b6d91d02ba8f4be02b28bddeb44ca896e28476cf,"This paper proposes identifying the robust overfitting as the early part of epoch-wise double-descent, which is caused by implicit label noise derived from the mismatch between the true label and assigned label distributions on the adversarial examples. The authors further provide a method that combines temperature scaling and interpolation to mitigate robust overfitting. Both theoretical and experiments are included to show the effectiveness of the proposed method on realistic datasets. ","In this work, the authors aim to show that double descent in adversarial training might be caused by implicit label noise, that is, the distribution mismatch between the true label distribution and the assigned label distribution of the adversarial examples. They empirically support this claim by showing that training with static adversarial examples would also encounter the double descent phenomenon. To further solve this problem, they propose to apply temperature scaling and interpolation to create a soft label for each adversarial sample in adversarial training. Experiments on CIFAR10/100 and Tiny-imagenet are conducted to validate the efficacy of the proposed method.","This paper demonstrates that ""robust overfitting"" during adversarial training is an early part of an epoch-wise double descent phenomenon for relatively large models. The authors also demonstrate that the overfitting behavior is observed as we select larger perturbations during training.  The authors explained this ""double descent"" phenomenon using ""implicit label noise"" and finally demonstrated that by temperature scaling and knowledge distillation, we can significantly mitigate the overfitting behavior for adversarial training.  ================= Post Rebuttal ================= The authors have answered most of my concerns. Hence, I have increased my score to 6. Please find additional comments to improve the organization of the paper and experimental results in the following.","This paper studies the double descent phenomenon in the adversarial training, with an aim of explaining the wide observation of robust overfitting. The authors made the connection by showing that with a large enough model, the robust overfitting can be seen as the early stage of an epoch-wise double descent. This observation is confirmed with extensive experiments and analyzed by a proposed metric called the implicit label noise. A method is proposed to mitigate such noise and the consequent overfitting.",0.5,0.3611111111111111,0.3333333333333333,0.22549019607843138,0.22549019607843138,0.19626168224299065,0.35294117647058826,0.24299065420560748,0.2962962962962963,0.21495327102803738,0.2839506172839506,0.25925925925925924,0.41379310344827586,0.2905027932960894,0.3137254901960785,0.22009569377990432,0.2513661202185792,0.22340425531914895
823,SP:b6df2a7bddd6112e6e0d21ed7525b5242f975d37,"This paper is an interesting ""re""search on a classical meta-learning method MAML. It shows that both the inner update iterations and task label assignment have a clear influence on the performance of MAML. It raises some questions and uses a set of in-depth experiments to draw conclusions. ","The authors study the MAML algorithm and propose two ways to improve the performance. First, they study the number of inner gradient steps. Second, they look at the permutation of labels when learning a new task which ideally should not make any difference. However, it seems that during testing this could lead to different test performances on a target task. Based on this observation, they propose a novel and very interesting solution to share the weights of the classification layer (which they call UNICORN MAML).","The paper mainly investigates the effect of permutation in the class label assignment in the tasks for the MAML algorithm. First, the authors show that MAML requires a higher number of inner loop updates than what is commonly used. Then, they show that MAML is sensible to the permutation of the class labels in the tasks and experimented with diverse methods to alleviate this problem. Finally, they proposed Unicorn-MAML, a modification of the MAML algorithm that learns a single weight vector for the classifier layer, to make the model permutation invariant. They show through experiments on two different datasets (mini-ImageNet and tiered-ImageNet) that it achieves or outperforms state-of-the-art performance.","This paper analyses the well-known and well-studied MAML algorithm and raises two key observations. First one is requiring a high number of inner-loop updates, and the second one is the variation in the meta-test accuracy when permuting the indices of the classes. Then it proposes to simply meta-train a single vector and duplicate it as initialization for the classifier head to make MAML permutation-invariant and improve its generalization performance on meta-test tasks. ",0.18,0.22,0.22,0.25882352941176473,0.21176470588235294,0.25217391304347825,0.10588235294117647,0.09565217391304348,0.13924050632911392,0.19130434782608696,0.22784810126582278,0.3670886075949367,0.13333333333333333,0.13333333333333336,0.17054263565891473,0.22,0.2195121951219512,0.2989690721649484
824,SP:b70491f033e70889fe76545b5ce7405f5104b7cc,The paper proposes a coordinated policy optimization algorithm to simulate self-driven particles and showcases its power in the autonomous driving application of traffic simulation. The algorithm proposes both local and global coordination mechanisms to tradeoff cooperation and competitiveness at the scene-level. The paper benchmarks the efficiency and safety of their method against a few baselines on a newly proposed simulator of heavily interactive scenarios.,"In this paper the authors propose a multi-agent reinforcement learning approach to finding controllers in self-driven particles systems. The problem of SDP is formulated as dec-POMDP where each agent has an individual reward function that is wishes to maximize using local observations only.  The algorithm consists of two components: local coordination relying on neighboring rewards and global coordination.   The local coordination part modifies the objective to be a combination of individual rewards and neighbor reward using a Local Coordination Factor. The LCF is used to compute a coordinated objective.   The global coordination component is responsible to find the best LCF, for tractability this objective is factorized into individual global objective.  The algorithm is evaluated on a set of traffic scenarios against multiple baselines: independent policies, mean field methods and curriculum learning.   The reward model using LCF and the split between local and global coordination is sound and interesting. However, the derivation lack of clarity, and it would have been useful to study the algorithm on other domains, especially given that the claim is on general SDP systems.   The authors should revise the claim formulation and provide more justification of the global coordination method. ","The authors aim to design controllers for traffic simulation agents in the mixed-motive reinforcement learning setting. They introduce a smooth interpolation between individual and mean global reward (social value orientation) which agents locally optimize while using a meta-optimization process to optimize the degree of interpolation. They show that in a suite of 5 traffic scenarios, their method generally produces agent populations that succeed at higher rates, are more efficient, and produce a smaller amount of critical failures.","The authors introduce a novel MARL learning algorithm, CoPO, that is designed to facilitate coordination in SDP systems. In addition to independent value functions, CoPO learns a neighborhood value function according to a return based on the normalized sum of neighboring agent rewards.  Both independent and neighborhood returns are mixed together according to a popular measure of social value orientation. Using a centralized bi-level optimization process, the agent social value distribution is optimised at the same time as the optimal low-level continuous decentralized control policies. The authors evaluate CoPO on a testbed of 5 common road traffic coordination tasks and according to three novel collective metrics. CoPO is shown to outperform other state-of-the-art CTDE MARL algorithms. ",0.36363636363636365,0.21212121212121213,0.25757575757575757,0.10714285714285714,0.17346938775510204,0.26582278481012656,0.12244897959183673,0.17721518987341772,0.14049586776859505,0.26582278481012656,0.2809917355371901,0.17355371900826447,0.18320610687022898,0.1931034482758621,0.18181818181818182,0.1527272727272727,0.2145110410094637,0.21000000000000002
825,SP:b737bf98086b86b71d6d8d6eee08c34d427eca12,"This paper focuses on a multi-dimensional time-series prediction task through deep learning - particularly towards predicting consumer behaviour. The authors utilised two real-world datasets (Cloudinary and bike-sharing dataset, with the bike-sharing data being publicly available) for predicting subscriber usage to identify consumer behaviour. Additionally, the authors also mention that experiments were also conducted on a publicly available exchange-rate dataset for time-series forecasting. The key theme in the proposed approach is the incorporation of auxiliary inputs to a CNN model (by making the conventional CNN deeper with multiple auxiliary layers), and ultimately utilising the model for time-series prediction. The paper, with multiple experiments conducted across baselines (such as ANNs, Random Forest and LSTMs), highlights that forecasting individual consumer behaviour (based on the bike-share dataset) yields better results when utilising auxiliary layers in the CNN model.   ","This paper is focused on the development of an automated model for predicting multi-variate future usage. This is an important problem for industrial companies that rely on subscriber systems. Long-term future prediction/forecasting is known to be a challenging problem, especially for systems with complex dynamics and high dimensional variables. In this paper, the authors propose a CNN-based model with multiple outputs/losses. Multiple auxiliary layers are constructed to incrementally produce predictions from short-term future to long-term future. Results have shown some improvement on two datasets. ","The paper presents a model architecture for time series forecasting based on CNNs. The proposed method incorporates multi-dimensional inputs as well as auxiliary outputs for different time scales. Compared to the demonstrated baselines, the paper's method generally outperformed across two datasets for product usage (cloud SaaS and bike rentals). ","This is an application paper about utilizing CNN for multi-dimensional time series from subscriber usage. The authors proposed to use auxiliary layers to produce multi-step predictions. Overall, this paper lacks technical and theoretical novelty. The presentation needs significant improvements and the experiment section is unsolid to justify the proposed methods.  ",0.14788732394366197,0.11971830985915492,0.13380281690140844,0.0989010989010989,0.15384615384615385,0.19607843137254902,0.23076923076923078,0.3333333333333333,0.36538461538461536,0.17647058823529413,0.2692307692307692,0.19230769230769232,0.18025751072961374,0.17616580310880828,0.1958762886597938,0.1267605633802817,0.19580419580419584,0.19417475728155342
826,SP:b86b8dc0e4ae576752edab8cf01cd5fcd0d7db43,"The paper analysis the problem of interlocking in cooperative rationalization, provide both theoretical and practical analysis for this problem, and derive a potential way to solve it: - The paper first shows theoretically, that, when formulating selective rationalization as a two player cooperative game, where a rationale generator generates a mask, and a predictor uses the masked input to predict the output, a problem of interlocking can happen if both the predictor and the rationale generator initially have a correlated (but potentially invalid) bias as to which part of the input best explains the output. In such cases, as it has been mostly train on one, potentially suboptimal, subpart of the input, the predictor will incur greater errors when using other (potentially better) parts of the inputs, which, in turn, will reinforce the generator in selecting the suboptimal subpart. The paper analyses this problem theoretically, by showing that the prediction loss provided optimal predictor is concave in the policy, meaning that some vertices of the simplex might be local optima. - In turn, the authors show that when using soft-attention as an explanation scheme instead of hard attention provides a convex prediction loss provided optimal predictor, but loses the property of being faithful. - The authors propose a methods that combine the soft-attention and hard-attention explanation scheme to better optimize the hard attention one, by training both an attention predictor and a rationale predictor, using the attention predictor to train the rationale generator, while regularizing the attention predictor to keep it close to the rationale predictor. - The authors validate the existence of the interlocking problem on a toy dataset (by manually favoring one of the subparts of the inputs by hand), and show that there approach partly alleviate interlocking. They proceed to demonstrate that their approach achieves state of the art on real world datasets.","The traditional selective rationalization consists of one rationale generator and one predictor. This cooperative rationalization structure reveals a major problem, i.e., model interlocking. To tackle this problem, in this paper, the authors propose a new rationalization framework, which introduces a third component into the architecture, a predictor driven by soft attention as opposed to selection. The additional predictor enables the framework to exhibit a better convexity property. ","The paper uncovers an issue in training rationale extraction models, which the authors call _interlocking dynamics_. When training a rationalizer, it turns out the optimization problem is concave, which means that in certain conditions the model can converge into a sub-optimal local minimum. When the model relies instead on a soft-attention as a rationalization, the problem is convex but the global minimum is not faithful. To solve this, A2R is proposed. A2R uses both hard-choice and soft attention rationales to obtain a better loss landscape. A2R is compared to other rationale extractors and obtains state-of-the-art in two datasets.","This paper describes ""interlocking dynamics"" that lead to local optima in selective rationalization techniques. The idea is simple: if the rational generator selects suboptimal features, you risk the predictor overfitting to these features, thereby removing a path to the optimal solution (since the generator cannot improve the predictor's loss by selecting better features if the predictor has overfit to suboptimal features). The paper builds intuition through a simple coordination game example, and describes sufficient conditions for their attention-augmented approach to overcome the coordination problem.",0.08852459016393442,0.12131147540983607,0.09180327868852459,0.25,0.20588235294117646,0.15384615384615385,0.39705882352941174,0.3557692307692308,0.32558139534883723,0.16346153846153846,0.16279069767441862,0.18604651162790697,0.14477211796246647,0.18092909535452323,0.1432225063938619,0.19767441860465118,0.18181818181818182,0.16842105263157894
827,SP:b8c221d46ae91c4f82d6b21d0f150b99d5bd80df,In this paper a subclass of SCMs whose structural equations are based on feed-forward NNs are considered. The authors show the expressiveness of this class and address the problem of training those model and evaluated identifiability of a causal task. Experiments on models (with few variables) are promising.,"This paper studies the expressivity, identification, estimation of causality with neural models. For expressiveness, it shows that Neural Causal Model (NCM) is expressive enough to represent any structural causal model (SCM), but in general, given observation data alone, without further constraints, the NCM is not able to identify higher layers of Pearl Causal Hierarchy (PCH). The authors then develop a necessary and sufficient condition to determine whether a causal effect can be learned from data (i.e., causal identifiability); if identifiability holds, it also estimates the effect from data (causal estimation). Empirical experiments on a simple synthetic dataset support the theoretical results.","The authors investigate the connection between structural causal models and neural networks from an learning perspective. Their theoretical results emphasize, notably, that the impossibility to learn neural networks that are interventionally equivalent to the true SCM based on observations only. The paper provides an interesting perspective on the connection between causal inference and learning with neural network, although clarity could be improved, notably regarding the limitations and the contribution with respect to previous work.","The paper considers a restricted class of SCMs (structural causal models) where each variable in a graph is generated by applying a neural-network-parameterized function of the variable's parents, called Neural Causal Models (NCMs). The paper shows a wealth of results mapping the relationship between the full class of SCMs and the smaller class of NCMs in terms of when and how identification holds and how estimation of the interventional distribution can be achieved. The method proposed to identify and estimate effects takes a graph as an input, computes the maximum and minimum of the effect of a particular intervention within the class of NCMs that are consistent with the observed data distribution (at the lowest level in the causal hierarchy, observational); if the maximum and minimum match, the effect is identified. The method is evaluated on a few experiments.",0.24489795918367346,0.20408163265306123,0.3673469387755102,0.11764705882352941,0.20588235294117646,0.22972972972972974,0.11764705882352941,0.13513513513513514,0.1267605633802817,0.16216216216216217,0.14788732394366197,0.11971830985915492,0.15894039735099338,0.16260162601626016,0.18848167539267016,0.13636363636363635,0.1721311475409836,0.1574074074074074
828,SP:b8cf416b2bb93f8d0cb03e65783a4e6c242e4f6f,"This paper introduces iterative decoding to improve the compositional generalization ability of seq2seq models. Iterative decoding predicts a series of intermediate outputs from an input, and then adapting these outputs into intermediate inputs that are fed back to the model until a sequence containing an end-of-iteration token is predicted. This approach explicitly encourage the model to learn to unfold the compositional generalization procedure. Experiments on the PCFG and cartesian product datasets show that iterative decoding can improve the compositional generalization ability of seq2seq models. However, additional experiments also show that iterative decoding may fail in more general datasets that do not have explicit intermediate steps defined.","This paper proposes a decomposed training target for transformers to improve compositional generalization. Specifically, a complex task is decomposed into sequential subtasks by external knowledge to exposure the procedure of composition. Then, they train transformers to reproduce these intermediate steps and perform iterative decoding during prediction. Besides, they adopt relative attention and copy decoders to improve the performance of each step, resulting in overall improvements. They achieve strong performance on PCFG and Cartesian product datasets and report negative results on CFQ dataset.","This paper studies compositional generalization in the context of Transformer models. Noting that ""one-shot"" sequence-to-sequence learning with Transformer models often results in models that do not generalize compositionally, the authors propose an iterative scheme whereby intermediate input-output pairs are constructed for each task (how this is constructed varies depends on the task), and the decoding proceeds as a series of seq2seq tasks. The approach is found to perform well on synthetic tasks (PCFG, Cartesian product) where there is a ""natural"" way to construct the intermediate input-output pairs, but fails to perform well on more real-world tasks such as CFQ.","This paper presents a type of “task re-formulation” that re-arranges the labels in task such that they are easier for transformer-based encoder-decoder models. This technique is specifically meant to address compositional generalization, which roughly means to generalize to instances longer than seen during training, with combinations of tokens not seen during training, and so on (further clarified in the paper). The paper presents interesting results on three tasks (PCFG, Cartesian Product, and CFQ). I find the problem of compositional generalization with transformers to be very important for the ML community, and that the results are mostly useful. Nonetheless, there is a substantial amount of missing related work, and the writing could greatly be improved.",0.18518518518518517,0.19444444444444445,0.17592592592592593,0.2073170731707317,0.21951219512195122,0.20952380952380953,0.24390243902439024,0.2,0.16101694915254236,0.1619047619047619,0.15254237288135594,0.1864406779661017,0.21052631578947367,0.19718309859154928,0.16814159292035397,0.18181818181818182,0.18000000000000002,0.19730941704035876
829,SP:b8db583b9ff9dcc21b284e2548f2b6bf0551cb80,"This paper introduces a method to quantify closeness in terms of  information divergence between a distribution of text sampled (generated) from a language model to human written text. These types of measures are useful in evaluating language models in open text generation tasks, but also could be extended for automatic detection of machine generated text.     The metric that the paper introduces is ""Mauve"", theoretically it is the AOC  of a plot composed of the fwd KL divergence $KL(P||R_\lambda)$ and  $KL(Q||R_\lambda)$ where $P$ is the model distribution , $Q$ is the human text distribution and $R_\lambda$ is the a mixture distribution of both (using different values of a mixing parameter $\lambda$).  Unlike different information divergence metrics  (e.g. vanilla KL or Jensen–Shannon) Mauve captures more general information about the divergence between the distributions. By analogy: in standard discrimination tasks ROC can capture the tradeoff between precision (Type 1 errors) and recall (type 2 errors), Similarly Mauve could capture Type 1 and Type 2 divergences than the human text distribution.  This is interesting as for example, due to its capacity of capturing type 2 errors, it can inherently asses the diversity of the generated text, covering information that can be captured by the self-bleu metric.    Now Approximating such KL values in the Mauve metrics is challenging in practice. Even when relying on Montecarlo estimation of to sample from $x \sim P(x)$  , Most of the sampled x from the model will not exist in the large dataset of human written text Q(x) therefore leading to inf values of KL divergence.   Instead authors propose to convert the P,Q from distribution over a high dimensional space (seq of text) to a multinomial distribution. This is done by converting discrete text into continuous embedding then binning them into 1 of k clusters. While this transformation doesn't guarantee unbiasedness in the KL estimations it allows tractability of calculations.  ","This paper presents a framework for evaluating open-ended text generation models: that is, evaluating how closely they match the distribution of human text (P). The proposed method (Mauve) evaluates for both Type I errors (false positives) and Type II errors (false negatives).  Mauve measures the divergence between P and Q by looking at the Pareto frontier of distributions R that are close to P and Q: that is, looking at the curve KL(P|R), KL(Q|R), which is optimized by R being an interpolation of P and Q.  This curve cannot be computed exactly since the true text distribution P is unknown. Mauve therefore takes samples, embeds them, clusters them, and approximates P and Q as multinomial distributions over the clusters, allowing us to interpolate their distributions and take KL divergences.  Results show that this metric (1) has some desirable properties like rating long text as worse (since it often is in these settings) and reproducing human judgments about different sampling schemes; (2) is robust and works with different hyperparameters; (3) correlates well with human judgments.","This paper proposes an automated metric named Mauve for open-ended text generation to measure the gap between model-generated text and human text. This metric quantifies the divergence between the generated text distribution and human text distribution. It computes this by plotting the area under the divergence curve using KL-divergence. Given samples of generated text and human text, the authors propose to embed them using a pre-trained language model (GPT-2), assign the embeddings into clusters to finally compute the categorical distribution probabilities, which are then used to compute points in the divergence plot, the area under the curve produces the scalar value of the metric. The authors show that Mauve fares well in terms of capturing known properties of generated text like text length, model size, and decoding strategy and correlates well with human judgment. They also show that Mauve is robust to its various design choices like quantization, embeddings, etc.","This paper proposes comparison metrics for open-ended text generation. The method, named Mauve, compares the model’s distribution against the distribution of human-written text.  There are two types of errors identified and established to motivate the metric. The main idea of the metric is to measure the mutual distance from P to Q and Q to P. Due to the computational tractability issue, an approximated version based on quantized discrete version of the measure is proposed. Experiments show that Mauve agrees well with human judgment compared to competitor methods.  ",0.1238390092879257,0.11455108359133127,0.09907120743034056,0.20670391061452514,0.17318435754189945,0.2129032258064516,0.22346368715083798,0.23870967741935484,0.3516483516483517,0.23870967741935484,0.34065934065934067,0.3626373626373626,0.1593625498007968,0.15481171548117156,0.15458937198067635,0.22155688622754494,0.22962962962962966,0.26829268292682923
830,SP:b97636e8a5ecee97ba23fc3c2cf27609e9928ed8,"The paper proposes Tensor Language (TL), a language with which popular GNN models can be uniformly studied, so as to yield insights about their expressiveness and separation power. More specifically, the paper shows how TL corresponds to the WL hierarchy, and shows that summation depth in TL, as well as the number of index variables, correspond directly to iterations and tuple size for k-WL, respectively. Moreover, the paper establishes a direct correspondence between GNN update equations and expressions in TL, thus offering a simpler means to understand the expressive and separation power of GNNs, even considering their number of layers, by mapping their architectures to expressions in TL. TL , as well as its guarded fragment GTL, are then used to re-establish a set of known GNN results from the literature.   Beyond expressive power, TL is also used to quantify the function approximation power of GNNs, and also shows that GNNs, characterized by a TL fragment, can learn functions with separation power upper-bounded by a refinement algorithm corresponding to this fragment. Finally, the TL framework is used to establish new results. In particular, it shows that k-IGN cannot achieve expressiveness beyond (k-1)-WL, as this model corresponds to tk iterations of (k-1)-WL, and also offers insights as to the expressiveness of k-IGN with a polynomial number of layers: Indeed, model power does not increase with more standard layers, e.g., those relying on the adjacency matrix, but rather, any increase in layers can only improve expressiveness by deriving GNN functions with e.g., increased treewidth.","The paper proposed a novel way of evaluating the expressive power of any arbitrary GNNs in terms of WL test order using tensor language. The main motivation of this paper is that there is no straight forward expressive power evaluation tool so far. All existing methods need architecture specific proofs. Main challenge is the translating given GNN into tensor language. Later, they easily parametrize given GNN by two numbers which are the number of indices of tensor language ($k$ ) and number of summation depth ($t$). Then, they claimed that given GNN's expressive power is equal to $k-1$-WL test when it applies $t$ coloring rounds.","This paper defined formal languages for describing tensor expressions. It showed that their expressive power is identical to (suitably parameterized) color refinement algorithms or (vertex/graph) WL algorithms. Then, by translating existing GNNs into the tensor languages, this paper provided upper bounds for the expressive power of GNNs, which recovered several existing results. In addition, this paper characterized the closure of a function class by functions whose expressive power is equal to or less than the functions (Theorem 6.1). As an application, this paper characterized the expressive power of several GNNs via the tensor language (Corollary 6.2, Proposition E.3).","This paper introduces a new approach to study the separation power and approximation properties of graph neural networks (GNN). The authors introduce the Tensor Languages (TL) and show the representation of GNN as TL expressions. In Section 4, they show the separation power of TL and relate it to color refinement algorithms like k-WL. In particular, they are able to characterize the separation power of such algorithms after a given number of iterations. These results allows them to compute in Section 5 the separation power of GNNs. They are able to recover all results in the literature in an unified way and prove some new results. In Section 6, they give consequences of their results in term of approximation of GNNs, recovering known results and proving new ones. This is a theoretical paper without any experimental results.",0.09541984732824428,0.10687022900763359,0.1450381679389313,0.16822429906542055,0.16822429906542055,0.21568627450980393,0.2336448598130841,0.27450980392156865,0.2753623188405797,0.17647058823529413,0.13043478260869565,0.15942028985507245,0.13550135501355015,0.15384615384615385,0.19,0.1722488038277512,0.14693877551020407,0.18333333333333332
831,SP:b9a5a096c7fe11e2c1806c756664de0f84eaf67c,"The paper considers the problem of online learning the Stackelberg Equilibrium in general-sum games. The authors identify a fundamental gap between the exact and the estimated version of the Stackelberg value. Then they consider three general-sum games setting: bandit games, bandit-RL games, and linear bandit games, designing algorithms for these settings and providing sample complexity results.","The paper considers learning Stackelberg equilibria in unknown games with noisy bandit feedback of the rewards obtained by leader and follower, respectively. The authors prove an information theoretic gap between the value of the true Stackelberg equilibrium and the one learned from finitely many data points. However, they show that approximate Stackelberg equilibria (defined according to a $\epsilon$-best response of the follower) can be learned efficiently using empirical reward estimates. Upper and matching lower bounds are provided on the number of required samples. Results are instantiated for bandit games, bandit reinforcement learning games, and linear bandit games.","This paper provides sample complexity results and analysis of algorithms that learn Stackelberg equilibria for three types of games: general bandit games, bandit-RL games, and linear bandit games. The novelty of this paper lies in the fact that it studies a different solution concept, Stackelberg game, in general-sum games without assumptions regarding the convexity or concavity of the reward functions. To the best of my knowledge, this is the first paper that studies the sample complexity for learning Stackelberg equilibria under such games.","The authors study the problem of learning Stackelberg equilibria from random samples in general sum games. The work will be of interest to both theoreticians of Stackelberg games and those who work on applications. The main contributions are: - In bandit games, which are two-step Markov games where the leader and follower only observe their own rewards, the authors show that there is a constant gap between the optimal and the best strategy the leader can attain with a finite number of samples. This gap is a central feature of the paper—it exists in all settings that are analyzed by the authors, and the sample complexity results give you performance that approaches the best possible up to the gap. The authors give a tight bound on the number of samples required which is linear in the size of the action sets of the leader and follower and quadratic in the approximation ratio relative to the gap. The algorithm queries each leader action a fixed number of times (which is the same across all actions) to construct an estimate of the follower's best response function that has the desired properties. - For bandit-RL games, where the leader selects an action that induces an MDP for the follower, the authors show that a quite similar algorithm can be applied. The main challenge is that the follower has to learn the optimal policy for each MDP (this is an RL setting with bandit feedback to the follower), and this requires an application of some recent work to yield appropriate estimates. The bandit-RL game setting reduces to bandit games when the horizon is of length 1. The derived sample complexity matches that of the bandit game for the horizon = 1 subcase, showing that the dependence on these parameters is tight. However, the scaling in horizon length is extreme—more than quintic. There is no known lower bound for the dependence on horizon. - In linear bandit games, which are bandit games, but where the rewards are a linear function of the features of each action. The algorithm the authors state for this setting scales quadratically in the dimension of the feature space and quadratically in the approximation ratio. There is a factor of the dimension between this and the lower bound they derive.",0.4067796610169492,0.288135593220339,0.5254237288135594,0.19387755102040816,0.3877551020408163,0.3764705882352941,0.24489795918367346,0.2,0.08136482939632546,0.2235294117647059,0.09973753280839895,0.08398950131233596,0.3057324840764331,0.23611111111111113,0.1409090909090909,0.20765027322404372,0.15866388308977036,0.13733905579399142
832,SP:b9f4d0da5aecbdc05d229d477eeb737c394c3393,"The paper studies the question of ensemble function diversity and how to preserve it on ensemble distillation to a single model. The authors use input perturbations that make the models, especially well trained models that are near 0 loss on the training set, disagree on the input, and then use knowledge distillation from individual members of the ensemble to individual subnetworks of the distilled model. The authors show that without this approach, the distilled models suffer from lack of function diversity, and that their approach enhances it and in accordance with their theory enhances performance as well.","This paper attempts to reduce the performance gap between an ensemble and a model that was distilled from it. The main idea is that, when distilling, the model should “absorb as much function diversity inside the ensemble as possible”. To do that, the paper proposes using ODS perturbation strategy, that samples diverse predictions for a given input form the ensemble, to use as label during distillation. ","This paper focuses on the topic of effectively distilling ensembles of models into a single model. Assuming that the distilled model should inherit the diversity of the ensemble, they attempt to make the distilled performance more comparable to the ensemble by using a (adversarial) perturbation strategy that directly identifies sources of diversity within the ensemble (i.e., where models within the ensemble disagree with each other). The authors demonstrate empirically that utilizing such a diversity-inducing objective within distillation improves the final performance of the distilled model in comparison to the ensemble on image classification experiments. They motivate this objective for distillation with intuitive descriptions of the importance of diversity within an ensemble, as well as by making connections to gradient matching between the student and teacher. ","This paper proposes to improve Knowledge Distillation (KD) for ensemble models. The authors apply ODS [Tashiro et.al, 2020] to each teacher model in the ensemble to obtain perturbed data. Thereafter, the perturbed data is used to train student models, each corresponding to one teacher model. The authors argue that ODS can improve the diversity among student models in the ensemble.",0.17525773195876287,0.26804123711340205,0.16494845360824742,0.3333333333333333,0.19696969696969696,0.14960629921259844,0.25757575757575757,0.2047244094488189,0.26229508196721313,0.1732283464566929,0.21311475409836064,0.3114754098360656,0.2085889570552147,0.23214285714285712,0.20253164556962025,0.22797927461139897,0.20472440944881892,0.2021276595744681
833,SP:ba70bbbf64ebd585b3fb6084860089397eead137,"The paper tackles the problem of cardinality-based decomposable submodular function minimization (Card-DSFM). DSFMs have been a popular topic of research, and the authors claim that the case when every component function is a cardinality based concave function is an important one in practical applications. For this subproblem, they derive the first approximation algorithms via reduction to sparse graph cuts.  Given a concave cardinality function, the aim is to model it via gadgets (directed graphs on which we will solve a max-flow instance). The authors show that the problem of finding such gadgets is equivalent to finding piecewise linear functions which can approximate the original function with minimum number of linear components. This problem is solved using a greedy method, which gives a better bound on the number of pieces than existing methods.  Once these are optimally computed, they are converted into equivalent gadgets. Once all gadgets are combined into a graph, a max-flow instance is run. The obtained minimum cut also gives a solution to the underlying Card-DSFM.","This paper proposes a fast approximate method for minimizing decomposable submodular function where each component is cardinality-based. The runtime is improved by exploiting sparse piecewise linear approximation to concave functions at integer points, which is combined with graph reduction to convert the original minimization problem as an s-t cut problem on a sparse graph. Empirical results show that the mehtod can be faster than state-of-the-art numerical methods. Moreover, the approximation ratios are generally much smaller than the worst-case approximation guarantee.","This work studies minimizing decomposable submodular functions, which are of the form f(S) = \sum_{e \in E}f_e(S \cap e), where each f_e is a ``simple'' submodular function. In many practical applications, f_e(A) is cardinality based, that is, f_e(A) = g_e(|A|) for some concave function g_e. Several discrete/continuous algorithms have been proposed to minimize such decomposable submodular functions. One combinatorial approach to tackle this problem is to introduce a gadget graph G_e so that, for any A, g_e(|A|) is equal to the minimum cut of G_e subject to A (in addition to some special vertex) being contained in one side of the cut. It is known that any cardinality-based submodular function can be represented this way as a gadget graph of O(|e|) vertices and O(|e|^2) edges.  The main contribution of this work is providing a method for constructing a gadget graph that ``approximates'' cardinality-based submodular function instead of exactly simulating it. Specifically, it is shown that, to obtain a multiplicative error of 1 \pm epsilon, we only need O(epsilon^{-1} |e| log |e|) edges. A lower bound of O(epsilon^{-1/2} |e| log |e|) is also shown. When g(x) = x(k-x), which often appears in practice, it is shown that the number of edges can be further improved to O(\epsilon^{-1/2}log log epsilon^{-1} \cdot |e|). These results are obtained by approximating the concave function g_e with a piecewise linear function l and then representing l as a gadget graph.  Empirical results on image segmentation tasks show that the proposed approximate reduction outputs a much smaller gadget graph compared to the exact reduction without much sacrificing the quality of the solution. For example, for some choice of the parameters, the size of the output graph becomes 0.013 (1/0.013?) times smaller while the multiplicative error is merely 1 \pm 4*10^-3. It is then confirmed that the trade-off of the approximation ratio and runtime of the proposed method is much better than those of other methods. It is also empirically investigated that the quality of hypergraph clustering does not deteriorate much by replacing the objective submodular functions with gadget graphs obtained by the proposed approximate reduction.","This paper presents an algorithm for minimizing submodular functions that are the sum of cardinality-based components. This is a special case of decomposable submodular function minimization. Formally, the problem considered is to minimize $f(S) = \sum\limits_{e\in E} g_e(|S \cap e|)$, where $E\subseteq 2^V$ and $g_e$ are 1-dimensional concave functions. One instance of such functions are the region potentials used in image segmentation, where $g_e(x) = c_e \cdot x (|e| - x)$ for some constant $c_e \geq 0$. Such minimization problems are also considered in hypergraph clustering applications.  The authors propose an approximation algorithm that works by approximately reducing their problem to a graph min cut problem. The fact that concave functions can be _exactly_ represented by graphs is known, and the resulting number of edges in the graph is $O\left(\sum\limits_{e\in E} |e|^2\right)$. The main result of the paper is to show how to reduce the number of edges by allowing _approximate_ reductions, leading to a graph with $\widetilde{O}\left(\frac{1}{\epsilon} \sum\limits_{e\in E} |e|\right)$ edges while incurring an $(1+\epsilon)$-multiplicative error in the objective.  The idea is to simplify each concave function before converting it to a graph. Viewing each concave function as a piecewise linear function, the size of the resulting graph depends on the number of pieces. The authors present an algorithm that is able to find the _best_ (in terms of the number of pieces) $(1+\epsilon)$-approximation to this function. By known results, the optimal number of pieces is $\widetilde{O}\left(\frac{1}{\epsilon}\right)$, thus follows the $\widetilde{O}\left(\frac{1}{\epsilon} \sum\limits_{e\in E} |e|\right)$ bound on the number of edges.  For the special case of the region potentials $g_e(x) = c_e \cdot x (|e| - x)$, the authors are able to get a better bound of $\widetilde{O}\left(\frac{1}{\sqrt{\epsilon}} \sum\limits_{e\in E} |e|\right)$ on the number of edges.  The theoretical results are accompanied by experimental results in the problems of image segmentation and hypergraph clustering. For image segmentation, the results show a speedup of at least 3x in 3 out of 4 experiments compared to previous methods, while it is slower in 1 of the 4 experiments. For hypergraph clustering, the F1 score is improved by 3 percentage points at the expense of a 30% runtime increase.",0.14450867052023122,0.23699421965317918,0.27167630057803466,0.36046511627906974,0.32558139534883723,0.24484536082474226,0.29069767441860467,0.1056701030927835,0.11380145278450363,0.07989690721649484,0.06779661016949153,0.23002421307506055,0.19305019305019305,0.1461675579322638,0.16040955631399315,0.13080168776371306,0.11222444889779559,0.23720349563046195
834,SP:ba752741818669b8202d934116b7e91f1ef0d13a,"This paper proposes a new framework for conducting approximate Bayesian inference in the context of function-space optimization. The method described is fairly general and with good theoretical justifications to some of the most important claims here. To achieve their goal, the authors define a new approximate objective function with an alternative divergence to the regular KL  and propose the SPGs as a generalization over previous work. Even though there is some important information missing in the experimentation phase, the performance of the method seems overall much better than previous state-of-the-art models in many contexts. This needs to be addressed since it constitutes an important issue, but the method is promising and interesting.    ","This paper worked on the approximation for the posterior distribution, especially when we perform Bayesian inference in the space of functions. The authors provided a new Kullback Leibler divergence for stochastic processes. The number of the measurement points is a random variable. This new KL divergence is well defined for stochastic processes which are generated from different network architectures. Based on the new KL divergence, the authors developed a scalable variational inference using the VAE to generate approximate posterior stochastic processes.","The authors propose ""Functional Variational Inference"" (FVI), a novel variational approach to approximate Bayesian inference in function space. The approach is based on the ""grid-functional KL divergence"", a novel form of KL divergence between stochastic processes. The authors show that this is a proper KL-divergence and that it is well-defined in cases where previous work (fBNN) is not.  Furthermore, the authors propose ""Stochastic Process Generators"" (SPGs), a novel family of variational distributions which are more flexible in comparison to distributions used in previous work (VIP). Using SPGs, the authors derive an efficient approximation of the ELBO corresponding to the grid-functional KL divergence. The authors demonstrate that this approximation scales better than the one used in fBNN.   The authors evaluate FVI on a range of experiments (interpolation with implicit priors, regression and classification with BNN priors, Thompson sampling). FVI tends to perform favourably in comparison to fBNN, VIP, and a range of other existing methods.","The paper presents an approach to probabilistic regression. The posterior distribution is estimated using a variational approach. A variational class of stochastic processes is presented, which is parameterized by neural networks - amongst other things by a product of experts variational autoencoder. For optimizing the variational distribution a variational bound and its sample approximation is derived. The proposed approach is evaluated on regression, classification and reinforcement learning tasks.",0.1896551724137931,0.25,0.10344827586206896,0.345679012345679,0.1728395061728395,0.13291139240506328,0.2716049382716049,0.18354430379746836,0.1791044776119403,0.17721518987341772,0.208955223880597,0.31343283582089554,0.2233502538071066,0.21167883211678834,0.13114754098360656,0.23430962343096232,0.18918918918918917,0.18666666666666665
835,SP:baca4cd5f93ae57d3907b61f1ae64a139011b382,"This paper proposed FedLite, which extends a specific federated learning setting called split-learning to be more transmission efficient by a vector quantization scheme. In split federated learning, the classification layer that has a larger parameter set is saved and learned in the server, while the client learns the relatively lightweight feature layers. In FedLite, the activation outputs of the cut layer are compressed by vector quantization before sending to the coordinate server. To alleviate the issue of noisy gradients caused by compressed activations, a regularization term is introduced into the gradient update step which is derived from the Taylor expansion. Empirical results show the effectiveness of the proposed approach, which largely reduces transmission costs without significant performance loss. ","This paper introduces FedLite, a compression method built on top of Split Learning (SL) for better communication efficiency. Under the cases where the size of the intermediate output is large, SL can introduce a communication bottleneck in model training. FedLite tackles that issue via compressing the intermediate output via a vector quantization-based method. A gradient correction scheme is proposed to further enhance the proposed compression method. Convergence analysis and experimental results are provided to justify the effectiveness of FedLite.","The authors propose a method to compress the intermediate activations of a neural network that is split across a client and server in the Federated Learning setting. In this 'Split Learning' scenario, transmitting activations and corresponding gradients incur significant communication costs. The authors discuss FedLITE, a method to compress activations during the forward pass and to correct the gradient to account for the quantization during in the forward pass. Their paper is augmented with an interpretation of the gradient correction term as minimising a surrogate loss function, as well as a convergence analysis of their approach and empirical evaluations. ",The authors propose end to end training framework that relies on a novel vector quantization.  The authors claim the following contributions: - End-to-end communication-efficient neural network splitting-based FL approach for resource-constrained clients. - New gradient correction scheme for backward pass. - Reducing communication rounds up to 490X without compromising the accuracy. - Providing ,0.18487394957983194,0.16806722689075632,0.07563025210084033,0.2,0.15,0.1414141414141414,0.275,0.20202020202020202,0.16666666666666666,0.16161616161616163,0.2222222222222222,0.25925925925925924,0.22110552763819097,0.1834862385321101,0.10404624277456648,0.17877094972067042,0.1791044776119403,0.18300653594771243
836,SP:badff4be17c894123719306f03a159896b259c6e,"In this paper presents a new set of activation functions based on a relaxation of the logical operators AND, OR, XNOR. This is motivated by the behavior of the operators under the logit space equivalence.  Moreover, the authors present approximations that are computationally more efficient based on simple max, min and addition operations. As these binary operators are reduction operations, i.e., they convert two inputs into one, the authors note the similarity to the MaxOut and provide different alternatives on how to adapt the architectures to use the boolean activation functions. Finally, the authors provide empirical results showing how the different potential combinations of logical activation functions behave for different datasets and architectures.","This paper introduces three novel activation functions for neural networks, by modifying the logit-space versions of AND, OR, and XNOR in order to only use addition and comparison operations.  A large number of experiments show that in certain architectures and on certain tasks, some combinations of these activation functions deliver good performance.  While the results are somewhat interesting, I find the activation functions under-motivated (e.g. the motivations assume independence of input features) and worry that many of the experimental settings are designed to favor their new functions.  Similarly, the breadth of experiments means that each one is relatively under-reported in its details.  It would be helpful to have a more developed theory of these activation functions and more experiments in settings that are more standard.","The paper develops and presents approximations of three Boolean logic based activation functions: AND, OR, XNOR. These activation functions are based on the principle that neurons encode logits to represent presence of features in the log-odds space (logit-space). The formulations are simple and straightforward. Empirical evaluation is conducted on several applications using the presented activations (under individual and ensemble configuration), where ReLU, max, min (with adjustments) are the competitive baselines. ","The paper proposes new activation functions based on the approximation of AND, OR, XOR operations. In addition, ensembling strategies are proposed to combine features from these operators. The experiments on image classification, transfer learning, abstract reasoning, and zero-shot learning tasks show that the proposed activation functions perform reasonably well, especially OR for ResNet and an ensemble for zero-shot and transfer learning.",0.21929824561403508,0.17543859649122806,0.16666666666666666,0.15503875968992248,0.13178294573643412,0.19444444444444445,0.1937984496124031,0.2777777777777778,0.30158730158730157,0.2777777777777778,0.2698412698412698,0.2222222222222222,0.20576131687242796,0.2150537634408602,0.21468926553672316,0.19900497512437812,0.17708333333333334,0.2074074074074074
837,SP:bb29ce05c176e7137d121e119ebe456eec38bc48,"This paper studies the problem of scene flow: estimating 3D motion fields between given point clouds from a dynamic scene. The proposed method predicts scene flows by optimizing a neural network at execution time, using randomly initialized weights.   This approach of using a network architecture as a prior is shown to have better generalization performance for out-of-distribution samples compared to learning-based methods which require offline datasets. In contrast to another recently introduced non-learning approach which uses graph Laplacian priors, this method provides a continuous scene flow representation, and has better time and accuracy performances in large scale scenes. ","This paper studies scene-flow estimation. Following the idea of deep-image prior, it proposes an unsupervised way to learn scene flow. The results are competitive, or even better than supervised flow-estimation methods trained on very different training set. Moreover, it also out-performs existing optimization based unsupervised flow-estimation methods."," This paper highlights the fact that deep learning has caused a paradigm shift in many computer vision applications from methods based on optimization with strong priors to ones that optimize  feed-forward neural networks in an offline process. The paper argues that these feed-forward models are bad at generalization and proposes to use the network in a radically different way. In essence, the network are used as a parametrization for a traditional optimization solution - allowing the network's inductive biases to be used as a “prior”. While using a network structure as a prior for optimization is not new (ala Deep Image Prior, DeepSDF, NeRF etc.), applying this type of parametrization to the problem of optical flow is new. The results are competitive with trained approaches and quite impressive for such a simple solution.","This submission presents a method for estimating scene flow between two point clouds that uses a neural network as an implicit regularizer during optimization. The submission revisits the classical optimization-based approach where instead of training a neural net to estimate flow for a given scene, it optimizes the flow per scene and does not need to train the network beforehand. Instead, the network is “trained” to produce flow for the given scene, such that representing the flow with the network introduces some form of implicit smoothness regularization.",0.12745098039215685,0.18627450980392157,0.17647058823529413,0.25,0.21153846153846154,0.13333333333333333,0.25,0.14074074074074075,0.20454545454545456,0.0962962962962963,0.125,0.20454545454545456,0.16883116883116883,0.16033755274261605,0.18947368421052632,0.13903743315508021,0.15714285714285714,0.16143497757847533
838,SP:bb7e2735e797f6de95fa32b70a4a2226c1eb2e4d,"This paper explores how  deep feed-forward or convolutional networks perform image classification tasks. The authors consider the layers of a trained network as a sequence of embeddings, from which nearest-neighbour predictors can be computed from the training examples and their predicted class. This allows one to compute successive class predictions for a given example, as it gets processed from the input layer to the final softmax. They introduce the Prediction Depth of an example as the earliest layer where its final classification is reached and stays without change.   Experimenting with three network architectures and four image datasets, the authors suggest that the Prediction Depth of an example is a good estimator of its accuracy, learning difficulty, and confidence (in term of distance both from the class separator and closest adversarial examples). They also propose that it is consistent, for a given data point, over different trained models and (to a lower degree) architectures.  Finally, they show how their findings relate to known properties of neural networks, and define a new typology of difficult examples in image classification.   ","The authors propose a new characterization of the difficulty of an example during training and testing of deep learning models: the layer at which an example is learned (which they call ""prediction depth""). They show that early layers are enough to learn easy examples, and that difficult examples are primarily learned by later layers. They also connect the prediction depth to two established notions of example difficulty from the literature: the consistency score and the iteration learned (closely related to the example forgetting statistic).  ","Intuitively, some examples are ""harder"" to learn than others.  Currently, there are two ways of measuring ""hardness"": the ""statistical view"", in which an example is hard if its classification is sensitive to small changes to the training set; and the ""learning view"", in which an example is hard if its label is learned only at the end of the optimization process.  This paper proposes a new ""computational view,"" in which an example is considered hard if many layers of processing are required to produce the prediction.   Computational hardness is formalized through a metric called ""prediction depth,"" which is the depth at which the prediction of a KNN classifier operating on the intermediate hidden representations starts to agree with the network's ultimate prediction.  For instance, consider a 20-layer trained network and a training example for which the network predicts ""airplane.""  Suppose that a KNN classifier operating on the first-layer hidden representations predicts this example as ""dog,"" and a KNN  classifier operating on the second-layer hidden representations predicts this example as ""cat"", while KNN classifiers on layers 3 through 20 predict this example as ""airplane.""  Then the prediction depth is 3.  The intent is: the higher the prediction depth, the harder the example.  The paper first confirms that prediction depth is robust across random seeds and relatively robust across architectures, and that visually 'easy' images have low prediction depths and visually ""hard"" images have high prediction depths.  Then the paper demonstrates that the ""computational hardness"" of prediction depth is correlated with the ""statistical hardness"" of sensitivity: examples with low prediction depths tend to be classified the same regardless of perturbations to the training dataset (e.g. bootstrap resampling), whereas examples with high prediction depths are not robust to changes in the training dataset.  Then the paper demonstrates that the ""computational hardness"" of prediction depth is correlated with the pre-existing notion of ""learning speed"" hardness: examples with low prediction depths tend to be learned earlier in training, whereas examples with high prediction depths are not learned until late in training.  Then the paper demonstrates that examples with low prediction depths have lower 'output margins' (the difference in logit between the predicted class and the other classes) as well as lower 'input margins' (the L2 distance in input space to the nearest adversarial perturbation).    ","This paper introduces the prediction depth (PD) as a measure of example difficulty in deep networks. They then conduct an empirical study on several vision tasks of the relationship between PD and some other quantities:  - difficulty of an example, as (qualitatively) measured by a human  - accuracy and consistency  - difficulty of learning an example (i.e. an example is difficult to learn if it is consistently correctly classified after a large number of iterations)  - margin and conclude that PD is useful for predicting these quantities",0.1340782122905028,0.2346368715083799,0.1340782122905028,0.38095238095238093,0.21428571428571427,0.05194805194805195,0.2857142857142857,0.10909090909090909,0.2857142857142857,0.08311688311688312,0.21428571428571427,0.23809523809523808,0.1825095057034221,0.14893617021276595,0.1825095057034221,0.13646055437100213,0.21428571428571427,0.08528784648187633
839,SP:bb9f3b337b8cd938bd748aeb9c8d1b71214285b7,"The paper proposes dealing with tree search planning in continuous action spaces by learning affordances. The authors propose an architecture composed of several modules, where they are able to plan ahead in a tree search manner by learning a module that expands K discrete possible affordance from each state node. The results suggest that the proposed technique is able to deal with difficult tasks in which standard tree search methods would not be easy to apply.",This paper looks incorporating the concept of affordances from ecological psychology into the planning process.  The basic premise is that affordances represent the possible relevant actions available to an agent that are potentially moderated by their goals and the state of the world. This has the potential to reduce the size of planning search trees.  In this paper the authors claim two contributions (1) the incorporation of affordance consideration into the planning process and (2) an approach for discovering affordances which the authors have called GrASP as it is a gradient based approach.   The paper implements the approach and tests it experimentally on a range of domains from the DeepMind control suite.  ,"This paper proposes a gradient-based method for selecting affordances in planning. Specifically, it includes an affordance module that maps state representations to continuous actions/options. The planning procedure is aided by a learned model with an encoding, dynamics, reward, and value module. Most prior work uses the gradients through the trajectory under the learned model to update the policy/actions, whereas this work is about using them to update the collection of K potential actions to select from.","The paper proposes a gradient-based affordance selection, i.e., action/option selection, in addition to the value equivalent modules for tree-expansion procedure(s) in planning. The claim is that GrASP can learn primitive-action and option selection and plan in a continuous state and action space to outperform model-free RL. The affordance module maps the state to K actions or options from continuous space. These affordances are later used for expanding the look-ahead search tree. The key idea is to compute the gradient of performance loss with respect to the affordance model's parameters through the planner. The method uses two expansion procedures: (1) shallow-depth complete trees, (2) UCT-based MCTS. The performance of TD3 is compared to different variants of the proposed algorithm on some tasks from the DM Control Suite.",0.17105263157894737,0.17105263157894737,0.23684210526315788,0.16071428571428573,0.24107142857142858,0.34177215189873417,0.11607142857142858,0.16455696202531644,0.13138686131386862,0.22784810126582278,0.19708029197080293,0.19708029197080293,0.13829787234042554,0.16774193548387098,0.16901408450704225,0.1884816753926702,0.21686746987951808,0.25
840,SP:bbab321537389d10abe5a4601e168c0d3e4252ce,"This paper proposed a new debiased model-agnostic meta-reinforcement learning algorithm. The main improvement is that the paper considers a more accurate setting where one does not have accurate estimate on the gradient for each task, which is assumed by previous works. One has to estimate the gradient from samples in each task, and the samples themselves depend on the global parameter $\theta$, there will be an extra term for the global gradient.  The authors hence write the exact gradient and show the convergence. Numerical results show good performance.","This paper deals with the problem of few-shot meta learning, i.e. where one explicitly optimizes for the performance of doing one (or more) steps of SGD on a new task. This was intially introduced through the MAML method. However, the authors argue that MAML only discusses gradient descent while in practise, approximations lead to doing stochastic gradient desecent instead, thus making the original formulation of the problem biased. To that end, this paper proposes an SGD based meta learning objective, which ensures simpler convergence analysis as well as better performance in two benchmark problems from the RL setting. ",This paper aims to tackle the non-diminishing bias in MAMRL (MAML for RL) due to the stochastic policy gradient update for subtasks. Such bias hinders the first-order optimality when solving MAML with gradient descent. The authors compute the exact gradient of the MAMRL objective and identify the additional term that arises in RL. The authors then estimate such a gradient with an unbiased approximation based on samples and propose SG-MRL that utilizes such approximation to solve MAMRL. The authors show that SG-MRL converges to first-order stationary points under the standard assumptions and further conduct experiments on SG-MRL against baselines in the MuJoCo environment.,"This paper studies the Model-Agnostic Meta-Reinforcement-Learning (MAMRL) from a theoretical perspective. In particular, it proposes a modified MAMRL objective along with its unbiased gradient estimator, establishing a new Meta-RL algorithm called SR-MRL. It theoreticallly proves the convergence of SR-MRL and experimentally shows its comparable performance to the original MAML algorithm.",0.17777777777777778,0.2,0.14444444444444443,0.17,0.12,0.11926605504587157,0.16,0.1651376146788991,0.23214285714285715,0.1559633027522936,0.21428571428571427,0.23214285714285715,0.16842105263157894,0.18090452261306533,0.1780821917808219,0.1626794258373206,0.15384615384615383,0.15757575757575756
841,SP:bbe804b5e42bec11f3dcfc3cb55d8097d081f71d,"This paper introduces a method for novel view synthesis of a dynamic scene with a person in varying facial expression. To enable the controllability of facial expression, a parametric 3D face model is combined with existing neural radiance field-based approaches. To overcome background artifacts (e.g., distortion) while animating, the spatial face prior is added in a form of binary mask. The method is evaluated on four customized videos.  ",This paper introduces a method for creating animatable portraits within a static scene based on dynamic neural radiance fields.  The main idea is to fit a 3DMM-like model to a set of portrait frames and use it to rasterize a binary mask which is used to enforce distanglement between the foreground and background when training a dynamic NERF. Both qualitative and quantitative results seem to be convincing.," This paper proposes a method that marries parametric face models and neural radiance fields to achieve view synthesis of a portrait that provides expression control. Another view of this work is extending Park et al.'s Nerfies to support expression control and driving.   The paper makes three contributions: Firstly, it proposes a dynamic NeRF model that supports certain explicit control. Secondly, the proposed method disentangles the facial expression from the appearance of the background with a simple masking scheme dubbed as spatial ray sampling prior. Finally, the method enables the simultaneous control of viewpoints and facial expressions.","The submitted paper focuses on novel view synthesis from portrait video with controllable facial expressions. The authors separate the modeling of the static background from the dynamic foreground by segmenting out the faces (using landmark detection and fitting). The faces are modeled with Nerfies with expression parameters as latent codes, while the static background is essentially modeled with a standard NeRF without conditioning on per-frame expressions. The proposed method was evaluated on 4 subjects with data collected from a phone. The authors also demonstrate an application of the proposed method where videos can be synthesized with consistent facial expressions as a driving video as input.  ",0.24285714285714285,0.2571428571428571,0.2571428571428571,0.25,0.16176470588235295,0.20618556701030927,0.25,0.18556701030927836,0.16981132075471697,0.17525773195876287,0.10377358490566038,0.18867924528301888,0.24637681159420288,0.2155688622754491,0.20454545454545453,0.20606060606060606,0.12643678160919541,0.19704433497536947
842,SP:bc44045edebd2f61c2a3c44b2087f47e19b0afe4,"This paper mainly investigates how to perform architecture search to obtain the efficient transformer for auto-regressive language modeling. Compared with previous works, this searched model introduces two simple and effective modifications: squaring ReLU and depthwise convolution. Experimental results also demonstrate that the proposed model can achieve comparable results with fewer computations.",This work performs a neural architecture search to find an alternative to Transformer language models for efficient training and inference. Primer is a resulting architecture and its two main modifications are (1) squaring ReLU activations and (2) adding depthwise convolution to attention multi-head projections. Primer and Primer-EZ (applied these two modifications only) achieve better efficiency-accuracy trade-off while also following a scaling law like Transformer. ,"This paper studies how to improve the transformer architecture for the task of _autoregressive_ language modeling. To do so, they start from the transformer architecture and do low-level architecture search. They find an architecture, Primer, which provides better training efficiency / scaling laws over the original transformer. Most interestingly, they show that a simple variant of the transformer Primer-EZ, which simply adds a convolution layer and a squaring operation to the ReLU activations, can provide similar or better improvements than the full Primer. They test their implementation of Primer-EZ in multiple codebases and datasets, and show that the gains transfer.",The paper defines a new neural architecture search framework based on generating TensorFlow-based neural network programs. The paper discusses the search space and the evolutionary search-based searching method. The authors perform the search algorithm on a language modeling benchmark and show two specific modifications to the original Transformer model in the resulting model generalizes well to other tasks: squaring ReLU activation and adding a spatial depth-wise convolution after the QKV transformation of the multi-head attention layer.  ,0.2692307692307692,0.3076923076923077,0.25,0.22058823529411764,0.23529411764705882,0.1568627450980392,0.20588235294117646,0.1568627450980392,0.1625,0.14705882352941177,0.2,0.2,0.23333333333333334,0.2077922077922078,0.196969696969697,0.1764705882352941,0.2162162162162162,0.1758241758241758
843,SP:bc53eb482a446c44519ef7fb452094b34170db0a,"This paper suggests a new model of behaviour based on conditional-value at risk principle that can provide interesting alternative interpretations to the ""two step"" task used in many ""model-free"" vs. ""model-based"" RL studies. Although the scope of the framework is somewhat narrow (albeit with a variety of modifications to address time consistency), the paper provides a number of interesting explanations to the data that should be considered for the sake of accurate interpretation of computational model-based analyses. The analyses are also very detailed and interesting although I would have preferred to see more focus on empirical data as opposed to theoretical considerations. Overall a strong paper for NeurIPS.",The authors adopt a risk measurement to a well-studied two-step RL task and show that many humans are risk-averse. This provides an alternative explanation (risk aversion) for previously assumed stickiness or perseveration. The authors also discussed time consistency issues with their models and provided a few better alternative models and a better experimental design to test it. ,"The paper considers using CVaR in human behavior modeling of a two-stage task. In particular, the paper uses CVaR in modeling the choice probability in the two-stage decision-making setting. To work with the time-inconsistency of CVaR, the authors have tried three approaches, namely the fixed, precommitted, and nested CVaR. Using the choice model in analyzing a dataset, the authors show that some participants exhibit risk-averse behaviors.","This study proposes how we can integrate distributional RL and CVaR to account for seemingly irrational human behavior. CVaR is widely accepted in finance but not yet in other fields provides a new way to measure the level of subjective risk. Recent findings suggest that the parameter associated with CVaR may well capture the individual differences in risk sensitivity.  This study shows that the model combining the distributional RL and CVaR indeed explain better human choices in two-step task. Furthermore, this study provides a potential solution of how this method can be extended in multistep decisions in a more complicated task. While this model is better predicts the actual choices of human subjects, many parts of this model have not yet fully tested, as the author pointed out, because the data was acquired from too simple a two-step task. Despite this limitation, this might be appreciated by broad research fields (e.g. from investigating psychiatric diseases to designing robots or self-driving cars). I have few comments to make this study to be appreciated more by other filed of researchers.",0.11607142857142858,0.125,0.20535714285714285,0.15,0.2,0.23943661971830985,0.21666666666666667,0.19718309859154928,0.12637362637362637,0.1267605633802817,0.06593406593406594,0.09340659340659341,0.1511627906976744,0.15300546448087432,0.15646258503401358,0.13740458015267176,0.09917355371900828,0.1343873517786561
844,SP:bc59e792b3f87e7aae750523d4d353063f90202b,The paper proposes to approximate matrix multiplications (and convolutions) by randomly sampling column-row pairs and only multiplying submatrices. This is used to compute stochastic gradients which subsequently speed up training. Different sampling modes are presented and compared in experiments. Theoretical and practical training speed-ups are reported on standard architectures and datasets.,"This paper adapts the CRS method for approximate matrix multiplication into a new method, Bernoulli-CRS, that yields unbiased gradient estimates. The paper then analyzes this method on linear regression and shows the new method and giving unbiased estimates of a gradient for the original objective with a regularization term. They then devise a new routine for approximate backpropagation that has unbiased estimates with bounded second moments. The paper then introduces a deterministic variant called top-k sampling and show the performance of the 3 methods (CRS, Bernoulli-CRS, top-k) in experiments in terms of performance, compute, and speed.","This paper focuses on unbiased approximations to matrix multiplication and convolution computations in deep neural networks (DNNs). The aim is to reduce the complexity of these computations without the resulting function the network as a whole computes. As practically all of the computation in a deep network is expended in matrix multiplications and convolutions this would provide a global acceleration. In practice this work demonstrates 4-37% improvement in training speed on ImageNet, depending on how the computation is distributed.","This paper proposes a method for speeding up training by approximating the computation of linear and convolutional layers. The approximation of such operation is done by taking K samples of the operands (e.g. K row-column pairs in a matrix multiplication) instead of multiplying them directly. The authors motivate the use of the CRS algorithm because is lightweight, can be applied to matrices/tensors of all shapes and, it is unbiased. The authors then study its use in Linear Regression settings and Convolutional layers and, propose their own Bernoulli-based sampling strategy. The authors evaluate their approximate training scheme on common image classification tasks, including a distributed learning scenario.",0.20754716981132076,0.20754716981132076,0.2641509433962264,0.16,0.18,0.2125,0.11,0.1375,0.12727272727272726,0.2,0.16363636363636364,0.15454545454545454,0.1437908496732026,0.16541353383458648,0.17177914110429449,0.17777777777777778,0.17142857142857143,0.1789473684210526
845,SP:bc6046ca3c7557d29ee40836d330d5444f96a21e,"The paper attempts to bring clarification to In-Context Learning (ICL) by relating it to Bayesian Learning over mixtures of HMMs. Each component of the mixture generates examples of facts of the same conceptual type, such as ""Albert Einstein was German"", ""Marie Curie was Polish"", etc (here the conceptual type is ""nationality""). The first part of the paper addresses a theoretical question, namely whether the mixture model, on being given a prompt, that is, a prefix such as ""Albert Einstein was German. Marie Curie was Polish. Isaac Newton was "", is able to implicitly identify the relevant component of the mixture (i.e. ""nationality""), and consequently, whether it is able to correctly complete the prompt, here with ""British"". The second part of the paper consists in simulations, where the mixture model is used to produce data on which a neural language model (LSTM or Transformer) is trained, and on evaluations of how well the neural LM is able to perform the completions, under different configurations and ablations.","This paper studies the question of why few-shot prompting works for large language models (LM) such as GPT? The paper is mainly focused on understanding why prompting works given that concatenating multiple examples separated by a delimiter introduces out-of-distribution prompts that the LM would most likely not have seen during its pre-training. The paper shows that when the pre-training distribution is a mixture of HMMs, prompting works as a result of Bayesian inference. The authors show the validity of their theoretical results by training transformers and LSTM on synthetic datasets sampled from some HMMs that they devised.","The authors propose to study the phenomenon of ""in-context learning"" through the lens of Bayesian prediction. They introduce a framework in which the data generating distribution is given as a mixture of HMMs parameterized by a latent ""concept,"" and they show that under suitable conditions the posterior predictive distribution over a completion given a structured prompt asymptotically selects the ""concept"" behind this particular structure. They illustrate this framework and reproduce the phenomenon in a simplified language modeling example.","The submission studies the phenomenon of the ""in-context learning"" behavior of large language models (LLMs). Two results are presented: First, the submission argues that in-context learning can be formalized as Bayesian inference in a mixture of Hidden Markov Models (HMMs). Second, the submission introduces small-scale settings in which the in-context learning behavior of LLMs is reproduced empirically.",0.13253012048192772,0.12650602409638553,0.0963855421686747,0.14705882352941177,0.14705882352941177,0.21518987341772153,0.21568627450980393,0.26582278481012656,0.26229508196721313,0.189873417721519,0.2459016393442623,0.2786885245901639,0.16417910447761197,0.17142857142857137,0.1409691629955947,0.16574585635359115,0.18404907975460125,0.24285714285714283
846,SP:bc7c5da15e95f74e94d67ae5725a8eedc29edb56,"The authors consider the problem of reinforcement learning using high-dimensional observations (eg, images) that may contain both exogenous and endogenous state information. Seeking to remedy the issues with learning that arise due to the exogenous state information, the authors propose a new model called an Exogenous Block MDP (EX-BMDP) and a new algorithm called Predictive Path Elimination (PPE) to learn a generalization of the inverse dynamics of the EX-BMDP. Additionally, the authors present some experimental evidence that PPE performs well in the EX-BMDP setting compared to alternatives.","This paper introduces a model called the Exogenous Block MDP (EX-BMDP) where the latent state contains both controllable (endogenous) and uncontrollable (exogenous) elements. The paper proposes an algorithm to find a policy cover with sample complexity that depends only on the size of the endogenous state rather than the observation or exogenous state. The algorithm works by training a classifier to predict the actions that were taken to get to a state. Since the exogenous state is not affected by actions, states for which the classifier returns similar results are likely the same endogenous state. The algorithm builds up a set of policies (sequences of actions are sufficient in the near-deterministic setting) which visit unique endogenous states by using the classifier to deduplicate redundant action sequences. Besides proving the sample efficiency of their approach, experiments are provided which show that this algorithm performs better than baseline approaches in terms of performance as well as in ability to decode the state from its representation in a simple combination lock environment as well as a grid world with distractors.","This paper proposes a new class of decision problems, the exogenous block-MDP (EX-BMDP), and an algorithm for acquiring minimal state representations for EX-BMDPs. EX-BMDPs are an extension of block MDPs that allows for additional ""exogenous"" state components that may have arbitrary Markovian dynamics, but cannot be influenced by actions, including through the actions' effects on ""endogenous"" state. The proposed algorithm, PPE, is able to recover a mapping from observed states to endogenous states (i.e. ignoring the latent distractor variables) in the setting where the endogenous state dynamics and initial state distribution are near-deterministic. It does this by comparing the set of policies that can reach each pair of states: informally, if $p(\pi \mid s_1) = p(\pi \mid s_2)$ for all policies $\pi$ (assuming a uniform $p(\pi)$), then $s_1$ and $s_2$ are treated as equivalent. In the deterministic case, policies reduce to action sequences (""paths""). Illustrative experiments on simple domains show that the proposed approach is able to quickly recover a state representation that depends mostly on the endogeneous component.","The authors propose a novel algorithm PPE, which they prove efficiently eliminates exogenous noise under certain assumptions (e.g. near deterministic dynamics). PPE works by growing a set of open-loop policies (action sequences) sufficient to reach all possible states for an increasing horizon. This is done by predicting the index of the policy from its final state, and eliminating states that are not sufficiently predictable. They show that both in theory and in practice that popular alternatives (noise contrastive and inverse-dynamics approaches) either fail to ignore exogenous noise or fail to distinguish between actually different states.",0.27472527472527475,0.21978021978021978,0.15384615384615385,0.24581005586592178,0.11731843575418995,0.09944751381215469,0.13966480446927373,0.11049723756906077,0.14285714285714285,0.2430939226519337,0.21428571428571427,0.1836734693877551,0.18518518518518517,0.14705882352941177,0.14814814814814817,0.2444444444444444,0.15162454873646208,0.12903225806451613
847,SP:bcacb4954de9b4c8c3d881bdc81d44d75297be87,"The paper proposes a data augmentation framework called BRACE that utilizes concept-based explanations from existing interpretability methods (such as Comprehensible Convolutional Neural Network (CCNN) and GradCAM) to identify informative examples to be added to the training set. Specifically, the proposed approach defines a utility function that focuses on finding images that are 1) under-represented in a class (i.e., images containing visual features of the class but are predicted as other classes with high confidence by the classifier), and 2) contain visual concepts that have led to the misclassification. Experiments on CUB, CUB-Families and Tiny Imagenet show better performance for the proposed approach as compared to previous works and baselines.","This paper proposes an explanation-based data augmentation method for image datasets. Technically, this method defines a utility function to choose the data samples collected from the Internet. The utility function is defined to select the samples that could better reflect the under-represented region in the data distribution. The authors compare the proposed method with some existing data augmentation techniques on three datasets and showcase the effectiveness of the proposed method to helping achieve higher finetuning accuracies.  ","The paper proposes to use interpretation to select effective instances for augmenting training data. A utility function is developed, where images with high scores mean that: 1) the images are helpful for better learning class-c data, 2) the images could significantly correct the drawbacks of the model M. The paper discusses two interpretation scenarios, interpretable models (GAP+linear model) and post-hoc interpretation. The experiment includes a comparison to two types of baseline methods, data augmentation methods and data selection methods. Ablation studies are also conducted to show the effectiveness of the two components in the utility function. Different parts in the current version are somehow fragmented. ","This paper proposed a utility function to rank new training samples with respect to their potential contributions of improving the model performance. The proposed utility contains two parts, one is for the under-represented region, another is based on explanation algorithms for finding the concepts that cause the misclassification. Experiments proved the effectiveness of the proposed method.",0.18584070796460178,0.19469026548672566,0.1592920353982301,0.2692307692307692,0.2948717948717949,0.1388888888888889,0.2692307692307692,0.2037037037037037,0.3157894736842105,0.19444444444444445,0.40350877192982454,0.2631578947368421,0.2198952879581152,0.19909502262443438,0.2117647058823529,0.22580645161290322,0.34074074074074073,0.18181818181818182
848,SP:bcb23bdcbb70499a10b7b481f65ef891a6cc9d9a,"This paper proposes a prime-dual network to extract discriminative relationships between support and query examples of the multi classes with few examples. While the prime network predicts labels of a query out of the support set, the dual network reverses the process and infer the category labels of the support set using query examples. For training, a weight self-supervised loss contains the sum of dual and prime networks and is evaluated on inner-domain and cross-domain cases.  ","The authors design a prime-dual architecture for few-shot learning to characterize the inherent relationship between the support set and the query set and introduce a self-supervision constraint for performance improvement. In particular, it proposes to correct query sample labels between the prime network and dual network and design an optimized prediction for the query samples with a selection mechanism. Extensive experiments are conducted on datasets with the ablation analysis, and the method achieves favorable results over existing algorithms. ",[1] This paper follows the line of metric-based few-shot learning methods with GNNs. The novelty of this paper is to design dual GNN graphs to capture the consistency of label prediction from the support set to the query set and its reversible task (from the query set to the support set).  [2] The proposed method achieves considerable performance gains compared with other state-of-the-art methods. ,The paper proposed a self-supervised learning framework and a dual network to improve the performance of few-shot learning. The dual network is based on GNN architectures and learn the relationship between the features from the support set and query set. Experiment shows improvement over the baselines on several few-shot learning benchmarks. ,0.2375,0.225,0.225,0.2345679012345679,0.2222222222222222,0.2608695652173913,0.2345679012345679,0.2608695652173913,0.3333333333333333,0.2753623188405797,0.3333333333333333,0.3333333333333333,0.2360248447204969,0.24161073825503354,0.26865671641791045,0.2533333333333333,0.26666666666666666,0.2926829268292683
849,SP:bcb3d5c67dba9ee98d0b38d7dae2a6fc79af5087,"This paper collects the QVHIGHLIGHTS, a unified benchmark dataset for text-based moment retrieval and highlight detection in videos. It contains over 1000 Youtube videos with a) human-written free-form queries and their relevant moments, and b) five-point scale saliency scores for all query-relevant clips. Different from existing text-based moment retrieval datasets, QVHIGHLIGHTS has a smaller temporal bias on moments and allows a text query to match multiple disjoint moments. Compared to other highlight detection datasets, it presents temporally dense annotations dependent on queries.  As a baseline, they propose a Moment-DETR model which can do the two tasks jointly.","This paper introduces a new dataset using language queries to localize moments in videos. The paper also introduces a DETR-like baseline for this task. Several existing models and the baseline are run on the proposed dataset. I'm concerned the contributions will not be significant, the dataset is small (10k videos) and the baseline is an adaptation of existing methods to videos.","In this paper, the authors focus on moment retrieval and highlight detection tasks from videos given text queries. They present the QVHIGHLIGHTS dataset which includes 10,000 videos collected from Youtube. Each video is annotated with a query, time span of a moment, and saliency scores. In addition, the authors propose an Encode-Decoder Transformer model (Moment-DETR) that predicts moments and scores in an end-to-end manner.  The Moment-DETR model provides superior performance in comparison to previous methods on the QVHIGHLIGHTS dataset for moment temporal localization and highlight detection tasks.","This paper introduces a new dataset for video moment retrieval and highlight detection named QVHighlights. The dataset contains around 10K YouTube videos. Each video is annotated with a free form text query and (i) the time intervals in the video corresponding to the query (multiple intervals can match the query) and (ii) for each annotated time interval a *saliency* scores (from 1 (Very bad) to 5 (very good)) describing how relevant are the individual frames within an annotated clip are to the query.   In addition to the dataset, the authors introduce a method named MomentDETR inspired from DETR in order to perform moment retrieval in their introduced dataset. The method performs better than existing techniques.",0.11538461538461539,0.23076923076923078,0.25961538461538464,0.19047619047619047,0.2698412698412698,0.3333333333333333,0.19047619047619047,0.25806451612903225,0.23478260869565218,0.12903225806451613,0.14782608695652175,0.26956521739130435,0.1437125748502994,0.2436548223350254,0.2465753424657534,0.15384615384615383,0.1910112359550562,0.29807692307692313
850,SP:bcf424f8006b1d5e0d5a5c982b3151693cce3d85,"The authors propose MODINV, a new algorithm to learn minimal and compressed representations. MODINV uses several predictors with different initializations and learning but all built on top of the same learnt representation.  After explaining the algorithm and its relation to minimal representation the authors evaluate it on vision and RL tasks.  On vision tasks the authors show improvement on CIFAR-10 by adapting their method on top of SimSiam in the specific case of a linear predictor. On RL they evaluate on control tasks from pixel inputs.  The authors also provide an experiment to show that adding a loss to discourage correlation within features helps to improve RL performance.","Inspired by the information bottleneck principle, the authors propose to learn minimal representations by independently training multiple predictive heads on top of a shared representation, such that the resultant predictor is more robust to spurious correlations. To test this proposal, the authors provide extensive set of experiments in RL from pixels and vision benchmark tasks. The ablation studies help make the case too.","Motivated from the idea that minimal representations — that encode information relevant to a task and nothing more — are likely to generalize well, the paper proposes an approach, titled Model Invariance (ModInv) to learn representations. The underlying idea is to use multiple diversified predictors solving auxiliary objectives on top of a shared representation/encoder. Different predictors are diversified in terms of their training dynamics — different initializations and learning rates. The approach relies on the hope that a shared representation that can lead to optimal performance for a diverse set of predictors is likely encoding some notion of implicit invariance to changes in predictors, thereby, introducing some notion of minimality. The authors conduct experiments on reinforcement learning and self-supervised (in computer vision) settings and claim that ModInv provides strong performance improvements in both. Based on further ablations,  the authors demonstrate how sensitive ModInv is to the number of predictors and the diversity in terms of training dynamics (in terms of learning rate), and whether ModInv is complementary to recent augmentation strategies in reinforcement learning.","This paper proposes using an ensemble of predictors or heads on top of a shared representation to improve performance. The ensemble of predictors/heads are made ""diverse"" by using different learning rates. Some performance gains are shown in RL settings, while some stability gains are shown in a particular vision setting.",0.1926605504587156,0.23853211009174313,0.10091743119266056,0.36507936507936506,0.20634920634920634,0.11560693641618497,0.3333333333333333,0.15028901734104047,0.21568627450980393,0.1329479768786127,0.2549019607843137,0.39215686274509803,0.24418604651162792,0.18439716312056736,0.1375,0.19491525423728812,0.22807017543859648,0.17857142857142858
851,SP:bd447c7d0f99059b9bece31a1693da9ba0ab0364,"This paper presents SfB, a novel method of handling a single RGB image of a significantly blurred fast moving object, by recovering the object's shape, texture, and trajectory, what are optimized towards a joint loss of image consistencies, via a differentiable render. The method can be applied to a variant of tasks of de-blurring, shape reconstruction, super resolution and view synthesis. The paper reports state-of-the-art results on a variant of real-world and synthetic datasets, both quantitatively and qualitatively.","This paper introduces an approach for predicting the textured 3D shape and motion from blurred video frames. The approach is based on the differentiable rendering that can predict shapes and image formations that can be regarded as a combination of background and foreground scenes. The paper shows interesting results that can reasonably recover shapes, motion, and a deblurred texture map.","The submitted paper focuses on the shape, texture, and motion recovery of a single fast moving object from two input images from a static camera - a clean background image, and a blurred image due to fast object motions. The authors tackle the problem using differential rendering and iterative optimization at test time. Specifically, the template mesh (""prototype""), the texture map, the initial translation and rotation, as well as the constant motion speed are jointly optimized in order to minimize a combination of loss functions, with the image formation loss being the key contribution. The method is demonstrated on both synthetic dataset and relatively simple real datasets. ","This paper presents a method for 3D shape reconstruction of objects with large motion blurs in a video sequence. The authors propose to augment an off-the-shelf 2D shape-from-blur method, whose output is used to fit a 3D mesh shape as well as a linear 3D motion trajectory within a small timeframe. The motion blur is synthesized by averaging the shape & texture predictions within the timeframe for the image reconstruction loss, in addition to other regularization terms to keep the optimization well-behaved. Temporal super-resolution can also be achieved by synthesizing the image at a continuous timestamp. Experiments are benchmarked on 3 real-world datasets containing sequences of small objects and a synthetic dataset for benchmarking reconstruction accuracy.",0.17857142857142858,0.2619047619047619,0.2619047619047619,0.2833333333333333,0.2833333333333333,0.19811320754716982,0.25,0.20754716981132076,0.18032786885245902,0.16037735849056603,0.13934426229508196,0.1721311475409836,0.20833333333333331,0.23157894736842105,0.21359223300970873,0.20481927710843373,0.1868131868131868,0.18421052631578946
852,SP:bda29cabe2a75ec86ba01f1774f77621caca564d,This paper proposes EE-NeT for contextual bandits which contains two neutral networks: one for learning the underlying reward function and another for learning the potential gains of arms if explored. They prove that  EE-NeT achieves better regret bounds than state-of-the-art neural bandits algorithms for both UCB-based and TS-based. This paper also shows that EE-NeT outperforms existing linear and neural bandit approaches.,"This paper studies the neural contextual bandit problem, and proposes a neural-based bandit approach with a novel exploration strategy, called EE-Net. Besides utilizing a neural network (Exploitation network) to learn the reward function, EE-Net also uses another neural network (Exploration network) to adaptively learn potential gains compared to currently estimated reward. Then, a decision-maker is constructed to combine the outputs from the Exploitation and Exploration networks. Theoretical guarantees and extensive experiments are provided to demonstrate the performance superiority of EE-Net over existing linear and neural bandit algorithms. ","This paper presents a neural net based bandit approach with a novel exploration strategy. Specifically, the solution uses an exploitation network to estimate rewards for each arm and an exploration network to predict the potential gain compared to current reward estimate. It then uses a decision maker to select arms using one of the linear or nonlinear modes. The key differentiator/strength of the paper is using neural net based exploration strategy and demonstrating that the proposed solution has a tighter bound than the existing state-of-the-art bandit algorithms. ","This paper studies neural network-based contextual bandits. Different from existing works which utilize UCB or Thompson sampling for exploration, the exploration component of the proposed method EE-Net is also modeled with a neural network.  Based on theoretical tools on over-parameterized neural networks, it is proved that EE-Net achieves the regret bound of $\mathcal{O}(\sqrt{T\log T})$. In contrast, existing neural network-based contextual bandit methods, such as NeuralUCB and NeuralTS, achieves the regret bound of $\mathcal{O}(\sqrt{T} \log T)$ based on the theoretical tool of the neural tangent kernel.  Experiments are conducted on four public datasets to validate effectiveness of EE-Net, compared with a variety of contextual bandit methods including both neural-based methods and linear contextual bandit methods.",0.2898550724637681,0.2318840579710145,0.3188405797101449,0.3804347826086957,0.2826086956521739,0.21978021978021978,0.21739130434782608,0.17582417582417584,0.171875,0.38461538461538464,0.203125,0.15625,0.24844720496894407,0.19999999999999998,0.22335025380710657,0.3825136612021858,0.23636363636363636,0.18264840182648404
853,SP:bdda04b701b73b4e5e5fec405a1b1219fbee5de7,"In this manuscript the authors test how DNNs trained on classifying scene categories react to texture based foveation, which they compare to undisturbed images, a foveated blur and a uniform blur matched in their SSIM to the original image. In overall accuracy the texture model performs close to the performance of the undistorted images and clearly better than the other two distortions. Going beyond accuracy the authors then test the effects of the input distortions on processing filtered images, occlusion, and cue conflicts between center and periphery. ","In this paper, authors study the functional advantages of a foveal transform of visual inputs. It is nicely introduced with a very comprehensive review of the literature. The method introduces a 2 two stage model of the visual system, where the first stage corresponds to the (fixed and non adaptive) foveation stage and the second stage to the higher level processing, typically associated with the categorisation operated in the ventral stream of the visual pathway. This second stage will be implemented by existing CNN architectures (AlexNet and ResNet) which are re-learned on the transformed inputs. To control for the functional consequences of the foveated processing, the first stage can also be a single isotropic blurring of the image. Both alternatives are manipulated such that their distortion (as computed with a SSIM measure) are equally balanced, leading to 1 standard mapping and three proposal retinal transformations :  Standard-NEt (unmatched) and Foveation-Net, Matched-Net and Ada-Gauss-Net (matched). Results show that for both perceptual systems which are foveated, ""Foveation-Net has the highest i.i.d generalization while Ada-Gauss- Net has the greatest o.o.d generalization"". Second result is that foveated processing allowed a better robustness to occlusions and third result is that such networks reproduce behavioural results of a Window Cue-Conflict. Last results propose to study that foveation introduces a focusing strategy, and keep high-french information on the fovea - which seem less striking results.","The paper aims to explore the effects on representation that emerge when a CNN is equipped with a foveated visual system. The authors build a model whereby images are artificially rendered into a form that captures foveation which are then fed into standard CNN architectures. Two different types of foveation are considered: one where the periphery is blurred (greater blurring as a function of eccentricity), and another where texture statistics are preserved over local regions which increase in size as a function of eccentricity.  These are compared against each other and two baselines:  - Reference: images are transformed by pushing them through a convolutional AE - Foveation-texture: images are transformed by pushing them through a convolutional AE with a foveated texture style transfer - Uniform-blur: each image is blurred uniformly, with the amount of blurring set such that the SSIM between the blurred image the its corresponding reference was the same as the SSIM between the corresponding foveation-texture image and reference. - Foveation-blur: as above, but with blur additionally increasing with eccentricity  Clearly the reference images contain more information than the foveation-texture images; the SSIM matching on the is done to resource match the blurred images for comparison to foveation-texture, and the reference serves as an upper bound. Experiments are performed on a scene classification task. ","I thank the authors for a thoroughly written paper studying an important question for both machine learning and neuroscience. The authors propose a biologically inspired modification to CNN architectures by introducing foveation. Several thorough experiments are performed to assess the benefit of foveation with reasonable control transformations. The proposed modifications seem novel and discuss the relevant prior work in this domain. Appreciate the detailed discussion of all implementational specifics, I'm fairly confident about the correctness of the experiments performed and results presented.",0.25287356321839083,0.27586206896551724,0.19540229885057472,0.15,0.08333333333333333,0.0963302752293578,0.09166666666666666,0.11009174311926606,0.20481927710843373,0.1651376146788991,0.24096385542168675,0.25301204819277107,0.1345565749235474,0.15737704918032788,0.2,0.1572052401746725,0.1238390092879257,0.13953488372093023
854,SP:beaef224ae4592604cb8398b67befc5d6263ac01,"This paper proposes a scalable approach to disentangled representation learning using the self supervised learning framework, rather than the typically used generative model framework. Their approach is principled, being grounded in the Higgins et al definition of disentangling from the symmetry perspective. It also appears to be scalable, which is a very exciting step in the subfield of unsupervised disentangled representation learning.",This paper proposes a self-supervised learning model which is a variant of the Invariant Risk Minimization with an iterative partition mechanism. The method can encourage the self-supervised learning of a disentangled representation. Empirical results validate the effectiveness of the proposed model on self-supervised learning and disentangled representation learning.,"This paper presents a new self-supervised learning technique that learns to map image inputs to ""disentangled"" vectors iteratively by alternating two steps. First, all of the data is assumed to be in the same subset and they learn a representation by minimizing a SimCLR like contrastive loss (here the positive pairs are augmented versions of the same image, while negative samples are other images in the batch). In the second step, they find the partition of the dataset into 2 subsets that maximize the contrastive loss. This new partition is added to the set of all partitions considered, and we repeat the two steps (again learning the representation that minimizes contrastive loss over all partitions and then finding the next partition and so on). They use a regularized form of the contrastive loss, which adds a term taken from earlier work (""Invariation Risk Minimization""). This terms measures the magnitude of the gradient of the contrastive loss wrt to the temperature parameter (at temperature=1). They evaluate their technique against alternatives on a wide range of disentanglement metrics and downstream classification accuracy, and show their technique performs well.","In this paper, the authors focus on the problem of learning unsupervised representations that are also disentangled. To this end, they build on top of the idea of invariant risk minimization and propose Iterative Partition-based Invariant Risk Minimization (IP-IRM) which iteratively assigns examples to partitions and constrains invariance in the loss among these partitions. They justify their approach mathematically according to a formal definition of disentanglement. The then perform multiple experiments qualitatively and quantitatively demonstrating the learned representations are disentangled. Furthermore, they show promising results on Out-of-distribution classification tasks. ",0.27419354838709675,0.27419354838709675,0.1935483870967742,0.4117647058823529,0.3137254901960784,0.13829787234042554,0.3333333333333333,0.09042553191489362,0.12903225806451613,0.11170212765957446,0.17204301075268819,0.27956989247311825,0.3008849557522124,0.136,0.15483870967741936,0.17573221757322177,0.22222222222222224,0.18505338078291814
855,SP:bee015aa2460abcf9b360339778c36db62d48bc7,"This paper investigates how to federate the Expectation Maximization (EM) algorithm in the case of a curved exponential family. The authors propose a novel algorithm, called FedEM, which translates the EM algorithm in the FL setting, which consists in iteratively sending local sufficient statistics to the server and updating the value of the model. This algorithm takes into account the distributed aspects, communication constraints (using an unbiased compressor with an error feedback mechanism for the local statistics to send), partial participation, and data heterogeneity of the FL setting. The authors prove the convergence of FedEM (Corollary 2). A variance-reduced version of FedEM is also proposed (VR-FedEM), also with convergence guarantees (Th. 4). Experiments are performed on toy synthetic data, MNIST and real data (missing value imputation from a tabular dataset, bird).",Authors propose and study a distributed variant of Expectation-Maximization for exponential families. Theoretical analysis is conducted to characterize the convergence of the proposed algorithms towards stationary points of the underlying objective function. The empirical performance of proposed algorithms is studied via numerical experiments on synthetic and open data.  ,"The paper proposes methods to implement Expectation-Maximization (EM) algorithm under Federated Learning (FL) setting. The methods proposed incorporate gradient compression and variance reduction techniques, and also allow for partial participation during training process. Moreover, the paper gives finite-time complexity analysis of proposed methods. Finally, paper demonstrates their methods by both simulated and real data experiments.","The authors study the problem of expectation-maximization in a federated setting. They propose two communication-efficient algorithms where the clients compress certain sufficient statistics to save communication. For each algorithm, convergence rates are presented that match their centralized counterparts in the absence of compression. ",0.12781954887218044,0.14285714285714285,0.10526315789473684,0.1836734693877551,0.1836734693877551,0.15789473684210525,0.3469387755102041,0.3333333333333333,0.3111111111111111,0.15789473684210525,0.2,0.2,0.18681318681318682,0.2,0.15730337078651682,0.16981132075471697,0.19148936170212766,0.17647058823529413
856,SP:bf1e7a56011b8b5ed73aee54bd2fa811fe864fb3,"This paper focuses on the setting of federated learning where the two agents are attempting to perform kernel regression using different kernels (and hence have different models). Their study yields an algorithm of using alternating knowledge distillation (AKD) imposes overly strong regularization and may lead to severe under-fitting. Their theory also shows an interesting connection between AKD and the alternating projection algorithm for finding the intersection of sets. Leveraging this connection, they propose an algorithm that improves upon AKD. ","The paper analyzes the dynamics of optimizing two kernel regression models via co-distillation in a distributed setup where local models may differ in the kernel used. In each round, each local client uses the other client's model to produce novel labels for its local dataset and then retrains its local model using these novel labels. The paper analyzes three variants of this approach, the vanilla variant, a variant where novel labels are an average of the actual label and the one predicted by the other client's model, and an ensembling approach that uses all model iterations to produce predictions. The approaches are analyzed theoretically and empirically.","This paper analyze knowledge distillation based model agnostic federated learning. They consider simple two agent kernel regression scenario, where each agent has its own dataset and predicting function. They propose to train agent 1 model on dataset 1, and then use agent 1's model to make predictions on dataset 2, and agent 2 will train model using these predictions. They analyze the dynamics of agent 1's model, and show that Alternating Knowledge distillation will degrade the model prediction to 0. They provide another algorithm ensemble AvgKD which can actually avoid this issue and converge to optimal solution. The experiments also show that AvgKD can converge to zero loss while other approaches do not work.","This paper introduce an interesting perspective on federated learning via knowlegde distillation, which allows participating clients to have their own choices of models. In particular, the paper develops theoretical results for a two-client federated regression scenario, which demonstrates (a) the degeneration of an alternating knowledge distillation where iterative distillations (i.e., a client model at each iteration is re-trained based on unlabeled input + prediction of the other client's model of the previous iteration) gradually lose information over the iterations and eventually converge towards a vacuous model; and (b) a new ensembling technique that aggregates intermediate models produced by both clients over the iterations such that in the limit (i.e., when the no. of iterations tends to infinity), the aggregated model is the same as the (oracle) centralized model. This developed intuition on ensembling intermediate models is then applied to more realistic 2-client federated classification scenarios on MNIST and CIFAR-10 datasets.",0.2,0.2125,0.225,0.22018348623853212,0.22018348623853212,0.1810344827586207,0.14678899082568808,0.14655172413793102,0.11538461538461539,0.20689655172413793,0.15384615384615385,0.1346153846153846,0.1693121693121693,0.17346938775510204,0.15254237288135594,0.21333333333333337,0.1811320754716981,0.15441176470588236
857,SP:c078726571ed80602888eae739195074494afb48,"The paper aims to ground language to vision via cross-modal contrastive learning to build an explainable shared semantic space. This is achieved in two parts (i) align visual and language representations with MS COCO dataset (ii) Retrieve visual objects with language queries through a cross-modal attention module and infer the visual relations between the retrieved objects through a bilinear operator on Visual Genome dataset. This allows model's language encoder to become a stand-alone language model that can encoder embedding concepts in a visually grounded semantic space. The model enables compositional language understanding based on visual knowledge and multimodal image search with queries. The authors show experiments on clustering, compositional reasoning, relationship with human-defined norms, and image search to showcase the usefulness of the model.","The paper implements a model which learns word representations from text grounded in images. The model is composed of a BERT-based text stream, a VGG-based image stream, and visual relation classification module. The text and image components are pre-trained and fine tuned on the COCO dataset of captioned images as well as on Visual Genome relations. The main contribution of the paper is the large number of analyses carried out on the induced grounded word embeddings, compared to plain BERT embedddings. The paper shows some evidence that the principal components of the grounded embedding space correspond to interpretable semantic dimensions (i.e. concrete-abstract), that words' semantic features can be predicted based on embeddings. The paper also claims that the model is capable of compositional understanding. ","This paper proposes a contrastive learning framework to learn visually grounded, and as a result, semantically interpretable language representations. It proposes two contrastive losses which make use of multiheaded cross-modal attention and bilinear interaction to compute similarity scores between subject and object representations, which can then we used in the contrastive loss.  They perform thorough experiments with the resulting representation to illustrate how well they correlate to human ratings along different language axes such as concrete-abstract and compositionality, as well as qualitative multimodal image search examples.","This paper analyzes the linguistic embedding in the joint image-language space learned by cross-modal contrastive learning. Comparing with the standard word embedding, the grounded language embedding is human explainable. Specifically, the PCA of the grounded langauge embedding is aligned with human rating. When clustering the language embedding, the cluster of grounded language embedding is more semantically coherent than the un-grounded language embedding. Furthermore, the paper showed that the grounded langauge embedding could benefit visual-linguistic downstream tasks.",0.16279069767441862,0.17054263565891473,0.13953488372093023,0.09302325581395349,0.16279069767441862,0.125,0.16279069767441862,0.25,0.225,0.13636363636363635,0.2625,0.1375,0.16279069767441862,0.20276497695852536,0.1722488038277512,0.11059907834101383,0.20095693779904306,0.13095238095238096
858,SP:c0976f3b1660215445d60cf4842db823bfaa37cd,"The paper considers optimization of MF two-layer neural nets (the infinite-width version with entropic regularization). Specifically the paper proposes to do dual coordinate ascent, which allows for exponential convergence rate, together with a particle approximation scheme. The paper shows that in the discrete-time setting, an improved convergence rate is obtained (w.r.t. desired error $\epsilon_P$, the number of data points $n$, the regularization $\lambda_2$).","This work solves the entropy regularized mean-field model for a two-layer neural network. As the Fenchel–Rockafellar dual of that problem is a finite dimension problem, they consider an SDCA-type scheme to solve the dual problem, which still contains an integral term and they use the Langevin iterate procedure to solve the subproblem approximately. In contrast to the existing analysis for SGD in the mean-field regime, the new algorithm has a fixed length dual step size and has a fast linear convergence rate.","This paper presents an optimization alogrithm for mean-field shallow neural networks. The algorithm optimizes the parameter measure using a particle-based implementation of the stochastic dual coordinate ascent algorithm. The authors establish convergence results, namely an exponential convergence rate owing to the convexity of the dual problem.","The problem of training an infinitely wide neural network with one hidden layer can be written as an optimization problem in the space of probability distributions, where each atom in the distribution corresponds to a hidden neuron. The paper proposes to solve instead the dual problem (of an entropy regularized variant), which in the case of finite data is a finite-dimensional optimization problem.  The main contribution is the conception and analysis of an efficient dual coordinate ascent scheme to solve the dual, which in theory and practice requires fewer iterations to find an epsilon-accurate solution than existing methods.  ",0.18571428571428572,0.21428571428571427,0.22857142857142856,0.14942528735632185,0.2413793103448276,0.2708333333333333,0.14942528735632185,0.3125,0.16,0.2708333333333333,0.21,0.13,0.16560509554140126,0.2542372881355932,0.18823529411764708,0.19259259259259262,0.2245989304812834,0.17567567567567569
859,SP:c0f9b2744ae990e34da449c1a6c64459d8f87cbc,The paper studies the performance of a greedy algorithm for online bipartite matching under the configuration model. In this model the vertex degrees of the input graph adhere to a fixed distribution. The authors estimate the competitive ratio of the greedy algorithm and show that in this model the greedy algorithm achieves better performance guarantees than related methods.,"This paper analyzes the performance of the GREEDY algorithm for online matching on random bipartite graphs generated by the configuration model with given degree distributions. The analysis is based the differential equation method, a stochastic approximation using an ODE to represent the time evolution of matching size under the GREEDY algorithm with overwhelming probability. The paper reports the following main results: - A proof that the matching size of the GREEDY algorithm is the solution to an ODE. - Competitive ratios of the GREEDY algorithm in different graph models (d-regular graphs, Erdos-Reny graphs). - A proof that GREEDY beats RAKNING on 2-regular graph, with experimental evidence suggesting that the same holds for higher degrees.","The authors investigate bipartite matching in an online setting were the graph edges are generated via the configuration model (i.e. from a chosen degree distribution). They bound the estimation error, measured via the expected ratio between the online matching obtained by Greedy and the hindsight-optimal matching. This allows them to compare the performance guarantee for Greedy versus the state-of-the-art Ranking algorithm. The authors prove that surprisingly, there exist problem instances (including simple and natural ones) where Greedy outperforms Ranking.","The paper studies the online bipartite matching problem with capacities in random graphs that are generated by a stochastic process called the configuration model. The configuration model generates random graphs whose degree distributions are sub-Gaussian. The main contribution of the paper is an analysis of the competitive ratio achieved by the well-studied Greedy algorithm for online bipartite matching. The main result relates the competitive ratio to the solution of a certain ODE. The ODE can either be solved in closed form for some special cases of interest or it can be solved numerically to obtain estimates of the competitive ratio. As a corollary of the main result, the paper derives competitive ratios for several special cases, including random d-regular graphs and Erdos-Renyi graphs. ",0.43103448275862066,0.3103448275862069,0.39655172413793105,0.19298245614035087,0.2982456140350877,0.30952380952380953,0.21929824561403508,0.21428571428571427,0.18110236220472442,0.2619047619047619,0.2677165354330709,0.2047244094488189,0.2906976744186046,0.2535211267605634,0.24864864864864866,0.2222222222222222,0.28215767634854777,0.24644549763033174
860,SP:c13f2002931fe71148d4991217f6b8439fa9390e,"In this work authors presented two analyses of the Riemannian gradient descent for eigenvector computation.  First authors provided a general tight analysis that holds for any real symmetric matrix. In addition, authors further give the first worst-case analysis that achieves a rate of convergence.  For real-world application, this gap-dependent analysis suggests a new promising learning rate for stochastic variance reduced PCA algorithms. Authors provided experiments to confirm the findings.  Overall this is a solid paper with both theoretical analysis and empirical studies.","This paper considers Riemannian gradient method for PCA. Contributions of the paper are following: 1) proved convergence rate of $O(\frac{1}{\max{\Delta, \epsilon}}\log\frac{1}{\varepsilon})$ for any real symmetric matrix; 2) showed without the need of the eigenvalue gap information that even if the gap $\Delta$ is significantly smaller than the target accuracy, it is possible to achieve a speed of convergence $O(\frac{1}{\epsilon}\log\frac{1}{\varepsilon})$. Numerical results verify obtained rate of convergence.","In this paper, the authors considered Riemanninan gradient descent for solving the top eigenvector/value problem for real-symmetric matrices. Rates in two regimes are given, depending on the gap of the top eigenvalues compared to the desired accuracy. The analysis is based on polynomial recursion derived from the algorithm updates. Additionally, for positive-definite matrices, a stepsize choice is proposed based on the upper bound on the contraction factor as suggested by the recursion. Numerical experiments are conducted for the power iteration, projected GD and Riemanninan GD.","The paper studies Riemannian gradient descent for largest eigen-vector computation real symmetric matrices. For matrices with eigen-gap $\Delta$ and (global) convergence error $\epsilon$, the paper gives a gap-dependent convergence rate of $\frac{1}{\Delta} \log(1/\epsilon)$ and gap-independent analysis which gives a convergence rate of $\frac{1}{\epsilon} \log(1/\epsilon)$. This improves upon previous work that achieves a $\frac{1}{\Delta^2} \log(1/\epsilon)$ rate and generalizes related work that that achieves the same rate for projected gradient descent, power iteration and Riemannian gradient descent restricted to PSD matrices.  The paper highlights an omission in previous work that mistakenly stated a rate of $\frac{1}{\epsilon} \log(1/\epsilon)$ which is in fact $\frac{1}{\Delta^2\epsilon} \log(1/\epsilon)$.   Experiments comparing the power iteration, projected gradient descent and RGD are done for proposed adaptively chosen step-sizes. ",0.17647058823529413,0.21176470588235294,0.24705882352941178,0.19753086419753085,0.345679012345679,0.25,0.18518518518518517,0.20454545454545456,0.14383561643835616,0.18181818181818182,0.1917808219178082,0.1506849315068493,0.1807228915662651,0.20809248554913296,0.1818181818181818,0.1893491124260355,0.24669603524229075,0.18803418803418803
861,SP:c18a5a73b62a8cc24efb784e93ff3745b12a147e,"The authors propose a spectrum of objectives for learning energy-based models that generalise maximum likelihood. These objectives--known as pseudo-spherical scoring rules--are similar to (persistent) contrastive divergence in terms of their implementation and computational cost, but have an extra degree of flexibility that can boost downstream performance for e.g. image generation.",This paper presents a generalization of the Contrastive Divergence that uses pseudo-spherical scoring rules to define a new family of loss functions for generative modeling. This family of objective functions has an auxiliary loss parameter that can balance the trade-off between diversity and quality. The method is applied to learn EBMs on image benchmark datasets.,"This paper provides an efficient framework to estimate energy based model (EBM).  While conventional KL or general f-divergence-based contrastive divergence learning suffers from instability and non-convergence issues, the proposed framework namely pseudo-spherical contrastive divergence (PS-CD) can be implemented as SGD type algorithm whose sample complexity and convergence property are provided.  PS-CD includes the conventional CD as a special case and thus provides gradient-based optimization framework for CD.  Experiments to evaluate the effectiveness of the proposed approach are conducted with both synthetic and image benchmark datasets. ",The paper introduces a class of divergence to learn EBMs. The class of divergence includes KL as a special case. The paper approximates the gradient of the class of divergence by self-normalized importance sampling and analyzes the convergence when optimizing with the approximate gradient. Experiments are conducted to evaluate the influence of $\gamma$.,0.23636363636363636,0.16363636363636364,0.14545454545454545,0.24561403508771928,0.19298245614035087,0.17391304347826086,0.22807017543859648,0.09782608695652174,0.14814814814814814,0.15217391304347827,0.2037037037037037,0.2962962962962963,0.23214285714285712,0.12244897959183675,0.14678899082568805,0.1879194630872483,0.1981981981981982,0.2191780821917808
862,SP:c223ac5c9fd6d71322ab0952f16ffdc738122e3c,"The paper considers the problem of unsupervised accuracy estimation and error detection of pre-trained classifiers. The contributions consist of (i) a bound on the generalization performance on estimated accuracy and on detected errors and (ii) a solution based on ensemble models. Specifically, the bound on the generalization error takes into account three quantities, namely (i) the error of the ensemble on correctly-classified test instances, (ii) the agreement of the ensemble and the pre-trained model on potentially wrong test instances and (iii) the diversity of the ensemble on mis-classified test instances. The solution based on ensemble models consists of an iterative training strategy, where at each iteration the ensemble is trained to minimise a training surrogate for quantity (i) combined with quantity (ii) and ensuring diversity among ensemble predictions across iterations. Experimental analysis on several benchmarks (including digits, Office-31, CIFAR-10, iWildCam and Amazon Reviews) show the potential of the proposed solution.  Overall, the paper is well written and well structured. The idea of using ensemble strategies in the context of unsupervised accuracy estimation and error detection is novel. However, there are some issues in terms of ORIGINALITY and QUALITY, which need to be addressed and/or discussed (see below for further comments)."," The paper proposes a framework to tackle two tasks: 1. unsupervised accuracy estimation (estimate accuracy on unlabeled data) and 2. error detection (identify the misclassified examples in the unlabeled data). The framework uses an ensemble of networks to identify mis-classified points as points where the current classifier disagrees with the ensemble. Then the mis classified points are used for self-training to improve the ensemble. The paper gives a theoretical analysis which depends on assuming that the ensemble is mostly correct where the classifer is correct, the classifier and the ensemble disagree on pseudolabeled data and that the ensemble is diverse in its predictions, which is claimed to be true empirically. ","- The paper considers two problems: unsupervised accuracy estimation and error detection, both in out-of-distribution settings. - The authors propose a practical approach, where they train a ‘check model’ h using self-training and ensembling. Points on which the prediction model and the check model disagree on are predicted as misclassified. - The theoretical analysis identifies conditions under which the approach should work and presents a bound on the accuracy estimation error - The authors empirically show that their algorithm outperforms baselines for both unsupervised accuracy estimation and error detection","The paper studies the problem of error detection and accuracy estimation of unlabeled data points. Particularly, the paper proposes to use a diverse ensemble of models and detect errors by checking the inconsistency between the ensemble prediction and the target model prediction. Moreover, the paper proposes a self-training procedure that iteratively identifies incorrect predictions and adds their pseudo labels as new training targets to detect more errors. The paper also gives some theoretical analysis given assumptions about the ensemble models' quality.",0.178743961352657,0.1497584541062802,0.13526570048309178,0.21428571428571427,0.20535714285714285,0.20454545454545456,0.33035714285714285,0.3522727272727273,0.34146341463414637,0.2727272727272727,0.2804878048780488,0.21951219512195122,0.23197492163009406,0.21016949152542372,0.19377162629757785,0.23999999999999996,0.2371134020618557,0.21176470588235297
863,SP:c22a9f67b984730605faaa950212890cd39a7bc0,"This paper looks at the problem of estimating physically-plausible 3D human motion and environment interactions from an egocentric RGB video. It proposes to combine the advantages of kinematic and dynamics-based approaches: first, a physics-based dynamics policy (UHC) is trained to imitate motion capture data; then a kinematic policy is trained to predict a target pose for UHC based on RGB and object & camera pose inputs. Experiments show that the combination of dynamics and kinematics outperforms either individually by a large margin, and that the method can reasonably predict a small set of person-object interactions. ",This paper presents a method for estimating the egocentric 3D pose of a person moving in a scene. The main idea is to build a dynamics-driven human motion controller which is provided target poses to mimic by a kinematics estimator and information about scene objects. Experiments show that improved performance compared to previous approaches.,"This paper proposes a method for pose estimation from first-person camera view. The work builds on a universal controller that learns to imitate a wide variety of behaviors. A kinematic policy is then either trained fully via supervised learning or in a hybrid manner, using a combination of supervised learning and reinforcement learning. The reinforcement learning optimization consists of a motion imitation and a physics-based objective. Specifically, the universal controller’s output is utilized to regularize the kinematic policy to become more physically plausible. The main contributions are a universal controller and the dynamics-regulated training procedure for egocentric pose estimation in human-object interaction scenarios. ","In this paper, the authors presents a new method for egocentric physically-plausible 3d human motion estimation. Specially, they designed a general-purpose humanoid controller (UHC) and a dynamics-regulated kinematic policy that can be directly trained and deployed inside a physics simulation. The proposed universal humanoid controller is trained on large-scale Mocap datasets and is for general-purpose. In the experiments, the authors evaluate their proposed method on both Mocap dataset and their collected real dataset. The authors show good performance improvements compared with PoseReg and EgoPose, as well as good imitation success rate compared with DeepMimic on Human3.6M.",0.16326530612244897,0.21428571428571427,0.22448979591836735,0.32727272727272727,0.32727272727272727,0.21296296296296297,0.2909090909090909,0.19444444444444445,0.21568627450980393,0.16666666666666666,0.17647058823529413,0.22549019607843138,0.2091503267973856,0.20388349514563106,0.21999999999999997,0.22085889570552147,0.22929936305732482,0.21904761904761902
864,SP:c235ef4d49668517d90b9b39128f9ed2e2a678ed,The authors propose a method for end-to-end deep clustering. Authors utilizes NCE loss and proposes a method to produce cluster level loss. They motivate their approach and supply empirical evaluation.,"This paper proposed a new clustering method called Twin-Contrast Clustering(TCC), which extends the mainstream contrastive learning paradigm to a cluster-level scheme. TCC simultaneously learns instance- and cluster-level representations by leveraging cluster assignment variables. And TCC uses Gumbel Softmax to reparametrize the assignment variables, which makes TCC is able to be trained end-to-end without auxiliary steps.","For image clustering, this paper extends the mainstream contrastive learning paradigm to a cluster-level scheme, and proposes twin-contrast clustering. Heuristic cluster augmentation equivalents are presented to enable cluster-level contrastive learning. Besides, this paper also derives the evidence lower-bound of the instance-level contrastive objective with the assignments. Experiments on several datasets demonstrate its superiority.","This paper extends the mainstream contrastive learning paradigm to a cluster-level scheme. Specifically, the author propose twin-contrastive clustering (TCC),  which links the instance-level learning track with the cluster-level one. Extensive experiments show the advantages of TCC.",0.21875,0.21875,0.15625,0.2786885245901639,0.3114754098360656,0.3620689655172414,0.11475409836065574,0.1206896551724138,0.125,0.29310344827586204,0.475,0.525,0.15053763440860216,0.15555555555555556,0.1388888888888889,0.28571428571428575,0.37623762376237624,0.42857142857142855
865,SP:c23d8a58c5a53540493da8acf5c157c666c78cbb,"The paper derives information-theoretic bounds for learning algorithms. The bounds are based on the predictions of the learned model, rather than the weights (output), which makes them tighter by the data-processing inequality and can yield non-vacuous bounds in cases where weight-based mutual information bounds are vacuous. The authors validate the analysis with a few experiments. ",This paper proposes new information-theoretic generalization bounds. These bounds extend some existing results and seem easier to compute. The authors also demonstrate their bounds through some applications and experiments.,"The paper continues a line of work which develops information-theoretic bounds on the generalization gap (i.e. the difference between e,pirical and population loss) in supervised learning. Its main contributions are: 1. the authors introduce a natural “transductive” variant of the conditional mutual information bound (Section 3) and show that it is stronger than all previous bound. (Eg one can use it to prove that any proper learner for a VC class generalizes). 2. The authors quantitatively improve some of the previous bounds on input-output mutual information (theorem 2.2, 2.3), and on conditional mutual information (Theorem 2.6), by moving the expectation outside the square-root in the RHS of the inequality. ",The authors build upon recent literature on information theoretic bounds on generalization error and specifically the conditional mutual information bound by Steinke and Zakynthinou [28]. They propose an improved bound that is tigther than [28] and easier to compute in practice. It is achieved by replacing the weight-parameter vector in the mutual information expression of [28] with a prediction function evaluated on a subset of in-sample (training) data and a subset of out-of-sample data.  This enables the authors to give performance guarantees on deterministic training algorithms. Several toy examples and experimental results are presented to illustrate the advantages of the improved generalization bound.,0.15254237288135594,0.3050847457627119,0.2542372881355932,0.3333333333333333,0.3333333333333333,0.2222222222222222,0.3,0.15384615384615385,0.14018691588785046,0.08547008547008547,0.09345794392523364,0.24299065420560748,0.20224719101123598,0.20454545454545456,0.18072289156626506,0.1360544217687075,0.145985401459854,0.23214285714285712
866,SP:c258d9eb2ba4be0d4e01ab7b969ea95ac032b181,"The paper studies stochastic multi-agent multi-armed bandits where the reward distribution could be heterogeneous, i.e., the mean reward for agents on an arm could be different.  The authors consider Nash social welfare as the fairness criteria and aim to learn a distribution on arm plays that result in optimal Nash social welfare. Thress algorithms based on Explore-Then-Commit, Epsilon Greedy, and UCB are given and their performance is analyzed.","In this paper, the authors propose to minimize the Nash Social Welfare (NSW) function -- the product of utilities, in a multi agent multi-armed bandit problem. The NSW strikes a balance between utilitarian and egalitarian optimization in a multi agent framework. The product of utility maximization is a log-concave optimization for which they propose three algorithms -- Explore-then-commit, Epsilon-Greedy, and UCB.  They establish regret upper bounds for the proposed algorithms, with UCB having the best \tilde{O}(\sqrt{T}) regret.  ","This paper studies a multi-armed bandits problem with multiple agents. Arms have agent specific reward distributions associated with them. An arm is pulled at each step and each agent receives a reward sampled from their corresponding agent specific reward distribution for this arm. The goal of the learner is to find a distribution over arms that maximizes an appropriate aggregate notion of rewards earned by all agents – the Nash welfare.  The paper develops variants of three standard algorithms – one that explores for a fixed number of rounds and then exploits (Explore-First), an epsilon-greedy style algorithm, and a UCB style algorithm, to minimize a notion of regret which measures the difference between the Nash welfare of the solution at time t and the optimal Nash welfare. The paper derives upper bounds on the expected regret of the algorithm in each case. For UCB, the upper bound matches the lower bound (in the appendix) in terms of its dependence on parameter T (time horizon).","This paper studies multi-agent multi-armed bandits under heterogeneity (where arm means vary across users), with the objective to optimize Nash social welfare to ensure fair allocation of pulls in the obtained policy. The authors present variants of explore-first, epsilon-greedy, and upper confidence bound strategies to achieve fair decision-making. Each algorithm obtains ""fair"" regret matching the standard counterpart, along with a $ \Omega(\sqrt{T})$ lower bound in the Appendix. The central technical (theoretical) contribution is a concentration result on the fluctuation of NSW that exploits the Lipschitz property of the objective coupled with a covering argument, which is then utilized subsequently to derive regret bounds for each of the proposed algorithms. ",0.2328767123287671,0.3287671232876712,0.273972602739726,0.3253012048192771,0.2891566265060241,0.18787878787878787,0.20481927710843373,0.14545454545454545,0.17391304347826086,0.16363636363636364,0.20869565217391303,0.26956521739130435,0.21794871794871795,0.20168067226890754,0.21276595744680848,0.21774193548387097,0.24242424242424243,0.22142857142857142
867,SP:c25b58e657e1b3744b53c1b476992f16688278cf,"In this paper, the authors discuss selecting rationales to improve classifier performance in an extract then classify setting. The motivation behind this paper is that prior work on rationale selection often seeks to identify a subset of tokens in a document that carry the most information about the target label. However, oftentimes such rationale selectors pick up on spurious correlations that are merely correlated with the label due to confounding and do not express the desired relationship with the label. So even though the desired predictor would rely on “the causal variable”, in this case, predictors often end up relying on the “non causal” factors.   To address this issue, the authors suggest assisting the rationale selector by lowering the mutual information between non causal features and the label (since a rationale selector will try to identify features that have the maximum mutual information with the label). To do so, the authors propose a framework in which they first use a rationale extractor (similar to that introduced by Lei et al.) to identify rationales for the label of interest. A class conditional masked language model then makes inference on these tokens (while keeping the other tokens constant) to produce new documents with a counterfactual class label. These replacements are performed by the MLM in one step (in a greedy fashion). A discriminator seeks to distinguish between the original document and the generated counterfactual document. This process is run iteratively until convergence. The core idea here is that when the original dataset will be augmented with the counterfactual documents, it should lead to a lower mutual information between the “non rationales” that remained unchanged during counterfactual generation and the label. As the authors recognize, one drawback of automatically generating counterfactual examples is that we assume access to a perfect rationale selector. However, since the motivation of the paper relies on a poor rationale selector which may also reduce the mutual information between causal features and the label, they conjecture that reducing the mutual information between spurious features and the label more than reducing mutual information between causal features and the label could allow us to improve the model performance. The authors conduct experiments on Beer reviews from RateBeer and Tripadvisor reviews. Compared to three baselines, the proposed method appears to generally perform better on the downstream classification metric, and these gains are attributed to better rationale extraction. I think overall the paper represents a valuable contribution but I have some concerns as I lay out below. I will be happy to update my score post rebuttal.","This paper addresses an important problem of correcting the spurious rationales that often get learned due to maximizing the mutual information. Instead, it proposes “counterfactual data augmentation” that lowers the mutual information between spurious signals and the document label. The counterfactual factual data augmentation is an unsupervised method. The paper finally shows strong results on two datasets. ","This paper aims to provide a novel rationalization scheme for text classification models in NLP. The authors use the standard setup of a selector and classifier model wherein the selectors selects specific words from the input text and classifier makes a prediction on the selected rationale. Standard issues in such models is that the selector may end up picking on spurious correlations (or ""artifacts"") when picking the rationale if they are correlated with the predicted quantity, instead of true predictive features. The core contribution of the authors is to augment the dataset in such a manner that the correlation between the artifacts and the label is broken.  The correlation is broken by generating counterfactual examples -- examples where predictive features are *flipped* along with the label but the spurious features are kept the same. Since we don't know a priori which features are predictive vs spurious, the authors hope that the dataset contains slightly less information about the label than the predictive features and provide an analysis in a simple binary feature case, wherein the show when the model may increase the mutual information for predictive features vs rationale error rate of initial selector.   The modeling part use transformer based models all the way. The setup learns the selector and prediction, in addition to a counterfactual generator (which replaces the rationale words with words predictive of the opposite class), and a discriminator to make sure the words are indeed from same distribution as input text.  Experiments are done on two sentiment analysis multi-aspect datasets and the generated rationales are shown to be more accurate than baseline MMI training. Some qualitative analysis seem to indicate better rationales are produced in terms of coherency and correctness. ","A rationale in NLP is a subset of an original input text on which a classifier operates. Extracting such a rationale is a popular approach for more interpretability because by definition we know the classifier only had access to this reduced text. However, rationales can be nonsensical and can consist of spurious patterns in the dataset.  To improve the coherence of rationales and to improve generalization, this paper proposes a technique to automatically create counterfactual data. This counterfactual data is created specifically to reduce mutual information between the true target label and irrelevant parts of the original text with regards to the true target label. For example, in “This beer smells great. It tastes terrific.” both sentences have positive sentiment but only the first is relevant when predicting whether the beer has a good smell. To help a model to automatically learn to distinguish, a helpful counterfactual example would be “This beer smells awful. It tastes terrific”.  Contributions  -	A method to automatically generate counterfactual examples to improve rationale extraction. -	An analysis of how good an initial rationale extractor should be to be able to expect performance improvements using the proposed method of generating counterfactual examples. ",0.06367924528301887,0.16037735849056603,0.1179245283018868,0.3508771929824561,0.3157894736842105,0.14035087719298245,0.47368421052631576,0.23859649122807017,0.2564102564102564,0.07017543859649122,0.09230769230769231,0.20512820512820512,0.11226611226611226,0.19181946403385047,0.16155088852988692,0.11695906432748537,0.14285714285714285,0.16666666666666666
868,SP:c264d829101068d0c0c4fd3a739841c379ad3b7a,"The paper proposes an unrolled denoising method for non-autoregressive text generation. Specifically, the method first corrupts the original input and then re-generate uncertain tokens in it, and the result is taken as the output of the denoising autoencoder to compute loss with the target. Experiments are conducted on machine translation and unconditional text generation. The method achieves good results on MT without utilizing knowledge distillation.","This paper presents a training method of non-autoregressive text generation that is trained from scratch without distillation. The method is motivated as an unrolled denoising approach, which starts with a sequence of random tokens and transforms it in multiple stages to a valid sequence. To make training feasible, the proposed approach starts with a corrupted target sentence instead of a totally random sequence and does only two steps of unrolling. The training loss is an approximation of this process and is computed as a sum of loss on direct denoising and two steps of denoising. During inference, it starts with a random sequence and applies several steps of denoising to arrive at the desired sequence. Several heuristics are also applied to make generation feasible. The main result of the paper on non-autoregressive machine translation on which the authors show decent improvements over baselines on non-distilled settings however slightly underperforms AR distilled models. The authors also show examples for the case of unconditional generation and text in-painting","This paper proposes a new training technique for an iterative non-autoregressive (NAR) model and achieves significant improvements over the previous model. Inspired by the success of the diffusion model, the author formalizes the iterative NAR decoding process with a denoising autoencoder framework, then unrolls the denoising process during training. The proposed method (SUNDAE) achieves new state-of-art performances in machine translation by deep iteration decoding and parallel reranking. More experiments on unconditional text generation tasks further show its promising capability.","The present paper proposes a Step-untolled denoising autoencoder (SUNDAE), a NAR text generation model which marries de-noising auto-encoder and Markov chain models. The major contribution of this paper is the unrolled denoising training scheme, which is a training step method. In the first step, the original text is randomly corrupted in the same way as what the denoising autoencoder did. Subsequently, in the second step, the noisy text from the first step is first denoised by sampling from the model, and the denoised text is then used as a sample for training the denoising autoencoder.   The proposed model has been tested on two tasks that involve text generation: machine translation as well as language modeling. ",0.3582089552238806,0.2835820895522388,0.34328358208955223,0.12941176470588237,0.1411764705882353,0.25609756097560976,0.1411764705882353,0.23170731707317074,0.19491525423728814,0.2682926829268293,0.2033898305084746,0.17796610169491525,0.20253164556962025,0.2550335570469799,0.24864864864864863,0.17460317460317465,0.16666666666666666,0.21000000000000002
869,SP:c266aef6e28ca51c310cb4c0e1bb20769a23d809,"This paper presents Pessimistic Bootstrapping for offline RL (PBRL), a model-free offline RL algorithm that purely relies on an uncertainty-driven method. Bootstrapped Q-functions are trained, and the standard deviation of their estimates is used for the uncertainty quantification. This uncertainty quantification is then used for pessimistic bootstrapping. Also, in contrast to the existing methods that only considers in-distribution target, PBRL optimizes Q-function even for out-of-distribution actions with the pseudo-target that is penalized by the uncertainty quantifier. A theoretical analysis is provided that PBRL is provably efficient in the linear MDP setting. Experimental results demonstrate that PBRL outperforms the state-of-the-art methods. ","This paper presents a model-free pessimistic bootstrapping approach for offline RL. Specifically, the paper considers an actor-critic approach with an ensemble of Q-functions and utilizing disagreements in their predictions (measured as standard deviation) for learning the Q-functions. The paper also presents a way to regularize the learning with out of distribution state-action pairs which according to the paper is a crucial part in obtaining improved results.   --- Update post author response: I have increased my score.","The paper looks at the offline RL problem and claims that existing techniques penalize out-of-distribution actions too aggressively and so perform poorly at generalization and extrapolation. To remedy this, the paper proposes an offline RL algorithm in which an ensemble of critics is trained with an objective composed of (1) a TD error based on actions seen in the dataset with target value penalized by standard deviation over the ensemble of the next Q-values, and (2) a squared error on the Q-values of OOD actions regressing to those same Q-values penalized by the standard deviation over the ensemble. The proposed algorithm is paired with a theoretical analysis connecting it to recent theoretical offline RL papers. The algorithm is evaluated on the D4RL benchmarks and shows favorable performance compared to baselines.",This paper proposed an uncertainty-aware offline reinforcement learning algorithm based on a Q-network ensemble. The proposed method penalizes Q-values on OOD actions and performs pessimistic offline Q-learning. The paper interprets the method with LCB framework. The proposed method outperforms existing offline RL algorithms on D4RL Gym and Adroit tasks. ,0.24324324324324326,0.22522522522522523,0.12612612612612611,0.2625,0.15,0.17777777777777778,0.3375,0.18518518518518517,0.2641509433962264,0.15555555555555556,0.22641509433962265,0.4528301886792453,0.2827225130890052,0.2032520325203252,0.17073170731707316,0.19534883720930232,0.1804511278195489,0.25531914893617025
870,SP:c30bae9a167445da4c7093e100830d87a21490d9,This paper proposes to use DNA as the side information for zero-shot species classification. The idea is novel and biologically sound. Experiments show that DNA embeddings achieve comparable performance as text embeddings when serving as side information.,"Authors propose to use DNA as side information for zero-shot image classification and a method that can leverage this information. The method is based on a Bayessian model that captures similarity between different classes as well as between images of a single class through global and local priors. Unseen classes are represented using the statistics of a fixed number of seen classes that are the most similar according to the side-information. Authors introduce an INSECT database and compare their method on INSECT and CUB with several other zero-shot methods. When using DNA as side information, the proposed method performs the best.","The paper proposes using DNA as side information for fine-grained zero-shot learning of insects, as opposed to visual attribute labels. Their proposed approach establishes a hierarchy over the images using DNA information, via a hierarchical Bayesian model over a learned embedding of DNA barcodes. Their method particularly outshines attribute-label-based approaches on an insect identification application, where visual attributes (from public Wikipedia pages, for example) are sparsely labeled or not available. The provide as an additional contribution their paired DNA/image benchmark dataset containing 21,212 examples of 1,213 insect species.","This paper proposed to use DNA as side information for the zero-shot learning tasks.  A CNN is used to generate DNA embeddings, and a Bayesian model is used for zero-shot learning. Experiments are performed on a newly collected fine-grained insect dataset with image and DNA pairs, as well as on the CUB dataset. Results showed that the Bayesian model outperforms other methods on the insect dataset using DNA as attributes. On the CUB dataset, using DNA is comparable with using word vectors, but pre-defined attributes still work better. ",0.47368421052631576,0.3157894736842105,0.4473684210526316,0.17307692307692307,0.25,0.22105263157894736,0.17307692307692307,0.12631578947368421,0.18478260869565216,0.18947368421052632,0.2826086956521739,0.22826086956521738,0.2535211267605634,0.18045112781954886,0.26153846153846155,0.18090452261306533,0.2653061224489796,0.2245989304812834
871,SP:c34f3c55587e94ef6619cdd2b92113f3f4c02564,"The paper considers the problem of prediction with missing (incomplete) features. The authors propose a class of generative models that includes missingness of features, and develop a discriminative learning algorithm that maximises the conditional (posterior) log-likelihood of the training data approximately. Experiments show that the method is competitive compared with existing approaches, and in particular with approaches based on VAEs. ","The authors propose a new method, DIG, for discriminative tasks with missing input features. It uses latent variable models to marginalize out the label and the missing part of the features given the latent variable, in order to compute the objective, the conditional log likelihood of the label given the corrupted features. As this objective is intractable, the paper builds a conditional evidence lower bound (CELBO) that can be unbiasedly approximated using Monte Carlo samples. CELBO consists of the regular ELBO as the lower bound for the log joint probability of the label and the observed features, and an evidence upper bound (EUBO) that bounds the log marginal probability of the observed features. The derivation of EUBO involves the alpha-renyi divergence and the exponential divergence. The stochastic CELBO contains a density ratio that can lead to large variance in the stochastic gradients during optimization, so the authors propose a surrogate parameterization to bound the gradient norm. Experiments on real datasets justify the effectiveness of the variational approximations to stabilize the optimization. When compared with VAE, CVAE, and MICE, the DIG algorithm shows better or comparable predictive performance and robustness against feature corruption.","This paper proposes a new method for learning with missing data. Compared with previous approaches, the authors choose to perform discriminative learning with generative modeling so as to borrow the benefits from these two types of methods. To optimize with the underlying intractable loss function, the authors start from the traditional variational lower bound ELBO and one upper bound CUBO from a previous work (the $\chi$-divergence lower bound [1]) and derives a lower bound for the original loss function. To solve the issue with the estimation bias as well as the potential huge variance, the authors change the divergence function in CUBO as well as add the surrogate parameterization so that the Monte Carlo estimation of the loss can be unbiased and (potentially) with smaller variances. Experiment results show the proposed methods run stably and perform comparably or better compared to baseline methods.  References:  [1] Adji Bousso Dieng, Dustin Tran, Rajesh Ranganath, John Paisley, and David Blei. Variational inference via χ upper bound minimization. In Advances in Neural Information Processing Systems, pp. 2732–2741. 2017.","The paper addresses the problem of predictive modelling with missing input features. The authors formulate the problem as a latent variable model, and in addition to the standard variational lower bound (ELBO) propose to use a variational upper bound based on CUBO (Dieng at al 2017) modified by an exponential divergence to solve the MC estimation of CUBO. They further propose surrogate parametrization to reduce the variance in the gradients. The experimental evaluation over standard regression UCI datasets with randomly dropped features shows marginal improvements over existing baselines.",0.32786885245901637,0.29508196721311475,0.3114754098360656,0.19689119170984457,0.15025906735751296,0.14857142857142858,0.10362694300518134,0.10285714285714286,0.2159090909090909,0.21714285714285714,0.32954545454545453,0.29545454545454547,0.15748031496062992,0.15254237288135594,0.25503355704697983,0.20652173913043478,0.2064056939501779,0.19771863117870722
872,SP:c3627df75302253f2491aa883facb864e8e52e07,"This paper proposes a continual learning method by learning module networks and a way to compose them. One notable difference is that the proposed approach does not reply on an external task id as the input. Instead, it learns to predict whether the input batch can be composed with the current modules or need to add another module. The authors show several experiments to validate the performance.","This paper tries to tackle continual learning with a modular approach and proposes the local module composition (LMC) approach. LMC composes modules dynamically by computing local relevance scores for each layer, computed with the input. This architecture makes LMC perform well for OOD tasks as well as outperforming existing approaches on a continual learning benchmark.",Authors propose an algorithm to perform continual learning without task-id labels at inference. The method involves local module-level OOD estimators which then output linear coefficients that informs the model how to combine outputs. The model also has the option of creating new modules which are first trained in a projection phase from downstream layer signals before starting training on their own. The proposed method is tested in standard continual learning benchmarks as well as OOD detection and meta-learning settings. ,"The paper proposes a new algorithm for addressing continual learning (CL) by leveraging modular architectures. In contrast to (the few) existing modular CL works, this one focuses on the task-agnostic setting, where the agent is not informed of the task ID. The proposed architecture assigns two components to each module: a functional component and a structural component. The latter is in charge of locally determining whether each data point (actually each mini-batch, in practice) is in-distribution for the given module. The training proceeds in stages, adding new modules whenever the new data point is out-of-distribution for all modules, then training the module such that its output is in-distribution for modules at later depths, and finally training the entire model together. Extensive experiments are carried out to demonstrate the strengths of the proposed approach.",0.13432835820895522,0.16417910447761194,0.34328358208955223,0.23636363636363636,0.2,0.2073170731707317,0.16363636363636364,0.13414634146341464,0.16546762589928057,0.15853658536585366,0.07913669064748201,0.1223021582733813,0.14754098360655737,0.1476510067114094,0.22330097087378642,0.18978102189781024,0.1134020618556701,0.15384615384615385
873,SP:c3a775bff03fdbfeb22a4f2936cb989c31eb4b9a,"This paper introduces a latent variable controller model that allows for reusable skill learning in behaviour cloning and reinforcement learning settings. The architecture comprises three stages or levels. At the highest level, input state information (eg. proprioception, visual input, object state information) is passed through an MLP to produce a discrete latent state skill selection variable, incorporating a discrete latent transition model. This skill selection prior is then used in a mid level network operating on the same input information, to produce a continuous latent state, conditioned on this discrete latent variable. Finally, this latent state, together with the input state is used by an actor network to produce actions.  The model is trained used a heuristic process depending on the deployment setting, with different elements frozen at different times, or through the use of a mixture agent that can be initialised from scratch or using previous skills (a similar approach is explored by Florensa et al., ICLR 2017). A KL regularisation on the discrete latent variable skill selector is used (also in prior work - Burke et al., CoRL 2019). Results show that this approach is effective on a range of tasks, and that skill re-use (as expected) out performs learning from scratch. Probably the most interesting result is the comparison with a hierarchical behaviour cloning network and the experiments around skills transfer. The idea seems sound, although the presentation is in need of some improvement, and at times the paper suffers from lack of clarity (in problem formulation and presentation of results). Moreover, some important work in the admittedly vast array of work on hierarchical models for RL and behaviour cloning is missed.  ","This paper presents a hierarchical model for skills extraction from an online dataset. In particular, the model can be seen as a 3-level hierarchy. The highest level of the hierarchy selects a discrete variable that corresponds to the index of a skill. The mid-level selects the continuous parameter that defines the skill. The lower level executes the skill.  The considered problem is crucial in reinforcement learning. In particular, the definition of skills to be reused across different tasks is essential to achieve better sample efficiency, and crucial to allow reinforcement learning to be applied directly on challenging real robotic tasks.   When dealing with an off-line dataset, one wants potentially to first extract a set of skills (say, low-level policies) and then to learn via RL the selector of such policies. In this way, one both simplifies the problem (increasing sample efficiency) and allows the skills to be reused in different tasks.  The authors propose a graphical model that consists, per time step, of four variables: $x_t$, $y_t$ $z_t$ and $a_t$. $x_t$ represents the MDP's state, $y_t$ is the discrete selector of the skill, $z_t$ the (continuous) parameters of the skill, and $a_t$ is the output at time $t$ of the selected skill (and can represent, for example, the joint velocities of the robot).  After learning via variational inference the probabilistic model, the authors propose to refine the learned behavior via MPO, learning the skill selector $p(y_t | y_{t-1},x_t)$, the skill parameters $p(z_t | y_t, x_t)$, and the skill p(a_t | z_t, y_t).  Further, the authors propose 1) to use an asymmetric information approach, where each level of the described hierarchy sees a different definition of the state. In particular, the lower layer sees the robotic state (proprioception), the mid-layer sees the position of objects in the scene, and the highest level sees both the information 2) the models use ""gated""-heads: the categorical variable $y_t$ is fed to the last part of the neural network.   Once the hierarchy has been learned, one can tune it on the same or different tasks. In particular, the lowest layer is kept fixed, while only the highest, or the highest, and the mid-layer are learned.   The authors propose to use MPO as a reinforcement learning algorithm. They perform a few ablation studies and experiments to empirically prove the efficacy of their method. ","This paper presents a method to learn a three-leveled hierarchy of skills offline, from a dataset of demonstrations, that can then be applied to accelerate reinforcement learning. The three-level architecture is novel. It encodes a discrete selection, a continuous contextual variable (dependent on the discrete selection) and a low level policy dependant of the continuous contextual variable. The results show some improvements over baselines, especially in a sparse reward context where the presented method capitalizes from the offline learned strategies for exploration.","This paper proposes a three-level hierarchy for learning motor skills. At the bottom, there is a low-level controller for action generation and at the top a high-level controller that generates sequences of skills, with continuous control signals generated by skills at the mid-level. When transferring motor skills from one task to another using reinforcement learning, it is possible to either retrain only the high-level controller or the mid-level one as well. In experiments, it is shown that for transfers to more complex manipulation tasks with sparse rewards, you are more likely to get convergence in training with positive rewards, if only retraining the high-level controller. Thus you are given more flexibility and are possibly able to learn more complex tasks, with the proposed hierarchy compared to earlier methods based on only two levels. ",0.19272727272727272,0.10545454545454545,0.10545454545454545,0.07228915662650602,0.08674698795180723,0.2261904761904762,0.12771084337349398,0.34523809523809523,0.20714285714285716,0.35714285714285715,0.2571428571428571,0.1357142857142857,0.1536231884057971,0.1615598885793872,0.13975903614457832,0.12024048096192384,0.12972972972972974,0.16964285714285712
874,SP:c3a98139d88d2e9811cc14fa392d888de7133c68,"Contextual embeddings of words are mapped to a sort of cluster of word embeddings with each one corresponding to one possible state in a linear CRF model. The idea is to build a network between the cluster of words (states of CRF) for analysis the manifold of language. To reduce quadratic cost of CRF, a sampling based approach is proposed to compute the partition function.   Empirical analysis is not very insightful, and the contributions are incremental. ","This paper proposes a latent network analysis, using a proposed a Randomized Dynamic Programming (RDP) approach, to study the topology of the representation space of contextual embedding models. By constructing an unbiased estimator for the partition function with subsampled DP paths, the authors are able to reduce the memory complexity of forward-backward computation on Linear CRFs by 2 orders of magnitude. Using this RDP formulation, inference with structured latent variable models is scalable to 1000s of latent states. Latent states are modeled in a linear chain CRF with a long-tailed prior on transitions and learned using amortized variational inference similar to Fu et al. 2020, Li and Rush 2020. State-word connections in the representation space provide linguistic information that encapsulates syntactic and semantic roles and state-state connections correspond to the construction of phrases. The authors find anchoring states in the representation space that provide enough information to perform unsupervised paraphrase generation. ","This paper proposes a new way to greatly reduce the memory requirements of inference on discrete latent variable models (LVMs). It does this by estimating the key inference steps in dynamic programming through enumerating a small subset of nodes and using importance sampling for the rest of the nodes. This technique is generally applicable to many dynamic programming algorithms and enables scaling LVMs to much more states by reducing memory requirements. Empirically, this work shows that the proposed approach gets better partition estimates than existing methods using top-k approximations. This work also demonstrates an application of the proposed approach on discovering meaningful latent codes and latent trajectories from pretrained language models.","In this paper, the authors address the issue in the computational and memory efficiency in Dynamic Programming when it is used in the Forward Algorithm for Linear Chain CRF. The author purpose  a Sample Forward Algorithm by randomizing the summation. The authors proves that such randomization is an unbiased estimator for the true sums, and, using Amortized Variational inference for optimization, shows that it empirically reveals meaningful latent topology with much less required resource. ",0.23684210526315788,0.17105263157894737,0.13157894736842105,0.14838709677419354,0.11612903225806452,0.14285714285714285,0.11612903225806452,0.11607142857142858,0.13513513513513514,0.20535714285714285,0.24324324324324326,0.21621621621621623,0.15584415584415584,0.13829787234042554,0.13333333333333333,0.17228464419475656,0.1572052401746725,0.17204301075268819
875,SP:c3bebbc9a809712fe06eac2a4adf0517ad38d900,"The submission deals with high resolution satellite images, given a low resolution image. The method is trained by learning to predict a high resolution image given a low resolution one, adversiarially, knowing the true high resolution. The point is to use LR imagery to predict HR at points in time where actual HR images are not available.   Experiments aim at quantifying the quality of generated images, both from a structural and reconstruction quality standpoint, as well as using the generated images in a potential downstream task of object counting, specifically, buildings and swimming pools. ","This paper proposes using a NeRF-like approach for generating high-resolution imagery at some time t based off of high-resolution imagery at time t' and low-resolution imagery at t. This is a practically useful application for different problems in conservation and sustainability as the authors note. They compare the proposed method with SR based methods and image fusion models (e.g. cGAN and Pix2Pix) on two datasets.  The approach consists of: 1.) An image encoder that takes the conactenated HR and LR images as input 2.) A decoder that produces pixel level features 3.) A learned positional embedding  4.) A pixel-wise decoder that produces the desired high-resolution imagery from the pixel level features and position embedding 5.) A discriminator network  The entire process is trained using a cGAN loss and L1 reconstruction loss.  Evaluation is performed with SSIM, FSIM, and LPIPs.","This paper presents a method to generate high-resolution satellite imagery from a pair of input low resolution image and a reference high resolution image of the same location but taken at a different time instant. The main problem the author(s) are addressing is the lack of high quality satellite imagery(specially temporally), and that can be resolved using multiple low resolution and a reference HR image. The authors claim that such a high resolution image will be useful for object counting tasks, specially over time when the intermediate high-res images are not available. Their model comprises of a feature mapper (for fusion of input low-res and reference high-res image),  feature encoder (for both the Fourier and spatial feature) and finally the synthesiser (inspired by nerf operating in the temporal domain keeping the spatial domain fixed) they curate a dataset from the publicly available datasets or services like google earth and perform their experiments on them. The authors also conduct a Human study for the verification of their approach along with the usual metrics for image similarity. ","The authors propose a conditional pixel synthesis approach which combines frequent low-resolution aerial imagery with less widely available high-resolution imagery to generate photo-realistic and accurate high-resolution imagery.  The approach is largely based on NeRF with modifications for the remote sensing domain.  Results are compared to other techniques such as image fusion models, Pix2Pix, SuperResolution GAN, and others.",0.18085106382978725,0.30851063829787234,0.13829787234042554,0.21232876712328766,0.1232876712328767,0.07734806629834254,0.11643835616438356,0.16022099447513813,0.21311475409836064,0.1712707182320442,0.29508196721311475,0.22950819672131148,0.14166666666666666,0.2109090909090909,0.16774193548387095,0.18960244648318045,0.17391304347826086,0.11570247933884296
876,SP:c3f4aee84ba462fb07797d24ba96b4b7cc6503a6,"This paper proposes a conditional gradient algorithm that uses heavy ball type acceleration to solve the constrained optimization problem. After introducing the vanilla Frank Wolfe method which is one of the common methods to solve these types of problems, authors introduce their FW with heavy ball momentum and provide convergence guarantees on L-smooth and convex objectives under various step-size choices. Furthermore, they also consider another version of the algorithm (FW with restart) which solves two subproblems to further improve the primal-dual error in the problem. Lastly, the authors present the performance of their algorithms on binary classification and matrix completion.  ","This paper considers the effect of the heavy ball momentum on the Frank-Wolfe method. The $O(1/k)$ last-term convergence (type II as described in the paper) of a generalized FW gap is provided as the main contribution of this paper. Such a result is missing in the previous literature without the extra computational burden (only $O(1/\sqrt{k})$ result is available). The extension of the FW gap to the proposed generalized FW gap is natural, and this proposed quantity can also serve as the stopping criterion in practice. Other than the classic step-size choice of $2/(k+2)$, a similar result is obtained under smooth step size with the additional beneﬁt of a non-increasing objective value, but at the cost of some prior knowledge of the smoothness parameter $L$. The problem of the pessimistic estimation of $L$ is then alleviated with the introduction of the directionally smooth step size. Finally, the authors show that the restart technique further improves the last-term convergence.","The authors provide an analysis of Frank Wolfe (FW) algorithms with heavy ball momentum. After a recap of standard FW including common assumptions, choices of step size and convergence guarantees, they introduce FW with heavy ball momentum. Different variants for step size and weighting parameter are considered and it is shown that using heavy ball momentum stabilizes convergence in terms of an improved rate. Subsequently, the authors consider FW with heavy ball momentum prove that this leads to further improvement of the convergence rate. Finally, numerical results for binary classification on multiple datasets and matrix completion are reported.","This work presents the heavy ball frank wolfe (HFW) method and shows that it establishes a $\mathcal{O}(1/K)$ last-iterate rate on a primal-dual gap (and hence also on the primal function-value suboptimality). In contrast, prior work established a $\mathcal{O}(1/K)$ last-iterates rate on the primal suboptimality or a $\mathcal{O}(1/K)$ best-iterate rate on the primal-dual gap. The work also presents variations of the HFW method and brief experiments.",0.24271844660194175,0.2524271844660194,0.17475728155339806,0.12352941176470589,0.14705882352941177,0.11224489795918367,0.14705882352941177,0.2653061224489796,0.225,0.21428571428571427,0.3125,0.1375,0.18315018315018317,0.2587064676616916,0.19672131147540986,0.15671641791044774,0.2,0.12359550561797752
877,SP:c42059bc7beacaf5bd9fea55b5cf8f3fa63fce56,"This paper studies the weak topology on three spaces of probability measures, each corresponding to a level of a causal hierarchy introduced by Pearl (Pearl and Mackenzie, 2018).  The three levels, in order of increasing expressivity, are described as observational, interventional and counterfactual; given a set $\bf{V}$ of variables (known as ""endogenous"" variables; these variables can influence each other and may also be influenced by a set of ""exogenous"" variables), the observational distribution is a joint probability distribution over the values of $\bf{V}$ fixed according to some given structural causal model, an interventional distribution on $\bf{V}$ is a distribution where the values of some finite subset of $\bf{V}$ are fixed and a counterfactual distribution is one where various combinations of interventions with respect to different finite subsets of $\bf{V}$ are permitted.  The main results are: (1) a topological characterization of the set of hypotheses in the second level of the causal hierarchy that are ""experimentally verifiable"", which means that there is a sequence of tests that almost surely converge in the limit on the true hypothesis while a type I error is incurred up to a fixed bound at each stage (the relevant distribution belonging to the second level of the hierarchy); (2) the set C_2 of all points in the second level of the causal hierarchy that have a unique inverse (with respect to a projection from the third level) forms a meager set, where the underlying topology is the weak topology; a similar result applies to the preimage of C_2 under projection from the third level of the hierarchy.   ","This paper proposes a series of topologies on the different spaces of probability spaces of structural causal model. The authors define certain causal projections on these spaces that should map higher levels of the causal hierarchy to lower levels. The non-injectivity of these projection maps is shown and illustrates, what is called, the collapse of the hierarchy. The authors show that for those probability spaces that collapse are topologically meager. Using the result that open sets in the weak topology corresponds to statistical hypothesis that can be verified, the authors show that the collapse can never be statistically verified, because of topological meagerness. ","The paper introduces a topological learning-theoretic version of the causal hierarchy theorem (Bareinboim et al. 2020).  This result shows that associational, causal, and counterfactual quantities are not only distinct in a measure-theoretic sense but also in a topological sense. This, in turn, implies that causal assumptions allowing the inference of arbitrary counterfactual quantities from experimental data would be statistically unverifiable. This result reinforces the notion that causal assumptions are needed to perform causal and counterfactual inference, as they cannot be accomplished solely from data. Such formal limit becomes even more important when considering the strong inclination of ML research towards finding methods that are successful at modeling the observed data but are blind to causal constraints not present in the data.","This paper presents a topological framework to theorize about causal inference, and defines a weak topology on each of three spaces of probability measures, corresponding to the three levels of Pearl's causal hierarchy. In this setup a topological version of the causal hierarchy theorem is derived, to the effect that collapse of the hierarchy is topologically negligible. An interesting learning theoretic interpretation of this result is also given, in the spirit of no-free-lunch theorems.       ",0.12313432835820895,0.09328358208955224,0.12313432835820895,0.18269230769230768,0.23076923076923078,0.15447154471544716,0.3173076923076923,0.2032520325203252,0.42857142857142855,0.15447154471544716,0.3116883116883117,0.24675324675324675,0.17741935483870966,0.1278772378516624,0.19130434782608696,0.16740088105726872,0.2651933701657459,0.18999999999999997
878,SP:c4429f4b779ee2488c7d2396b6fdede463f4ee6a,"This paper studies the global convergence of SGD/GD with label noise. Under suitable assumptions, it shows that if initialized properly and with suitable stepsize/batch size, label noise SGD/GD converges approximately to some stationary point of a regularized loss function. The results reveal a novel implicit regularization effect of label noise that approximately equals to adding an explicit regularizer. Notably, this regularization effect is more favorable than the implicit bias of vanilla SGD, as vanilla SGD cannot escape from poor global minima, while adding label noise regularizes certain Hessian norm and thus helps the iterates to escape and converge to flat/good minima (that achieve both small main loss and small regularization loss). Empirical results are provided to demonstrate this advantage of label noise regularization.","-The paper studies the behavior of SGD with label noise for over-parametrized models. The label noise setting is linked to an implicit regularization scheme, which is helpful to improve the search for ""better"" optimal as well as to understand the optimization landscape. The study focuses on the analysis for the ""flat"" regions. Theoretical analysis has been provided and empirical studies on ResNet18 and CIFAR10 are shown.","## Post-rebuttal     I thank the authors for their clarifications. We had a long discussion about the technical details, and some aspects became more clear to me, but it did not lead to a resolution of the concern that the theory is somewhat weak. I remain concerned about the restrictive nature of the assumptions regarding the quality of the local minimizer. The authors argue that existing theory guarantees us convergence to such minimizes, but there seems to be a mismatch between the requirements of this work and the proved statements in the cited papers.     ######  This paper studies the question of minimizing a smooth loss function with noise injection inside labels. The authors show that SGD with label noise converges to a stationary point of a certain regularized loss function, where the regularization depends on the amount of noise, batch size, and the stepsize. The paper improves the results of prior work, namely it does not need small stepsizes of the work by Blanc et al., and the model is much more general compared to the paper of HaoChen et al. I find the topic of the work to be quite interesting and worthy of investigation, although it is hard for me to call this paper very insightful.  The results have a lot of novelty but they are also somewhat incremental and many limitations are still present in this paper. I particularly criticize the following aspects of the results: restrictive assumptions, requirement for local initialization (close to a global solution), and decreasing effect of regularization when the target stationarity is small.","This work investigates the implicit regularization effect of stochastic gradient descent (SGD) with noisy labels.  The authors show that SGD with label noise converges to a stationary point of a regularized loss function. The regularization weight can be controlled with the step size, the batch size, and the noise level of the labels.  The authors assume that the loss function to be minimized satisfies standard regularity conditions, e.g., lipschitness and smoothness,  the step size is not too large (relative to the lipschitzness constant), and that the loss is not too flat around any global minimizer.  Assuming that the SGD is initialized close to an approximate global minimizer of the loss $L$ satisfying the above properties, the authors show that SGD with noisy labels will converge to an approximate stationary point of the regularized loss $\widetilde{L}$ (they give the expression of the regularized loss as a function of the step size, batch size, and noise level).  Since the authors do not assume the step size to tend to $0$ as in [1] they find that the reguralization term interpolates between penalizing all eigenvalues of $\nabla^2 L$ equally that happens for small learning rates and penalizing larger eigenvalues more (happens for large learning rates).",0.1732283464566929,0.29133858267716534,0.2755905511811024,0.3283582089552239,0.29850746268656714,0.20463320463320464,0.3283582089552239,0.14285714285714285,0.17073170731707318,0.08494208494208494,0.0975609756097561,0.25853658536585367,0.2268041237113402,0.19170984455958548,0.21084337349397592,0.13496932515337423,0.14705882352941177,0.22844827586206898
879,SP:c479299a8b67c6d9d051e6a84383cd5e256835b9,"This paper introduces ""ExMix,"" a collection of 107 pre-existing NLP datasets across 8 identified task-families. ExMix is proposed as a pre-training dataset, to be mixed with the typical self-supervised pre-training prevalent in NLP. The authors propose to pre-train a T5 model on a mixture of ExMix and C4 (the self-supervised pre-training dataset for T5), which they term ExT5. To do this, they convert every task to a text-to-text problem, so that all tasks, supervised and self-supervised, use standard sequence-to-sequence cross-entropy loss. They pre-train ExT5 for the same number of steps and on the same number of tokens as T5. They then evaluate how ExT5 and T5 perform when fine-tuned to individual tasks. They find that ExT5 largely outperforms T5 on the majority of downstream evaluated tasks, including tasks whose training data was included in ExMix as well as tasks which were not seen during pre-training.  The authors perform an experiment to identify whether or not a manually curated subset of ExMix can provide better pre-training generalization than the full set of tasks, due to negative transfer between tasks. To do this, they examine negative transfer between pairs of task families during fine-tuning. They select the task-families which exhibited the highest amount of fine-tuning transfer across all task-pairs, and use only 48 tasks from these task-families as a pre-training objective. They compare this model to a model trained on a slightly larger subset (55) of randomly selected tasks, as well as ExT5 (trained on all 107) tasks. The authors find that their method of task-selection is largely unhelpful for selecting pre-training tasks - the model trained on a random subset of 55 tasks outperforms the manually curated 48-task model, and ExT5 significantly outperforms both. This suggests that, for pre-training, selecting tasks that complement one-another with respect to downstream performance is not straight-forward, but increasing the number of tasks seems to alleviate negative transfer, rather than exacerbate it.  Finally, the authors perform some additional analysis of the training decisions made around ExT5. They find that: (a) pre-finetuning, which introduces multi-task learning as an intermediate step between self-supervised pre-training and fine-tuning, is not better than mixing all tasks for pre-training when using ExT5, (b) ExMix without self-supervised pre-training is significantly worse than self-supervised training without ExMix, suggesting that self-supervised pre-training is still crucial for pre-training, (c) downstream performance generally increases as the number of tasks scales up, and (d) pre-training with ExMix is significantly more sample-efficient than self-supervised pre-training along, i.e. it achieves strong downstream results very early, even 20k steps into pre-training.",This paper revisits the idea of multi-task learning for natural language processing (NLP) and scales it up to 107 supervised NLP tasks as EXMIX (Extreme Mixture) across diverse domains and task families. It extensively analyzes the co-training effect between the different families of NLP tasks and proposes a model pre-trained using a multi-task objective of self-supervised span denoising and supervised EXMIX named EXT5. They show that a massive and diverse collection of pre-training tasks is generally preferable to an expensive search for the best combination of pre-training tasks. It further shows the effectiveness of EXT5 on a range of supervised NLP tasks. ,"The paper presents an analysis of multi-task learning with the T5 model at a large scale. The tasks are formatted as text-to-text tasks and trained alongside the span denoising objective on C4, same as the multi-task strategy used in the original T5 paper. The paper evaluates this approach for a few different sizes of the T5 model on various downstream datasets, compared with vanilla T5 model. The paper shows improvements on average accuracy over multiple downstream benchmarks. In addition, there are some ablations on the number of tasks, the ratio of C4 vs multi-task, and comparing multi-task with pre-finetuning. ","This paper presents a corpus (ExMix) and a model (ExT5) which extend work on multi-task pretraining for large language models.   Their ExMix covers 8 families of supervised natural language processing tasks (eg. summarization, classification) with three individual tasks in each of them. These tasks are all converted into one text-to-text task (like prior work does) in order to allow uniform training on mutliple tasks. This is an impressively comprehensive collection of multi-task data sets, twice as large as the previously biggest multi-task corpus.   They also perform thorough experiments on a variety of intersting research questions.  They look at how to use cheaper fine-tuning to select a sub-set of tasks to use for pre-training - and they show that it is best to use all the supervised tasks in pretraining, not just the subset which worked best in fine-tuning.  They looked at whether adding and additional fine-tuning step before the final GLUE task fine-tuning would be better than pre-training with all this supervised multi-task data, and they show that in fact it is best to actually pre-train with them.  In their pre-training they mix masked-unsupervised learning with the task-supervised learning and they explore what is the right mix of the two, showing that most mixture settings are good, but that without the unsupervised signal, the model performs much worse. Also increasing the number of tasks generally helps downstream performance.   They also look at the efficiency of their approach. Training on ExMix allows the model to reach the performance of the T5 model trained only on plain text with many fewer (approximately half) the number of steps.   Finally they build a large pre-trained model ExT5 on which they perform really extensive evaluations and the demonstrate that the multi-task pretraining clearly helps and improves on a strong baseline across many tasks, including tasks which have not been included in the ExMix training data like translation.  ",0.08779443254817987,0.08779443254817987,0.14989293361884368,0.1743119266055046,0.3302752293577982,0.3113207547169811,0.3761467889908257,0.3867924528301887,0.21212121212121213,0.1792452830188679,0.10909090909090909,0.1,0.1423611111111111,0.1431064572425829,0.1756587202007528,0.17674418604651163,0.16400911161731208,0.1513761467889908
880,SP:c486c642174e8df3f218c44bd5d11adba52f28c1,"Inspired by applications of fair online learning, the authors initiate the study of *lower bounds* on the regret of OCO algorithms. They provide a general lower bound for FTRL algorithms and show the implications for fixed regularizers and for some special kinds of time-varying regularizers (such as fixed regularizers with time-varying learning rates or regularizers dependent on the past losses). They also provide regret lower bounds on AdaGrad-like algorithms derived from the FTRL framework, and show how linearizing the losses can yield to algorithms with potentially $-\Omega(T)$ regret in $T$ rounds. They also explicitly build the best-case losses for the experts problem with 2 experts, giving sharp bounds for this special case;  Their main application is to fairness for decision theoretic online learning (experts' problem), strengthening previous fairness bounds. They show a way to get fairness guarantees without needing to know the size of the groups, and extending the fairness notion in online learning in a way that is independent of the length of the game.","The paper establishes lower bounds on the best-case regret of the classical FTRL algorithm in online convex optimization. Specifically, the FTRL algorithm is considered in the general form where the regularizer can change over time (e.g., due to a time-varying learning rate). In the case of a fixed regularizer, it is known from earlier work that the regret is always non-negative, i.e., the algorithm never performs better than the best static expert. For time-varying regularizers, this is no longer the case, meaning that the best-case regret can be negative. The paper proves a simple generic lower bound theorem and applies it to several standard settings (Decreasing Hedge; a standard timeless method for decision-theoretic online learning (DTOL); an adaptive gradient version of FTRL), where it is found that the best-case lower bounds are roughly symmetrical to the known worst-case upper bounds on the regret (i.e., if A is a worst-case upper bound, then -O(A) is a best-case lower bound). In other words, these algorithms cannot outperform the best static expert by much, for any input. For DTOL with 2 experts, they show their lower bound to be tight.  For a different setting of the linearized FTRL algorithm, they show that an analogue result does not hold: Although the known regret upper bound is O(sqrt(T)), the best-case regret can be as low as -Omega(T).  An application and the original motivation for the work is in the setting of ""group fairness"" for DTOL: Here, time steps are partitioned into groups, and the algorithm knows which group the current time step belongs to. It is assumed that for each expert, the average loss per time step is balanced across the groups. Then recent work of Blum et al shows that the learning algorithm also has comparable loss in each group. This assumes that the time horizon T is known. This paper allows extending this result to an unknown time horizon, and shows an ""anytime"" extension of the result: If the assumption is strengthened so that for each expert, the losses are balanced across the groups not only after the last time step but also on every prefix of the input, then the learning algorithm also has balanced losses per group at all timesteps.","This work investigates the best-case lower bounds in the online learning setting -- meaning a lower bound on the regret value of an algorithm. Even though an algorithm is usually concerned with minimizing the regret and thus upper bounds on regret are used to describe algorithm performance, information about lower bounds on the regret can also provide information about the algorithm performance, its adaptivity, and in some cases certain notions of fairness. In this submission, the authors consider the FTRL family of algorithms, anytime (no prior knowledge of $T$) and timeless (first-order bounds), with constant, time-varying or adaptive learning rates. The results mainly show that the magnitude of regret of such algorithms stays within the range of the upper bound (or is nonnegative sometimes) with some exceptions. An application to online learning with group fairness is provided as a use case for such bounds.","This paper studies the best-case lower bound, which characterizes the minimum regret that a specific algorithm can achieve in the most benign environments. The authors provide the best-case lower bound for the general FTRL algorithm and show that the bound can be applied for many instances, including Hedge with non-increasing step size, adaptive gradient FTRL. The best-case lower bound for the Hedge algorithm can be used to prove the group fairness in the prediction with expert advice setting. Case studies on the linearized FTRL and Hedge with two experts are also provided.",0.2807017543859649,0.1871345029239766,0.15789473684210525,0.10335917312661498,0.10594315245478036,0.17123287671232876,0.12403100775193798,0.2191780821917808,0.28125,0.273972602739726,0.4270833333333333,0.2604166666666667,0.17204301075268819,0.20189274447949526,0.20224719101123595,0.15009380863039398,0.16977225672877846,0.2066115702479339
881,SP:c4bca2bc4d8731452a48ab1f5482a687b29fff88,"The paper offers a novel training scheme—called SmoothMix—for fitting deep net classifiers with improved adversarial robustness while maintaining accuracy to the extent possible. The key idea is to combine the mixup loss with smooth classifiers. The former interpolates the input sample with its adversarial perturbation, designed to “mislead” the smoothed classifier. The effectiveness of the proposed idea was studied on MNIST, CIFAR10, and ImageNet, showing competitive performance to that of state-of-the-art adversarial defense strategies (Gaussian smoothing, stability training, adversarial smoothing, MACER, and consistency).",This paper looks to improve upon SmoothAdv - a methodology that employs adversarial training on a smoothed classifier to further improve certified robustness. The methodology employed looks to tackling overconfident examples rather than adversarial examples by utilising MixUp. They demonstrate that we can combine MixUp with SmoothAdv to improve certified accuracy.,"The paper proposes a new method for training models to achieve better certified L-2 robustness. To this extent, SmoothMix generates adversarial examples by relaxing the perturbation budget and minimizes a mixup loss. The proposed procedure identifies instances classified with high confidence and near-off-class samples as causes of limited robustness in smoothed classifiers and offers an intuitive way to adaptively set a new decision boundary between these samples for better robustness. Finally, the method obtains improved certified accuracy for certain radii ε that each adversarial perturbation must be in.","The paper observes a connection between the confidence with which a smooth model classifies an input and the robustness radius with which the input's prediction can be certified. In particular, the paper argues that the model's over-confidence on some inputs can be a source for detrimental effects on the certified radius of other (nearby) instances. Inspired by this, the paper proposes to find over-confident near off-class samples (somewhat) near training instances, and conduct mixup training on this pair of instances as a way of improving the certified accuracy of the resulting smoothed classifier. The proposal can be regarded as another way of adversarial training that is better crafted for smoothed classifiers. Experiments conducted on MNIST, CIFAR and ImageNet show marginal but consistent improvements in certified accuracy.",0.11363636363636363,0.19318181818181818,0.22727272727272727,0.3,0.26,0.2111111111111111,0.2,0.18888888888888888,0.15267175572519084,0.16666666666666666,0.09923664122137404,0.1450381679389313,0.14492753623188406,0.19101123595505617,0.182648401826484,0.21428571428571427,0.143646408839779,0.17194570135746606
882,SP:c4cd2e9aff8b8065db56bc72441bbe3631bc4ed0,"The paper analyzes the generalization error of MAML in 2 scenarios that are different in the new tasks at test time which are (i) the training tasks and (ii) unseen tasks, under the assumptions that the loss function is (strongly) convex and the number of steps of SGD in the adaptation step of MAML is 1. In the former case, this work proposes a new notion of stability that is capable of capturing the dependence on the number of data points per task in the bound. In the latter case, this work formulates a generalization error bound based on the total variation distance between the distribution of the new task and those of the training task.","In this paper, the authors provide a generalization and uniform-stability-type analysis for MAML training. In specific, the authors first present a generalization error bound (based on the uniform stability bound) of $O(1/mn)$ for strongly convex objective functions when the new task at the test time belong to one of the training tasks. Then, for a more interesting setting where the new task does not belong to the training tasks, the authors provide a generalization bound consisting of a total variation distance between the data distributions of this unseen task and the training tasks. One toy experiment is provided in the supplementary materials to verify the impact of the sizes $m,n$ on the generalization performance. ","This paper analyzes the generalization bound of MAML-type algorithms from two aspects: (1) when the test task is a sample from the training tasks and (2) when the test task is unseen during training stage. A generalization bound on strongly convex case is given in case (1) showing the generalization error is deceasing at a rate of $O(1/mn)$. On the other hand, a bound under the total variation assumption is given in case (2), showing the bound depends on the average total variation between all the training tasks and the unseen task. The paper focuses on theory and thus no experiments are included.","This paper studies the generalization properties of MAML in the strongly convex setting. It's assumed that there are $m$ training tasks, each with $n$ samples, and the inner loop adaptation is one step of SGD using $K$ samples. It's proved that if the new task is a uniformly random task from one of the $m$ training tasks, MAML incurs a generalization error of $O(\frac{K}{mn})$; if the new task is an unseen task, the error depends on the TV distance between the new task distribution and all the training task distributions as well as their average. The analysis is based on a modified version of algorithmic stability tailored to the MAML setup.",0.2672413793103448,0.3103448275862069,0.3017241379310345,0.25210084033613445,0.2773109243697479,0.3113207547169811,0.2605042016806723,0.33962264150943394,0.3017241379310345,0.2830188679245283,0.28448275862068967,0.28448275862068967,0.26382978723404255,0.3243243243243243,0.3017241379310345,0.2666666666666667,0.28085106382978725,0.2972972972972973
883,SP:c4db31f87600254a03404325ba0560f04e1fc1c7,"This paper investigates the technique of “Holographic Reduced Representation” (HRR), developed in the early 90’s by Tony Plate as a tool for symbolic reasoning, for use in modern neural network architectures trained using backpropagation. The authors find that Plate’s original formulation leads to low accuracy in this setting and present a simple fix that improves performance. The authors apply the technique to classification problems in which the outcome is a high-dimensional binary vector and show that HRRs can be used effectively in conjunction with gradient based training methods in this setting. The paper is well written, technically sound, and discusses an interesting avenue for bridging classic work on connectionist AI with more modern techniques.","The authors introduce an improved formulation of learning using holographic reduced representations (HRRs). The paper shows that the improved HRR formulation allows HRRs to be used for eXtreme Multi-Label (XML) classification tasks, and that using an HRR-based approach leads to improved performance over FC, CNN and LSTM baselines. In addition to comparing against baselines, the paper also assess the impact of various hyperparameters. ","The authors improve Plate's HRRs, which opens up new possibilities for differentiable neuro-symbolic components in neural networks. The authors demonstrate the effectiveness of the introduced projection step in one setting: extreme multi-label problems. The paper is very well written, and the results are convincing with an exemplary analysis.","This paper introduces a modification to the Holographic Reduced Representation (HRR) method, which are a pair of encoding/decoding operations based on circular convolution; these operations can be used to efficiently approximate symbolic querying, information storage, and retrieval operations in a low-dimensional vector embedding space.  used for approximating symbolic storage and retrieval operations.  The first contribution of this paper is a modification to the original HRR method (introduced by Plate 1995 and primarily used in biologically-plausible computation models) that increases its query accuracy.  The second contribution to this paper is a proof-of-concept loss function that allows the modified HRR encoding to be incorporated into differentiable neural architectures.  The final contribution of this paper is a set of experiments showing that concrete HRR-based models can be used in extreme-multi-label (XML) classification tasks, in some cases exceeding baseline accuracy with standard fully-connected layers used for lookup and retrieval (while using a comparatively smaller and more efficient encoding/decoding scheme.)",0.1282051282051282,0.1623931623931624,0.2564102564102564,0.16923076923076924,0.3076923076923077,0.27450980392156865,0.23076923076923078,0.37254901960784315,0.18181818181818182,0.21568627450980393,0.12121212121212122,0.08484848484848485,0.16483516483516483,0.2261904761904762,0.2127659574468085,0.1896551724137931,0.17391304347826086,0.12962962962962965
884,SP:c537ad2ec0cc6d0c6d01c07e4474e7c1eda31f37,"The paper proposes approaching learning a language as a learning problem that is grammar, alphabet, etc. agnostic. Such formulation has resulted in the so called Explanatory Learning (EL) framework that is paired up with an environment to test it. The starting point for learning a language is to extract an interpreter based on a given set of observation and their explanation and then use the interpreter to determine whether an observation belongs to the language, in a binary classification setting. ","The paper proposes a new framework for studying explanation driven machine learning problems called Explanatory Learning. The goal is to learn an interpreter model from explanations paired with observations for a particular phenomenon. The explanations might be in an unknown language, but the explanations paired with observations can be used to learn a good interpreter. Once learnt, the interpreter should be able to follow new explanation for an unseen phenomenon. They refer to this problem as the communication problem. They also define the scientist problem where explanations for the unseen phenomenon is not available. The paper also introduces an Odeen dataset to facilitate experiments with Explanatory Learning. The authors propose a neural network architecture as a solution to the scientist problem. It has two components: a conjecture generator which generates a set of candidate English explanations, and a second interpreter model. Experimental results show better generalization compared to end to end neural systems.  ","The authors introduce a novel learning framework that revolves around learning a map between concepts and their description, where the latter are expressed in a language unknown to the learner.  Two learning problems are defined: (i) learning a new concept from a description of that concepts as well as examples and descriptions of other concepts, (ii) learn a new concept from examples of that concept as well as examples and descriptions of other concepts.  The underlying assumption is that the map (interpreter) from sentences to sets of examples is shared.  The authors propose an environment to evaluate these tasks and a neural architecture to tackle them.  ","The paper considers the problem of classification learning where each data point is accompanied by some explanation. The explanation is in some arbitrary language. The paper proposes a way to increase the performance of the classification task by learning to predict the explanation associated with a given data point, and then using that explanation along with the data point to proceed to make a prediction about the data point's label. The paper implements this pipeline using neural networks, and presents empirical results on a new benchmark dataset to demonstrate its performance.   ",0.3125,0.2375,0.2375,0.1568627450980392,0.13071895424836602,0.16981132075471697,0.16339869281045752,0.1792452830188679,0.20652173913043478,0.22641509433962265,0.21739130434782608,0.1956521739130435,0.2145922746781116,0.2043010752688172,0.22093023255813957,0.18532818532818532,0.163265306122449,0.18181818181818182
885,SP:c6277eddf25a838e91d62d83d6e9af2cf70ade56,"Offline reinforcement learning could enable one to train a data-driven decision making engine from purely offline data. However, the performance of polices learned through existing offline RL methods heavily depend on the quantity and quality of the offline data. When the data is scarce for a task, existing offline RL methods trained on limited offline data could perform poorly.   This work tackles the data efficiency issue of offline RL by transferring knowledge from a source offline dataset to a target offline dataset following an environment dynamics different from the source offline dataset. The proposed method augments the reward function by penalizing the state-action-next state transition tuples that are rare in the source environment. This reward augmentation strategy directly accounts for the dynamics shift between the source and the target domain. On several simulation a real-world tasks, the proposed method significantly improved the data-efficiency of existing offline RL algorithms. ","This paper presents a new offline RL algorithm that aims to adapt the policy to a target domain with insufficient data with the help of abundant data in a source domain. The authors propose their method, DARA, which applies a reward penalty that correct the dynamics shift between the source and target domains. The authors combine DARA with previous model-based and model-free offline RL methods and show that DARA is able to achieve performance improvement over prior offline RL methods without the reward augmentation in both simulated and real-world tasks.","The paper focuses on the dynamics shift problem in the offline reinforcement learning setting, with the assumption that the estimated MDP from the offline dataset is deterministic. It proposes DARA, a framework that relaxes the requirement of data (both the amount of data and the reward information contained in the data) needed from the target task. By capturing the dynamics shift between the source and target environments, DARA modifies the reward gained in the source environment to discourage transitions that have a smaller probability to happen in the target environment. The framework works with both model-free and model-based settings. ","The given work proposes the addition of a dynamics aware framework for Offline Reinforcement Learning. It motivates that  offline reinforcement learning suffer from a dynamics shift in SARSA transitions, which need to be effectively handled. They propose modifying reward in source offline dataset, penalizing it with the dynamics aware reward transition, setting the lower bound. The reward augmentation is done with explicit policy constraints as well as implicit policy constraints.They evaluate this approach on various robotic manipulation task from the Gym-MujoCo suite, evaluating and showcasing perforomance on four offline RL algorithms.  ",0.16339869281045752,0.1830065359477124,0.1568627450980392,0.26881720430107525,0.16129032258064516,0.18811881188118812,0.26881720430107525,0.27722772277227725,0.25806451612903225,0.24752475247524752,0.16129032258064516,0.20430107526881722,0.2032520325203252,0.2204724409448819,0.1951219512195122,0.25773195876288657,0.16129032258064516,0.19587628865979384
886,SP:c64a08182848150d35f6cc4e49bc1bd087631995,"The authors propose a novel method to learn disentangled representations from data. Their approach uses some inductive biases on the transformation between the latent and observed spaces, as well as on the latent distribution. Namely, they assume the transformation satisfies the assumption of the local isometry, and that the latent factors are (non-jointly) non-gaussian. The disentangled representations are learned by first applying a spectral transform to the data, followed by a linear ICA unmixing.  Their method is backed by a theoretical analysis, as well as an empirical validation.","The paper follows the recent question of when disentanglement is possible and shows that 'local isometry' is sufficient to ensure that it is possible (not guaranteed) to learn a disentangled representation. This result largely follows from know results in the ICA community alongside the isometry assumption. A proof-of-concepts algorithm that combines HLLE with ICA demonstrates feasibility on synthetic data constructed to satisfy the isometry assumption. While the paper is largely correct, I would argue that it is uninteresting as it (as far as I can tell) only applies to flat manifolds, which is the case we already understand well through linear models.","The paper shows that having local isometry together with non-Gaussianity of latent variables and sufficient number of samples guaranteed to learn a disentangled representation, without any domain-specific knowledge. The authors discussed some potential manifolds for which local isometry is satisfied. The authors also conducted several experiments to evaluate the disentanglement of the obtained representation by their method in contrast to the comparable approaches. The authors demonstrated the latent embedding and the mean correlation coefficient as qualitative and quantitative disentanglement measures, respectively.    ",This paper studies the disentanglement of samples generated by GANs. It argues that the local isometry and non-Gaussian property are the keys to disentanglement. Some intuitive experiments based on GAN latent space discovery are conducted to support this insight.,0.16666666666666666,0.15555555555555556,0.12222222222222222,0.19230769230769232,0.08653846153846154,0.12048192771084337,0.14423076923076922,0.1686746987951807,0.275,0.24096385542168675,0.225,0.25,0.15463917525773194,0.16184971098265896,0.1692307692307692,0.21390374331550804,0.125,0.16260162601626016
887,SP:c689f91072b181c48ac16c15d7c0875bcb55849c,"(Implicit)^2 models bring implicit computation to bear on implicit representation. Deep equilibrium network editions of the SIREN and MFN implicit representation architectures are defined and experimentally compared and contrasted with their standard explicit counterparts. The (implicit)^2 editions are optimized with two proposed approximations: reuse of fixed points during forward and truncation during backward. When these approximates are used with the proposed implicit architectures the training time and memory alike are reduced to constant w.r.t. the effective ""depth"" of the model as counted in fixed point iterations. This differs from explicit models, where time and memory scale with the number of layers (L), and standard use of deep equilibrium nets, where time scales with fixed point iterations (M) and memory is constant. As a further point on optimization, regularization by spectral normalization is found to be useful for (implicit)^2 training. Analysis experiments justify each approximation, in that the effects are clear in Figures 3 & 4, but the conditions of these experiments are not specified. The quality of results for explicit and implicit^2 are compared for a single input each for the three modalities of image, audio, and video. (implicit)^2 improves PSNR in all cases, and even by 10 or more percent relative for a 512x512 grayscale image and 23 percent relative for a 7-second sound. Accuracy (PSNR) and computation (memory usage in GB and training step time in sec.) are measured across three architectures for one input image: accuracy is higher or equal across model sizes, the text claims 3x faster training speed (in total?) and the text specifies memory saving though the reported charts do not (see Figure 5). The text motivates the computational attractiveness of implicit computation for implicit representations, but does not cover alternatives for reducing time or memory with explicit computation for implicit representations (like sampling/mini-batching, gradient accumulation, reversibility, model decomposition like in KiloNerf or ACORN etc.).","This work suggests that combining implicit-depth models (e.g. DEQs) with implicit representation approaches (e.g. SIREN) can bring performance improvements. The method is evaluated on image, audio and video representation.","Summary ------------------ This manuscript proposes the use of implicit differentiation (i.e. using the implicit function theorem) to train variants of so-called ""implicit"" representations (also known as ""coordinate based networks"")  the authors develop. Specifically the manuscript extends the formulations of SIRENs and MFNs in the form of Deep Equilibrium  Networks (DEQ) that can thus be trained through implicit differentiation. To make training of those  architectures efficient, the manuscript suggests two approximations. The first approximation lies in the  forward pass and circumvents the use of an expensive Fixed Point Solver. The second approximation lies in the backward pass and circumvents the expensive inversion of typically ""large"" Jacobian matrices. The authors evaluate their new architectures and training schemes on image, audio and video fitting and show that smaller Implicit^2 networks can fit signals better than SIREN or MFNs.","The paper proposed $($Implicit$)^2$, which replaces the internal layer of implicit neural representation (INR) with implicit layers. The proposed method is efficient in terms of memory and training time. For the fast training, the author proposed the re-usage of fixed-point and also used truncated backpropagation (approximated gradient). The method is sensible and also empirically demonstrates its effectiveness.",0.0375,0.10625,0.071875,0.34375,0.21875,0.13138686131386862,0.375,0.24817518248175183,0.38333333333333336,0.08029197080291971,0.11666666666666667,0.3,0.06818181818181818,0.1487964989059081,0.12105263157894737,0.1301775147928994,0.15217391304347824,0.18274111675126903
888,SP:c6c2321e9e898765d71153adf6602f548a376b6b,The paper proposes an approach for large-scale vision-language representation learning that focuses on achieving fine alignment between modalities through a late interaction strategy.  Experiments on zero-shot image classification and image-text retrieval tasks show that the proposed approach is able to achieve state-of-the-art performance. A large-scale image-text dataset is also constructed from the web for pre-training.,"The paper proposes to utilize the fine-grained alignment between visual tokens and text tokens in the contrastive loss for language-image pertaining. More specifically, the similarities between images and captions are calculated by averaging the token-wise maximum similarities. The experiments on both zero-shot image classification and image-text retrieval with different pre-trained datasets validate the effectiveness of the proposed model.    ","The paper tackles the problem of large-scale vision-language pre-training which has been quite popular recently. The main issues identified by the paper are: 1) fine-grained information might be lost due to the use of global features, and 2) alignment between text and language is typically done using cross- or self-attention, which are inefficient. The authors proposed a dual-stream method that 1) uses visual ""patch"" tokens from ViT models and 2) perform late fusion of the two modalities using a variant of the method from Khattab & Zaharia (2020). ","(1) The core idea of the proposed FILIP method is to achieve word-patch alignment by token-wise similarity matrix through cross-modal late interaction by modifying only contrastive loss, which is both training and inference efficient. The authors collected FILIP300M, a large-scale cleaned image captioning dataset from the Internet for FILIP’s V-L pre-training.   (2) FILIP achieves SoTA performance on ZS image classification and ITM tasks. Visualization results show its promising ability of fine-grained (visual-textual token) classification and localization.   (3) To the best of our knowledge, FILIP is the first work with such high accuracy on F30k that not only does it reach double full mark of R@K (i.e. i2t[R@5 and R@10] are both 100.0), but also it achieves 580+ of rsum with only one single model (no ensemble). ",0.2923076923076923,0.24615384615384617,0.26153846153846155,0.234375,0.25,0.17204301075268819,0.296875,0.17204301075268819,0.12056737588652482,0.16129032258064516,0.11347517730496454,0.11347517730496454,0.29457364341085274,0.20253164556962028,0.1650485436893204,0.1910828025477707,0.15609756097560976,0.13675213675213677
889,SP:c6ff324e9ef95b932ab583b024c66eb30036538d,"This study takes use of pruning-rewiring-pruning method to get a high unstructured sparse model. Broadly, this is one dropout regularization method. With step-by-step growing and pruning, most of the weights are close to $0$ and will be pruned in the next step. Experiments show that this training method can improve the sparsity and performance for a wide range of architectures and applications,","This paper presents a scheduled grow-and-prune approach to produce a sparse CNN model. In particular, a sparse model is achieved by repeatedly growing and prune subsets of layers during network training. The main difference between this approach and other approaches that do pruning during the training time is that this approach guarantees that all the weights are explored when a pruning decision is made.  In the paper, two variations of the approaches are discussed: C-GaP and P-GaP. The theoretical convergence of the C-GaP is analyzed. Experimental results on various datasets and tasks are presented, which shows improvement compared to baseline approaches. Additionally, key variables such as partition number are explored and empirical results are provided in the supplementary results. ","Note: I have previously reviewed this paper for NeurIPS 2021 and I will be reusing my review from then and add the additional comments along the way based on how the paper has been updated along with the promised updates from the previous revisions.   This paper proposes a way to obtain sparse neural networks by repeatedly growing and pruning parts of the model in a scheduled fashion. Figure 1 gives a good idea of the whole technique. It basically divides the whole network into K parts and only one of these K parts is dense while the rest are sparse. The dense part keeps cycling ensuring over-parameterized learning when needed. The paper proposes that these scheduled routines help in improving the accuracy of the sparse neural network models compared to other sparsity-inducing techniques.  The paper has extensive experimental evaluation on tasks like classification, object detection, 3D object part segmentation, and Transformers. The authors also try to provide some theoretical explanations to these proposed ideas.  The paper has updated visualizations to make the algorithm much more clearer from the earlier version which is commendable. ","This paper presents a new framework for model pruning. Unlike most previous pruning works, this framework does not require pre-training a dense model initially. They have proposed a schedule grow-and prune strategy to fulfill such a goal where the whole network has been split into multiple partitions. There will be an alternation of pruning and growing during the pruning, which ensures that all network parameters are updated in certain loops. Moreover, the authors provide theoretical insight behind such a design that all the weights trained per round are crucial for convergence. To accelerate the pruning process, this paper presents a parallel version that the growing and pruning of multiple partitions would be computed simultaneously. The accuracy and efficiency of the proposed framework have been verified on multiple tasks, including 2D images classification/detection, 3D detection, and text understanding.",0.2878787878787879,0.22727272727272727,0.25757575757575757,0.20967741935483872,0.23387096774193547,0.16216216216216217,0.1532258064516129,0.08108108108108109,0.12142857142857143,0.14054054054054055,0.20714285714285716,0.21428571428571427,0.19999999999999998,0.11952191235059761,0.1650485436893204,0.16828478964401292,0.2196969696969697,0.1846153846153846
890,SP:c72108efe2051fe059603ad6b97d98cccd11da16,"In this paper the authors propose a learning algorithm for accelerating solving of bipartite minimum weight perfect matching problems, which are combinatorial optimization problems that can be solved in polynomial time. These problems can be represented as linear programs, and they propose to learn the initial point to the dual problem, which, when feasible, can be fed to the Hungarian algorithm to kick-start the optimization process. Given a collection of training and testing instances, they propose to solve the training instance to obtain their optimal dual solutions and take their median as a learned initial dual point. Given a new test instance, they propose to take this training median as initial point, make it feasible for the test instance using a greedy heuristic, and solve the test instance using the Hungarian algorithm initialized with this feasible dual point. They compute (Theorem 1) the sample and time complexity of the algorithm, and show empirical improvements on two distribution of problems compared to a vanilla Hungarian algorithm.","The paper gives a learning-augmented algorithm to improve the running time for the minimum weight perfect matching (MWPM) problem in bipartite graphs. For a given bipartite graph, they consider the setting that the vector of (integral) edge weights is sampled from a probability distribution. For any given (unknown) distribution, they show that - one can efficiently learn dual variables that minimize the expected l1-distance to the optimal dual variables for a random instance sampled from the given distribution - these learned dual variables can be efficiently turned into a *feasible* dual solution for the given instance at the expense of increasing the aforementioned l1-distance by a factor at most 3. - using this feasible dual solution to warm-start the known Hungarian method algorithm to solve MWPM, the running time improves from O(mn) to O(min{mn,m\sqrt{n}alpha}), where alpha is the aforementioned minimal expected l1-distance for the given distribution.  Additionally, they provide an extensive experimental evaluation confirming an improved running time when the input distribution is sufficiently concentrated, and a running time comparable to the classical Hungarian method algorithm when the distribution is not concentrated. On instances generated from real-world data, the observed running time improvement was of the order of roughly a factor 2.","any combinatorial optimization problems may be solved or approximated with primal-dual algorithms built on a linear programming relaxation of the problem. These algorithms maintain an infeasible primal solution that is driven towards feasibility and a (matching) feasible dual solution that is driven towards optimality. The algorithm terminates once the primal solution becomes feasible and uses the feasible dual as a quality guarantee in the analysis.  The aim of the submission is to improve the running time of one particular primal-dual algorithm -- the Hungarian method for weighted bipartite matchings -- by using machine learning. The goal is to augment the algorithm with learned predictions that improve the (theoretical and empircal) running time if good, and that fall back to the theoretical worst-case otherwise.  Instead of trying to predict a primal solution (something the literature has determined to be difficult for bipartite matchings), the authors propose to predict a dual solution that is already driven some way towards optimality and to use that solution to ""warm-start"" the algorithm. The difficulty here is that the prediction might not be a feasible dual solution (and thus useless for the Hungarian method). The authors fix this issue by providing a (linear time) procedure that turns an infeasible predicted dual into a feasible dual while increasing the distance to the optimum dual by at most a factor of 3. Finally, the authors prove that duals can be learned approximately from O~(C^2n^3) samples if the matching instances are drawn from a fixed, but unknown distribution (C being the maximum weight of any edge).   To argue that the learning provides an empirical improvement, the authors compare their warm-started method against the classical Hungarian method. The results convinced me that the warm-started method works well.","The paper proposed a learning-augmented algorithm that receives a learned dual solution and uses it as a warm-start to improve upon the running time of the Hungarian method for weighted bipartite matching. The paper provides a neat 3-stage approach for obtaining “faster” algorithms for matching problem: 1) Learn a helpful dual solution for a given instance, 2) Convert a given predicted dual solution to a feasible dual solution, and 3) Lastly, get a near-optimal solution starting from the feasible “warm-start” solution. The paper provides an end-to-end algorithm for this problem using the described approach.",0.2469879518072289,0.2289156626506024,0.14457831325301204,0.2358490566037736,0.17452830188679244,0.13945578231292516,0.19339622641509435,0.1292517006802721,0.2376237623762376,0.17006802721088435,0.36633663366336633,0.40594059405940597,0.21693121693121695,0.16521739130434784,0.17977528089887637,0.19762845849802368,0.23642172523961658,0.20759493670886076
891,SP:c737b2cfe28f927b68c3a3f2ae434cba74cfb5ab,"This work studies the representational power of Residual Networks and try to explain theoretically a desirable property of using ResNets, namely: does a residual network guarantee that stacking more residual blocks does not lead to a degradation in performance of the network. The main approach of the authors is to view the output of a ResNet as an ensemble of several shallow networks as opposed to a non-ResNet which is simply a very deep composition of non-linear functions. Given this view point, the authors identify some simplifying assumptions which make the associated Empirical Risk Minimization problem tractable: 1) introducing a sequence of ""de-coupling"" weights (H) which allow the weights (W) in the ensemble to be independent of each other 2) removing the non-linearity at the end of the residual block and before the final affine transformation and 3) a few assumptions on the loss function (differentiability and convexity). With these assumptions, the authors prove a few desirable properties of ResNet architectures. Firstly, the authors show that stacking more residual blocks cannot hurt in the representational sense, and can improve if the new feature is linearly independent of the other features in the ensemble basis. Secondly, the authors show that ""wide ResNets"" always attain the empirical risk minimization lower bound, which has an implication on the quality of local optima attained by the ResNets. The authors also prove analogous results for a related class of architectures (DenseNets), in particular showing that DenseNets may be approximated by wide ResNests.","The authors proposed a couple modification of ResNet in order to improve the loss landscape. The main idea behind the modifications, if I understand correctly,  is to guarantee a direct skip connection (dense matrix or product of dense matrix) from the hidden features to the logits. The authors provide a  couple theoretical claims regarding the proposed models. No experimental results are provided. ",The authors present several theoretical properties of a ResNet variant that is similar to a standard ResNet only without one of the non-linearities. Variants of DenseNets are also suggested with improved guarantees. Such variants are much closer to the real ResNet that is used in computer vision applications than the models that were previously analyzed. ,"This paper studies a variant of ResNet/DenseNet, which removes the final nonlinearity. This makes the output a linear combination of a set of basis functions, which can be viewed as features extracted by the various blocks of the architecture. Based on this formulation, results regarding the optimization minima are examined. An augmented version of the architecture in question is also analyzed.",0.08366533864541832,0.0796812749003984,0.09561752988047809,0.20967741935483872,0.20967741935483872,0.17857142857142858,0.3387096774193548,0.35714285714285715,0.3870967741935484,0.23214285714285715,0.20967741935483872,0.16129032258064516,0.134185303514377,0.13029315960912052,0.1533546325878594,0.22033898305084748,0.20967741935483872,0.1694915254237288
892,SP:c757b5252941fe843fefc36a537b7bd6cb404b12,The authors study the problem of minimizing matrix factorization squared error with gradient descent. They show convergence with larger learning rates than previously considered. They further show that gradient descent has an implicit bias towards finding matrix factors which are magnitude-balanced in a certain sense.  ,"This paper studies the properties of gradient descent with large learning rate in the matrix factorization problem. The goal is to understand when gradient descent converges to a global minimum where the two factors are roughly balanced in norm, which is a shallow minimum that may be hoped to generalize better. For small learning rates (i.e. where classical optimization results guarantee convergence to a critical point), this does not happen. Hence, this paper considers learning rates beyond this regime.  For two special cases (when the matrices are 1xd or dx1), it is shown that under a large learning rate, gradient descent still converges to a global minimum, and this minimum is roughly ""balanced"". For the general case, it is shown that for almost any initial point, if the learning rate is such that gradient descent converges to a global minimum, then this minimum is roughly balanced.","This paper studies gradient descent optimization of matrix factorization (various forms, including the most generic case) under large learning rate. The authors prove convergence results that apply when the learning rate is larger than 2/L and up to ~4/L, and also show that optimization with large learning rates leads to dynamic balancing between the two matrix factors in the factorization, an effect that does not arise when using small learning rates. In proving some of the theorems, the authors observe the optimization can have two phases: one in which gradient descent is driven to search for flat regions, and a second phase near the global minimum where convergence occurs towards a balanced solution for the factorization problem. ","This paper is concerned with the non-convex matrix factorization problem. This paper considers the square loss, which is optimized via gradient descent. Due to symmetries of the problem, there are infinitely many different global minima. This paper aims to understand to which minima gradient descent with large learning rate (=step size) converges to.   The main claim of this paper is that gradient descent with large learning rate has an implicit regularization effect, meaning that it converges to factors of the underlying matrix, which are implicitly balanced in norm. To substantiate the claim, the paper provides several theoretical results as well as some numerical experiments.  ",0.30434782608695654,0.2608695652173913,0.391304347826087,0.21768707482993196,0.20408163265306123,0.20168067226890757,0.09523809523809523,0.10084033613445378,0.17142857142857143,0.2689075630252101,0.2857142857142857,0.22857142857142856,0.14507772020725387,0.14545454545454545,0.2384105960264901,0.24060150375939848,0.23809523809523808,0.21428571428571425
893,SP:c7b4e1f0ffad07811d84c1115a0c83e8388b2c00,"This paper introduces a new framework to remove non-biological variability to facilitate data aggregation across studies. The problem is viewed as a transformation of the input data, such that the transformed output is not informative of the “domain” (e.g., experimental condition or study) while simultaneously capturing relevant information from the data. This high-level procedure is implemented via an adversarial training strategy, where the mutual information terms of the objective function are approximated via a standard cross-entropy or MMSE loss. The authors validate their method using two murine datasets, in which the neural recording procedures are slightly different; the proposed framework outperforms the standard Harmony and scGen in the supervised and unsupervised settings.","The paper casts the removal of inter-experimental variability from functional data in systems neuroscience in the theoretical framework of domain adaptation. The paper adapts various approaches and subsequently demonstrate improved performance of cell type assignment. The paper also shows that the proposed theoretical framework can produce predictions that are best aligned to the anatomical data, as well as effectively reduces inter-experimental variability to reveal unobscured biological effects.","In the paper ""Removing Inter-Experimental Variability from Functional Data in Systems Neuroscience"", the authors propose a novel framework for alleviating inter-experimental variability. They present a supervised as well as an unsupervised variant of their approach and evaluated the framework on cell type datasets. They further compared their results to EM data, giving further credibility to their findings.","The authors develop a method for removing inter-experimental variability in systems neuroscience experiments. The general idea is to learn a new representation that maximizes the information retained about the data (and in the supervised case, also the signal of interest), while minimizing the information about the individual experiments. To do this, they use an adversarial training approach, where one network tries to remove information about the individual experiments while retaining total information in the new representation, and another network tries to predict the individual experiment from this new representation. They demonstrate their approach on experimental 2-photon recordings of retinal bipolar cells, recorded in 2 different experiments.",0.1206896551724138,0.09482758620689655,0.20689655172413793,0.2898550724637681,0.2318840579710145,0.288135593220339,0.2028985507246377,0.1864406779661017,0.2222222222222222,0.3389830508474576,0.14814814814814814,0.1574074074074074,0.15135135135135136,0.12571428571428572,0.2142857142857143,0.31250000000000006,0.1807909604519774,0.20359281437125748
894,SP:c84533f878f15e8c94a1291bd69c9173d754d1ed,This paper demonstrates how to use pretrained CLIP model as a reward function for an RL agent. This approach enables flexible goal specification using language. The proposed reward generation method uses CLIP to identify the relevant objects and a separate module to compute the reward based on a specified spatial relationship. This separation produces a better reward function than using the dot product between the embedded image vector and language vector. ,"The paper proposes a framework to train policies for tasks that are specified via language text without the use of any expert trajectories or underlying state information to engineer reward functions. The authors leverage a state-of-the-art visual grounding model (CLIP) to ground object nouns from the text into the current visual observation in order to derive a proxy for the rewards signal. Through some toy experiments they demonstrate performance of a simple baseline that implements this concept as well as enhancements to the baseline (saliency grounding + spatial relationship based rewards) that overcomes drawbacks of the naive approach. They also train their language-conditioned on several tasks to obtain a generic policy that can learn to solve an arbitrary task, given paired text descriptions and trajectory rollouts from the training tasks.",The paper proposes an approach for zero-shot reward specification using natural language groundings without expert demos and state information. The work proposes using CLIP’s image+language encoder to find the saliency maps and separately encoding the target locations for the object. These objects are identified by parsing the task sentence. The work proposes automatically getting a reward function by again using the sentence. The work claims that automatically parsing the goal text and using a heuristic to get a reward function works better than using the CLIP's language encode to goal text and get the rewards with a dot product.,"The paper presents a hybrid learning/rule-based approach to language-conditioned reward specification, particularly for robotic control tasks from images. The core contribution of the work is in specifying the reward 0-shot. Specifically, the method parses a language instruction into an object noun phrase and desired spatial configuration. Then it uses CLIP embedding of each object phrase as a query, and uses Grad-CAM to get a saliency map in the image which is used to localize the target object. Finally a set of heuristics are used to compute the reward for a particular spatial configuration given the target object(s) coordinates. The reward can then be used to train either single or multi-task policies. Experiments suggest that in Simulated Fetch robot domains this approach can nearly match an Oracle reward.  ",0.22535211267605634,0.30985915492957744,0.28169014084507044,0.19548872180451127,0.18796992481203006,0.1941747572815534,0.12030075187969924,0.21359223300970873,0.14925373134328357,0.2524271844660194,0.1865671641791045,0.14925373134328357,0.15686274509803919,0.2528735632183908,0.19512195121951217,0.22033898305084745,0.18726591760299624,0.16877637130801687
895,SP:c8ab5a370b3e417da65e6a4aa86afc7c4452da2a,"This paper studies the problem of fair ranking under a new notion of fairness. The main point of the paper is that uncertainty is a primary reason for the lack of fairness. In defining this new notion of fairness, authors base their arguments on the axiom that if $A$ has merit greater than or equal to $B$ with probability at least $p$, then a fair policy should treat $A$ at least as well as $B$ with probability at least $p$. Based on this, they say a ranking policy is fair if the probability that it ranks an agent in the first $k$ positions is at least the probability that it actually is among the top $k$ agents. They similarly say an algorithm is $\phi$-fair if the probability that it ranks an agent in the first $k$ positions is at least a $\phi$ fraction of the probability that it actually is among the top $k$ agents.  Authors then try to design algorithms for fair ranking. They first propose two very simple algorithms which achieve fair and approximately fair ranking without considering its impacts on the utility of the principal. They then show how to compute rankings that optimally trade-off approximate fairness against utility to the principal. Finally, they conduct empirical studies to evaluate the impact of their approach on a paper recommendation system that they built and fielded at a large conference. ","The paper considers ranking given access to posterior distributions on true scores. They propose a notion of fairness based on these posterior distributions, essentially requiring that probability of inclusion among the top $k$ scale with the probability of being ranked in the top $k$ according to the posterior distributions. These posterior distributions are presumed to be fair, in the sense that an algorithm which is fair (according to this definition) need only be fair with respect to the posterior distributions. They show that a convex combination of Thompson Sampling and ranking by means is approximately fair but can be suboptimal in terms of utility. They give an optimal algorithm (subject to their fairness constraint) by formulating the problem as a linear program. Finally, they run an experiment on a movie recommendation dataset and implement their algorithm in a real-world paper recommendation setting, comparing their two algorithms and measuring cost of fairness. ",The paper proposes a new definition of fairness in rankings. An LP-based algorithm is given for finding best (max utility) ranking for a given fairness level. Two empirical evaluations of the proposed frameworks are given 1) based on ranking movies from a public dataset and 2) an experiment was run during a large virtual conference in which participants were given paper recommendations according to a ranking.  ,"This paper studies the fairness issue in algorithmic decision-making caused by uncertainty. Since the actual merits are hard to observe, the authors propose a new notion to capture the uncertainty, named approximate fairness in ranking. Based on the notation, they propose different algorithms to achieve approximately fair ranking distributions. In addition to the theoretical analysis, extensive empirical studies are conducted to evaluate their approach.",0.17167381974248927,0.09012875536480687,0.12446351931330472,0.11842105263157894,0.10526315789473684,0.16417910447761194,0.2631578947368421,0.31343283582089554,0.4461538461538462,0.26865671641791045,0.24615384615384617,0.16923076923076924,0.20779220779220778,0.14,0.19463087248322147,0.1643835616438356,0.14746543778801843,0.16666666666666669
896,SP:c8dca3ada843453bf9dfa42609225955d731d0ec,This paper propose a novel method for knowledge distillation on object detection by measuring the correlation between each object and human observed instances. They have introduced a conditional modeling paradigm to find the crucial regions for distillation and apply the auxiliary tasks to supervise its training. Good experiment results have been achieved on COCO.,"This submission proposes a novel knowledge distillation approach for object detection. The authors attempt to use an auxiliary task with the key-query (transformer-liked) method to determine where and how the feature map can be transferred from a teacher to a student. In the paper, the target instances are encoded as queries to determine the knowledge distilled from the teacher. Extensive experiments show the effectiveness of the proposed approach.  ","This paper proposes a new method named Instance-Conditional knowledge Distillation (ICD) to deal with the knowledge distillation problem in image object detection task. Motivated by the fact the traditional classification-level distillation methods cannot focus on the locations of objects, they insert additional instance information to the distillation training together with auxiliary tasks, i.e. recognition and localization. Their proposed distillation framework improves the performance of the student network, and even outperforms the teacher network under some specific settings.","The paper proposes a target-oriented solution towards knowledge distillation for object detection. Unlike prior works which focus on distilling the global representations or foreground-region representations, the paper aims to find relevant regions for distillation based on the instance-conditional information. The auxiliary task optimization helps in finding the related knowledge. The paper performs extensive experiments on MS-COCO to show the efficacy of their method.",0.25925925925925924,0.2962962962962963,0.2962962962962963,0.22857142857142856,0.2857142857142857,0.2375,0.2,0.2,0.23880597014925373,0.2,0.29850746268656714,0.2835820895522388,0.22580645161290322,0.23880597014925375,0.2644628099173554,0.21333333333333332,0.291970802919708,0.2585034013605442
897,SP:c8ef4f1d48dfc8bd7d1d81302e689297b0046252,"This paper theoretically proves that the contrastive learning results in the dimensional collapse in the feature representation space. By analyzing the covariance matrix of the embedding space, the authors claims that it is specifically arise from the strong augmentation and the implicit regularization. To circumvent this issue, the authors propose a novel method of contrastive learning without using the projector at the end of an encoder.","This paper shows that contrastive methods also suffer from the “dimensionality collapse” phenomenon, a milder version of the “total collapse” problem that originally motivated the development of using explicit negative pairs to prevent learning trivial solutions. Two underlying causes for this problem, i.e too strong data augmentation and implicit low-rank regularization, are proved in simplified settings of shallow, linear networks. The dimensionality collapse problem is then related to the embedding projector component of SimCLR, and theoretical analyses are used to motivate an alternative training technique without the need for the embedding projector. ","This paper investigates the collapsing problem of contrastive learning. It attempts to attribute the collapsing phenomena to strong augmentation and implicit regularization, using simple linear network models. Based on the above analysis, the paper then propose a simple sub-vector based CL method called DirectCLR.",This paper firstly studies the dimensional collapse problem in existing contrastive learning (CL) methods. The authors provided both empirical and theoretical results to reveal that the popular SimCLR method may incur dimensional collapse by showing the low-rank property of the covariance. They thus propose a new framework called DirectCLR to solve the dimensional collapse in CL. Experiments on ImageNet demonstrate the effectiveness of the proposed method. ,0.3181818181818182,0.24242424242424243,0.2727272727272727,0.14893617021276595,0.1702127659574468,0.3111111111111111,0.22340425531914893,0.35555555555555557,0.26865671641791045,0.3111111111111111,0.23880597014925373,0.208955223880597,0.26249999999999996,0.2882882882882883,0.2706766917293233,0.2014388489208633,0.1987577639751553,0.25
898,SP:c8ef5a9da4ec3e9d8b550977a58859cafa6a1b18,"This paper presents a a new approach to solve the Robust PCA problem. They use matrix factorisation to formulate RPCA as a more scalable problem, then learn (or optimise) parameters during the algorithm's each iteration. They also add a recurrent layer at the end to allow for ""unlimited"" iterations. The methods is compared against classic RPCA solvers, but not against learned methods.","The paper studies the problem of robust PCA. The authors first proposed an improved algorithm based on the previous work ScaledGD. Then they proposed to use ""learning to optimize"" to select the hyper parameters dynamically and adaptively. The numerical results showed that the proposed method is more effective than two baselines.","The paper propose a new technique for robust PCA that can be realised as a trainable neural network. The technique provides similar theoretical guarantees to existing methods, yet scale better to larger problems. The paper is well-written and mostly easy to follow.","The authors study the problem of robust PCA inspired  by a LISTA type approach. In particular, the authors consider  a hybrid model of feedforward, and recurrent NN architectures and ""learn the parameters of (scaled) GD"" that is used to solve RPCA. I am not extremely familiar with the literature, but essentially the hybrid model allows for a non-apriori (in terms of the architecture) number of iterations that the algorithm can run for. This seems particularly interesting to me. The authors show that under certain conditions of the parameters, the algorithm enjoys linear convergence. The authors also show synthetic data experiments and a use-case of background subtraction to validate their results. ",0.1746031746031746,0.1746031746031746,0.19047619047619047,0.17647058823529413,0.29411764705882354,0.23255813953488372,0.21568627450980393,0.2558139534883721,0.10714285714285714,0.20930232558139536,0.13392857142857142,0.08928571428571429,0.19298245614035087,0.20754716981132076,0.13714285714285712,0.1914893617021277,0.18404907975460122,0.12903225806451615
899,SP:c8f0c34d2107caa7c162382fe75eb2bb1cd26cdc,Authors present a tensor kernel ridge regression method where the covariance kernel is replaced with random Fourier features to reduce the typical O(N^3) cost down to O(NM^2) for M random frequencies. RFFs typically have slow convergence in approximating the full kernel w.r.t. to the number of random frequencies. The tensor is assumed to have a rank-R CPD decomposition to further reduce the complexity of the problem.,"In this paper, the authors propose a novel kernel approximation scheme based on deterministic Fourier features and tensor decomposition. The proposed approach improves previous efforts in the sense that (i) it deduces the (kernel) approximation error to decrease exponentially with the number of frequencies (or Fourier features); and (ii) it overcomes the ”curse of dimensionality“ with the low-rank tensor decomposition approach, the complexity of which scales linearly in the input dimension. Simulation results (on the performance of tensor kernel ridge regression) are provided to validate the proposed method, on both small and large dimensional datasets.","This work proposes to use tensor decomposition techniques to approximate the deterministic Fourier features (DFF), in large-scale kernel learning. The main idea is that, after the DFFs are generated in the tensor form, low-rank approximation by tensor decomposition is applied, which can be formulated as a regularized convex optimization. Block coordinate descent is then used to solve the problem effectively. Experiments on kernel ridge regression and Gaussian process regression with synthetic and real datasets confirm the effeciency of the proposed approach.","The authors propose a method for inference of large-scale gaussian process (GP) regressio problems with relatively high-dimensional inputs. The method achieves state of the art performance on real-world datasets using 2 innovations 1) formulation of the regression problem in the primal form with deterministic Fourier features, 2) reducing the dimensionality of the regression weights in the primal space by  defining them as a low-rank tensor. The authors describe the advantages and disadvantages of different choices for the tensor decomposition and demonstrate computational and inferential performance on both synthetic and real datasets. Of note is their ability to achieve comparable performance using a laptop computer to competing methods using clusters.",0.2876712328767123,0.2054794520547945,0.2465753424657534,0.22916666666666666,0.2604166666666667,0.21686746987951808,0.21875,0.18072289156626506,0.1592920353982301,0.26506024096385544,0.22123893805309736,0.1592920353982301,0.2485207100591716,0.19230769230769232,0.19354838709677416,0.24581005586592175,0.23923444976076555,0.1836734693877551
900,SP:c91d970eeb6460b65d0752d2da6605556b27d242,"This paper proposes a different density function for popular density-based clustering algorithms, i.e. truncated symmetric and asymmetric Gaussian kernels for DBSCAN and DPC. It offers some diffusion-based argument for why that is a better density function for clustering than naive function, and provides a computationally more efficient surrogate function as well.  Performance of proposed method on many clustering experiments are provided.","The authors propose a new density function for density clustering models such as DBSCAN and Density Peaks Clustering (DPC). Given a kernel $K$ and the corresponding normalized random walk transition matrix $P = diag(K\mathbf{1})^{-1}K$, the authors propose to use the density function which corresponds to the stationary distribution attained in the limit. Since this function is expensive to compute, the authors propose a surrogate density function which is easier to compute.  The main experiments compare various density functions for DPC on UCI classification datasets and the DPC on density functions to face detection methods on two face identification datasets. ",The paper presents a methodology for defining the density reference function required in density-based clustering methods such as DBSCAN and DPC. The density function is obtained as the limiting probability density of a diffusion process and it takes into account the local characteristics of the dataset. A surrogate density is also proposed that is fast to compute. The method is compared to typical density functions used in DBSCAN and DPC.  ,"The paper introduces a new approach to performing clustering, based on constructing a Markov process on the input space, in which transitions are determined according to normalised kernel similarity scores, as in Spectral Clustering (SC). Unlike SC, however, the proposal utilises the stationary distribution of the process, or an approximation thereof, evaluated on the data set, as a density function to be used within existing density based clustering algorithms, such as DBSCAN and Density Peaks Clustering (DPC). Experimental results are given to illustrate the methods potential improvement of standard kernel density estimates, and also its potential improvement over purpose-built models for face clustering.",0.265625,0.265625,0.21875,0.27184466019417475,0.22330097087378642,0.2535211267605634,0.1650485436893204,0.23943661971830985,0.1346153846153846,0.39436619718309857,0.22115384615384615,0.17307692307692307,0.20359281437125748,0.2518518518518518,0.16666666666666669,0.3218390804597701,0.2222222222222222,0.2057142857142857
901,SP:c98d774dd80597f4313298221eec6b72f524ae28,"This paper provides the argument stability bound for adversarial training for linear models and 2-layer NNs with or without noise injection. The stability has a direct connection to the generalization bound. The contribution is mainly on the theoretical side. Motivated by the non-smoothness in AT which causes poor stability, the paper proposes to inject noise during training for both weight and data. Theory and simulation results justify noise injection on improving stability and shrinking generalization gap.","This work builds upon Bassily et al. and extends stability analysis to adversarial training algorithms. The authors use uniform algorithmic stability to derive lower and upper bounds on the robust accuracy.  Based on the theoretical analysis, they argued that non-differentiability (or gradient-masking) reduces the stability of adversarial training, which leads to the significant gap between training and testing robust accuracy. To reduce the generalization gap, the authors suggest using noise injection into the parameters and the input of the models.","This paper studies the theory of adversarial examples from the angle of algorithmic stability. More precisely, the authors characterize the algorithmic stability of the now famous adversarial training scheme. Their main findings can be summarized as follows.  1) Even if the loss function utilized during adversarial training is smooth, the adversarial perturbation applied to the input image causes some non-differentiability issues which impede guaranteeing a good algorithmic stability in the worst-case scenario. 2) The numerical approximation error of adversarial attacks used during training can have an impact on the algorithmic stability of the scheme.  3) Injecting noise to the learning procedure helps to make the loss function smooth again; hence restoring the algorithmic stability with high probability.","The paper investigates the stability of adversarial training when using SGD/GD, motivated by the stability analysis of Bassily et al. (2020) and Hardt et al. (2016) in the standard learning model. In order to improve stability, the authors suggest injecting noise into the weights and data (this was already investigated, see for example - Adversarial weight perturbation helps robust generalization, Neurips2020). Moreover, theoretical analysis is also provided for Linear Regression, Logistic Regression and learning with hinge loss. ",0.24358974358974358,0.24358974358974358,0.19230769230769232,0.24390243902439024,0.1951219512195122,0.14285714285714285,0.23170731707317074,0.15966386554621848,0.19480519480519481,0.16806722689075632,0.2077922077922078,0.22077922077922077,0.23750000000000002,0.1928934010152284,0.1935483870967742,0.19900497512437812,0.20125786163522014,0.173469387755102
902,SP:c9b5b2c81e275cf61c8358872200d758297b1de0,"This paper proposes an architecture or a reversible NMT system. In contrast to an encoder-decoder network, the two components of the network for each language are identical. When a language is being analyzed or generated depends on the direction of data flow in the network. To enable this, the authors propose reversible components in each layer of the network (inspired from revnet). The model is trained with CTC loss along with auxiliary losses for cyclic consistency and layerwise agreement. The paper is an interesting proposal for a reversible NMT system, and it brings together work on revnet, CTC loss, NAT in a simple, elegant framework to enable reversible NMT. ","The paper proposes a novel machine translation model that can be simultaneously applied to bidirectional translations. The model, named REDER, is symmetric at both input and output ends and consists of several identical reversiable duplex Transformer layers. REDER adopts the design of revnet for reversibility and reverses the first L/2 layers for homogeneity, which is a fully non-autoregressive NMT model. Extensive experiments on WMT14 En$\leftrightarrow$De and WMT16 En$\leftrightarrow$Ro datasets show that the proposed model consistently outperforms (non-)autoregressive baselines with improving decoding speed.","This paper proposes a duplex reversible network based on Transformer and revnet for neural machine translation. With the proposed approach, one can use one single model to translate between a language pair from both directions. The model uses CTC loss and allows non-autoregressive decoding for faster inference time. Experiments are well executed on standard NAT benchmark datasets with close performance to that of the standard Transformer model.  **Contributions**: the proposed model is new in machine translation from the best of my knowledge and achieves good performance compared to non-autoregressive models and multi-task learning. The proposed method saves parameters in terms of translation from both directions and improves inference speed.","This paper proposes duplex sequence-to-sequence learning where data streams can flow from both ends of a seq2seq model. More precisely, they design a parameter efficient Transformer (REDER) that can simultaneously input and output a distinct language from each end of the model. In this way, REDER enables reversible machine translation by simply flipping the input and output ends. The experimental results show that the proposed REDER improve the multitask trained baseline by over 1.3 BLEU while maintaining over 5x speed-up at inference time.",0.15454545454545454,0.18181818181818182,0.12727272727272726,0.24719101123595505,0.2247191011235955,0.17857142857142858,0.19101123595505617,0.17857142857142858,0.16091954022988506,0.19642857142857142,0.22988505747126436,0.22988505747126436,0.1708542713567839,0.1801801801801802,0.14213197969543148,0.21890547263681592,0.22727272727272727,0.20100502512562815
903,SP:c9ccef4594e834001f59c73e24c5c468d2923174,"This paper first proposes a proximal variant of the gradient method with shuffling strategy for a class of composite finite-sum minimization problems, and then specifies it for an optimization problem in federated learning (FL). In fact, the proposed algorithm performs n gradient descent updates per epoch and then applies the proximal operator at the end of each epoch. Both single shuffling and randomized reshuffling strategies can be used. The authors analyze the convergence rates of this algorithm under the strong convexity of the losses and the strong convexity of the regularizer. Next, they apply the proposed algorithm to FL, with and without regularizer. This variant is similar to FedAvg, but it replaces SGD by a shuffling gradient scheme for local updates. They establish the convergence of this variant for both the strongly convex case (where the regularizer is strongly convex) and the non-convex case in both heterogeneity and homogeneity settings, respectively. Numerical experiments are conducted to illustrate the algorithm and compare it with two existing methods: local SGD and SCAFFOLD. ","This paper proposed two shuffling based stochastic optimization methods, ProxRR and FedRR. ProxRR is the proximal version of SGD with shuffling, and they prove that ProxRR achieves better convergence rate than vanilla proximal SGD on strongly convex function, similar to its counterpart SGD with shuffling. Another advantage of this paper is that their convergence results do not need bounded gradient assumption, which was needed in the several prior shuffling SGD papers. In FedRR algorithm, each client uses SGD without replacement to optimize the local model, instead of vanilla SGD employed in local SGD. They also have analysis of FedRR for convex and nonconvex functions under both homogeneous and heterogeneous settings, and the convergence rates are faster than local SGD. The experiments also show the faster convergence of shuffling based algorithms. ","This paper studies a proximal extension of random reshuffling algorithm, namely ProxRR for finite-sum composite optimization. ProxRR requires only one proximal steps per round, which reduces the complexities incurred by costly proximal operators. This paper also studies a federated version of ProxRR, namely FedRR, and proved theoretical advantages over Local SGD.","In this paper, the authors propose two new algorithms for finite sum minimization: Proximal Random Reshuffling (ProxRR) and Federated Random Reshuffling (FedRR). ProxRR is used to solve composite functions, while FedRR is a special case of ProxRR as used in the context of federated optimization. They provide convergence rates under various convexity assumptions, and they also show convergence to small gradient norm (in expectation) for smooth non-convex objectives.",0.1744186046511628,0.10465116279069768,0.13372093023255813,0.1076923076923077,0.14615384615384616,0.21153846153846154,0.23076923076923078,0.34615384615384615,0.3333333333333333,0.2692307692307692,0.2753623188405797,0.15942028985507245,0.1986754966887417,0.16071428571428573,0.19087136929460582,0.15384615384615385,0.19095477386934673,0.18181818181818182
904,SP:c9e79237aecc2d27b98c48d545255bcf8fc3f2ba,"This paper addresses the problem of how to leverage data collected from on a robot to reduce/remove the need for robot specific data. The authors propose a model based RL policy based on coupling robot-agnostic and robot-specific dynamcis modules. Experiments with simulated and real robots are demonstrated, showing transferability of visual model based policies. ","This paper proposes a robot-aware RL-based approach to transfer learning from a robot to another. The approach factorizes the model into two models, a robot model and a world model. The method is evaluated against several baselines on simulated and real data. The paper claims that it permits zero-shot transfer onto new robots for the very first time.","The presented work is can be seen as a more sample efficient improvement/augmentation of the Visual Foresight method (Ebert et al.) in the context of transfer learning for robots. The authors present the notion of robot-aware models - the overall dynamics model is explicitly decomposed onto a known analytical dynamics model for the robot and a learned dynamics model (from pixels) for the objects in the scene. The analytical model, combined with off-the-shelf image plane projection methods, tell us how a robot would look like from the camera's PoV after a set of taken actions. This in turn allows us to exclude any robot-related pixels from the scene observations and provide the learned dynamical model only with object-related pixel information, effectively making the learned model invariant to the robot's appearance. The same logic is applied to a novel planning cost function used for performing visual MPC. As a result cross-robot transfer can be more easily facilitated in the context of table-top manipulation tasks, where the goal is specified through a goal image.","The paper presents an extension of previous works on training a visual policy to control a robot arm in a manipulation context. The policy is pre-trained on a large robot experience dataset (e.g. different robots interacting with different objects in a table-top setting). The dataset is used to train a model for the visual dynamics (i.e. next visual & robot state given current visual & robot state and applied action). The visual dynamics are used to reach a desired visual state using an existing technique: visual foresight (VF) by Finn & Levine, 2017 and Ebert et al., 2018.   The main novelties of the paper of the paper are two. First, authors propose a technique to learn a decoupled visual dynamic model: the world model (i.e. a model to propagate forward the world-observations consisting of an image of all objects in the scene excluding the robot) is decoupled from the robot model (i.e. a model to propagate forward the end-effector pose and a mask which represents the robot body in the visual space). Second, the visual foresight planning relies on a newly proposed cost which combines and decouples two costs: a desired robot-state cost and a desired world-state cost.",0.2807017543859649,0.22807017543859648,0.3157894736842105,0.26229508196721313,0.3114754098360656,0.22099447513812154,0.26229508196721313,0.0718232044198895,0.08780487804878048,0.08839779005524862,0.09268292682926829,0.1951219512195122,0.27118644067796605,0.1092436974789916,0.13740458015267176,0.1322314049586777,0.14285714285714288,0.20725388601036268
905,SP:ca01c5fc3fd91f25d8c46bc6fc68c6ab2832b8d0,"This paper proposes a new method to sequentially detect covariate shift which is the problem encountered when training and test covariates are not equally distributed. This methods builds on existing work that uses classifiers to distinguish between the two datasets and its accuracy as the statistic. The contribution of this paper is proposing a sequential version, where the classifier is trained in an online manner  and the accuracy computed on a sliding window (of training and test samples) is used for each test set observation.  A criterion to reject the equality of the distributions in the sliding window is given using on the well known Clopper-Pearson interval. A lower bound on the probability of correctly accepting the null hypothesis is derived from the properties of the aforementioned interval. A lower bound on the probability of correctly rejecting is derived, under the assumption that the classifier has at least some discriminating power (i.e., that it is non trivial).  ","In traditional machine learning, there is a basic assumption that training and test sets are from the same distribution. When this assumption holds, we can expect low prediction error in the test set. However, in the real world, this assumption may be broken. For example, when the images change from daylight to night, the distribution changes. As a result, we cannot fully trust a classifier trained with daylight images. To address this issue and make predictions reliable, detecting such covariate shifts seems promising. Although we can directly use a two-sample testing method to complete this detection task, it cannot fit the online requirement. This paper presents a new sequential classifier two-sample test to address these dynamic covariate shifts. This paper proves that their optimization preserves the correctness—i.e., the proposed algorithm achieves a desired bound on the false positive rate. In the experiments, they also show that the proposed algorithm efficiently detects covariate shifts on ImageNet.","This draft proposes an online covariate shift detection algorithm based on the two-sample test with source and target data in a fixed sliding window. The proposed method is based on the accuracy of the source-target classifier to identify the covariate shift: if the source-target classifier cannot distinguish the source or target data, then the proposed method considers they are generated from the same distribution and vice versa. Experiments on the modified ImageNet dataset validate the effectiveness of the proposed method.","This paper proposes a new covariate shift detection method, which distinguishes whether training data and test data come from the same distribution. This method sequentially evaluates each example before taking a gradient step on that example, avoiding constructing a held-out test set. Experiments on ImageNet validate its effectiveness at detecting both natural and synthetic covariate shifts.",0.16352201257861634,0.16352201257861634,0.12578616352201258,0.12578616352201258,0.10062893081761007,0.1927710843373494,0.16352201257861634,0.3132530120481928,0.3508771929824561,0.24096385542168675,0.2807017543859649,0.2807017543859649,0.16352201257861634,0.21487603305785125,0.1851851851851852,0.16528925619834714,0.14814814814814817,0.22857142857142856
906,SP:ca033d28bfcc2124fb637ebe305b023664bb9638,"This work analyzes several one-shot and zero-shot NAS methods on multiple NAS benchmark datasets. The authors inspect NAS algorithms from various aspects, including training bias, stability, variance, loss ranking v.s. accuracy ranking, etc. . The authors conclude that one-shot methods and zero-shot methods might fail to work due to multiple reasons and propose several solutions to alleviate these issues.","This paper gives evaluations of methods to evaluate the performance of a neural network before it is fully trained. This has applications to neural architecture search (NAS). Specifically, the authors study two types: one-shot estimators (OSEs) and zero-shot estimators (ZSE) which have both been popular recently. The authors give a large study of various aspects of these methods. They give several new insights about biases in their performance and ideas to mitigate them. All experiments are conducted on three popular search spaces: nas-bench-1shot1, nas-bench-201, and nas-bench-301.","Neural Architecture Search aims to identify neural network architectures from a human-designed search space which have good accuracies, or good accuracy/size tradeoffs. The submission explores how a number of different design choices affect how well existing heuristics can identify/rank promising architectures from a search space. The submission focuses on two classes of heuristics: * One-shot models/estimators, which use a single shared set of weights to evaluate/rank many different candidate architectures, * Zero-shot estimators, which try to evaluate/rank different candidate architectures without any training at all.  Experiments are conducted on the NASBench-101, NASBench-201, and NASBench-301 benchmark tasks.","Weight sharing is a very important technique in NAS to speed up the search and reduce resources. However, such technique suffers from performance drop, and attracts a lot of interset from the community to investigate into it. This paper is one of the works that investigate the behavior and underline principles of such weight sharing technique (aka one-shot). Besides it also investigate current popular zero-shot estimators. The paper analyzes the one-shot and zero-shot estimators from multiple perspectives on three benchmark datasets, with sufficient experiments and further suggestions.  Though some of the points/findings are not novel and well-known, the sufficient experiments still make a convening verification on these. It will bring much to the community. The findings and suggestions will make using one-shot and zero-shot NAS easier for practitioners.",0.2222222222222222,0.14285714285714285,0.2698412698412698,0.24468085106382978,0.2127659574468085,0.17142857142857143,0.14893617021276595,0.08571428571428572,0.125,0.21904761904761905,0.14705882352941177,0.1323529411764706,0.178343949044586,0.10714285714285714,0.1708542713567839,0.2311557788944724,0.17391304347826086,0.14937759336099582
907,SP:ca6f076e93aa15f861bc1d6366d880d759b2bdcb,"The paper studies instance-optimal mean estimation in the central model of differential privacy, with straightforward extensions to the shuffle and local models. The specific notion of instance optimality used is, I believe, original to this paper: the goal is to find an algorithm minimizing error over a given database D and a restricted neighborhood of databases that are neighbors and only include elements also present in D. The algorithm consists of four steps:  1) randomly rotate the database, 2) center it using a privately estimated component-wise median, 3) privately estimate a certain quantile of the L_2 norm of the resulting database and clip data to that estimated L_2 norm, 4) take a noisy empirical mean of the clipped data and de-bias.  The paper gives instance-specific upper and lower bounds that are nearly matching, up to logarithmic factors in the dimension and a discretization parameter. Experiments suggest that the algorithm performs well against existing methods. ","This paper considers the problem of private mean estimation and attempts to develop algorithms that are instance-optimal, that is, adapting to the difficulty of the underlying instance. The authors modify the existing instance-optimality definition in differential privacy, and develop an algorithm that is nearly instance-optimal. The caveat here is that nearly instance-optimal here hides factors that depend on the dimension and privacy parameters, hence making the optimality guarantee less strong than what claimed by the authors.","This paper does differentially private mean estimation for empirical estimation, as opposed to statistical estimation (statistical estimation is an application of their work). It focusses on instance optimal mean estimation by introducing a new target error function of the dataset, $\mathcal{R}_{\text{in-nbr}}$. It shows both and upper bound and a lower bound for this problem. It has both theoretical and empirical results.  The theoretical results say that for this measure of error, the optimal error scales according to the diameter of the dataset in question. To prove the upper bound, the authors first use a clipped-mean-estimator, with the clipping threshold decided via a simple derivative of the error. Since that clipping estimate is a function of the dataset, it could not be used directly. So, they came up with a simple quantile estimator to do the same. They, however, first performed a random rotation, followed by a translation to fix scaling issues. Then they finally use the aforementioned clipped-mean-estimator.  They finally showed application in statistical mean estimation of high-dimensional Gaussians. They showed that they require very weak prior knowledge of the covariance matrix of the Gaussian for this. They finally compared their empirical results with the Coinpress algorithm of Biswas et al.  They extend these results to the local and shuffle models of DP, too.","The paper proposes a new algorithm for the mean estimation problem that gets around the ""curse of global sensitivity"" and achieves the so-called instance optimality. Experiments including a real-data one are carried out to verify the practicality. The results are also extended to the shuffle and local models of DP.",0.1375,0.19375,0.11875,0.2875,0.1625,0.09417040358744394,0.275,0.13901345291479822,0.36538461538461536,0.1031390134529148,0.25,0.40384615384615385,0.18333333333333335,0.1618798955613577,0.17924528301886794,0.15181518151815182,0.196969696969697,0.1527272727272727
908,SP:ca75e0ed198013edfaa89fa2746f0f5b95535918,"They implement a differentiable simulator based on PyTorch (actually it was a bit unclear to me whether they implemented it, or just used Isaac Gym; I have a question about this below, and hope to receive an answer from the authors), as well as a method to utilize the differentiable simulation to optimize the policy more efficiently. The method operates as follows: 1. It computes a batch of episodes each of length H using a stochastic policy. 2. It splits the episodes into subsequences of length h. 3. It adds a terminal value function to each sequence, as well as discount factors on the rewards (starting from no discounting at the beginning of each subsequence), and computes policy gradients via backpropagation for optimization. 4. The value function is learned by computing lambda return targets (note that lambda returns are not used in the policy optimization though).  There is also some discussion about exploding gradients that hinder optimization through long horizons, which is why they truncated their episodes into subsequences.  They perform experiments on cartpole swingup, ant, humanoid and humanoid MTU (a humanoid actuated by 152 muscle tendon units), and compare to PPO, SAC, Backpropagation through time and PODS. The proposed method (SHAC) outperformed the other methods both in terms of number of steps (by a large margin) as well as computation speed (though PPO was faster in terms of wall-clock time in the early stage of learning after which SHAC overtook it). Particularly on the humanoid MTU task, the final performance was much better than the other methods. In terms of wall-clock time, the required computation time was a few minutes for cartpole and ant, and around 1h for the humanoid tasks.  They also performed ablation studies looking into the necessity of the terminal value function (it was necessary) and also the length of the subsequences (intermediate length sequences were optimal).  Recently some similar simulators, e.g. Brax or Isaac Gym have been made, but these works have not yet shown impressive results by taking advantage of differentiating through the model.","In this paper, the authors introduce a novel differentiable simulator (built on PyTorch) that focuses on enabling smooth gradients with respect to expected return, especially with a view to ameliorating discontinuities in contact dynamics. In order to leverage this differentiable engine as effectively as possible, they also introduce SHAC, which is an analytical policy gradient (APG) method that features a terminal value function to truncate the value gradient calculation through time. The authors' show this results in a smoother objective function with lower training time, which empirically gives rise to faster/improved convergence in policy learning.","This paper focuses on the setting of reinforcement learning with differentiable simulation, where the authors assume access to a simulator which can compute the derivatives of the next state and reward with respect to the current state and action exactly. Under this setup, the authors propose an actor-critic style reinforcement learning algorithm that leverages the simulator derivative information to achieve better training sample efficiency and wall-clock time. Specifically, the authors expand the value function of the policy in h steps, and combine the sum of the simulator gradient of these h steps and the gradient of the learned value function at the h + 1 step to obtain the overall gradient for training the policy. The value function is then trained in the same way as in other actor-critic style methods.  The authors evaluate the proposed algorithm empirically on a few domains and the experiment results suggested that the proposed method achieves superior sample efficiency and wall-clock time compared to prior methods.   ","This paper demonstrates the effective use of differentiable simulators for significantly speeding up the training of policy gradient methods. The main idea is to backpropagate through the simulator only on shorter partitions of the trajectory, which seems to lead to a smoother loss landscape and better gradient signal. To avoid getting stuck in local minima, a critic is used to incorporate a terminal cost. As the implementation can be parallelized and the simulation run on the GPU, the experiments show drastic improvements not only in sample efficiency but also in run time.",0.07871720116618076,0.11661807580174927,0.06997084548104957,0.23958333333333334,0.1875,0.15757575757575756,0.28125,0.24242424242424243,0.2608695652173913,0.1393939393939394,0.1956521739130435,0.2826086956521739,0.12300683371298407,0.15748031496062992,0.11034482758620692,0.17624521072796936,0.1914893617021277,0.20233463035019456
909,SP:ca8120df14fbcc14c39ca61b4576234b81e2e1e8,"The paper looks at the problem of improving the generative quality of GANs. The paper makes improvement along two dimensions: a) The paper adjust the samples distribution of GANs using adversarial attacks (I-FGSM), and thus effectively samples from a potentially multi-modal distribution. b) The paper improves the diversity of generated samples using using adversarial attacks on the mode-seeking objective of Mao et al 2019.","This work proposed a sample shifting method in GAN, formulated as adding intermediate latent space to generated pixel space. Such method is based on observation of continuous mapping limit: image quality in pixel space is not as continuous as latent space; limited latent space will incur mode collapse thus poor image diversity. The main contributions are: a new optimization problem as sampling method to improve image generation by quality and diversity, propose to use I-FGSM optimization method to achieve this sampling optimization problem. The experiment showed improvement on public dataset of STL-10, CIFAR-10.","The authors demonstrate that the generator in a GAN is a continuous function two latent codes that are close in the latent space are mapped to two images that are close in the pixel space. However, the quality of the generated images is not preserved as quality is not a continuous function in pixel space. To address this issue, the authors propose to transform the original latent codes and demonstrate that it results in better generation quality and diversity.","This paper applies two existing techniques derived from adversarial example literature and MSGAN to improve the quality and diversity of generated samples by GAN. The former idea from [Goodellow et al., 2014b] is used to shift a latent vector z originally sampled from the Gaussian, which is different from the original adversarial attack paper that transforms the input image. The direction to move the latent vector is calculated by using I-FGSM with a standard loss for the generator proposed in the vanilla GAN paper. The experiments show that this technique can improve the quality of generated samples. Furthermore, this paper also consider how to improve the diversity of generated samples by transforming the latent vectors before putting them into the generator network. The approach for that is based on MSGAN's idea that mines hard latent vectors for the discriminator. The authors combine these two techniques and achieved better results compared to DCGAN, WGAN, WGAN-GP, and SNGAN models on several relatively small scale datasets such as CIFAR-10, STL-10, etc.",0.13432835820895522,0.16417910447761194,0.3283582089552239,0.19791666666666666,0.16666666666666666,0.26582278481012656,0.09375,0.13924050632911392,0.12716763005780346,0.24050632911392406,0.09248554913294797,0.12138728323699421,0.11042944785276074,0.1506849315068493,0.18333333333333332,0.21714285714285714,0.11895910780669144,0.16666666666666666
910,SP:cab2f82f4973fb8e420a6a6ed6e48807da5e12bc,"The paper presents generalization bounds for meta-learning (Corollary 1) that obtain O(1/sqrt{m*n} * log{covering number}) convergence rate. Unlike previous work, the proposed bound can utilize all m*n training samples (n tasks, m samples per task). In a special branch of meta learning that involves representation learning with neural networks, i.e., the algorithm learns a common embedding that is shared across tasks, the paper establishes spectrally-normalized bounds for both classification and regression problems.  The paper relies on two critical assumptions: relatedness between tasks and closure property of the hypothesis space. The paper shows that the task-relatedness assumption is satisfied if the sample space is a complete and separable metric space. The paper also briefly discusses the conditions for which the closure property assumption holds. ","The paper presents novel generalization bounds for meta-learning based on a notion of task-relatedness that allows one to compare two tasks by notably allowing a mapping only in subregions where the similarity can be measured in a sense. The theoretical results a covering number bound, a covering number meta-learning bound with representation learning, and spectrally-normalized bounds for meta-learning with neural networks adapted to binary, multi class and regression. The contribution is theoretical.","The paper: 1) proposes a new notion of task-relatedness for meta-learning termed ""almost $\Pi$-relatedness"" that assumes isomorphisms between two tasks in the environment. A PAC-style generalization bound of $\mathcal{O}(\sqrt{\frac{C}{mn}} +  \sqrt{\frac{\ln(1/\delta)}{mn}})$ is shown, improving upon Baxter et. al's. 2) For meta learning that involves representation learning, they bound $C$ with two covering numbers that are both defined over a single task, making their results suitable to be combined with recent works of deep neural network in the single task learning. 3) They demonstrate that any two tasks in the environment are almost $\Pi$-related if the focused sampled space is a complete separable metric space.","In this paper, the authors proposed two key concepts: almost PI-Related tasks and almost PI-related environment. Intuitively, if two tasks are PI-related, then if a function space H contains a good solution to task A, it should also contain a good solution to task B.  With these two concepts, the authors went forward and proved that in an almost PI-related environment, the generalization bound has a convergence rate of O(1/\sqrt(mn)). In other words, when tasks are similar, then meta-learning algorithms should be able to utilize all data points across all tasks.",0.1893939393939394,0.24242424242424243,0.09848484848484848,0.2857142857142857,0.16883116883116883,0.15966386554621848,0.3246753246753247,0.2689075630252101,0.13131313131313133,0.18487394957983194,0.13131313131313133,0.1919191919191919,0.23923444976076552,0.2549800796812749,0.11255411255411256,0.22448979591836735,0.14772727272727273,0.17431192660550457
911,SP:cb134bcde26b849ab53339cb0172990a6422e624,"This paper investigates the regularizing effect of different output layer designs in deep neural networks. The authors propose several different design choices of the output layers, which are all very simple but effective as the authors demonstrated in the experiments. They also propose two terms named neural dependency and expressivity between the neurons and classes. With these simple designs of the different output layers, the experiments on multiple classification tasks and resnet/densenet CNN models improve the performance for a large margin, which helps avoid the overfitting problem.   The main contributions: 1. Propose simple yet effective output layer designs for deep neural networks. 2. Introduce neural dependency and expressivity factors to measure the overfitting problem. 3. Experiments on different classification tasks are expressive. ","This paper designs and studies regularization effects for different types of output layers on training deep neural networks, which is very useful in practice. They propose five output layer designs and conduct various experiments including a case study. The results show that their proposed designs can achieve improvements of up to 10% on evaluation datasets. Overall, this paper is well-written and easy to follow. ",This experimental work studies various sparse output-layer types for transfer-learning. Authors report interesting gains through usage of certain learned/fixed output layers. Although I found the experiments interesting and believe that they would spark more research on the topic; the story and results needs a little bit more work.,"Usually a fully connected layer is used as the final layer of an image classification network. To reduce the overfitting of this layer, this paper studies the use of activation scaling, fixed randomization, sparsity and in-layer ensembling as a regularization technique. This paper also introduces neuron dependency and expressivity as two factors contributing to overfitting and proposes ways to measure these factors. ",0.15447154471544716,0.11382113821138211,0.12195121951219512,0.15384615384615385,0.16923076923076924,0.11764705882352941,0.2923076923076923,0.27450980392156865,0.23809523809523808,0.19607843137254902,0.1746031746031746,0.09523809523809523,0.2021276595744681,0.16091954022988506,0.16129032258064516,0.1724137931034483,0.171875,0.10526315789473684
912,SP:cb9f6aa2cd551858d15e2c238c014715681e1d72,"The paper studies collapse phenomena of representations from neural classification models trained beyond zero training error with cross entropy. The theoretical contribution of the paper is an analysis of such phenomena in the so called unconstrained feature model. In this setting, the authors derive the global loss minimizers, and show that these can be obtained via gradient based optimization, as the critical points of the loss function are benign. These theoretical results are underpinned by experiments, which investigate the collapse phenomena for a ResNet-18 architecture when trained via different optimization algorithms or with randomly labeled data. The theoretical results also indicate, that in practice, the last linear layer of a neural network can be fixed a priory and of low dimension. ","This paper provides a landscape analysis for neural collapse with regularization analysis. The theoretical results are nice, thus I voted an accept decision. But reviewer has a concern about the novelty of the proof and the implications of the theoretical results.","This is an extremely well written (articulate and readable) and diligently prepared (completely by the book) manuscript. For example, it draws carefully on more than 100 other manuscripts. This alone makes the manuscript valuable to other people, as it teaches how work can be exposited and papers can be written.  The results are interesting and instructive, and in some way connect these questions to some recent work in non-convex optimization. The practical benefit of this work -- tens of percentage points speedups -- are meaningful.  ","Neural collapse is an intriguing phenomenon that has been observed in neural network classifiers as the training error goes to 0 where the last layer of the neural network has collapsed input points to corners of a simplex, thereby minimizing variability, and maximizing separability. While this phenomenon seems well studied the authors extend it by studying the critical points of this optimization geometrically by relating it to a convex problem. They show in Thrm 3.2 that the critical points are either global minimal or saddle points with negative curvature which can be escaped by a minimizer.  ",0.11475409836065574,0.12295081967213115,0.1885245901639344,0.1951219512195122,0.2682926829268293,0.14285714285714285,0.34146341463414637,0.17857142857142858,0.23711340206185566,0.09523809523809523,0.1134020618556701,0.12371134020618557,0.17177914110429446,0.14563106796116504,0.2100456621004566,0.128,0.15942028985507245,0.13259668508287292
913,SP:cbb1c7743096ddc3876e36deaa21a13b288c6e52,"Recent work in RL has been seeking ways that RL can draw the kinds of generalization that we see in supervised learning. Recently, a couple groups obtained efficient RL algorithms by assuming that the reward and transition probabilities are linear combinations of some set of basis functions. A somewhat more appealing, weaker assumption would be for merely the Q-function (state-action value function) to be linear. Unfortunately, recent works also found hard examples of environments with linear Q-functions, that require exponential sample complexity. Indeed, even with a simulator, there are exponential lower bounds -- only recently have any kind of polynomial-time guarantees, with a polynomial dependence on the smallest gap between the value of any optimal action any any sub-optimal action.  This work proposes a new model in which linear Q-functions are learnable. The model can be seen as a weakening of the simulator model: whereas the  simulator model allowed the learner to query any state-value pair to get a  sample of the reward and transition functions, this model merely allows the  learner to return to states that it previously visited on some trajectory. ","Disclaimer: I am not an expert in theoretical RL. My review is thus restricted to the overall soundness of the manuscript.  The authors describe theoretical work proposing a new sampling strategy for the online linear Q* problem making it sample efficient. They modify and extend the LSVI-UCB algorithms for linear MDP and show via regret analysis, that their sampling scheme with state revisiting leads to sample efficient algorithms -  a feature missing in previous studies in this particular setting with classic episodic sampling.  ",This paper introduces a new problem setting of RL with state resetting. It then proves a polynomial (in ) sample complexity bound under linear approximation with assumptions of (1) $Q^*$ realizability and (2) suboptimality gap $\Delta $ bounded away from 0. This setting and bound helps to reconcile recent results under assumptions (1) and (2) of exponential lower bounds in the online setting with polynomial upper bounds in the generative model setting.,This paper studies the sample complexity of RL with the linear Q^* assumption. Previous work has shown that this problem can not be solved with polynomial samples neither in the generative setting nor in the online setting even with a large gap between the value of the best action and the second-best one. Here the authors show that the additional assumptions of a sampling procedure which allows the learner to backtrack to previously-seen states (together with a large gap) is sufficient for polynomial sample complexity (albeit with sample complexity scaling with the inverse gap). The algorithm is based on the “standard” LSVI-UCB template. ,0.08994708994708994,0.08465608465608465,0.15343915343915343,0.13253012048192772,0.1686746987951807,0.22857142857142856,0.20481927710843373,0.22857142857142856,0.27358490566037735,0.15714285714285714,0.1320754716981132,0.1509433962264151,0.125,0.12355212355212357,0.1966101694915254,0.14379084967320263,0.14814814814814817,0.1818181818181818
914,SP:cbb2ef0a292693fffd77e7a95183e4725bf21dcd,"This paper considers improving uncertainty estimations for meta-learning. Previous work [1] has done this via using a hyper-prior over the prior parameters for meta-regularization. This hyper-prior, however, took the form of a simple Gaussian distribution over the prior parameters, which imposes smoothness regularization on the prior but may not be ideal for enforcing uncertainty in regions where no meta-training data is available. To remedy this, the authors propose a hyper-prior in the function space in the form of a stochastic process. They then show how the meta-learning algorithm can be defined so that it uses stochastic processes for the prior, hyper-prior and hyper-posterior. The experiments are conducted on a Bayesian optimization setup, where meta-training involves using a sequences of related BO problems corresponding to n target functions and evaluation involves using the meta-learned model to perform BO on a new target function. For example, one such setting is hyper-parameter optimization for machine learning models, where the training task functions correspond to training and testing the machine learning model on different datasets. Across different benchmarks in this setting, they show their proposed algorithm performs better than [1].  [1] Rothfuss et al. PACOH: Bayes-optimal meta-learning with PAC-guarantees. ICML 2021.",This paper observes that meta-learning algorithms typically underestimate epistemic uncertainty because of a failure of regularization to respect the structure of meta-learning. They propose a meta-regularization step that builds on functional KL-divergence to a GP functional prior. Their experiments in sequential BO suggest that this functional prior outperforms alternatives.,"This work introdcues a new way to regularize priors in meta-learning through the use of stochastic processes as hyper-priors. The authors rely on sampling-based measurements to estiamte the KL divergence between the meta-learned prior and a stochastic process hyper-prior.   Through experiments on toy datasets, the algorithm, F-PACOH, demonstrates principled uncertainty estimates. It provides higher levels of uncertainty far from training data, which is in contrast to previous methods for imposing meta-learning priors and hyper-priors. Finally, the authors apply their method to a Bayesian Optimization meta-learning problem and demonstrate decreased regret compared to other methods benchmarked. ","This paper discusses a meta-learning method with priors defined on function spaces. Building on PACOH, a meta-learning framework with PAC-Bayesian bounds, the paper proposes F-PACOH where the hyper-priors and priors defining meta-learning models in function spaces. In original PACOH, the hyper-priors are set to be parametric distributions (usually Gaussians), and the authors argue that such parametric priors cannot effectively capture the functional structure of the tasks that we ultimately want to enforce for meta-learners. On the other hand, functional priors directly regularize functions of interest to be close to prior knowledge, e.g., standard Gaussian processes, so result in more reliable uncertainty estimates.   The inference is done by optimizing the PAC-Bayesian bound, and the caveat here is the computation of KL-divergence between functional hyper-priors and hyper-posteriors. This can be approximated by the technique introduced in Sun et al., 2021, where a set of measurements points are sampled and the KL-divergence is approximated with the evaluations on those measurement points.  The paper mainly focuses on the application of the proposed method to Bayesian optimization. On several benchmarks, the proposed F-PACOH is demonstrated to quickly meta-learn and improves on similar BO tasks. ",0.08490566037735849,0.12735849056603774,0.1792452830188679,0.22641509433962265,0.32075471698113206,0.2692307692307692,0.33962264150943394,0.25961538461538464,0.18536585365853658,0.11538461538461539,0.08292682926829269,0.13658536585365855,0.13584905660377358,0.1708860759493671,0.18225419664268586,0.15286624203821658,0.13178294573643412,0.18122977346278318
915,SP:cbd719c71fc3cb758dd49373c7941ea072ebb7a5,"This is an interesting paper to discuss the variance in the adder neural network. The authors have an in-depth comparison between ANN and CNN. Also, the adder operation in the neural network can enjoy some nature advantage over CNN in terms of the adversarial robustness. The authors have conducted comprehensive experiments to support these analyses.","The paper reveals the existing instability of Adder Neural Networks (ANNs) through the variance estimation of output features and proposes adaptive weight normalization to tackle this issue. Additionally, the adversarial robustness of ANNs is highlighted through further analysis of perturbation variance in both ANNs and CNNs, and the paper proposes to automatically eliminate the perturbations based on the statistics in the batch normalization layer. Experiments on several tasks validate the effectiveness of the proposed method in improving the stability and robustness of ANNs.","This paper aims at enhancing the stability of AdderNets for performance improvement in downstream tasks. For an adder layer, approximations of feature variance and expectation are derived, with which the variance of weight is found to be the major cause of instability. Adaptive weight normalization (AWN) is thus introduced for addressing this problem. Together with the feature variance approximation, the potential adversarial robustness of AdderNets is also analyzed through comparing the perturbation variance in both AdderNets and CNNs. A simple robust inference strategy with BN layer is thus designed for disturbance elimination.","This paper investigates the mean and variance of features for AdderNets and reveals that the instability of AdderNets mainly comes from fluctuations of running mean and variance in batch normalization layers. To this end, this paper proposes the adaptive weight normalization (AWN) to optimize adder weight distributions adaptively. Extensive experiments demonstrate that the proposed method improves AdderNets on both stability and robustness.",0.23214285714285715,0.23214285714285715,0.17857142857142858,0.2891566265060241,0.26506024096385544,0.1956521739130435,0.1566265060240964,0.14130434782608695,0.16129032258064516,0.2608695652173913,0.3548387096774194,0.2903225806451613,0.18705035971223022,0.17567567567567563,0.1694915254237288,0.27428571428571424,0.30344827586206896,0.23376623376623376
916,SP:cc340a040f20a0107c099eec3c2f0bb5aa55cf19,"Maintaining functional diversity between ensemble members are a large, and still not completely solved problem. The authors are introducing a kernelized repulsive term to the update rule of an ensemble. They show empirically that this increases the functional diversity between ensemble members. The authors also demonstrate that their improvement translates into a proper Bayesian inference.","The paper proposes a particle based inference method that, unlike Deep Ensembles, approximates the Bayesian posterior of a deep neural network. The proposed approaches are variants of Stein Variational Gradient Descent. They employ a repulsive force (in the form of a kernel) that prevents the particles from collapsing into the same mode.  The main contribution of the work is showing that this particle-based training method is equivalent to gradient descent minimizing the KL-divergence of the true posterior and the particle approximation in Wasserstein space.  The paper also presents empirical results. On a 1d synthetic benchmark, it is shown that the proposed methods are able to capture in-between uncertainty. On a 2d synthetic benchmark, it is shown that the uncertainty estimates are akin to that of HMC. On larger, image classification datasets, it is shown that the methods outperform both deep ensembles and svgd.","The paper tackles the problem of building ensemble methods to improve prediction accuracy and uncertainty estimates. The authors propose adding a repulsive term to the gradient update rule for each ensemble element to produce diverse ensembles, and they design repulsive terms for which the empirical distribution over ensemble members converges to the true posterior distribution. Specifically, the authors propose three repulsive terms that arise from three alternative approximations of the gradient of the empirical distribution over ensemble members, and they compare the resulting ensembles with standard deep ensembles in synthetic tasks and image classification tasks. Finally, to account for the fact that different weights can parameterize the same function (and therefore a repulsive term in the weight space may not achieve the desired effect), the authors consider updates (and repulsive terms) in both weight space and function space and compare both approaches in each evaluation task.  The claims are that the proposed approach: * enables using deep ensembles to perform proper Bayesian inference, * improves the quality of the uncertainty estimates in synthetic tasks, * improves OOD detection on image datasets.","This paper introduces a novel but simple repulsive term to the training objective for deep ensembles which prevents the ensemble members from collapsing to the same point in *function* space. This training procedure is closely related to that of Stein variational gradient descent. Furthermore, the authors show that this repulsive term turns deep ensemble training into approximate Bayesian inference. Finally, the authors demonstrate that this method improves on both SVGD and deep ensembles in a range of benchmark tasks. ",0.2545454545454545,0.41818181818181815,0.2727272727272727,0.22602739726027396,0.1643835616438356,0.16292134831460675,0.0958904109589041,0.12921348314606743,0.189873417721519,0.1853932584269663,0.3037974683544304,0.3670886075949367,0.13930348258706468,0.19742489270386265,0.22388059701492538,0.20370370370370366,0.21333333333333335,0.2256809338521401
917,SP:cc5e07934c1c3d37a29d27c7012ec1c0a9eb0300,"The authors extend gradient flow methods for variational inference to handle inequality constraints. These inequality constraints can capture fairness, safety, interpretability, and other such desiderata. They introduce two methods to solve this. First, they introduce a 'primal-dual gradient method': they formulate the equivalent minimax problem to the constrained optimization, and then they derive the dynamics for gradient descent ascent of the zero-sum game. Convergence is shown by explicitly providing a Lyapunov function. Next, they introduce a 'constraint controlled gradient descent', which first finds the feasible set, and then stays within the feasible set afterward (inspired by controlled barrier functions (CBFs) from control). Convergence is shown similarly to the work in CBFs.","The paper presents SVGD variants that can handle constraints on the optimized distribution in the form of an expectation under it. The methods are based on Lagrangian multiplier (primal-dual form) and constraint controlled optimization, respectively. Their convergence analyses in the context of a constraint are developed. Experiment verifies a good fit to the target distribution with the satisfaction of the constraint.","The authors propose two ways to extend Unadjusted Langevin Algorithm (ULA) and Stein Variational Gradient Descent (SVGD) for sampling from a distribution known up to its normalization constant, under moment constraints. The first one is a primal-dual gradient descent scheme, while the second one is a controlled gradient descent one.  They provide convergence guarantees in continuous time for both schemes: a 1/T rate for a given optimality criterion Delta in the more generic case (Th 3.4 and 3.8), and linear convergence rate in KL if some form of log-concavity is satisfied for the current target p_{lamba_t}* (Th 3.5 and 3.9).  These methods perform reasonably well on synthetic and real world experiments, with a better performance for controlled-based methods. ","This paper proposes to modify existing approximation posterior inference algorithms to satisfy an additional moment constraint that might be related to trustworthiness, fairness, or something else. Two algorithms are proposed, which can be used with either a discretized Langevin algorithm or Stein variational gradient descent. Convergence properties of the underlying gradient flows are also characterized. ",0.12389380530973451,0.21238938053097345,0.08849557522123894,0.24193548387096775,0.11290322580645161,0.078125,0.22580645161290322,0.1875,0.18181818181818182,0.1171875,0.12727272727272726,0.18181818181818182,0.16,0.1991701244813278,0.11904761904761907,0.15789473684210525,0.11965811965811965,0.10928961748633881
918,SP:cc6aa977ce561a2493ae74bb694205cd67c8d890,"This paper focuses on addressing the domain generalization problem through the lens of causality. Instead of learning environment-invariant representation/inference p(z|x) like [31,97], the paper proposes to learn the other direction p(x|z). The paper proposes a novel graph, where it assumes the latent semantic-object representation (s) and domain-specific representation (v) cause the image. The generative mechanism works (s,v) -> x, while the prior p(s, v) can be prone to environment change at test time.   Therefore, the model proposes a CSG-ELBO to maximize the likelihood of E_{p(x, y)}[log p(x,y)], and further derive a CSG-ind to disentangle v from s. The paper conducts three experiments on different datasets for both OOD generalization and domain adaptation to show the advantage of their proposed model.","The authors propose a specific graphical model, Causal Semantic Generative model, which is based on the principles of casual reasoning. The graphical model attempts to explicitly distinguish between meaningful semantic factors of variation and spurious correlations. The design of the model is targeted at out-of-domain generalization and domain adaptation performance. The authors describe an approach for training such a model using variational inference, and demonstrate theoretically and experimentally that their approach gives superior OOD generalization and domain adaptation performance compared to relevant baselines.","This paper proposes a Causal Semantic Generative model (CSG) for out-of-distribution (OOD) generalization. It assumes a semantic latent variable $s$ and a variation latent variable $v$, following the causal graph that $s,v$ are the causes of the input $x$ and $s$ is the only cause of the label $y$. The prior $p(s,v)$ is domain-specific, but the causal mechanisms $p(x|s,v)$ and $p(y|s)$ are invariant across domains. To learn the model, the authors adopt variational inference by introducing $q(s,v|x)$ and training with ELBO. The authors consider single-domain OOD prediction and domain adaptation. For single-domain, they advocate to use an independent prior p(s)p(v) to eliminate spurious correlation. For domain adaptation, a new prior is learned on the target domain.  The authors provide theoretical analysis, showing that under additive noise assumption, the ground-truth CSG is identifiable, and subsequently the OOD generalization error and domain adaptation error are bounded. Experiments are conducted on Shifted MNIST, ImageCLEF-DA and PACS datasets. The proposed model achieves superior performance compared to CNBB in single domain and DANN, DAN, CDAN in domain adaptation.","The paper addresses the problem of out-of-distribution prediction by proposing a causal model which captures the underlying data generation process. The causal model consists of a latent variable which generates both the input images and the associated output labels. The novelty of the paper stems from the fact that the authors split the latent variable into two parts. The first part (s) captures the semantics of the generation process and the second part (v) captures the spurious domain specific variations. Since maximizing the likelihood of the data under this model is intractable, the authors propose an approximate and efficient training and inference mechanism. The authors show theoretically that their model will learn semantic features which are responsible for the final prediction. The claims are backed by a collection of experiments on simple datasets.   ",0.145985401459854,0.27007299270072993,0.1897810218978102,0.35294117647058826,0.29411764705882354,0.1958762886597938,0.23529411764705882,0.19072164948453607,0.1925925925925926,0.15463917525773196,0.18518518518518517,0.2814814814814815,0.18018018018018017,0.22356495468277945,0.1911764705882353,0.21505376344086025,0.22727272727272727,0.23100303951367782
919,SP:cd94adaeb740b8a9396ce90cd02ecc6be761d769,"This paper proposes to learn collaborative policies for routing problems. Collaborative policies firstly learn to generate many diversified and primitive solution candidates. Then for each solution candidate, they segment the original problem into several sub-problems. Another neural network model is trained to improve these sub-problems. The improved solutions of the sub-problems can be merged to form a new solution for the original problem.  They teste their methods on the TSP, PCTSP, CVRP problems and achieve the best performance among all the baselines. ","The paper addressed the hard routing problems based on a hierarchical strategy consists of two DRL policies called the seeder and reviser. Together with the two policies, to assess the diversity of candidate solutions, the entropy-regularized reward is adopted. The revise divides the tour into subtours and optimizes the length of them. Experimental results suggest that the proposed LCP framework works on TSP, PCTSP, and CVRP instances.","This paper introduces Learning Collaborative Policies (LCP) for learning to optimize TSP-style routing problems. The goal of this was not to outperform all other optimizers, but to outperform RL optimizers, which are usually less effective than traditional approaches, but also are more scalable to task-variation. This makes this class of optimizers more suitable for real-world problems. LCP uses 2 policies to optimize a TSP: a seeder, and a reviser. The seeder generates many valid but diverse solutions (ensured with an entropy maximization term in the reward). The reviser learns to relax small sections of each seed to create more optimal solutions. Revision is repeated a set number of times and the best solution is given.","Deep reinforcement learning (DRL) has recently been applied to variety of optimization problems, outperforming traditional approaches. This paper investigates the application of DRL to the context of routing problems such as the classical travelling salesman problem. The methodology proposed involves two DRL agents: the seeder, which is tasked with generating candidates for the solution that provide a good coverage of the solution space, and the reviser, which is tasked with attaining better solutions based on the reduced solution space provided by the seeder. To generate diverse candidates, the seeder utilizes entropy regularization reward. The authors present experiments indicating that their framework, called learning collaborative policies (LCP) improves over previous DRL frameworks for various routing challenges (e.g., TSP and capacitated vehicle routing).",0.17647058823529413,0.2235294117647059,0.2,0.23529411764705882,0.3088235294117647,0.15254237288135594,0.22058823529411764,0.16101694915254236,0.13934426229508196,0.13559322033898305,0.1721311475409836,0.14754098360655737,0.196078431372549,0.18719211822660095,0.1642512077294686,0.17204301075268816,0.22105263157894736,0.15000000000000002
920,SP:cdb6a46f10ff6ffab7a2284788c2d9483d843f3e,"VISER tackles the problem of 3D shape and pose reconstruction from an input video with masks and flows. The main competitor on this task is the recent LASR approach (CVPR 2021) The main technical novelty in VISER is joint embedding of pixels and surface points learned with a self-supervised contrastive loss. These embeddings match each surface points to all corresponding pixels in the video, thus addressing long-range correspondences in the video. ViSER outperforms the state-of-the-art on human and animal videos and the supplementary videos are very impressive.","This paper presents a new method for reconstructing a 3D shape template and dense 3D part trajectories from a monocular video. This work is very similar to a recent work LASR. The main difference, which is also the main contribution, is to establish long-range correspondences by matching the features between each 2D frame to a 3D canonical shape. ","Authors propose a method for reconstructing deforming objects from videos (with the target object pre-segmented). The output is a mesh with bone assignments, plus temporal deformation parameters (i.e. bone transformations). The approach predicts correspondences between frame pixels and points on the mesh by mapping them into a joint embedding space, using a contrastive loss. The method is demonstrated on videos of humans and animals, where it out-performs a recent baseline.","This paper presents an optimization-based framework that recovers 3D shape, articulated pose, and weak texture from a monocular video of an articulated object. The proposed method extends a recent method (LASR) with a surface embedding matching module for establishing long-range correspondences, and achieves significantly better results on highly deformable objects, such as break dancers.  This is an extremely challenging task, and has a long history in computer vision. Current methods either require extra information, eg depth or keypoints, or rely on category-specific template shape models. The results achieved by this method is very impressive, relying only on off-the-shelf category-agnostic segmentation and optical flow models.  The paper also demonstrates the possibility of extending the method with different instances in multiple videos, which presents exciting potential of capturing category-level priors for fast inference. ",0.18478260869565216,0.17391304347826086,0.21739130434782608,0.2033898305084746,0.3050847457627119,0.2054794520547945,0.288135593220339,0.2191780821917808,0.14492753623188406,0.1643835616438356,0.13043478260869565,0.10869565217391304,0.22516556291390727,0.19393939393939394,0.17391304347826086,0.18181818181818182,0.18274111675126906,0.14218009478672985
921,SP:cdf6b2a136592442b46bec5d42306398350b0b58,"This paper considers cardinality constrained submodular maximization, both monotone and non-monotone, in the random streaming setting. In the random streaming setting the universe is received in a uniformly random permutation, as opposed to the adversarial setting where the universe could be received in any order.  In the random streaming setting, better approximation guarantees are possible compared to the adversarial setting (where 1/2 is the best possible). An algorithm developed by [ASS19] was shown to give a $1-1/e-\epsilon$ guarantee for cardinality constrained monotone submodular maximization, but has memory exponential in $\epsilon$ making it impractical. This paper introduces a streaming algorithm that gives the $1-1/e-\epsilon$ needing only $O(k/\epsilon)$ memory. They also prove $1-1/e$ is the best possible. Further, they introduce a streaming algorithm for the non-monotone version of the problem that gives a $1/e$ approximation guarantee also in $O(k/\epsilon)$ memory.  The algorithms are analyzed empirically, and shown to be practical.","The paper studies the submodular maximization problem with a cardinality constraint in the random stream model where the algorithm makes a single pass over a random permutation of the elements. The main contribution of the paper is an algorithm for monotone functions with a nearly-optimal 1-1/e-eps approximation and O(k/eps) memory. The algorithm also extends to non-monotone functions via sub-sampling, giving a 1/e-eps approximation  using the same memory requirement. On the hardness side, the paper shows an unconditional hardness result showing that an approximation of 1+1/e is best possible for any single-pass random stream algorithm, even if the algorithm is allowed unbounded computation time.","This work studies cardinality-constrained submodular maximization in a one-pass streaming model (with a random ordering). The main results are: - A $(1-1/e-\varepsilon)$-approximation algorithm for monotone functions that   uses $O(k/\varepsilon)$ memory, where $k$ is the cardinality constraint. - A $1/e$-approximation algorithm for non-monotone submodular functions that   also uses $O(k / \varepsilon)$ memory. - A new hardness result which shows that any   $O(1-1/e+\varepsilon)$-approximation algorithm requires $\Omega(n)$ memory.  The algorithm for monotone functions in this paper improves on the recent work of [Agrawal-Shadravan-Stein, ITCS 2019] by removing an exponential dependency on $\varepsilon$ in the space complexity. Similarly, they close the previously best-known streaming hardness gap of $7/8$ to $(1-1/e+\varepsilon)$, which was also proven in [Agrawal-Shadravan-Stein, ITCS 2019]. Lastly, the authors provide a strong set of experiments that demonstrates their streaming algorithm is very competitive with an offline greedy algorithm.","This paper studies the problem of maximizing a submodular function. For monotone submodular functions subject to a cardinality constraint, the paper gives a $(1-1/e-\epsilon)$-approximation. The space requirement is $O(k/\epsilon)$ where $k$ is the cardinality constraint. This is in contrast to a previous algorithm of Agrawal et al. which gave an algorithm with the same approximation ratio but with an exponential dependence on $\epsilon$. Hence, the new algorithm obtains an exponential improvement on the dependence on $\epsilon$. The new algorithm is also straightforward to implement. For the case of non-monotone submodular maximization subject to a cardinality constraint of size $k$, the authors gives an algorithm that achieves a $(1/e - \epsilon)$-approximation with space complexity $O(k / \epsilon)$.  The authors also prove a $(1-1/e+\epsilon)$ hardness result for monotone submodular maximization. This improves upon the previous best hardness result of $0.7$.  Finally, the authors show some experimental results and it is apparent that their algorithm performs quite well when compared to offline greedy.",0.21341463414634146,0.23170731707317074,0.24390243902439024,0.33620689655172414,0.3103448275862069,0.3018867924528302,0.3017241379310345,0.2389937106918239,0.23255813953488372,0.24528301886792453,0.20930232558139536,0.27906976744186046,0.25,0.23529411764705882,0.2380952380952381,0.28363636363636363,0.25,0.2900302114803625
922,SP:cf5c0de0dd44797b0ea8d3cbb4c911cdf36a6fbc,"The paper studies the problem of random order online convex optimization introduced previously by Garber et. al in 2020. Random order online convex optimization is a variant of standard online convex optimization where the loss functions may be chosen by an adversary (and do not even need to be individually convex), but the order in which they arrive is uniformly random.  This modification allows one to obtain efficient algorithms with stronger guarantees than for true worst-case analysis.    This paper improves upon the guarantees obtained by Garber et. al, getting nearly optimal rates in the general setting that essentially match offline/stochastic rates.  They remove a factor of $d$ and a factor of $1/\lambda^2$ (where $\lambda$ is the strong convexity parameter) from the regret guarantee.  However, their optimal algorithm requires remembering all previous observations (so $O(dT)$ memory).  Thus, the authors also give an efficient algorithm that requires only $O(d)$ memory but sacrifices a $1/\lambda$ factor in the regret guarantee (which is still better than the result in Garber et. al).   Roughly, the authors draw a parallel to stochastic online convex optimization.  The difference in the random order model is that the observations are not  independent but are instead sampled from a set \textit{without replacement}.  However, the authors are able to modify  techniques from the stochastic case to this without replacement setting.","The authors study a variant of OCO introduced by Garber, Korcia, and Levy in which the costs are adversarial but are revealed to the online learner in a uniformly random order. The authors propose a new gradient-based algorithm for this model and claim that the regret incurred by their algorithm has a strict improvment on the problem parameters compared to that of Garber et al. Specifically, the regret of the new algorithm does not depend on the ambient dimension $d$ at all and has improved dependence on the strong convexity parameter $\lambda$.","Problem description: The paper studies the problem of online convex optimization in the random order model. This problem was recently proposed by Garber et al. (ICML 2020). It is a generalization of online convex optimization, where at each step the learner incurs a loss function $f_t$ and the goal is to minimize the cumulative loss $\sum_{t=1}^{T} f_t$. The authors inspired by Garber et al. (2020), considered the scenario in which each $f_t$ is smooth and Lipschitz but not necessarily (strong) convex. However, they imposed this assumption that $\sum_{t=1}^{T} f_t$ is (strongly) convex. Garber et al. (2020) assumed that at each step $t$, a new loss function $f_t$ is sampled uniformly without replacement from $\{f_t\}_t=1^T$, and they call this setting random order online optimization. The same setting is peruse by the authors of this paper and they improved the results in Garber et al. (2020), under the same assumptions.  Assumptions: 1. $f_t$ is Lipschitz  2. $f_t$ is smooth 3. $\sum_{t=1}^{T} f_t$ is $\lambda$-strongly convex.  Approach: Their main idea is to use the stability properties of stochastic gradient descent, see Shalev-Shwartz et al. (JMLR 2010). The authors defined a new notion of stability (page 5, line 176). This new definition of stability is suitable for sampling without replacement, which is beneficial for random order online optimization. Note that relevant definitions of stability Shalev-Shwartz et al. (2010), works only for i.i.d sampling.   By this conviction, they proposed Algorithm 1 (page 7) which is inspired by Vitter (1985). They proved the stability property of Algorithm 1 in Lemma 6 (page 7), which implies their main result in Theorem 1 (page 8). Since Algorithm 1 requires an intense memory usage, they proposed Algorithm 2 (page 9), which is more economical in that regard. Theorem 2 (page 9) uses Algorithm 2 but with a worse dependency w.r.t $\lambda$ (strong convexity parameter), compare to Theorem 1.  Contributions: With a different and a novel approach, they studied the problem proposed by Garber et al. (2020) and they could improve the results which I summarize as follows:  1. The upper bounds in Garber et al. (2020) have a linear dependency with respect to the dimension of $f_t$. They could remove this dependency and the rates are dimension free.  2. In Theorem 1 (page 8) they improved the results in Garber et al. (2020) up to a factor $1/\lambda^2$  3. In Theorem 1 (page 8) they improved the results in Garber et al. (2020) up to a factor $1/\lambda$  4. As a corollary (page 9) of Theorems 1 and 2, they could obtain similar results for the case when $\sum_{t=1}^{T} f_t$ is convex but not necessarily strongly convex. ","In this paper the authors tackle the problem of online optimization with functions (picked by an adversary) arriving uniformly at random. Moreover, instead of assuming each function is (strongly) convex individually, they assume only that the sum of these functions is (strongly) convex, as recently proposed in previous work. Their main results are two algorithms for the random order model with strongly convex cumulative loss function, one of them attaining regret matching the assymptotics of the traditional adversarial case and the second only paying an extra $1/\lambda$ factor (both up to lower-order terms) but with a smaller memory footprint. As a corollary they obtain two algorithms for the plain convex case with regret bounds on the order of $\tilde{O}(\sqrt{T})$ and  $\tilde{O}(T^{2/3})$, respectively. To obtain these results they extend the connection between generalization and stability to the ""out-of-sample""/sampling without replacement case, which may be interesting by itself.",0.1277533039647577,0.31718061674008813,0.15418502202643172,0.3870967741935484,0.21505376344086022,0.09915611814345991,0.3118279569892473,0.1518987341772152,0.22151898734177214,0.0759493670886076,0.12658227848101267,0.2974683544303797,0.18125,0.20542082738944364,0.18181818181818182,0.12698412698412698,0.1593625498007968,0.14873417721518986
923,SP:cf629f8dda8893dc20b8882ad7fb80d0c6f36052,"The authors propose PACE, a Transformer-based architecture for directed acyclic graphs (DAGs). By providing a DAG-specific positional encoding (using canonicalisation and GINs), the claim is that a fully-connected self-attentional model can be executed over such an input, ameliorating the requirement of following the DAG's topological structure in an RNN-like fashion. The proposed model is evaluated over neural architecture search datasets, showing improvements over relevant baselines.","This paper proposes an encoder architecture for directed acyclic graphs which is parallelizable in computation and thus more efficient than asynchronous message passing alternatives. The model is based on representing the graph in its canonical form, i.e. a unique representation of its isomorphism class, which is transformed into a positional encoding and combined with a Transformer-based encoder. In experiments on neural architecture searches and Bayesian networks, the proposed architecture outperforms related methods.","This paper introduces PACE, a method for converting a labeled directed acyclic graph into a sequence so that it can be fed to a Transformer architecture, along with a small modification to the Transformer to incorporate DAG structure. The authors cast their approach as a parallelizable alternative to some existing methods for encoding DAGs, which include GRU based approaches and undirected GNN based approaches, and show that their method outperforms those techniques on neural architecture search tasks. (However, the authors do not discuss or compare with existing methods for encoding of directed but not-necessarily-acyclic graphs, including other parallelizable approaches using Transformers.)  *Edit: Comparisons to other methods have been added in replies to other reviewers, although the paper has not been updated.*","The paper presents a new architecture for encoding the directed acyclic graphs into embedding vectors to benefit the downstream applications. The proposed architecture breaks the limitation of sequentially encoding the nodes of a DAG, and could parallelly encode the nodes of each graph so that the learning and inference speed could be much improved. The proposed method is validated on encoding neural architectures collected in the NAS benchmarks and shows better effectiveness and efficiency compared to existing graph encoding methods.",0.19718309859154928,0.22535211267605634,0.23943661971830985,0.22972972972972974,0.20270270270270271,0.16260162601626016,0.1891891891891892,0.13008130081300814,0.2125,0.13821138211382114,0.1875,0.25,0.19310344827586204,0.16494845360824742,0.22516556291390727,0.17258883248730966,0.1948051948051948,0.19704433497536947
924,SP:cf8e43276e85ef5532e34a8bd6a77bf26b37e6b5,"The paper introduces FLINT, a framework that jointly trains a classifier with an “interpreter”, an interpretable version of the model. The interpreter is a linear classifier over a set of attributes (each associated with a “concept”), which are derived from hidden states of the neural network. The interpreter is trained with three objectives: (1) matching the output of the original classifier, (2) a sparsity regularizer, and (3) an anto-encoding objective to reconstruct the input. Each attribute in the interpreter can be visualized by finding input that maximizes the attribute (with some regularization), which allows the interpreter to produce global interpretation for a class and local interpretation for an example. The method is tested on four image classification tasks. It has higher accuracy than two previous interpretable models (SENN and PrototypeDNN) and higher fidelity than two post hoc interpretation methods (LIME and VIBI). According to a human survey, the visualizations are human interpretable in most cases.","This paper proposes an architecture which jointly learns to classify as well as provide an explanation behind the classification. The explanation is generated from a separate part of the model which doesn’t affect the model performance much as the classification performance is achieved by a backbone network. The explanation generation part takes inputs from the intermediate layers of the backbone network which helps to produce better and more meaningful explanations. The results are promising on experiments shown on MNIST, FMNIST, CIFAR10 and QuickDraw datasets.","The paper proposes FLINT a Framework to Learn with Interpretation to solve Supervised Learning with Interpretation task. FLINT involves training 3 models on a preditor model an interpreter and a decoder. This framework provides both global and local interpretations.  The interpreter network takes as an input the output of different hidden layers from the predictor network then computes an attribute dictionary of functions phi g(x)= softmax(Wphi(x)). Different phi corresponds to different concepts  phi_j(x)= Psi_j (f_I(x)) where Psi_j is shallow network and f_I(x) is the output of hidden state I.  So The model parameters of g depend on the parameters of Psi and Weights. Given interpreter with parameter and input x the contribution of phi_j = phi_j(x)*w_j,y_hat/max|ph_i*w_i_yhat| where y_hat the class output.  Global Interpretation is a set of class-attribute pairs (c, ph_j) such that their global relevance is greater than some threshold. Local interpretation for a sample x provided is the set of attribute functions ph_j  with a local relevance score greater than some threshold.  FLINT has multiple loss functions, first to ensure that the interpreter output is similar to that of the model output cross-entropy is used. An additional loss is used to ensure diversity of ph_j and ensure input fidelity the input is reconstructed from phi using a decoder network which gives us the final loss.  FlINT was evaluated on 4 different image datasets, it was compared with other self explain networks SENN and PrototypeDNN to terms of accuracy and saliency methods LIME and VIBI in terms of fidelity.","The paper proposes FLINT, a new learning schema in which a neural net and a gray-box model are learned jointly by minimizing a composite loss: a loss on the labels + a set of losses controlling the concepts acquired by the gray-box models + a distillation loss that encourages the gray-box models to behave similarly to the neural net.  The concepts in the gray-box model are built from intermediate neurons of the neural net to encourage faithfulness.  The authors also describe a visualization pipeline and a post-hoc explanation protocol.",0.14743589743589744,0.2948717948717949,0.14743589743589744,0.2823529411764706,0.21176470588235294,0.0967741935483871,0.27058823529411763,0.16487455197132617,0.25,0.08602150537634409,0.1956521739130435,0.29347826086956524,0.19087136929460582,0.21149425287356322,0.18548387096774194,0.13186813186813187,0.2033898305084746,0.14555256064690028
925,SP:cfaaba04a565ee82c253d37bbb5552fb340a4ea5,This submission proposes a dynamic resolution network that adaptively selects the suitable resolution for image recognition. A tiny resolution predictor is proposed to predict a smaller resolution for the input image of the large classifier. Thus the computational costs can be saved. The experimental results are good and the novelty of this paper is sufficient.,"The paper presents a method to reduce FLOP requirements by resizing images dynamically using a resolution prediction network, prepended to an image classifier network.  The resolution is selected from one of a handful of candidates, and resized before being passed to the classifier.  A training-time loss encourages smaller resolutions to be selected, effecting an accuracy vs resolution tradeoff controlled by two parameters.  Since selected resolution for inference is a one-hot categorical, the method uses Gumbel softmax to enable backpropagation during training.  A resolution-aware batchnorm layer is also described, which tracks batchnorm stats separately for each resolution.  The system performs very well, with over 40% reduction in average operations resulting in just one or two percent accuracy loss on imagenet, and small accuracy gains with 10% reduction in computation. ",A dynamic resolution network is proposed in this submission. There is a computationally efficient predictor which selects the suitable resolution for the large classification CNN model. The predictor generates hard decision of resolutions. The usage of Gumbel softmax trick enables the gradient to propagate to the predictor. A FLOPs loss is also proposed to control the computation budget. Experiments on ImageNet show that this method could largely reduce the FLOPs with similar accuracy or improve the accuracy with fewer FLOPs reduction.,"This manuscript proposes a dynamic resolution network that can be used for the base CNN model to dynamically change the resolution of the input images. The intuition of the proposed method lies in the fact that the difficulty of the input may differ. For the easy sample, one does not need to use the full resolution for the classification. The experiments show that the proposed DRN can reduce the computational cost a lot while keeping a similar performance and even better one.",0.2545454545454545,0.2909090909090909,0.34545454545454546,0.16030534351145037,0.12213740458015267,0.2716049382716049,0.10687022900763359,0.19753086419753085,0.23170731707317074,0.25925925925925924,0.1951219512195122,0.2682926829268293,0.15053763440860213,0.23529411764705882,0.2773722627737226,0.1981132075471698,0.15023474178403756,0.26993865030674846
926,SP:cfcd5f329e4fe8e412c2c5c3a2bdee851333f29b,"To train language GANs, previous approaches use the discriminator score as a reward, and train the generator through RL. Instead, this paper proposes to use the discriminator to guide the decoding process, searching for outputs with high discriminator scores, and then train the generator to generate these outputs. To find outputs with high discriminator scores, the authors employ Monte Carlo Tree Search and use the discriminator as the value network. Experiments include question generation and summarization, and the proposed method shows improvements to MLE and ColdGAN.","This paper presents a method of training (or rather fine-tuning) text generation models using a GAN-like setup. Instead of using the discriminator directly to update the generator as is usually the case, the authors propose using the discriminator as a way of cooperative decoding to generate high-quality samples which are then used as the gold output to train the model in the usual teacher-forcing setup. The authors explore a sampling procedure based on MCTS to generate more diverse sentences rather than using beam-search or sampling during training and inference. On summarization and question generation, they show improvements on the standard and proposed evaluation metrics. ","This paper attempts to address the difficulties of adversarial training for language generation, as discrete generator outputs make conventional GANs difficult to train.  The authors propose combining cooperative decoding proposed by Gabriel et al (2019), along with MCTS-based decoding, to train a discriminator and generator jointly. The generator outputs are trained with the results of their MCTS-based cooperative decoding procedure.  The authors demonstrate their method's performance on summarization (CNN and Daily Mail dataset) and question generation (SQuAD). ","This work proposes selfGAN. Instead of using RL techniques to utilize the discriminator. They do Coop-MCTS which is termed as a ""cooperative decoding"", which is designed to generate sequences that is better than naïve sampling or decoding. And the coop-decoded sequence is then used to train the generator in turn, in the standard teacher-forcing way. I understand this approach as the process in which the discriminator is helping the generator to refine its generations. Better results comparing to MLE and ColdGAN are shown in the experiments. ",0.22093023255813954,0.22093023255813954,0.23255813953488372,0.2018348623853211,0.23853211009174313,0.175,0.1743119266055046,0.2375,0.2222222222222222,0.275,0.28888888888888886,0.15555555555555556,0.1948717948717949,0.2289156626506024,0.2272727272727273,0.23280423280423282,0.26130653266331655,0.16470588235294117
927,SP:d0218a8986184d625564ab11b9e7887615e9edb3,"This paper introduces a novel method based on spectrum-based fault localization (SBFL) from the software testing domain to identify the states in which decisions made by an RL policy are most influential in determining whether or not the agent achieves its objective. More specifically, the authors create ""mutant trajectories"" where, with probability $\mu$, a state is mutated and the action that the agent must take in that state is the ""default action"" (e.g. ""repeat the previous action"") or unmutated, where the agent acts according to policy $\pi$ when in that state.  From these mutant trajectories, the authors use SBFL to determine a ranking of states.  SBFL takes as input the vector $\<a_{ep}^{s}, a_{ef}^{s}, a_{np}^{s}, a_{nf}^{s}\>$, where $a_{ep}^{s}$ is the number of successes achieved when state $s$ is a non-mutant state, $a_{ef}^{s}$ is the number of failures achieved when state $s$ is a non-mutant state, and $a_{np}^{s}$, $a_{nf}^{s}$ log the number of successful and failed trajectories when state $s$ is a mutant state.  In order to assess the quality of the ranking returned by SBFL, the authors create ""pruned policies"" where an agent can only use the policy $\pi$ for the top $r$ states, and must use the default action for all other states.  Overall, in a wide variety of settings ranging from MiniGrid and Cartpole to Atari games, the authors find that, by ranking states using SBFL, they are able to encourage agents to vastly reduce the length of their trajectories while still attaining high reward.  This is a significant contribution from the perspective of creating interpretable AI.","The goal of this work is to leverage fault localization techniques from software development and debugging as a way to simply learned RL policies. The main premise is that learned policies are unnecessarily complex. In order to simplify their complexity, it is critical to rank the states where the learned policy decisions are critical. Motivated by spectrum-based fault localization, authors leverage it in RL by using similar cost functions to compare complex policy functions to so called mutant executions which are policies that recommend default or previous actions as a way to simplify the complexity of the policy class. Based on such mutant policies (identified on the fly by parametrizing specific aspects), the authors demonstrate they are able to rank important states (based on the policy's impact on outcome). The proposed method is evaluated on standard RL environments (atari, minigrid, cartpole) to quantify efficacy and visually demonstrate interpretability via saliency maps. ","The paper uses an existing ranking mechanism borrowed from software testing to rank the states of trajectories in reinforcement learning. After ranking the states, the authors propose to replace the top performing states with a default decision/action showing that that proposed ranking correlates with the `importance` of the trajectory states. The results show that the performance of the pruned policies (after replacing the states with a pre-defined action) is comparable with the original (non-pruned) policies.","This paper studies the problem of ""ranking policy decisions,"" which uses spectrum-based fault localization to compute an offline score for each action, conditioning on a state. The main idea is that the action with a higher success frequency should be ranked higher than other actions. Studying the action rank of policies presents an interesting angle to investigate RL policy from simplifying the DRL policy, interpretability, offline evaluation, while the experiments conducted might not be sufficient to support either one aspect solidly.  ",0.1223021582733813,0.11151079136690648,0.08633093525179857,0.13071895424836602,0.12418300653594772,0.16666666666666666,0.2222222222222222,0.3974358974358974,0.2926829268292683,0.2564102564102564,0.23170731707317074,0.15853658536585366,0.15777262180974477,0.17415730337078653,0.13333333333333333,0.17316017316017315,0.16170212765957448,0.16249999999999998
928,SP:d02418bb68ae61bc060c0c4359b280bd162aa1f9,"The paper proposes a mixing strategy for graph structured samples. This strategy is works as follows : 1) take a pair of graphs 2) assign a node index from 1 to N to each node in the graphs, where N is the number of nodes in the larger graph. 3) for the smaller graph, create dummy nodes so that the total number of nodes in smaller graph becomes N. 4) Mix (interpolate) the node features and edge connectivity of the nodes with same indexes from the two graphs.  The main claim of this paper is that, such kind of mixing avoids the ""manifold intrusion"" problem.","This work studied the research problem of data augmentation for supervised graph classification. The authors proposed ifMixup, which is a Mixup-based data augmentation method on graph data. The authors also showed that ifMixup is free of the known manifold intrusion problem of Mixup-based methods.","This manuscript develops a mixup strategy for graphs. Specifically, as claimed by the authors, the proposed method is manifold intrusion-free. This is achieved by mixing two input graph pairs (some dummy nodes are needed, see Figure2) based on the standard mixup formulation. ","This paper shows a very simple technique for performing input-space mixup on graphs, in which the vertices are put into an arbitrary order and dummy vertices are added to make the two graphs being mixed have the same number of vertices.  Mixing is then performed on the vertices and edges.  The improvements are excellent and seem to be SOTA on all benchmarks considered, often by nice margins.    Detailed Notes from reading paper:    -Mixing on graphs.  Theoretical guarantee that mixed graphs avoid intrusions.     -Example of intrusion on mixed graphs is interesting, simple, and logical.     -Proposed algorithm is ""Intrusion-Free Mixup"" ifMixup.     -Edges and nodes take interpolated values.  Labels are interpolated based on edges.     -Can recover source graphs from mixed graphs.     -Task is to convert each graph into a single class (as opposed to each node having a class label).     -Each node is assigned a distinct index, dummy nodes are created to make each ""graph"" same size.  Dummy nodes have no edges.     -First construct ""soft"" edges by mixing edges between two graphs.  Network needs to be able to take soft edges.     -Proof that this graph mixing operation is invertible.     -ifMixup also shows benefit from more depth, as compared to baseline (which is probably badly overfitting).   ",0.1346153846153846,0.09615384615384616,0.22115384615384615,0.21739130434782608,0.2826086956521739,0.3488372093023256,0.30434782608695654,0.23255813953488372,0.11274509803921569,0.23255813953488372,0.06372549019607843,0.07352941176470588,0.18666666666666668,0.13605442176870747,0.14935064935064934,0.2247191011235955,0.10399999999999998,0.12145748987854252
929,SP:d07d63b62ed4a5d317100e18885eeebb3ee41e19,The paper provides a novel two-stage pipeline for 3D scene graph prediction tasks on point cloud data. Novel relation and object embedding methods are proposed. Experiment results show that the proposed pipeline gives higher performance compared with related methods. ,"In this paper, the authors study the 3D scene graph prediction task. The major contribution is the proposed “object/predicate meta-embedding,” which learns class-specific embedding to assist scene graph prediction. Ablation study verifies the effectiveness of the proposed embedding learning method.","The paper proposes a scene graph prediction method for 3D point clouds. The paper uses the graph network to learn class label based embeddings. It does not use appearance information from the point clouds to avoid perceptual inconsistencies. The class label dependent prototypical embeddings are learned using a graph auto-encoder with directed edges and a message passing mechanism tuned for such edges. The scene graph prediction works by extracting embeddings from subject and object points and object pair relationship in form of an edge embedding. The embeddings (geometric, meta embeddings) are fused with an MLP. The network is trained with focal loss. The proposed method is evaluated on 3DSSS dataset and achieves state of the art results on various metrics. ","This paper proposes a knowledge-inspired method for 3D scene graph prediction on point cloud. The key is to decompose the problem into two sub-tasks. First, a graph auto-encoder called meta-embedding is introduced to learn class-dependent prototypical representations only from the class labels to avoid visual confusions. Then a scene graph prediction model selects related meta-embedding from the first task as prior knowledge to predict credible relation triplets of 3D scene graph. Benefited from the learned commonsense knowledge, the scene graph prediction model achieves good performance on 3DSSG dataset.",0.225,0.35,0.325,0.3488372093023256,0.4418604651162791,0.2231404958677686,0.20930232558139536,0.11570247933884298,0.13829787234042554,0.12396694214876033,0.20212765957446807,0.2872340425531915,0.21686746987951808,0.17391304347826086,0.19402985074626866,0.18292682926829268,0.2773722627737226,0.2511627906976744
930,SP:d0b35f80df03a480d19e0d3e41a3d854471325c9,"This paper proposes a new probabilistic model named PC for efficient encoding and decoding. The authors claim that PC greatly reduces the time complexity for inference. Experiments show that PC achieves 5-20x faster than SOTA neural compression algorithms, and performs SOTA results on MNIST datasets.","The paper showcases an application of Probabilistic Circuits to lossless compression, and achieves competitive compression performance to state of the art method. The PCs output a marginal distribution over the data, which the authors then use to compress with arithmetic coding or other methods. This has several advantages, one being that bits-back coding is not needed, enabling single-sample compression.  ","Probabilistic circuits are a formalism, developed relatively recently, for describing multivariate probability distributions. PCs are represented using directed acyclic graphs (DAGs), with an operational semantics, and it is relatively straightforward to deduce which operations (marginalization, maximisation, estimation of moments, etc.) are tractible based on the structure of the DAG and the locations of input variables.  This paper studies the application of PCs to lossless compression, identifying the specific type of marginalisation which is necessary for auto-regressive, arithmetic coding-style compression, and the necessary conditions a PC must satisfy in order for this marginalisation to be tractible. In section 3, a procedure is described for particularly efficient marginalisation by sharing computation.  Experiments show that the approach is competitive in terms of compression rate with other recent 'neural compression' works, and results are presented showing that PCs can be much faster than some existing methods, although I have serious concerns about these (see below).","The manuscript addresses the question of using deep generative models in lossless compression. As highlighted by the authors, the main issue here is related to the cost of computing probabilistic queries or, in some cases such as GANs, the inability to compute them. Thus, this paper suggest using Probabilistic Circuits (PCs) for lossless compression. PCs allow for tractable probabilistic inference, which enables efficient compression.  Experimental results are favourable in two ways. First, PCs are faster for compression, achieving results from 5 to 20 times faster than competitor neural networks. Second, PCs achieve competitive compression rates on various datasets.",0.15217391304347827,0.30434782608695654,0.2826086956521739,0.26229508196721313,0.18032786885245902,0.12418300653594772,0.11475409836065574,0.0915032679738562,0.1326530612244898,0.10457516339869281,0.11224489795918367,0.19387755102040816,0.13084112149532712,0.1407035175879397,0.18055555555555558,0.14953271028037382,0.13836477987421386,0.15139442231075698
931,SP:d0db395de8a01b89b69c7dd1f1d8075c5a026b0f,"This paper studies a multi-armed bandit problem where the reward of the arm depends on the hidden state. The state changes according to the dynamics of the MDP, which means that the arms chosen in the past will influence the states chosen in the future time steps. The number of states and available actions at each state in the MDP is assumed to be finite, and the reward is assumed to be drawn from a distribution with finite support. This work studies the setting where the learner the transition matrix and the reward distribution of the arm for a given state and also the learner doesn't observe the reward. The main contribution of the paper is the algorithm that is shown to achieve T^2/3 regret in the described setting, where the regret is measured against the best policy that doesn't observe the current state and is not restricted to be a memoryless policy.","In this paper, the authors consider a variation of the well-known MAB problem where the Multi-Arm bandit can be in different regimes modelled by a common finite-state Markov Chain unobservable to the learner.  The means of the different arms can vary in the different regimes and the learner needs to infer both the transition matrix and the reward distribution in the different states. The contribution is threefold: a new bandit formulation, an algorithm for solving this new problem and the theoretical analysis of the proposed algorithm.","This paper studies a non-stationary and finite bandit model with Markovian regime-switching rewards, which is termed “regime switching bandit”. As mathematically formalized in the paper, in this new bandit model, the reward distributions of various arms are modulated by an underlying finite-state Markov chain, with unknown transition matrix, and whose state is assumed unobservable. The paper develops a learning algorithm called SEEU, which combines spectral estimators for the Markov chain involved with the optimism principle (together with some forced exploration). Ignoring logarithmic factors, SEEU is shown to achieve a regret of $O(T^{2/3})$, with high probability, against an oracle who knows reward distributions and the transition function of the Markov chain, but as the learner, does not observe the state of the Markov chain. ","The authors study a variation of the multi-armed bandit problem where the rewards are neither adversarial nor stochastic. The rewards are actually random variables drawn from distributions that change based on an unknown state.  This changing state is modeled by a finite-state Markov chain, and the learner does not know the current state, nor the matrix and reward distribution. The difficulty of this problem seems to be controlled by two different aspects: how strong is the oracle you compare the learner to, and how much budget has the environment to change the state that it is in.    The oracle is the best action in hindsight, then results for adversarial MAB would apply, which give  $ \tilde O(\sqrt T)$ types of regret bounds. The authors consider a much stronger oracle that is aware of the transition and the reward matrix of the MDP, but not of the current underlying state. In their problem setting, the changing budget is linear in T.   The authors present an algorithm for this problem and derive a O(T^{2/3}\sqrt{\ln{T}}) regret bound against the previously mentioned oracle. Their algorithm combines several concepts such as the spectral method of moments methods to estimate the parameters of the MDP and the commonly used UCB method.    ",0.2088607594936709,0.2088607594936709,0.2848101265822785,0.25842696629213485,0.4606741573033708,0.24031007751937986,0.3707865168539326,0.2558139534883721,0.2112676056338028,0.17829457364341086,0.19248826291079812,0.14553990610328638,0.26720647773279355,0.22996515679442509,0.24258760107816713,0.2110091743119266,0.271523178807947,0.18128654970760236
932,SP:d0dd82b1e121c91644e81190c03cfc8e45a07b70,"The paper presents a method for polarization-based dehazing of images. Using a camera with a polarization filter array, multiple polarization images can be captured in a single snapshot. These images are used to estimate an unpolarized image $I$ and a degree-of-polarization image $P$. A series of neural networks are trained on synthetic data to estimate the polarization and radiance of the ambient and transmitted scene light conditioned on $I$ and $P$. The results of the dehazed estimated scene radiance are compared with other methods for dehazing using polarization images or a single unpolarized image. Quantitatively, the approach appears to offer roughly 1-2 dB in improvement over previous methods in terms of PSNR. Qualitatively, the method appears to generate improved results on captured data as well.  ","This paper proposes a method for image dehazing by polarization-based learning pipeline. It is based on a physical formation model which is considering the spatially variant real-world scattering and taking into account the polarization effects of both transmitted light and airlight. Experiments demonstrate that the proposed method can dehaze robustly with fewer artifacts, and performs better in recovering details. ","This paper presents dehazing algorithm using characteristics of polarized images. It first introduces generalized physical formation model of polarized hazy images, regarding spatially-variant real-world scattering. Taking three differently polarized hazy images as input, the proposed algorithm estimates and refines degrees of polarization (DoP) of the transmitted light T and airlight A, and utilizes these predictions to finally obtain original scene radiance R. By doing so, this paper was able to show outstanding performances, especially on real-world images due to its consideration on spatially-variant DoP and global airlight.","This paper proposes a neural network for image dehazing where the input is expected to contain polarization information (e.g. multiple polarized images of the exact same scene). The total irradiance and total degree of polarization can be obtained directly from the polarized input frames. The paper then follows the image formation model of [77] (and others) to identify key quantities to estimate: the infinite airlight color, and the degree of polarization of the ambient and scene light. Multiple neural networks are designed to estimate these components; they are coupled using closed-formed expressions derived from the image formation model to produce the final dehazed image. The networks are trained independently, then finetuned jointly on a synthetic dataset. ",0.13178294573643412,0.15503875968992248,0.2248062015503876,0.3442622950819672,0.2786885245901639,0.2087912087912088,0.2786885245901639,0.21978021978021978,0.2457627118644068,0.23076923076923078,0.1440677966101695,0.16101694915254236,0.17894736842105263,0.18181818181818182,0.2348178137651822,0.27631578947368424,0.18994413407821228,0.18181818181818182
933,SP:d13d450b463804798cc8c6a51bae4bd710c1eba7,"This paper proposes to distill the features from a LiDAR teacher model to a monocular-based student model. To align the feature maps between the teacher and student model, the teacher model uses the same networks as the student and the only difference is that the teacher model takes the sparse/dense depth map as input. Several techniques include scene-level distillation, object-level distillation in feature/outputs, feature fusion. In the experimental part, this paper extensively ablates several key analyses including cross-modal evaluation between baseline model and full model, depth estimation. ","The paper presents MonoDistill, a way to enhance RGB-based 3D object detection through knowledge distillation from a LiDAR-based teacher. A 3D detection pipeline (built around MonoDLE) is trained both on RGB images and densified LiDAR input. Three main mechanism enable teacher-student transfer: On feature level, scene-level distillation aligns affinity maps between teacher and student while masked features are trained on object-level, additional masked pseudo labels are leveraged around the teacher centre predictions. Each component is carefully ablated and experiments are conducted on the KITTI benchmark. ","In this paper, authors propose a monocular image-based 3D object detection method based on knowledge distillation. Specifically, the monocular image-based 3D object detector [Ma et al 2019] is infused with the depth cues via distilling knowledge from LiDAR-based teacher model. At the test-time, the model detects 3D objects without any intermediate depth prediction and ranks 1st on the KITTI benchmark dataset. Hence, the proposed monocular-based 3D detector is end-to-end and more accurate than the other methods in this category, without adding extra depth-estimation overhead in between. ","This paper proposes an approach to leveraging knowledge distillation (KD) for training image-based monocular 3D detectors. Different from the ways to incorporate the LiDAR signal in prior works, this paper takes it as the input of a teacher network and further supplements the image-based student network in terms of spatial information. Three levels of distillation, scene-level, object-level in the feature space, and object-level in the result space, are devised. Experimental results show that this method can effectively boost the performance of image-based monocular 3D detection while still maintaining outstanding efficiency.",0.1935483870967742,0.20430107526881722,0.23655913978494625,0.2111111111111111,0.2,0.18085106382978725,0.2,0.20212765957446807,0.22916666666666666,0.20212765957446807,0.1875,0.17708333333333334,0.19672131147540983,0.20320855614973263,0.23280423280423282,0.20652173913043478,0.19354838709677422,0.17894736842105263
934,SP:d14a523212d330ca3527e7f4daef361f17d5fc84,"Memorization has been investigated in deep neural network classifiers but not that much in adversarial training. The authors investigate the ability of adversarial training to memorize random datasets. They use PGD-AT and TRADES to do the adversarial training and see different convergence properties, which differs significantly from training on true labels. They also discover that robust overfitting can occur in adversarial training with memorization and use temporal ensembling to mitigate it.","This paper provides comprehensive studies on several facts about the memorization of adversarial training algorithms (AT). Specifically, the authors uncover two findings: (1) Given a sufficient model capacity, PGD Adversarial Training cannot fit the whole training set (with 100% labels flipped). While, TRADES can fit the whole random flipped set. It may be because the objective function of TRADES has a clean loss, which can improve the stability of training. (2) Memorization of one-hot labels in AT methods can be one important factor to the robust overfitting issues. Based on these findings, the authors propose a method Temporal Ensembling (TE) approach to avoid fitting all adversarial examples with one-hot labels. ","This paper presents the memorization effect/behavior in adversarial training in perspective of model capacity, convergence and generalization. They first investigate the difference of memorization behavior between two adversarial training methods, PGD-AT and TRADES, with random labels and provide the proof of gradient instability for PGD-AT in terms of Lipschitz constant. Also they suggest that previous complexity measures are inadequate to explain robust generalization. Lastly, they investigate the cause of robust overfitting in AT and propose a mitigation algorithm by a regularization term for avoiding the excessive memorization of adversarial examples.","This paper presents a thorough investigation of the dynamics of adversarial training (AT) when a network is trained on a dataset with random labels. This methodology was very successful at identifying theoretical gaps in our understanding of neural networks for standard training, so here the authors propose to replicate those experiments in the context of adversarial robustness. Surprisingly, the authors find that PGD-AT is incapable of fitting random labels when trained from scratch, while TRADES or a warmed-start version of PGD-AT can. They also suggest plausible explanations for this behavior. Finally, the authors use their novel findings to explore the reasons behind robust overfitting, and propose a new method to prevent it. ",0.2361111111111111,0.2777777777777778,0.25,0.20535714285714285,0.23214285714285715,0.2903225806451613,0.15178571428571427,0.21505376344086022,0.1565217391304348,0.24731182795698925,0.22608695652173913,0.23478260869565218,0.1847826086956522,0.24242424242424243,0.19251336898395724,0.22439024390243903,0.22907488986784144,0.2596153846153846
935,SP:d14d3d8dad95993281291491657fdb7b3371fece,"This paper proposes to learn adaptable policies in an offline RL setting. One of the limitations of offline RL methods is decision-making in the out-of-support region. This paper tries to leverage existing meta-learning algorithms to learn a latent code z from trajectories as a context variable. Then, a context-conditioned policy would take in both states and context variables for generating actions. They experimentally show this method outperform previous methods on standard offline RL benchmarks. ","The authors propose a new offline model-based RL method MAPLE. The method relies on training an ensemble of dynamics with the offline data in addition to an environment-context extractor model that maps from dynamics to a latent embedding, which is part of the input to the policy. The authors claim such an environment-context extractor could both eliminate inconsistent dynamics and thus the remaining policy, after the probing stage, is the optimal one w.r.t. the ground truth dynamics during deployment. The experiments show that MAPLE achieves competitive results compared to state-of-the-art offline RL algorithms. ","Conservatism, i.e. avoiding out-of-distribution actions (and states), is a major theme in offline RL because of the need to avoid erroneous extrapolation beyond the data. This paper takes an alternative approach and instead trains a policy that adapts when deployed beyond the data distribution. To do this, a context encoder RNN is trained to produce latent codes given the episode history, and the encoder and policy are jointly optimized to maximize average performance across a large ensemble of pretained dynamics models. The algorithm, offline Model-based Adaptable Policy LEarning (MAPLE), is shown to achieve strong results on the MuJoCo tasks from the D4RL benchmark. ","This paper presents Model-based Adaptable Policy LEearning (MAPLE), a model-based offline RL algorithm that learns a policy that can quickly adapt to an unknown environment during execution. Existing offline RL algorithms constrain the policy to visit only in-support regions, but MAPLE aims to model all possible dynamics models for out-of-support regions and try to learn a dynamic-conditioned (context-conditioned) policy. Policies for various uncertain dynamics models are prepared first, and gradually filter out policies that do not correspond well to transition observations when deployed. However, solving it exactly is intractable, and a practical approximation is presented, which uses an ensemble dynamics model set and an RNN-based environment-context extractor. Experimental results show that MAPLE outperforms baselines in D4RL benchmarks, showing better generalization ability. ",0.20253164556962025,0.17721518987341772,0.26582278481012656,0.2079207920792079,0.19801980198019803,0.18691588785046728,0.15841584158415842,0.1308411214953271,0.16153846153846155,0.19626168224299065,0.15384615384615385,0.15384615384615385,0.17777777777777776,0.15053763440860216,0.20095693779904308,0.2019230769230769,0.17316017316017318,0.16877637130801687
936,SP:d1e8d333ad54d841bd0bbac76f74722998ffb2ce,"This paper presents a Mixed Supervised Object Detection (MSOD) framework that is built on the Fast R-CNN framework and employs Multi-Instance Learning (MIL) to deal with weakly-supervised images. It adds two novel components to the underlying framework: Mask prior and Semantic similarity. Mask prior which is a semantic activation mask (e.g., CAM) works to enhance the ability to generate robust region proposals. Semantic similarity is used to remove inprecise proposals by checking its semantic similarity with other proposals. The proposed framework yields the better accuracy than other WSOD and MSOD methods on VOC 07 dataset.","This paper introduces a mixed supervised object detection method with several improvements (mask generator/prior, semantic similarity transfer) proposed to improve the detection performance in the mixed supervision setting. The ability of detect novel categories is enhanced by the output of the mask generator that is supervised by only image-level labels. A MIL classifier is used to generate pseudo labels (detections) for the novel categories and the quality (average similarity scores) of the pseudo labels are determined by the semantic similarity network trained on only the base categories. Then, the pseudo labels are used in the detector's subsequent training and their losses are weighted by the average similarity scores obtained from the semantic similarity network. This encourages ""good"" pseudo labels to contribute more to the training and vice versa.","The paper proposes a method to do `mixed supervision` object detection, i.e. some part of the dataset is annotated fully while some part only has weak image level annotations.  The first contribution is to use semantic mask prior in object detection network. Prior works have already used semantic masks for weakly supervised detection, but the networks were separate. The paper proposes to integrate the mask prediction networks with the object detection network and use the masks to augment features for prediction candidate bounding boxes. The hope is that this would also generalize and produce better candidate boxes for target/novel categories.  The second contribution is to transfer semantic similarity. The paper incorporates a similarity (metric learning) network in the architectures and learns it using the bounding boxes of the base categories. Then the network is used to estimate the average similarities of pseudo bounding boxes for the novel categories. With the intuition that the average distance of a correct example wrt the rest of the class examples would be lower than that for an incorrect example, they use a weighted loss for object class prediction for the bounding boxes.  The training is done in an iterative manner and results are provided on public benchmarks COCO and ILSVRC datasets as source, and Pascal VOC 2007 as the target dataset.",This paper studies mixed supervised object detection where some object categories are fully supervised and the rest are only labeled with class tags. It introduces a (1) weakly-supervised segmentation module and (2) a semantic similarity learning module to the existing framework to improve the performance. Experimental results on two benchmarks validated the effectiveness of the proposed framework.,0.25252525252525254,0.30303030303030304,0.1919191919191919,0.2900763358778626,0.13740458015267176,0.091324200913242,0.19083969465648856,0.136986301369863,0.3275862068965517,0.1735159817351598,0.3103448275862069,0.3448275862068966,0.21739130434782608,0.18867924528301885,0.24203821656050956,0.21714285714285714,0.1904761904761905,0.1444043321299639
937,SP:d203f496a8be2c04c5dd967e0f6cf26b2622e0c8,"This is a very interesting manuscript and an enjoyable read. Essentially, it extends random walks on graphs which perform probabilistic watersheds to random walks on directed graphs which perform directed probabilistic watershed. This is not a trivial extension and is hence a significant contribution - if correct. The caveat is necessary since it will be difficult for any conference reviewer to find the time needed to verify the result.","This paper proposes an extension of a previous graph-based method for semi-supervised learning. Specifically, the proposed method builds upon the probabilistic watershed algorithm, which is a transductive graph-based algorithm for semi-supervised classification, based on building ""separating forests"" (i.e., in which each tree contains a single labeled node) and defining a Gibbs distribution on this set of forests.  The main contribution of the paper is a generalization of this method to directed graphs, and showing a couple of theoretical results related to the proposed generalization.","The paper proposes a semi-supervised learning algorithm for segmenting directed graphs - obtaining extensions to several previously proposed methods for segmenting undirected graphs. The authors' build on  the 2019 Probabilistic Watershed (ProbWS) paper [12], a graph segmentation algorithm, to construct the directed graph extension. The author's use the Matrix Tree Theorem (MTT), obtaining a Gibbs distribution characterization that they prove is equivalent to the Directed Random Walker (DRW) image segmentation algorithm [15], in analogy to the undirected case studied in [12].  analytical computations of probabilities. They then obtain a directed version of the Power Watershed (PW) graph partitioning algorithm [6]. A numerical study is performed to illustrate and compare the directed ProbWS approach.      ","Suppose we have a weighted directed graph with 2 (or more) nodes designated as ""seeds"" (call them 0 and 1), and we want to assign a ""score"" to each node depending on its proximity to seed 0 relative to seed 1. This is a sub-routine that can be used for some semi-supervised learning tasks: for example, think of the nodes as images of cats and dogs, and the two seeds as one image of each type whose classification is known; the goal is to classify the remaining images.  This paper proposes a natural method for assigning a ""score"" to each node in the above setup. The method is based on sampling a random forest from a certain Gibbs measure. This is proven to be equivalent to the following more intuitive interpretation: imagine starting a random walk at some node q, where the transition probabilities depend in a natural way on the directed edge weights; the ""score"" assigned to node q is the probability that this random walk reaches seed 1 before reaching seed 0. It is shown that these probabilities can be computed in closed form using the directed matrix tree theorem, so there is no need to actually simulate the random walk. All of this generalizes to the case where there are more than 2 seeds, in which case each node gets assigned a probability distribution over the seeds.  In the case of an undirected graph, the results of this paper were already known and called the ""Probabilistic Watershed algorithm"" (NeurIPS 2019). This paper provides the natural generalization to directed graphs.  The paper includes some experiments on three different datasets, including classifying images of digits 0-9. To construct the graph, each image is connected to its 5 nearest neighbors (in some metric), with a weight depending on the distance. A fraction r of the nodes are chosen at random to have their labels revealed (these are the ""seeds""). The experiments show that the new method seems to perform a bit better than the prior work [9], particularly on the digits dataset.",0.20588235294117646,0.20588235294117646,0.3088235294117647,0.2696629213483146,0.3595505617977528,0.2982456140350877,0.15730337078651685,0.12280701754385964,0.06086956521739131,0.21052631578947367,0.0927536231884058,0.09855072463768116,0.17834394904458598,0.15384615384615385,0.10169491525423731,0.23645320197044334,0.14746543778801843,0.14814814814814817
938,SP:d23bd8b8cc0451f5646a44ce0394ed625d4eb9dd,"The authors present MILO an offline imitation learning algorithm that uses a learned model of the environment to generate trajectories from arbitrary policies. This allows to reuse similar algorithms than in the online case such as GAIL or Wasserstein GAIL.  In addition, the authors use a bonus (pessimism) to penalize data that is generated in places with high model uncertainty. This can be achieved using an ensemble. They present experimental results as well as an ablation study on OpenAI Gym.  ","This paper studies imitation learning without access to the environment. The agent is provided with two datasets: one containing expert policy state-action pairs and one containing state-action-next state triplets collected from the environment. The authors propose the algorithm MILO that leverages pessimism to mitigate covariate shift and provide performance upper bound in discrete MDPs, kernelized nonlinear regulators, and Gaussian processes. The authors show improved performance achieved by MILO compared to other baselines such as behavioral cloning in MuJoCo control tasks. ","This work proposes a new method for offline imitation learning. The setup is slightly different than usual as it supposes that, additionally to the expert dataset we want to imitate, we have access to a large dataset of non-expert interactions. The method is the following:   1) learn a model of the transition kernel from the data.  2) Along with the model, learn an “uncertainty penalty” (the converse of an exploration bonus) that will encourage the agent to stay on the expert support.  3) Train *offline* a min-max model-based policy-optimization with the uncertainty penalty.   The contributions also include theoretical analysis on the proposed method in simplified cases.","This work considers an offline imitation learning setting where both expert and sub-optimal demonstrations are assumed to be known. While the covariate shift issue (the gap between training and test distributions) is a critical problem in offline imitation learning, e.g., behavioral cloning, authors address this issue by training a dynamics model through the sub-optimal demonstration (having state-action-successor state triples) and applying pessimistic penalty (incentivizing visiting the state-action pairs in the expert demonstration). With the theoretical derivations for various settings for dynamics modeling and the assumption that the demonstration does not have to cover the entire state-action space, authors show the theoretical guarantee of their proposed method. Also, the empirical results support their claim by showing that MILO (proposed algorithm) outperforms ValueDICE and BC.",0.1625,0.2,0.15,0.18072289156626506,0.26506024096385544,0.21818181818181817,0.1566265060240964,0.14545454545454545,0.09230769230769231,0.13636363636363635,0.16923076923076924,0.18461538461538463,0.15950920245398773,0.16842105263157894,0.1142857142857143,0.155440414507772,0.20657276995305165,0.2
939,SP:d241ad628c57a7f9bd06c735cbe72375119fdbc8,"The paper aims to train a GCN-based multivariate time series classifier by leveraging the pre-processed data (representation per instance) from multiple sources (data owners). The overall model training is decomposed into two steps. First, the client extracts representations using its pre-trained model. Second, the server aligns client-wise representation, and then trains a GCN-based classifier to leverage the structural information across clients.  ","This work is motivated by the problem of federated inference, which arises when clients own different parts of a datum, and learning must be collectively performed on the complete data representation. This application is presented as an extension of vertical federated learning. The proposed approach is based on the training of local models at the clients’ level. From the local models, an opportune data representation is extracted and shared in a centralized setting, which is then used to perform global inference. To cope with the mismatch of data representations across clients, a centralized alignment  procedure is proposed, which provides the input for a graph neural network encoding the relational structure among clients. Experiments are provided for the specific application of modeling real-life time series datasets from data owners of the US power grid, showing that the proposed local-global analysis pipeline outperforms both local and global training. ","The authors proposed a local-global framework to perform “federated inference” in a less addressed scenario where a datum consists of multiple parts, each of which belongs to a separate owner. Different from federated learning, to perform “federated inference”, joint efforts are required only in inference stage. To enhance the proposed framework, the authors propose two alternative alignment methods and a consensus graph learning among the new representations of different clients. In the experiments, both the theoretical analysis and empirical results are presented to support the effectiveness of the proposed framework. The main technical idea is the isolated learning of representation of local data and then aligning them to make the prediction. But this learning paradigm may heavily hurt the privacy of federated learning, since the new representations may leak the local data, and alignment of multiple representations may also lead to inference attack of these data.","This paper introduces a new problem, called federated inference, and proposed a framework of solutions including local representation alignment and learning a consensus graph. The technical contribution mainly comes from a theorem (Theorem 1) on the convergence of learning permutation matrices, and a new approach (icdf) for parameter inference of matrix Bernoulli distribution. The results demonstrate the proposed approaches are useful in improving the prediction accuracy.  ",0.24242424242424243,0.21212121212121213,0.12121212121212122,0.19594594594594594,0.13513513513513514,0.1292517006802721,0.10810810810810811,0.09523809523809523,0.12121212121212122,0.19727891156462585,0.30303030303030304,0.2878787878787879,0.14953271028037385,0.13145539906103285,0.12121212121212122,0.19661016949152543,0.1869158878504673,0.17840375586854462
940,SP:d245cee75ea007498bfba7b6f8b2ba20a195a3c0,"The paper introduces a new way to enforce orthogonality in convolutional layers. The main idea is to make the computations in the frequency domain where convolutions (or cross-correlations) become products. Hence, exact orthogonality is achieved, while keeping the computational costs low using FFT.","The paper proposes a novel normalization technique for controlling the channel-wise Lipschitz of the convolutional layers. The normalization technique operates on the Fourier domain which could reduce the burden of doing full SVD and at the same time be more accurate compared to using approximate methods. The paper also empirically demonstrates the effectiveness of such normalization by showing experimental results on robustness under different sources of noises, easier training, and extended usage in GAN. ","This paper exploits the convolutional structure to maintain orthogonality on the convolutional kernel for guarantee the energy preservation in CNN, rather than the naively used methods using flattened convolutional kernels. It orthogonalizes the channel-wise convolutional kernel via preconditioning its circulant matrix transferred by FFT. The proposed ConvNorm also reduces the layer-wise spectral norm of the weight matrices. Experiments on small network (e.g, VGG 16 and 18-layer Resnet) using CIFAR-10 ImageNet verify the effectiveness of ConvNorm in model robustness and training GANs. ","The paper proposes to orthogonalize the convolutional kernel in a channel-wise manner. To do the computation more efficiently, they calculate the matrix inversion via FFT in the frequency domain. They compare with the baselines on the tasks of adversarial attack, image classification and GAN.",0.22727272727272727,0.22727272727272727,0.25,0.21333333333333335,0.2,0.18604651162790697,0.13333333333333333,0.11627906976744186,0.24444444444444444,0.18604651162790697,0.3333333333333333,0.35555555555555557,0.1680672268907563,0.15384615384615383,0.24719101123595505,0.19875776397515527,0.25,0.2442748091603054
941,SP:d24be5f3b53e8dc839087d4c93ab14bcbc5d7ace,"The paper describes a universal law of robustness via isopemitry, which states that smooth interpolation requires d times more parameters than usual interpolation. The authors then include a discussion of their results for the MNIST and ImageNet datasets, and go through the proof of the main results. The authors conclude with a discussion of their results for deep learning.","This paper shows that if one wants to interpolate the data smoothly, the number of parameters required is $d$ times more than mere interpolation. More precisely, they show that for any function class smoothly parametrized by $p$ parameters, and for any $d$-dimensional dataset satisfying mild regularity conditions, any function in this class that fits the data below the noise level must have its (Euclidean) Lipschitz constant larger than $\sqrt{nd/p}$.   To demonstrate this, the authors show that for random labels, if the distribution satisfies isoperimetry then either the 0 level or 1 level set has probability upper bounded by $\exp(-d/C^2)$ where $C$ is the Lipschitz constant of $f$. A union bound then gives an upper bound on the probability of a function fitting the data.  ","It is generally possible to interpolate train data as long as the number of parameters of our model exceeds the number of training points. The present paper asks how many parameters do we need if we want to interpolate in a smooth manner. If the smoothness is measured by Lipschitzness, the paper proves that one needs at least n d parameters to do so, where d is the ambient dimension (Theorem 1). The main assumptions are i) Lipschitz parametrization of the model with bounded parameters, ii) isoperimetric distribution of the train data, iii) positive noise level. This result, in particular, settles a conjecture by Bubeck, Li, and Nagaraj modulo assumption iii). ","This paper provides a lower bound on the *global* Lipschitz constant (with respect to the 2-norm) of any function within a parameterized function class that fits data below the noise level that scales inversely in the number of parameters p, showing that overparameterization is *necessary* to ensure a smooth fit of noisy training data. These lower bounds hold under the following assumptions/caveats:  - the d-dimensional input data satisfies the assumption of c-isoperimetry, which includes the Gaussian measure, strongly log-concave measures and distributions over d-dimensional manifolds with a positive Ricci curvature.  -- the parameters (""weights"") of the function class are polynomially bounded in (n,d).  -- the training data has positive label noise, and the function is constrained to fit below the noise level (which includes the case of interpolation of training data).  -- the smoothness (Lipschitz) constant is evaluated with respect to the 2-norm.  The paper contextualizes these results in the context of deep neural networks and provides two lower bounds on the Lipschitz constant that (in addition to the scaling given in n, d, and p) scale inversely either in the depth or the product of spectral norms of layers. The latter is a well-known upper bound on the Rademacher complexity of deep neural networks. Both results show an additional dependence on the number of parameters p that is required to ensure small Lipschitz constant --- such a dependence is not present in typical Rademacher complexity bounds used to upper bound clean generalization error.",0.2542372881355932,0.2033898305084746,0.3220338983050847,0.17692307692307693,0.3076923076923077,0.24324324324324326,0.11538461538461539,0.10810810810810811,0.07661290322580645,0.2072072072072072,0.16129032258064516,0.10887096774193548,0.15873015873015872,0.1411764705882353,0.12377850162866451,0.1908713692946058,0.21164021164021166,0.15041782729805012
942,SP:d252c0f9201728a356b2eca95fb30d390f7dec0d,"This paper proposes a novel post-training backdoor attack detection algorithm using reverse-engineering defense. The authors design a new statistic called expected transferability (ET) for detection. The advantage of ET is that the detection threshold is a constant (1/2) and is independent of classifier domain and attack algorithms, so the proposed detection algorithm can be applied to a wide range of backdoor attacks on different databases.","This paper proposes a new engineered reverse-engine based backdoor model detection method. It has stronger assumptions including: 1) no access to training data; 2) no access to benign reference model; 3) 2 or multi-class models. To detect backdoors in such scenarios, it leverages a new statistical metric known as the expected transferability, which can derive a fixed threshold for backdoored models detection. Results on six datasets show that the proposed method is effective for detecting backdoored models.","This paper proposed a post-training BA detection approach for two-class classification problems, which can also be extended to scenarios with multiple attacks. The approach is derived based on a proposed notion of 'expected transferability', which works with a principled threshold that is irrespective of the domain nor the network or attack configurations. Empirical experiments verified the effectiveness of their approach.","The paper proposes a class-specific metric called Expected Transferability (ET) that can determine if a classifier is targeted by a backdoor attack (or not). The threat model assumes no access to the model's training data. By the virtue of being class-specific, the metric allows one to examine classifiers regardless of the number of output classes. The author reason theoretical what values of ET indicates the presence (or absence) of backdoor attacks. Then they empirically demonstrate how this value can be estimated and leveraged. ",0.22058823529411764,0.25,0.23529411764705882,0.189873417721519,0.17721518987341772,0.1935483870967742,0.189873417721519,0.27419354838709675,0.18604651162790697,0.24193548387096775,0.16279069767441862,0.13953488372093023,0.20408163265306123,0.26153846153846155,0.2077922077922078,0.2127659574468085,0.1696969696969697,0.16216216216216217
943,SP:d2a98e886903c265c23e40b9956857e72cbf603b," This paper uses a generative model for ""curated"" labeled datasets that was initially developed by Aitchison 2020 for another seemingly unrelated purpose (explaining the ""cold posterior"" effect). The assumed task is classification, where there is a set of mutually exclusive possible class we wish to assign to each image. To obtain labels for a training set given specific images (or other example features), the generative model assumes that each example is assigned a label from S independent, identically-distributed annotators. Only if *all* annotators agree is consensus reached and the class label is provided, otherwise the image is considered ""noconsensus"" and is usually not included. It is suggested that many common SSL datasets (e.g. CIFAR-10) use such a consensus curation process, and that even unlabeled sets are subjected to consensus curation.  Using this model, they provide a principled explanation for the empirical success of several well-known semi-supervised learning (SSL) objectives -- entropy minimization, pseudo-label, and FixMatch -- applicable to discriminative deep learning. The key idea is that each objective can be viewed as a lower bound of the log likelihood of the unlabeled set under the proposed model for curation, assuming that the unlabeled set contains only ""consensus"" images.  In Sec. 4.1, the paper presents a Bayesian SSL analysis on a toy dataset. The key result is that if unlabeled data is generated without multi-annotator curation, then the unlabeled data does not improve classifier accuracy when included in training using SSL. However, if only consensus examples are included in the unlabeled set, then reasonable improvements in accuracy might be expected from SSL vs. labeled-set-only learning.  In Sec. 4.2, the paper analyzes astronomy images with 9 class labels, prepared using the Galaxy Zoo dataset (to my knowledge not a common dataset for SSL). The existence of multiple annotators for this dataset allows creating both curated and uncurated versions. Again, they find that for *curated* datasets (where many annotators agree), both test-log-likelihood and test accuracy improve from the inclusion of unlabeled examples. However, for *uncurated* datasets, including too many unlabeled examples ""dramatically worsened"" test-set performance, though small unlabeled sets help.","This paper shows that low-density separation semi-supervised learning (SSL) objectives can be understood as a lower-bound on a log-probability that arises from a principled generative model of data curation. This gives a theoretical understanding of recent results showing that SSL is more effective when unlabelled data is obtained by throwing away labels from the carefully curated training set, and is less effective when unlabeled data is taken from uncurated images. Experiments on toy data generated from a known model and on real data from Galaxy Zoo confirm the importance of data curation for SSL.","The paper is built on the previous work (Aitchison, 2021) that provides a generative model of data curation. In this model, the likelihood of a labeled data obtained with consensus is given by product of the probability that each labeler labels correctly. Under this model, the paper shows 1) the log likelihood of the consensus is lower bounded by entropy or pseudo labeling, 2) the log likelihood of the consensus is also lower bounded by the FixMatch type average likelihood of the augmented data. In toy experiments and Galaxy Zoo experiments, authors show that the test likelihood improves when unlabelled curated data is added, but decreases on the uncurated data.","The paper analysis semi-supervised methods taking into account the curation process employed in the creation of popular datasets, like ImageNet. The authors model this process via a generative model which they use to show two common objectives in semi-supervised learning, namely entropy minimisation and pseudo-labelling, are actually lower-bounds on the log-likelihood of the data under this generative model. The paper also shows experimentally, that unlabelled data improves the accuracy of classifiers when the dataset has been curated but actually hurts performance when uncurated datasets are used.",0.09192200557103064,0.09749303621169916,0.0947075208913649,0.24489795918367346,0.17346938775510204,0.21818181818181817,0.336734693877551,0.3181818181818182,0.37362637362637363,0.21818181818181817,0.18681318681318682,0.26373626373626374,0.14442013129102846,0.14925373134328357,0.1511111111111111,0.23076923076923075,0.1798941798941799,0.23880597014925373
944,SP:d2c9222cae209f534ea378029345b8cc726c245f,"This paper proposes an efficient end-to-end object detection architecture based on Deformable DETR, called Sparse DETR. Its main contribution is solving the computation budget by selecting only a few tokens as queries in attention computing. Sparse DETR can use only 10%-50% of the original encoder query while achieving comparable results. This is done by several components. Firstly, the authors add a subnet named Scoring Net before encoder to predict saliency scores for input. And only the tokens which have top-k scores will be chosen as queries in the encoder attention computing. Then, to train the Scoring Network, this paper use binarized attention weight from cross attention, named DAM, as pseudo labels. It also adds a detection head to predict the class scores for outputs of the encoder. The scores are used to select the appropriate token as the decoder`s query. The extensive experiments show the effectiveness of the proposed method.","This paper proposes methods to further improve the efficiency of Deformable DETR. It observes that the encoder queries referenced by the decoder account for only 45% of the total, and the detection accuracy does not deteriorate significantly even if only the referenced queries are polished in the encoder block. To this end, this paper proposes Sparse DETR that selectively updates only the queries expected to be referenced by the decoder. An auxiliary detection loss in the encoder also improves the performance. Experiments prove the effectiveness of the method. ","This paper tries to solve the problem of expensive computation on the encoder of Deformable DETR. The hypothesis is the encoder inputs a large number of image feature queries, but only a small number of them are actually referred by the decoder. The paper has shown that with a help with some selection mechanism, the encoder and only inputs partial of the feature queries, such that the computation can be reduced. With some other components, e.g. “top-k decoder queries” and “encoder auxiliary loss”, the Sparse DETR can achieve slightly better performances at fewer computations.","A modification to the recently proposed Transformer-based object detector, DETR, is proposed. The authors are motivated by low performance of DETR on small objects, and high computational cost of self-attention on large feature maps. To improve both, they propose to sparsify input feature maps by learning a classifier whether to include an input feature or not. Supervision for this classifier comes from Transformer decoder attention weights. The authors apply their approach to Deformable DETR and evaluate on the COCO bounding box detection benchmark, and show that it allows to trade detection performance for computational efficiency by tuning the classifier threshold.",0.2064516129032258,0.15483870967741936,0.12903225806451613,0.2840909090909091,0.18181818181818182,0.15625,0.36363636363636365,0.25,0.19607843137254902,0.2604166666666667,0.1568627450980392,0.14705882352941177,0.26337448559670784,0.19123505976095617,0.1556420233463035,0.2717391304347826,0.16842105263157894,0.15151515151515152
945,SP:d3f095d57a009ca037f52b7b524fd00b3416bf0f,"This paper studies the phase retrieval problem under bandit feedback. Regret bounds for this problem can be obtained from prior work by casting the problem as a low-rank bandit, which yields a $\tilde{O}(d^{3/2} \sqrt{n})$ regret upper bound. While several authors have conjectured that the upper bound for low-rank bandits is tight using information-theoretic heuristics, and these heuristics carry over also to the phase retrieval setting, this paper shows that these heuristics can be quite misleading. Indeed, this paper proves that the optimal regret for this setting is $\Theta(d \sqrt{n})$ (providing matching upper and lower bounds on the regret), which they obtain by ensuring that their algorithm can _increase_ the rate of information accumulated over time (thus circumventing the information-theoretic heuristics of prior works). "," The paper studies the quadratic bandit problem, where the reward at time $t$ is given by $\langle \theta_\star, a_t \rangle^2 + \varepsilon_t$, where $\varepsilon_t$ is i.i.d. gaussian noise, $\theta_\star$ is an unknown $d$-dimensional vector and $a$ is the action. The action set is fixed to be the unit ball, and the learner can choose any action $a_t \in \mathbb R^d$ such that $\lVert a_t \rVert_2 \le 1$. The norm of the unknown vector $\lVert \theta_\star \rVert$ is revealed to the learner at the beginning of the bandit. (This assumption can be removed, as discussed in the final section.) The main contributions are as follows.  1. The paper proves tight up to log factors upper and lower regret bounds $\tilde{\Theta}(d \sqrt n)$ for the quadratic bandit problem (Theorems 1, 2, respectively). 2. The paper extends the regret results to the pure exploration settings (Theorem 3). 3. The paper suggests that optimal regret is not attainable by fixed policies (Theorem 4).  The paper concludes that standard algorithms in bandits literature, including UCB, Thompson sampling, and information-directed sampling, might be insufficient for the quadratic bandit. Furthermore, the paper refutes previous regret lower bound conjecture, suggesting that naive guess based on signal-to-noise (SNR) ratio is wrong. ","This paper studied the bandit phase retrieval problem that can also be framed as a low-rank bandit problem where the actions and the unknown coefficient are rank-1 matrices. Their algorithm achieves a \tilde{O}(d\sqrt{n}) regret that matches their lower bound result up to log factors. By a simple conversion, they prove a \tilde{O}(d/\sqrt{n}) simple regret and show this can only be achieved by adaptive algorithms.","The paper studies a stochastic bandit optimization problem in which the expected reward can be expressed as $\langle a , \theta_\star \rangle^2$, where $\theta_\star$ is an unknown parameter. The authors provides an algorithm that achieve a nearly tight (up to logarithmic factors) cumulative regret bounds. Nearly tight bounds for simple regret have been shown as well.",0.22388059701492538,0.208955223880597,0.11940298507462686,0.10454545454545454,0.1318181818181818,0.21621621621621623,0.13636363636363635,0.3783783783783784,0.27586206896551724,0.3108108108108108,0.5,0.27586206896551724,0.16949152542372883,0.2692307692307692,0.16666666666666663,0.15646258503401358,0.20863309352517984,0.24242424242424243
946,SP:d3fb5cbdbbf8b2e342e8da4ebb34111d5c63939c,The paper discusses the relationship between deep declarative networks(DDNs) and deep equilibrium models(DEMs). The paper starts with a specific set of DDNs with a kernelized generalize linear model and shows that some DEMs are equivalent to the DDNs. The paper discusses the insight from such observation and potential usage of DDN inspired initialization for weights for implicit models.,"The paper discusses a new perspective that establishes the connection between optimization-based layers (i.e., DDNs) and the fixed-point forward computations of the deep equilibrium networks (DEQs). In particular, the paper shows that a kGLM optimization layer, under certain regularity and contractivity assumptions, can be written as an (arguably) simpler form of deep equilibrium model layer. The paper provides solid proof and a thorough discussion of the implication of this connection, such as 1) how we might want to parameterize the matrices; and 2) how this may induce a good initialization scheme for (this particular kind of) DEQ models which improves training stability and convergence.","The authors draw a resemblance between two classes of models, namely deep declarative networks (DDNs) and deep equilibrium models (DEQs), under mild assumptions. In particular, they show that solving an empirical risk minimization problem of a kernelized generalized linear model (kGLM) over a RKHS in a DDN admits a fixed point that, when viewed with an outer minimization, resembles the optimization of a simple DEQ with a fully connected implicit layer. They use their result to motivate an initialization scheme for DEQs that matches the solution of a trained kGLM.","This paper describes a particular DEQ formulation that is motivated by a kGLM-based deep declarative network (DDN). The MAP of a kGLM admits a closed form solution and is guaranteed to exist due to the convexity of a kGLM's log probability. The paper further discusses making the solution unique through a Lipschitz constraint. While strongly convexity and Lipschitz conditions for making fixed points unique is known, the main advantage of this kGLM approach seems to be being able to have a closed form expression for the solution, which is useful as an initialization for training DEQs. Experimental results validate that careful initialization can benefit training compared to random initialization, and a kGLM approach is useful in cases where the kernel can include domain knowledge.",0.36666666666666664,0.3333333333333333,0.2833333333333333,0.205607476635514,0.205607476635514,0.2222222222222222,0.205607476635514,0.2222222222222222,0.1349206349206349,0.24444444444444444,0.1746031746031746,0.15873015873015872,0.2634730538922156,0.26666666666666666,0.18279569892473116,0.2233502538071066,0.1888412017167382,0.18518518518518517
947,SP:d3fe2c50bccf3e3a529890c22153258f64610865,"The authors present a goal-conditioned RL method, which performs relabelling given a set of demonstrations. The main claim is that demonstrations will help guide exploration a lot more efficiently. This method is inspired from HER + LfD methods. The agent starts with a database of demonstrations. At training time, the agent samples a goal from this database, which it updates with successfully reached goal-states. In the hindsight relabelling process, the goals are sampled from the trajectory or from the demonstration database. This approach is evaluated on a set bimanual cable insertion tasks, where it outperforms prior approaches such as HER and is able to learn with fewer demonstrations than RL + LfD. ","This paper aims to improve the learning effectiveness and data efficiency for long-horizon control tasks under sparse rewards with demonstrations. Building upon prior works on DPGfD and HER, the authors proposed a new hindsight goal relabeling technique, which is referred to as “task-constrained goal-conditioned RL” to differentiate against “general goal-conditioned RL” as in prior works. Specifically, instead of selecting goals from the learned agent’s rollouts, the new technique only selects goals from the successful rollouts (and demonstrations). Rigorous experiments demonstrated that the proposed algorithm outperforms prior works on a very challenging bidextrous peg insertion task in simulation, in the aspects of the learned agent’s final performance (e.g. task completion rate) and data efficiency (e.g. number of demonstrations).","This paper studies long-horizon manipulation tasks given sparse rewards and a few demonstrations from a hindsight relabeling-based approach. Specifically, it leverages the demonstration states as relevant goals in the goal relabeling process to guide task-specific exploration.  When evaluating the policy, the online goals are sampled from a goal dataset, composed of successful states and final states from the demonstrations. When relabeling the goals for training, the goals are sampled from the set of states visited by the demonstrations. Rather than operating over the raw state observations, this method utilizes an encoder, which can be either engineered by an expert or learned through self-supervision, to extract a latent representation. Then, the rewards are relabeled in hindsight based on a thresholded distance in the latent space.","This paper contributes a method called HinDRL, which tackles the important problem of sparse reward robotic RL tasks when supplied with demonstrations. HinDRL seeks to build off of hindsight relabeling to learn a goal-conditioned policy which is trained in a self-supervised manner. Specifically, by relabeling transitions with task-specific goal candidates from expert demonstrations, the agent will be given more task-relevant feedback. By combining the goal selection strategy with goal-conditioned rewards which are either engineered or learned through temporal time-consistency, HinDRL is demonstrated on simulated sparse-reward robotics tasks where it achieves improved final performance and demo efficiency compared to HER.  ",0.1875,0.19642857142857142,0.20535714285714285,0.224,0.184,0.1796875,0.168,0.171875,0.2169811320754717,0.21875,0.2169811320754717,0.2169811320754717,0.1772151898734177,0.18333333333333335,0.21100917431192662,0.2213438735177866,0.19913419913419914,0.1965811965811966
948,SP:d427bf72ee54ff7f134560c3224b17da9b57ba94,"The paper studies representation learning in a setting where the source tasks and the target task use different prediction function spaces beyond linear functions, and asks the following question: how many source tasks are required to achieve *diversity* (an important quantity related to the generalization ability from source tasks to the target task)?  The paper first shows that diversity over the source tasks' prediction function space implies diversity over the target task's prediction function space. As a result, even when the target task uses multi-layer neural networks as prediction functions, generalization is still guaranteed as long as the source tasks use linear prediction functions. However, when the source tasks use nonlinear prediction functions, there exist some fundamental limits for generalization: the paper proves some hardness results, including a novel lower bound on the number of required source tasks in terms of the eluder dimension. The paper further provides an upper bound in terms of the generalized rank. Furthermore, the paper gives some theoretical results indicating that simpler tasks generalize better. Such insights are validated by numerical experiments.","This paper studies the importance of diversity in representation learning. The novelty over previous work is that this work studies the case where the prediction classes for the source and target can be different and nonlinear.  This paper first shows that diversity over the source prediction class $F_{so}$ implies diversity over the target prediction class $F_{ta}$, both of which could be nonlinear, as long as $F_{ta}$ is more complex than $F_{so}$. On the other hand, if $F_{so} \not\subset F_{ta}$, then 0-bias diversity cannot hold. When diversity does hold, error bounds on the target task are derived in terms of the diversity constants.  The second contribution of the paper is a negative result for nonlinear $F_{so}$: the number of tasks required to achieve $(\nu, 0)$-diversity is lower bounded by $\Omega(2^d)$ for 1-layer NN, or by the eluder dimension for the general case which is known to grow exponentially.  Finally, the theoretical results are verifies empirically on synthetic data. Different than the multi-task setup for theory, the experiments are on a single task with multiple outputs. The results show that diversity can still be achieved in this setting, with the representation learning performance improving with the diversity as desired.","This paper studies a transferred representation learning problem. In the considered setting, there are a set of source tasks and a target task. Each task consists of representation learning and prediction. The assumption is that all tasks share the same best representation function. The core question is: if we run ERM over source tasks, is the learned representation a good representation for the target task, and what is the generalization error for the target task? The authors quantify the performance through the Gaussian complexity of the function classes, and a quantity called ""diversity"", which relates the generalization error between the source tasks and the target task. Then the authors discusses several special cases, including ""source tasks are more complex than the target tasks"", ""source tasks are simply learning representation"", ""source tasks are representation followed by non-linear functions"". The main message is the simpler the source tasks are, the better chance transfer representation learning will succeed. ","The paper studies theory of representation learning. It differs from existing papers by considering scenarios when (a) the source and the target function class are different, and (b) non-linear prediction functions are used. There are three main results. The first result provides an upper bound for the generalization error of the target task if the source tasks are diverse. The second result shows the hardness of being diverse if the source tasks use nonlinear prediction functions, by relating diversity and eluder dimension. And the third result connects diversity and generalized rank.",0.2737430167597765,0.24022346368715083,0.21787709497206703,0.1848341232227488,0.14218009478672985,0.1858974358974359,0.23222748815165878,0.27564102564102566,0.42391304347826086,0.25,0.32608695652173914,0.31521739130434784,0.2512820512820513,0.25671641791044775,0.2878228782287822,0.21253405994550406,0.198019801980198,0.23387096774193547
949,SP:d462785f315f77c3607ab78f4c5d93c06f114822,"The work proposes a UDA pipeline for object detection based on self-training (via pseudo-labeling) and adversarial alignment. To this end, Monte-Carlo dropout inference is used to estimate detection confidence (or uncertainty). Certain detections (with  associated pseud-labels) are used for self-training while  uncertain ones are used to extract tile for alignment.  "," In this paper, authors propose an unsupervised domain adaptation method for object detection based on pseudo-labeling and adversarial training. The main idea is two-fold: 1) Monte-Carlo dropout is used to produce the detection uncertainty information that is used to generate pseudo-labels; and 2) regions surrounding uncertain detections are used for adversarial training against the regions from the source domain surrounding the ground truth bounding boxes. The proposed method is compared with the state-of-the-art single-stage and two-stage object detectors on Cityscapes-to-Foggy Cityscapes, KITTI-to-Cityscapes and Sim10K-to-Cityscapes datasets. Additionally, an ablation study examining the contribution of each of the proposed modules to the increased detection precision is presented.  ",This paper measures predictive uncertainty on class assignments and the bounding box predictions. The uncertainty is further used to guide the self-training and adversarial learning for domain adaptive object detection. The experiments cover various domain shift scenarios and prove the approach improves upon existing state-of-the-art methods. ,"Authors address the issue of domain adaptation for object detection, to detect objects in images with same label space but drawn from different data distributions. To this end, the paper proposes to mine uncertain tiles including object-like regions for adversarial feature alignment, and mine certain pseudo-labels for self training. Authors argue that the uncertainty guided training strategy can lead to better calibration under domain shift and improve the model's generatlization. Experiments are carried out on street driving datasets and demonstrate the effectiveness of the proposed method.",0.41818181818181815,0.14545454545454545,0.2545454545454545,0.15833333333333333,0.18333333333333332,0.26,0.19166666666666668,0.16,0.15730337078651685,0.38,0.24719101123595505,0.14606741573033707,0.26285714285714284,0.1523809523809524,0.19444444444444445,0.2235294117647059,0.2105263157894737,0.18705035971223022
950,SP:d4871e7b6a83391ebca2fbe00f5646ee7c773a86,"A model is proposed to perform 2d human pose estimation from a monocular image. To train, the model requires 2 images from the same video of the same person: one supervised with a 2d pose and one without supervision. The system simultaneously learns self-supervised keypoints and supervised keypoints, as well as a transformation between them using a transformer network. During inference, this design allows fine-tuning of some parts of the network with additional non-annotated images from the video. The results show that this self-supervised personalization scheme brings improvements over the only-supervised baseline.","This paper proposes a 2D human pose estimation learning framework that uses self-supervision for test-time personalization. The model is first trained with labeled data with both a self-supervised and a supervised task, and then is fine-tuned only with the self-supervised task at test time with unlabeled personalized data. The paper shows consistent improvement over the fully supervised baseline on three datasets under three scenarios (without, with online, and with offline test-time personalization).","This paper proposes to train personalized human pose estimators for different people by combining ideas from test-time training [57] with self-supervised keypoint estimation [26]. There is a self-supervised component during training using an image reconstruction task based on [26], and additionally, a transformer is trained in a supervised fashion to map the self-supervised keypoints to supervised keypoints (illustrated in Figure 2). There is a decent set of experiments performed on common human pose datasets - Human3.6M, Penn Action, and BBC Pose. Results demonstrate that the proposed method that is tuned for each subject is able to outperform baselines on pose estimation metrics.","Paper proposes to personalize a human pose estimator during test time without any supervision. This is achieved through training model with self-supervised and supervised keypoint detection objectives jointly. During test time, the model is finetuned using the self-supervised objective with test instances. Doing so ensures personalized keypoint estimations. Results on video 2D pose datasets supports that personalization is helpful. ",0.2268041237113402,0.18556701030927836,0.15463917525773196,0.2948717948717949,0.28205128205128205,0.20754716981132076,0.28205128205128205,0.16981132075471697,0.2459016393442623,0.2169811320754717,0.36065573770491804,0.36065573770491804,0.25142857142857145,0.17733990147783252,0.189873417721519,0.25,0.31654676258992803,0.2634730538922156
951,SP:d4cc38da893b17cc3febc2a98b3e5728ae140e2a,"This work introduces a GAN compression method that exploits the role of the discriminator. The main contributions are two-fold: (i) maintain the capacity balance between the compressed generator and discriminator by learning to selectively activate the convolutional channels of the discriminator, (ii) promote the performance of the compressed generator by distilling knowledge from the teacher generator and discriminator to the student models in the feature space. The method is validated on synthesis, translation, and super-resolution tasks. ","In this paper, the authors revisit the role of discriminator in GAN compression and design a new generator-discriminator cooperative scheme GCC for GAN compression. Specifically, they use a selective activation discriminator to automatically select the convolutional channels, aiming to help maintain the Nash equilibrium and avoid mode collapse. Besides, they also propose an online collaborative distillation method to further boost the compression performance. They achieve impressive performance on three widely-used GAN tasks--image generation, image-to-image translation, and super-resolution.","This work revisits the role of discriminator in GAN compression and finds that the capacity mismatch when compressing the generator with a fixed discriminator will lead to model collapse. To this end, this work proposes a generator-discriminator co-operative compression framework to automatically adapt the size of discriminator to maintain the Nash equilibrium with the lightweight generator. The extensive evaluation results validate the superiority of the proposed framework over SOTA GAN compression methods.","In this paper, the authors revisit the role of discriminator in GAN compression and propose a selective activation discriminator to automatically activate the convolutional channels. An online collaborative distillation is adopted to guide the optimization process of the lightweight generator. Experiment results demonstrate the effectiveness of the proposed method.",0.2564102564102564,0.2692307692307692,0.24358974358974358,0.27710843373493976,0.3493975903614458,0.32432432432432434,0.24096385542168675,0.28378378378378377,0.3877551020408163,0.3108108108108108,0.5918367346938775,0.4897959183673469,0.2484472049689441,0.27631578947368424,0.2992125984251968,0.29299363057324834,0.43939393939393945,0.39024390243902435
952,SP:d528a2625df56eb253e1c16a0b6e3250a4407751,"Explicit and implicit model training have their own advantages and limitations. The author proposes a joint training framework, which aims to combine the benefits from both worlds. Specifically, the author proposes a training framework called **Stein bridging** that allow joint learning of explicit and implicit models.  Particularly, the explicit model considered in this paper is the energy-based model (EBM) and the implicit model is the GAN-like generator. The implicit model is trained based on GAN framework, the explicit model is learned via Stein discrepancy, and the bridge between them is also the Stein discrepancy between the generated samples and the explicit model.   Theoretically, the author proves that the Stein bridging can (1) act as a dual Sobolev norm regularization for the discriminator that helps the GAN training; (2) act as a Moreau-Yoshida regularization for the Stein discrepancy to smooth the energy function; (3) also help to stabilize the training dynamics.   Empirically, the author conducts extensive experiments to confirm the effectiveness of the proposed bridging, in terms of generating quality and OOD detection. ",The authors propose to jointly train implicit (like GANs) and explicit (EBMs) density models by proposing a new regularization technique that connects the training of the impicit and explicit model via Stein bridges that is based on stein discrepancy. The authors show that such a learning objective with a Stein bridge helps to overcome the drawbacks encountered while training an implicit or explicit model individually. The authors provide sufficient theoretical details in support of the method under the Wasserstein distance and perform experiments on toy examples and image datasets.,"===== After Rebuttal =====  I have bumped up my score for the enhanced empirical results author(s) have provided in the discussion round. I remain conservative in my final recommendation because I am not fully convinced the gains are actually from the Stein energy penalty (i.e., some other regularization may also offer similar results) and the cost in modeling complexity may not be justified by the gains.   =====================  This paper proposes to improve generative modeling via joining the strength of unnormalized density estimation (based on Stein discrepancy) and Wasserstein GAN. Numerical experiments verify the proposed solution enjoys better convergence on both synthetic and real-world datasets. Some theories are developed to justify the use of training loss. ","This work proposes to train two models jointly, one explicit density estimator, and one implicit generative model. The idea is that the explicit (energy) model specifies a valid and easy-to-evaluate likelihood function, while the implicit model is flexible enough to draw high-quality samples.  Because the explicit model is an energy model, we don't have the normalizing constant, meaning that we cannot evaluate how close the explicit model is to the implicit model. The authors propose to minimize the stein discrepancy between the implicit model and the energy model, and the Wasserstein distance between the implicit model and the data.   The contributions are proposed as follows:  * A joint learning framework from training an explicit energy model and an implicit model to learn to sample from a target distribution where we only have access to samples.  * A training algorithm that trains the implicit model to sample via minimizing the Wasserstein distance then trains the energy model to learn an explicit density by minimizing the stein discrepancy between the energy model and the implicit model. ",0.15428571428571428,0.13142857142857142,0.26285714285714284,0.15730337078651685,0.2808988764044944,0.16521739130434782,0.30337078651685395,0.2,0.26136363636363635,0.12173913043478261,0.14204545454545456,0.10795454545454546,0.20454545454545453,0.15862068965517243,0.2621082621082621,0.13725490196078433,0.18867924528301885,0.13058419243986252
953,SP:d5a81edf7d4cc7c6a1498a2e3298d603c94dcdb7,"This paper proposes a novel method for comparing reward functions without policy optimization, the Dynamics-Aware Reward Distance. DARD improves on the state-of-the-art EPIC distance by using an approximate transition model to evaluate reward functions on transitions close to the training distribution, while EPIC evaluates on arbitrary transitions (which can be infeasible). The authors prove that DARD is also invariant to reward shaping, and experimentally demonstrate on the Bouncing Balls and Reacher environments that DARD is a better predictor of policy return than EPIC and works for coverage distributions induced by either random or expert actions. ","This paper considers a reward comparison method in RL, where the goal is to evaluate how close an alternative reward function is to the ground truth reward function in terms of how well the set of optimal policies for each reward function is aligned. The proposed method (DARD) builds upon EPIC [Gleave et al.] and addresses an issue of EPIC by taking into account the actual dynamics of the environment when canonicalizing/transforming reward function as opposed to considering all possible (s, a, r') triples in EPIC. This allows DARD to be more robust to out-of-distribution transitions compared to EPIC while still being invariant to reward shaping like EPIC. The empirical results on Bouncing Balls and Reacher environments show that DARD can represent the reward distance more reliably EPIC does.","The paper proposes a new reward pseudometric called Dynamics Aware Reward Distance (DARD) which uses approximate transition model of the environment to compare reward functions while being indifferent to reward shaping. Previous work Equivalent Policy Invariant Comparison (EPIC) addresses the same problem, but it uses all possible state transitions feasible or not. The authors show that this can cause problems for reward functions that are technically equivalent in the feasible state transition state, but unequal on the infeasible transitions (which should not matter). The authors evaluate the method in ""bouncing balls"" (a discrete navigation task) and ""reacher""  (robotic manipulation) environments. The results show that the proposed distance is more accurate compared to EPIC (distance is zero for ""FEASIBILITY"", 2 reward functions that are technically different but practically the same) and the distances are more aligned with the learned policies scores.","The paper proposes a new reward pseudometric (DARD), which is invariant to reward shaping and computationally efficient. Assuming access to the transition model of the environment, DARD could avoid the error due to samples out of the training distribution. The numerical results in simulated physical domains show that DARD outperforms previous methods. ",0.29292929292929293,0.3333333333333333,0.15151515151515152,0.21212121212121213,0.1590909090909091,0.15,0.2196969696969697,0.2357142857142857,0.28846153846153844,0.2,0.40384615384615385,0.40384615384615385,0.2510822510822511,0.27615062761506276,0.19867549668874174,0.2058823529411765,0.22826086956521738,0.21875
954,SP:d5f94c0510ad71cf646676a40b84c017fdf7e9bc,"In this work, the authors propose an iterative co-pruning framework using a lottery ticket learning for GNNs.   Graph lottery ticket learning implies that a sparse subnetwork exists in a dense and randomly initialized network for both GNNs and graph data.  UGS was proposed by Chen et al. to apply the lottery ticket learning to GNNs.  The basic concept is quite similar but it is distinguished from Chen et al. in that it can be applied for the inductive setting.  In Inductive Co-Pruning of GNNs (ICPG), edges of input graphs and model parameters are pruned according to the importance scores for each.  The authors exploit generative probabilistic models named AutoMasker to specify the importance of each edge and then configure core subgraphs.  Compared to the previous Graph Lottery Ticket (GLT) model, ICPG has a nice generalization ability for both node classification and graph classification.","The authors introduce ICPG, a novel method for pruning in graph and node classification tasks. The technique achieves good accuracy and is able to generate sparse graphs. Further, unlike previous work, the method is able to generalize to unseen graphs.","This paper is on generalizing UGS, (Chen et al., 2021) from transductive setting to inductive setting. The main idea is to learn the importance of each edge using the MLP over the learnt embedding from GNN, and pruning them through sorting the importance score. And weight pruning is done just by sorting the magnitude of weights in GCN.     ","This paper proposes a method to perform pruning of graph neural networks. The novelty of the paper is that the method works for inductive graphs, and thus can generalize to unseen graphs. By virtue of the masks being inductive, they also have the potential to take the global graph into consideration and not apply locally on each edge.",0.08275862068965517,0.12413793103448276,0.12413793103448276,0.15,0.3,0.1896551724137931,0.3,0.3103448275862069,0.3103448275862069,0.10344827586206896,0.20689655172413793,0.1896551724137931,0.12972972972972974,0.17733990147783252,0.17733990147783252,0.12244897959183673,0.24489795918367346,0.1896551724137931
955,SP:d61b3918d8777855d79533cffa6b10be3b447a03,"This paper analyses two client sampling strategies for federated learning algorithms. This work compares Multinomial distribution and Uniform distribution. The authors provide a decomposition theorem, that gives some insights into the impact of client sampling. Moreover, they provide convergence guarantees for the general non-convex case under some additional assumptions such as Bounded Dissimilarity. An experimental comparison of two sampling schemes is done at the end of the paper. ","The paper studies a framework for studying the impact of client selection strategies for unbiased aggregation at the server for federated learning. The framework separates the impact of client selection, client model drift, global model drift thereby providing new insights regarding the impact of client selection. In particular, the paper proposed a decomposition theorem which quantifies  the dependence on client sampling. Finally, experimental results on Shakespeare back the theory and demonstrate the effectiveness of one sampling choice over another based on the loss function setup in a FL framework.","The author(s) studied the impact of the client sampling for FL. In particular, the author(s) proposed a decomposition theorem and used it to obtain an improved convergence analysis of FedAvg. Numerical simulations are conducted to support the author(s) theoretical analysis. ","This paper considers the problem of Federated Learning (FL) with a random selection of clients in order to minimize $\sum_{i} p_i \mathcal{L}_i(\theta)$, where $\mathcal{L}_i$ is the loss function of client $i$ and $p_i$ is its weight, which is often proportional to the dataset size on client $i$. The authors study the impact of assigning weights $\omega_i(S_t)$ and probabilities $P(i\in S_t)$ to client $i$ when averaging the result of running SGD for $K$ steps locally on each sampled device. The authors first prove a decomposition bound (Theorem 1), which establishes a simple recursion for the distance to the solution. As far as I can see, this result is never used to establish a convergence rate. In addition to Theorem 1, under smoothness, bounded variance, and bounded dissimilarity assumptions, the authors study convergence rates of unbiased client sampling in the nonconvex regime (Theorem 2). As a side contribution, the authors also consider using different stepsizes $\eta_l, \eta_g$ locally and globally.",0.2463768115942029,0.15942028985507245,0.2898550724637681,0.1797752808988764,0.29213483146067415,0.3488372093023256,0.19101123595505617,0.2558139534883721,0.11494252873563218,0.37209302325581395,0.14942528735632185,0.08620689655172414,0.21518987341772153,0.1964285714285714,0.1646090534979424,0.2424242424242424,0.19771863117870725,0.1382488479262673
956,SP:d64cafba4b42667dbe78cd024bccce9097ff3e0f,"This paper proposed to learn to predict the 3D human pose for real-world images via adapting the model from the source data (label constraints) to unlabeled data. The data used for training contains: (1) the labeled source domain data (e.g. human3.6m or surreal); (2) real human video sequence without pose annotations; and (3) motion capture data (3D pose without images). (2) and (3) are unpaired. The authors proposed to devise: (a) image-to-latent; (b) pose-to-latent; and (c) latent-to-pose, to learn and predict 3D poses. In particular, (a) and (c) forms the test time predictions for the real-world target domain, while (b) and (c) constitutes an auto-encoder. The core problems in here is that the latent space in (ac) and (bc) might be mismatched. The authors proposed a smart way to make sure they are aligned (see Strength below) thanks to the contrastive loss (for setting up latent space non-local grouping rules) as well as the loss from the source domain data for regularization to make sure the two latent domains align with each other. ","This work regards monocular 3D human pose estimation as a domain adaptation problem to solve the problem of insufficient outdoor training data. The framework is composed of an image-to-latent encoder, which is pre-trained on the source domain and adapted to the target domain using unlabeled videos, and a latent-to-pose decoder, which is trained on unpaired 3D poses. During the encoder adaptation, non-local relations are utilized to improve the performance, which is the main contribution. ",## Summary  Paper proposes a self-adaptation framework to align the unpaired samples of 3D pose and video frames to solve 3D human pose estimation from in-the-wild images. This cross-modal alignment is realized through a relation distillation scheme. The proposed approach achieves compelling results on 3D human pose datasets among self-supervised and unsupervised methods.,"The paper focuses on 3D human pose estimation in a scenario of domain adaptation. To avoid dataset-bias, the authors address the problem in a more restrict setting: any form of paired supervision or auxiliary cues should not be used.   The authors first use AAE modules to learn pose and motion latent spaces. Secondly, the authors train a pose encoder which tries to map the input image to the learned pose latent space. Finally, given the target-domain videos, the authors train the pose encoder together with the decoder of the pose AAE. The model is optimized by minimizing relational energies. ",0.13513513513513514,0.07027027027027027,0.14594594594594595,0.1375,0.3375,0.22807017543859648,0.3125,0.22807017543859648,0.26732673267326734,0.19298245614035087,0.26732673267326734,0.12871287128712872,0.18867924528301885,0.10743801652892562,0.18881118881118883,0.16058394160583941,0.2983425414364641,0.16455696202531644
957,SP:d69461c3fd2bfb86266a11a52161be20c7b6ff3a,"The authors theoretically analyzed the dual of minimum norm optimization of over-parameterized deep neural networks. Specifically, they focused on deep neural networks with linear and ReLU activation functions, as well as parallel architectures where the output of each branch is a scalar. First, the authors considered the linear NNs and showed that the duality gap of general deep NN is non-zero, but for parallel NN with enough number of parallel branches, that would be zero. Next, they considered neural networks with ReLU activation functions and derived the duality gap for standard 3 layers neural networks. However, for an arbitrary deep parallel NN with ReLU activation, the duality gap would be zero if the L-th power of Frobenius norms of the weights is used as the regularizer.","In this paper, the author studies the duality gap in the neural network training problem. They show that in general (deep linear networks and three-layer ReLU networks), the duality gap is not zero. However, when restricting the parallel architecture, the authors show that the duality gap is indeed zero. ","The topic of this paper is the duality theorem for deep neural networks with linear and ReLU activation functions. In particular, this study shows that the strong duality (i.e., zero duality gap) does not hold for the standard deep models but it holds for parallel deep neural networks which are ensembles of networks with an appropriate regularization. These results are certainly useful because it enables global optimization of parallel deep models via the convex dual programs. ","Using duality theory, this paper studies the duality gap between prime and dual optimization problems for deep linear (or relu) networks that have sequential or parallel structure. The major claim is that, assuming the networks is sufficiently over-parameterized, linear networks of depths at least 3 have nonzero duality gap, while strong duality holds for deep parallel networks. Similar conclusions are given for relu networks.",0.14728682170542637,0.1937984496124031,0.14728682170542637,0.3,0.32,0.23376623376623376,0.38,0.3246753246753247,0.2923076923076923,0.19480519480519481,0.24615384615384617,0.27692307692307694,0.2122905027932961,0.24271844660194172,0.1958762886597938,0.23622047244094488,0.27826086956521734,0.2535211267605634
958,SP:d6b805bbc01bf14835dcf6b4172ac4a2ad3ec2ed,"This paper presents algorithms for fair division of items to agents. Each item is a job that needs to be processed within a given interval for its utility to be extracted by the agent processing it. This adds a layer of complexity since given any set of items, it is NP-hard to find the subset of feasible items that maximizes utility for the agent (utilities are additive but heterogeneous across agents). Two notions of fairness are considered. The first part of the paper focuses on max-min fairness. The main results are to improve the bounds for (a) the best existential max-min fair allocation, and (b) the best approximation to max-min fairness that can be obtained algorithmically in polynomial time. The difference between the two is primarily because of the NP-hardness stated above of finding the optimal subset of feasible jobs in a given set of jobs. The second paper of the paper focuses on variants of envy freeness. Here, the goal is to obtain envy-free solutions that are also Pareto optimal. But, this is impossible, and so are various extensions such as relaxing to EF1. Nevertheless, some positive results are obtained by relaxing EF1 an approximate notion, or relax Pareto optimality to individual optimality and focus on rigid jobs, etc.","Problem considered is the following: Given a m intelligent agents or machines and a set of Jobs (with release times and deadlines, processing time requirement and a utility) the goal is to allocate a maximum utility subset of jobs feasibly under some fairness criteria. They consider two fairness criteria namely: Minimax fairness and envy freeness.   The paper obtains novel approximation algorithms for various problems arising in this setting, improving upon the existing best results for some of them. ","The paper addresses the following job scheduling problem. There are n jobs and m processors. Job i “arrives” at time r_i, has a processing time p_i, and needs to be executed non-preemptively by time d_i (deadline). It is scheduled on a processor for execution, which can be working on one job at a time and derives utility u_i from execution of a job i. Each processor accumulates the sum of the utilities of the individual jobs it processes.  Two core solution concepts are used for allocating/scheduling the jobs onto processors. 1) maximizing the minimum utility accumulated by any processor, and 2) having the accumulated utility of each processor be no less than that of any other. The latter is matched with a Pareto optimality concept. Variants of those core concepts are investigated. ","The paper considers the scheduling problem of assigning jobs, each with a release time a deadline and a duration, to heterogeneous machines with the objective of *fairness*. More specifically, each machine has a utility for each job and its total utility is defined as the sum of utilities of jobs that were assigned to it and can be feasibly scheduled. With respect to fairness two concepts are considered:  - Maximin Share (MMS): the MMS of a machine can be thought of as the value it would obtain in the worst case by partitioning the jobs into bundles and was the last machine to select a bundle. If every machine obtains a utility that is no less than an $\alpha$-fraction of its MMS.  With respect to MMS the paper proves the existence of an $1/3$-approximate MMS schedule and shows that a $(0.24-\epsilon)$ one can be constructed in polynomial time. These improve upon the best known $1/5$-approximate MMS schedule due Ghodsi et al.  -  Envy freeness up to one item (EF1) + Pareto optimal (PO): EF1 means that although envy between different machines may exist, it disappears after removal of one item, and an allocation is PO if there does not exist another allocation that makes nobody worse off but somebody strictly better off.  The authors show that no algorithm can return a schedule satisfying EF1 and PO simultaneously for all input instances. When all durations are unit and the valuations are identical then such a schedule can be computed in polynomial time. It is also shown, that the schedule maximising Nash social welfare is PO and simultaneously 1/4-approximate EF1 (and 1/2-approximate EF1 if all jobs have unit processing times.)",0.08333333333333333,0.12962962962962962,0.20833333333333334,0.21794871794871795,0.3076923076923077,0.2898550724637681,0.23076923076923078,0.2028985507246377,0.15734265734265734,0.12318840579710146,0.08391608391608392,0.13986013986013987,0.12244897959183675,0.1581920903954802,0.17928286852589642,0.1574074074074074,0.13186813186813187,0.18867924528301888
959,SP:d6b8a3d2b1336b22f07c9ddd1c61380da516eda9,"In the setting where a pretrained value function is used in combination with MCTS, the authors show that because of the distribution shift problem, using the pretrained value function as the evaluation function in MCTS results in a performance worse than directly using the value function. Then, a correction term that in the end is based on the bellman error is proposed to resolve this issue. Experiments show that this correction term is effective.   The second part of this paper proposes a more GPU compatible search method that seems to scale well when a GPU simulator is available. ","This work investigates the problem of improving pretrained agents on discrete RL problems by using tree search. Using a ground-truth environment model, it finds that naively implemented tree search sometimes results in worse performance than no tree search at all. The authors provide analysis of this phenomenon in a simplified setting which shows that performing search on noisy Q estimates, especially with greater noise for states outside the span of the policy, results in taking incorrect actions. They introduce a correction term for tree search and show that it provides superior performance. The paper also includes a discussion of using batching to make tree search faster on accelerators, including using it to train agents.","This paper is about combinations of Tree Search (TS) and trained value functions, where the value functions may already have been trained without involving any TS during training, in RL problems such as Atari games. Claimed contributions:  1) Identification and analysis of a distribution-shift phenomenon, where, if a value function was trained standalone (without TS), adding TS on top of it at evaluation time can actually reduce an agent's performance (relative to, say, a plain agent that is greedy w.r.t. the trained value function). The basic intuition for why this happens is as follows: during training, states that are more likely to be visited under the greedy policy (w.r.t. value function) show up more frequently in collected experience, and hence also get a more accurately-trained value function. A tree search (especially an exhaustive one) will run the value function significantly more often to also evaluate states that the greedy policy would rarely/never reach, and in these states the value function will have larger errors (which may be overestimating or underestimating errors). Because there are usually more paths through the tree that the greedy policy would not visit than there are paths that the greedy policy would visit, the number of branches with large errors are likely to outnumber the branches with smaller errors. This outnumbering leads to a maximisation bias, where, when there are multiple such branches with large errors, likely at least one of them will be overestimating, causing the tree search to prioritise that branch over one with a more accurately-trained value function.  2) A correction term that allows a new ""BCTS"" tree search algorithm to function well with a pre-trained value function.  3) An approach named Batch-BFS to efficiently, on a GPU, run a breadth-first-style tree search with trained value function (assumes, I think, that the forward model of the environment, like an Atari game, itself can also be implemented to run on the GPU).  The resulting Batch-BFS approach outperforms DQN and Rainbow on several Atari games.",The authors introduce a new tree search technique that can be used to improve the score of trained RL agents online. They propose an analysis of the counter-intuitive phenomenon that applying standard MCTS with a pre-trained value function is less performant than using directly the value function in spite having access to more information.   The first contribution is an analysis of TS and the introduction of an off-policy correction term based on the analysis. The second contribution is a GPU adaptation of TS making it fast at inference time but also possible to train tree based agents in a scalable way.  The paper is well written and addresses an interesting problem and the proposed method could be used in many RL algorithms and applications. The GPU implementation seems somewhat disconnected to the rest of the paper.  ,0.20408163265306123,0.32653061224489793,0.2653061224489796,0.24347826086956523,0.2,0.10174418604651163,0.17391304347826086,0.09302325581395349,0.18705035971223022,0.08139534883720931,0.16546762589928057,0.2517985611510791,0.18779342723004694,0.14479638009049772,0.21940928270042195,0.12200435729847496,0.1811023622047244,0.14492753623188404
960,SP:d73fe5f1308fe7ffcb38abcd98cac251c5ea7c06,"This paper proposed a transformer-based architecture for online action detection. The architecture contains a long-term memory that stores information from the past 8 minutes and a short-term memory that processes the recent 8 seconds. Some techniques are used to reduce the complexity of transformer operations. The proposed method gets good results on THUMOS’14, TVSeries and HACS datasets. ","This work proposes a two-stage processing of video frames by Transformer decoders for video classification.  In the first stage a set of learnt queries “decode” a sequence of fixed-number of earlier video frames. Then in the second stage, features from recent frames act as queries and further “decode” the vectors output in the first stage. At the end of the second stage, final classification is carried out.   The main contribution is this two-stage design of the video classification model. The fixed-length “long-term” context of video frames is shown to be useful for “online” frame classification (i.e., w/o access to the future frames).  This work presents a set of experiments for different design choices, which is also informative.  The key experiments are online video classification on — THUMOS’14, TVSeries, and HACS datasets, where the proposed model outperforms the baselines reported in this work. ","The paper addresses an interesting and practical problem in videos – online detection of actions by examining only frames available till the present time and without any access to future frames. The problem is not only challenging but has practical significance too as online action detection is important in autonomous driving or surveillance scenarios and handling this with fast and light memory-weight training and inference enables one to address the problem real-time. The solution approach is based on the intuition that recent frames are more instrumental about the ongoing action while frames from extended past offer potential context to the present action. While differential exploitation of recent and extended past can definitely help (compared to using frames only from the recent past), it comes with extra computation and memory overhead. Based on the recently popular transformer philosophy, the authors came up with an encoder-decoder architecture with short and long term memories for recent and extended memories respectively. While related works have explored long and short term memories, the integration of the two outputs from them is not explicit and rather ad-hoc (pooling, concatenation etc.). The proposed work, on the other hand, made intelligent use of transformers with a two-stage memory compression design based on transformer decoders. Such a design is shown to be not only effective but also both memory and compute efficient. The experiments are well-designed. Extensive comparison with SOTA approaches and well-thought ablations show its superiority and robustness of design choices. The writing is crisp and adequate with insightful experimental analysis. The figures are also self-explanatory with descriptive captions. The related works are also covered good.","This paper presents a novel long video model named long short term transformer, with online action detection as the study case. The proposed transformer uses the encoder to model long-term historical information and uses the decoder to model the short-term information i.e. a few frames right before the current time. Experiments have been done on three standard benchmarks (THUMOS, TV, HACS) and the performance gains are clear compared to SOTA. Detailed analysis has been conducted as well.",0.32786885245901637,0.3114754098360656,0.3114754098360656,0.21476510067114093,0.14093959731543623,0.07636363636363637,0.1342281879194631,0.06909090909090909,0.2375,0.11636363636363636,0.2625,0.2625,0.1904761904761905,0.11309523809523811,0.2695035460992908,0.1509433962264151,0.18340611353711792,0.11830985915492959
961,SP:d74f66d3a2b18605c3defe56ae5073b2d2c7525d,"This paper proposes a Bayesian hierarchical modelling framework to learn shared information across multiple multi-armed bandits. It extends both meta MAB that doesn't uses task specific metadata and linear bandit models that does not assume inter-task heterogeneity. It defines a new regret, called multi-task regret and provides theoretical analysis on the regret bound compared to other methods. Simulated experiments confirm the superior performance against other baselines when the model is well specified. It also shows robustness when under some model specification. Unfortunately, there are no real data experiments to show the performance in practice.","The paper considers the multi-task bandit setting with a contextual information related to the task. The setting is defined in a fully Bayesian way, pathing the way to a Thompson Sampling (TS) bandit algorithm to handle this setting. A worst-case regret bound with respect to the parameters of the task-generator is proven and experiments on synthetic data show that the proposed model has smaller regret than baselines independently of the entropy of the task with respect to the contextual information. "," The paper studies the multi-task multi-armed bandits, where tasks are played sequentially and each task is governed by some unknown hidden parameter (metadata) which follows certain prior distribution.  1. The paper comes up with a Bayesian framework for the metadata-based multi-task multi-armed bandits.  2. The paper designs Thompson sampling-type algorithm, called MTTS, and demonstrates its supremacy over other algorithms, by proving regret bounds for the common Bernoulli / Gaussian prior cases.  3. The paper shows detailed experimental results to corroborate the effectiveness of the algorithm MTTS.",Authors provide a new solution of the problem of transfer learning across different bandit tasks. Specifically they assume policy to leverage external task-features describing task similarities. They focus on Bernoulli and Gaussian bandits as learning instances. The theoretical findings have been tested on synthetic data.,0.16326530612244897,0.16326530612244897,0.10204081632653061,0.2289156626506024,0.10843373493975904,0.07692307692307693,0.1927710843373494,0.17582417582417584,0.21739130434782608,0.2087912087912088,0.1956521739130435,0.15217391304347827,0.17679558011049723,0.16931216931216933,0.1388888888888889,0.21839080459770116,0.13953488372093026,0.10218978102189781
962,SP:d80b8d36a9b97bd38ff3913d66ed960ca585bb09,"The submission investigates the problem of multi-task reinforcement learning, which is where a collection of agents simultaneous learn similar tasks, ideally while exploiting the similarities in the task structure. The submission (i) proposes a formalism for this setting and (ii) designs and analyzes a model-based algorithm (Multi-task Euler) for the aforementioned formalism. The main engine Multi-task Euler is the exploitation of ""subpar state-action pairs"" --- i.e., state-action pairs which are suboptimal across all tasks. When there are many subpar state-action pairs, Multi-task Euler strictly outperforms Euler without information sharing. In any case, Multi-task Euler is never outperformed by Euler without information sharing.","This paper is about multi-task learning in episodic tabular MDPs. M learner's interact with M tabular MDPs with a shared state/action space but (potentially) different transition and reward functions. The authors assume that the mean rewards are epsilon close over all state/action pairs and that the transition kernels are epsilon/H close in L1 over all state/action pairs where H is the episode length. The learner's have unlimited communication at the end of every episode.  The main contribution is a new algorithm and analysis showing that the information sharing can help when epsilon is small and the number of state/action pairs with a large V^*(s) - Q*(s,a) gap is large. The basic idea is to maintain empirical estimates of transitions and rewards for each learner individually and also an aggregate of all estimates. Of course the latter is somehow an estimate of some average reward/transitions, so the algorithm adds to the corresponding confidence bounds a term that depends on epsilon. Besides the modified bonus terms, the learner uses essentially an instance of the Euler algorithm for each MDP. That's quite a natural idea, and the resulting bounds make sense.   The authors also provide lower bounds. These are of a minimax-ish flavour and show that there /exist/ MDPs where the upper bounds are tight. I find this to be only quite weak evidence that the upper bounds are expressed using the ""right"" quantities, if such things even exist.","This paper studies a multi-task RL problem, called epsilon-Multi-Player Episodic Reinforcement Learning. This study got inspired by a recent study on multi-task multi-armed bandits. The assumption is that all tasks are defined with the same state and action spaces, and $epsilon$-dissimilar in terms of transition and reward functions. As the first main contribution, the authors define the notion of subpar state-action pairs which captures the amenability of information sharing among tasks. Then, the author propose a model-based algorithm MULTI-TASK-EULER algorithm, which exploits the given dissimilarity parameter as an advantageous information sharing among tasks to achieve better regret upper bounds in comparing to being without knowing $epsilon$. The authors also present the gap-dependent and gap-independent regret lower bounds.  ","This paper studies multi-task reinforcement learning (RL) in tabular episodic MDPs. It studies an online setting, by showing that when the agents are facing similar but different MDPs, their collective performance, in terms of both gap-dependent and -independent regrets, can be improved through information sharing. Almost matching lower bounds are also provided.",0.18018018018018017,0.23423423423423423,0.0990990990990991,0.14859437751004015,0.07630522088353414,0.14728682170542637,0.08032128514056225,0.20155038759689922,0.2037037037037037,0.2868217054263566,0.35185185185185186,0.35185185185185186,0.1111111111111111,0.21666666666666667,0.13333333333333333,0.19576719576719576,0.12541254125412543,0.20765027322404372
963,SP:d821aedc71baf45b7dcfc59a6c429325e4185913,"The paper is about imitation learning, and it particular it considers the task of replicating the style of behaviours. The authors propose a novel framework, called Style Behavioural Cloning that can learn the representation of different behaviour styles from demonstrations by experts. The approach is an extension of behaviour cloning, that tries to derive the behaviour style embedding by optimising the model to predict the action accurately. The paper presents the mathematical formulation of the framework and an experimental analysis of different settings.","The paper proposes a method to learn different behavior styles from expert demonstrations via imitation learning. To achieve this, the method learns a latent style embedding of behavior styles and condition the policy on it. A style policy that takes the style embedding as additional input is learned alongside a target policy without the style embedding. Both policies are trained with behavior cloning (BC). Experiments are performed on simple environments such as Grid-World and Cartpole.","This paper presents Style BC, an extension to standard behavior cloning that uses an additional style embedding to capture different modes in expert demonstrations. The style embeddings are discrete, fixed-size vectors which the policy uses to predict actions from states. Style BC trains two imitative policies: the target policy and the style policy. The target policy directly predicts actions from states; the style policy additionally takes as input the selected style embedding. The style embeddings are trained by back-propagating action prediction gradients. Style BC outperforms the selected baselines (BC, GAIL, InfoGAIL) on both diversity and final performance in the CartPole environment. It is shown to be able to capture different modes from expert demonstrations in the Highway environment.  ","This paper focuses on learning different behavior styles from expert demonstrations for imitation learning. More specifically, the contributions are three folds: 1. This paper proposes a novel style behavior cloning approach that learns the various representation of behavior styles from expert demonstrations; 2.  The style embedding which can provide valuable guidance for the generation of style policies is obtained automatically; 3.  The authors empirically demonstrate the effectiveness of the proposed method on a Grid-World environment, cartpole control tasks from OpenAI gym, and a collection of environments for decision-making in autonomous driving named highway-env.",0.20481927710843373,0.21686746987951808,0.30120481927710846,0.3157894736842105,0.32894736842105265,0.18333333333333332,0.2236842105263158,0.15,0.2604166666666667,0.2,0.2604166666666667,0.22916666666666666,0.21383647798742136,0.1773399014778325,0.2793296089385475,0.24489795918367346,0.2906976744186046,0.2037037037037037
964,SP:d8c8f65665c0895d53a8635ee212bc85a5a8c763,"The paper studies the model learning aspect in model-based reinforcement learning (MBRL). Standard Dyna-based MBRL algorithms rely on maximum likelihood estimation (MLE) or its variants for model learning. In contrast, the authors propose a Value-Gradient weighted Model (VaGraM) loss. Using the loss, the authors mitigate the objective mismatch in standard MBRL: models with high likelihood might not be good for maximizing returns since they optimize a different objective. The paper builds on top of value-aware model learning (VAML) but points out two problems with the original algorithm: 1) value function estimates can be arbitrary outside of the empirical state distribution 2) the entanglement of value and model learning creates the possibility for bad local optima. The experimental results provide evidence that the proposed VaGraM loss alleviates the problems and outperforms in the MLE baseline when the true model cannot be estimated accurately.","The authors propose a new loss function for training dynamics models in MBRL that is aware of the environmental value. This loss function is a tool to mitigate the ""objective mismatch"" issue and seems well motivated. The authors show the usefulness of their VaGraM in simple experiments. ","The paper introduces a new value-aware objective for model learning that directly builds on VAML — specifically, using a Taylor series approximation for the next state value prediction w.r.t the state under consideration. This yields a somewhat intuitive form upon simplification — essentially, dimension-wise weighted L2 distance between predicted and actual states of the model where the weights correspond to the gradient. The experimental results in restricted domains indicate that the proposed VaGram method outperforms MLE. ","The paper proposes a new loss for model-learning in model-based RL, called “Value-Gradient weighted Model loss” (VaGraM). The loss belongs to the family of value-aware model learning losses and is inspired by the VAML and IterVAML losses by Farahmand et al. The paper designs this loss with two main issues of VAML in mind -- (1) value function being evaluated on irrelevant model predictions and (2) model converges to optimas that are sensitive to value function perturbation, being de-stabilized easily when value function is slightly updated. The proposed method is shown to fix both of these issues and is studied in the pedagogical Inverted Pendulum environment and in the MuJoCo Hopper environment. The key takeaways from the Hopper experiments is that the proposed method does better than the baseline MLE model loss in two scenarios -- (1) reduced model capacity and (2) distracting dimensions.",0.11643835616438356,0.17123287671232876,0.2534246575342466,0.2978723404255319,0.3829787234042553,0.28205128205128205,0.3617021276595745,0.32051282051282054,0.25170068027210885,0.1794871794871795,0.12244897959183673,0.14965986394557823,0.17616580310880828,0.22321428571428573,0.2525597269624574,0.224,0.18556701030927833,0.19555555555555557
965,SP:d8f59891be2b61a961a1a91e71229d70aee00631,"This paper proposes a novel LSH based approach for efficient forward and backward pass of Partial Differential Equations (PDEs). The proposed LSH-SMILE approach focus on local updates and use LSH to find the local neighbors. Moreover, LSH-SMILE propose a bucket merging method to avoid empty buckets.","In learned Partial Differential Equation (PDE) solvers, the forward simulation (thereby data collection) and backward learning processes are very time consuming. This paper tries to mitigate this issue by using Locality Sensitive Hashing (LSH).   The motivation for this approach comes from three critical observations -  1) that most PDE updates in a close vicinity tend to be similar ,i.e., in a one step update, most elements change only on few steps and remain unchanged in the rest of the process. 2) one step updates for a element is dependent on the neighborhood (neighborhood can be thought of a context in NLP). 3) most points share similar temporal dynamics  In learned PDE, the practical simulation happens by discretizing finite-difference approach and obtaining learnable vectors.  Starting from an initial state and hashing to one bucket per element, LSH-Smile iteratively pools elements that have similar hash codes into the same buckets. On each hash bucket, there will be a classic PDE update (same update for all elements in a bucket). Then the hash codes of buckets are updated to  obtain the ones with significant change (>r, a threshold). Then the elements in those buckets are rehashed.  The backward process is similar with an additional gradient computation step that I presume is also done a bucket basis.  I’m not entirely sure abt the input structure and the hash function used (I may have missed some details).  But essentially, LSH-SMILE does the standard updates process on a chunk by chunk basis instead of on all elements.     ","The paper proposes a method (LSH-SMILE) that applies LSH to speed up PDE forward simulation and backward learning. Observing that most of the elements are very similar, they can be hashed into buckets and each bucket can be represented by a single value, thus reducing computational and memory cost.  The method is validated empirically on two problems, grain growth and nanovoid simulation, showing that it is much faster than standard brute-force computation while maintaining reasonable accuracy.","The paper applies locality sensitive hashing to neural networks for solving Partial Differential Equations (PDEs). The paper proposes a solution called LSH-SMILE and claims several contributions: (1) LSH-SMILE achieves a drastic improvement in scalability than conventional approaches. (2) Theoretically, the paper proves a novel bound on the errors introduced by LSH-SMILE (3) Experimentally, the paper demonstrates LSH-SMILE exhibits comparable quality with exact approaches but costs less time and space.  ",0.3333333333333333,0.25,0.2916666666666667,0.09019607843137255,0.0784313725490196,0.14102564102564102,0.06274509803921569,0.15384615384615385,0.1917808219178082,0.2948717948717949,0.273972602739726,0.1506849315068493,0.10561056105610561,0.1904761904761905,0.23140495867768596,0.13813813813813813,0.12195121951219512,0.1456953642384106
966,SP:d921e6e659c09ae947fe3466735a368d9c3f9100,"This paper considers the multi-arm bandit problem. It starts from the observation (already present in the literature) that classical bandit algorithms such as UCB or TS estimate the mean rewards of the arms by simple empirical averages without taking into account the fact that they are adaptively sampled and are therefore biased. The paper thus proposes to make these estimators unbiased by using importance weighting (inverse propensity score weighting) as well as variance reduction tools (adaptive doubly robust estimator and clipping). Applying it to TS, they empirically demonstrate that their algorithm significantly outperforms classical TS and UCB while maintaining a theoretical worst-case regret bound of order $O(\sqrt{T})$. ","The paper investigates stochastic MAB algorithms, UCB and Thompson sampling, where the usual choice for estimating the means of arms, importance weighting, is replaced with a ""doubly adaptive"" estimator. This estimator, proposed by Leudtke and Van Der Laan 2016, is self-normalized in that the weight of each sample in the mean estimator is not uniform but rather proportional to the inverse square root propensity weight; it also uses an AIPW-style estimator with these weights.  The authors claim that this new estimator can have better practical performance, since the variance of the mean estimates is greatly reduced, and this claim is verified with empirical simulations. Additionally, the authors provide an upper bound on the worst case performance of O(K^2 sqrt{T log T}), which captures the correct dependence in T. ","The paper proposes a Thompson Sampling-based online algorithm for the stochastic K-armed bandit problem, which exhibits improved numerical performance in terms of the expected cumulative regret and its variance, and smaller stopping times for best arm identification. The improvement is over the popular UCB and Thompson Sampling baselines common in literature. Although, the improved empirical performance of the proposed algorithm does not translate to provably better regret guarantees vis-a-vis the aforementioned benchmarks (a sub-optimal K-dependent scaling factor is present in the upper bound). The improved numerical performance is primarily attributed to a new estimator for the true arm-means (as opposed to the standard sample-average approximation used by vanilla UCB/TS), which is provably unbiased, and at the same time, offers a better variance control compared to other unbiased estimators popular in ex post (adaptive) causal inference literature. The punchline, in my understanding, is that it is possible to improve and (better) control the numerical performance of bandit algorithms by employing sophisticated estimators from adaptive inference literature, without compromising on rate-optimality in the regret upper bound (order of dependence on the sampling horizon, modulo multiplicative constants). ","The authors propose a Thompson Sampling (TS) based cumulative regret-minimsation algorithm for stochastic MAB. Their approach pivots on the unbiased estimator of mean using inverse propensity score, and then it applies weighted averaging scheme [Luedtke and Van Der Laan, 2016] to reduce the variance of the estimate. Further, they establish theoretical worst-case upper bound on the expected cumulative regret, and present multiple simulations to show empirical efficiency of their proposed algorithm.",0.1891891891891892,0.1981981981981982,0.15315315315315314,0.21804511278195488,0.15789473684210525,0.10824742268041238,0.15789473684210525,0.1134020618556701,0.2328767123287671,0.14948453608247422,0.2876712328767123,0.2876712328767123,0.1721311475409836,0.1442622950819672,0.18478260869565216,0.17737003058103978,0.20388349514563106,0.15730337078651685
967,SP:d949d2392264497b1043f8b7cb0e5ea1bf838eec,"In this work, the authors propose to characterize neural networks with a tool from topological data analysis, called persistence diagram, in order to be able to compare neural networks with different numbers of layers and/or neurons. More specifically, they show that one can filter the neural network computational graph using the edge weights (that are learnt during training of the network) to produce a filtered flag complex, from which persistent homology can be computed. Then, the authors interpreted the distances between the persistence diagrams obtained from networks with varying parameters (number of layers, neurons, labels), and showed that often, the distances have intuitive correlation with the complexity of the neural networks.","This paper presents the similarity measure between neural networks based on the graph characterization strategy of neural networks. This graph characterization strategy is based on Persistent Homology in which neural networks can be represented as abstract simplicial complex. In this work, neural networks are considered as weighted directed graphs which can be further considered as simplicial complexes and we can obtain Persistent Diagrams of these graphs. Then, we can compute distances between Persistent Diagrams of different neural networks. Only MLP layers are considered to compute the PDs. Results are shown on four popular datasets from two different areas of images and text. Experiments with different persistence summaries are provided to compute the distance matrices and results are compared between them on different datasets.","This paper proposes a method to compare trained neural network based on extracting topological descriptors from their graphs. Two networks are then compared through the lens of these descriptors yielding a similarity score between networks, with the expectation that networks aiming at solving a similar task should be measured as being similar. Authors showcase their approach on qualitative experiments on different datasets of different nature. ","This paper tries to measure the structural similarity of the neural networks by associating them with simplicial complexes. The similarity between neural networks is then measured by the topological feature via persistence homology. The graph is generated by taking the neurons as graph nodes, and the edge weights are defined as the network parameters for the connected neurons. The directed flag complex is attached to the constructed weighted directed graph, then the filtration, a family of simplicial complexes for the neural networks. The persistence diagram and persistence barcode are calculated for the filtration. They are used to show the three persistence summaries: persistence landscape, weighted silhouette and heat vectorizations.  Experiments were carried out for various benchmarks for typical CNN architectures.",0.19642857142857142,0.125,0.22321428571428573,0.13821138211382114,0.21951219512195122,0.18461538461538463,0.17886178861788618,0.2153846153846154,0.20833333333333334,0.26153846153846155,0.225,0.1,0.1872340425531915,0.15819209039548024,0.21551724137931033,0.18085106382978722,0.22222222222222224,0.12972972972972974
968,SP:d9ba9873c54481c86387adb7a3ef66cf6c9a3350,"This paper is motivated by the problem of learning n-qubit quantum states in different learning models, such as differentially private PAC learning and online learning with imprecise feedback, and proves information-theoretic equivalences and implications between them. One of the main results of the paper is proving that pure DP PAC learning of quantum states implies learning of quantum states in the online learning model with imprecise feedback.  The line of implications goes as follows: a bound on the sample complexity of Pure DP PAC learning implies a bound on the probabilistic representation dimension of the class, which implies a bound on the one-way communication complexity of quantum learning, which bounds the *sequential-fat-shattering* dimension of the class (a generalization of the Littlestone dimension from boolean to real-valued function classes).  If the sequential-fat-shattering dimension is bounded then there exists an online learning algorithm which is robust to imprecise feedback (as must be the case for the training samples-measurements of quantum states), called Robust-SOA.   The authors then also prove that this online learner implies the existence of a learning algorithm that satisfies a type of stability, called quantum stability, which says that there exists a state such that the algorithm’s output is within a ball of this state with high probability.  Among others, a significant implication included in the results is a new bound on the complexity of shadow tomography which includes the sequential-fat-shattering dimension (sfat) instead of the number of qubits n (and sfat<=n for any class but there exist classes with sfat significantly smaller than n). ","This paper derives relationships between several models of approximately learning quantum states. Specifically, it extends results in learning boolean functions to learning real-valued functions with imprecise feedback, and shows that the complexity of learning in Pure DP PAC upper bounds quantum CC and the sequential fat-shattering dimension. The Sfat dimension in turn implies online learnability. A conceptual contribution of this paper is quantum stability, and the authors show that online learnability implies the existence of stable learners, but contrary to results on boolean functions, quantum stability does not imply Approximate DP PAC. The authors also propose the RSOA algorithm, which is central to proving the results. ","This paper presents a number of connections between private learning, online learning and stability in the quantum and real-valued settings, generalizing recent work of Bun, Livni and Moran '20 in the Boolean-valued setting. The main results are:  - An algorithm (termed RSOA) that accomplishes online learning of real-valued functions (requiring feedback only within an $\epsilon$ precision) with a mistake bound given by the sequential fat-shattering (sfat) dimension of the class  - A notion of real-valued and quantum stability (generalizing Bun et al's ""global stability""), along with a proof that online learnability implies this kind of stability  - A no-go result for (one approach for) converting stability to approximate PAC learnability  - An improved shadow tomography result with sample complexity in terms of the sfat dimension  - Bounds on the sfat dimension of quantum state classes in terms of Holevo information  Many of the key notions and proofs are natural generalizations of the Boolean and (exact) real-valued analogs as studied e.g. in Bun et al '20 and Rakhlin et al '10. The concrete applications to quantum state learning are novel and interesting, and address some questions raised in earlier works such as Aaronson et al '18.","The paper develop a series of relationship between quantum learning model.  (1) Pure DP PAC learnability implies finite sequential fat shattering dimension. (2) Finite sequential fat shattering dimension implies online learnability (3) Online learnability implies certain notion of quantum stability. (4) quantum stability may not implies approximate PAC learnability.  The work (especially point 3,4 above) is inspired by the recent breakthrough of Bun, Livni and Moran (Journal of the ACM'21) on proving online learnability implies private PAC learnability, extends the result to real-valued setting.  ",0.17100371747211895,0.17843866171003717,0.11524163568773234,0.3055555555555556,0.25925925925925924,0.135678391959799,0.42592592592592593,0.24120603015075376,0.3563218390804598,0.1658291457286432,0.3218390804597701,0.3103448275862069,0.24403183023872677,0.20512820512820512,0.17415730337078653,0.21498371335504884,0.28717948717948716,0.18881118881118883
969,SP:d9eb6faff73a95ce9102a2fac9067531dda4d74e,"This paper studies the problem to learn physical equations by neural networks. To address the limitations of traditional methods using sparsity to learn physical equations, this paper proposes to use the physical neural network as a container, with additional constraints to explore “Range, Inertia, Symmetry, and Extrapolation (RISE)”. Using the proposed method, it is expected that less local minima will be explored for better solution. Experiments have been conducted to support the presented method.","A framework for learning succinct and humanly interpretable descriptions of physical systems from data is outlined. The main focus of the approach is eliminating/avoiding local optima, which is achieved through both architectural design of the model and modification of learning algorithm. Empirical evaluations conducted on simulated data suggest improved error metrics as well as recovery of ground truth equations.","Robust methods for equation discovery respecting physics are of utmost importance for the application of data-driven methods in science en engineering. The present contribution proposes a new framework based on Physical Neural Networks (PNN) : - it is assumed that the function $\mathbf{y} = \mathbf{f}(\mathbf{x})$ is polynomial (with possibly negative exponents), - the network has only one hidden layer (as far as I understood), - this hidden layer takes as input the state variables $x_i$ of the system and outputs $z_j = \exp \left( \sum p_{ij} \textrm{ln}(x_i) \right)$, - In the above expression, $p_{ij} = \textrm{tanh}(c_1 w_{ij}) \cdot \sigma(c_2 w^{\prime}_{ij})$ with $\mathbf{W}$ and $\mathbf{W}^{\prime}$ parameters to be learned. - The final output layer is a simple weighted sum, i.e. $y_k = \sum_j m_{jk} z_j$.  Sparsity is of the model results from the use of the sigmoïd activation function in the hidden layer while the hyperbolic tangent makes sure $p_{ij}$ is an integer (either positive or negative).  Along with this simple architecture, the authors propose the so-called RISE framework to include realizability conditions (e.g. physical constraints) the model needs to satisfy. These constraints are enforced only weakly by penalizing the loss function to be minimized in the training state. The overall model is tested on a number of non-standard benchmarks to illustrate its capabilities.","This paper tackles the problem of model inference and prediction, while preserving physical correctness. This is an interesting and important problem in supervised learning, for interpratability and generalization reasons. It is also quite important in model-based reinforcement learning, in the context of nonlinear system identification.  The authors start by challenging existing sparse regression approaches (such as SINDy) and symbolic regression approaches (such as EQL): - Because they lead to non-physical solutions with low loss, - Because they depend on hyperparamters  Hence they propose what is coined a Physical Neural Network (PNN), where: -  layers have a Laurent polynomial shape with learnable coefficients and powers, - weights corresponding to the layer input powers are decomposed into a tanh activation multiplied by a sigmoid activation.  This last design choice allows to enforce some kind of sparsity (when sigmoid converges to 0) and constrains powers in the range {-1, 1} when active (because of tanh). Furthermore, it ensures differentiability of the integer valued layer-input powers.  Additionally, the authors also propose to penalize the model training with what they call a RISE constraint. The latter is meant to enforce physically plausible range, inertia, symmetry and extrapolation properties in the inferred model.",0.0945945945945946,0.20270270270270271,0.2972972972972973,0.25,0.2,0.1440677966101695,0.11666666666666667,0.0635593220338983,0.11224489795918367,0.0635593220338983,0.061224489795918366,0.17346938775510204,0.1044776119402985,0.0967741935483871,0.16296296296296295,0.10135135135135134,0.09375,0.15740740740740738
970,SP:da0445341a459e59978e93485ee66fdacd7bcf82,"This paper studies the problem of feature mapping for reproducing kernels. The focus is when the data size is too large, the SVD for the kernel Gram matrix would be prohibitively difficult. This paper proposes to use the least-squares loss to achieve a low-rank approximation to the Gram matrix, to use Legendre transformation to disentangle quadratic terms in the least-squares loss, and to use a two-phase Hebbian parameter update rule to achieve the approximating feature map.","This paper considers the problem of finding a low-rank approximation to a kernel matrix. In the low-rank approximation problem, one would want to find a low-rank matrix such that the Frobenius distance between the low-rank matrix and the original matrix is minimized. The paper shows that this optimization problem can be upper bounded by the cost function of a Hebbian neural network with a recurrent layer. Using some experiments the paper shows that this neural network can produce accurate low-rank approximations to some synthetic datasets as well as the MNIST dataset for the Gaussian and Arc-cosine kernels.",The paper describes an online stochastic optimization of a neural network to learn an embedding such that the inner-product between embedded points approximates a non-linear kernel based similarity. This can be a radial basis function network that approximates the embedding of a Gaussian kernel. It can also use a homogeneous kernel to approximate cosine-similarities. The network approach can be seen as a sort of Nyström approximation where each output dimension is associated with a different Nyström support point (landmark). The approximation cost is in terms of minimizing an upper bound on the squared error of the similarities. Experimental results show that the method is able to learn meaningful representations. ,"The paper studies the linear similarity matching problem, which represents input $x_t$ by a high dimensional vector $y_t$ such that $y_s \cdot y_t \sim f(x_s, x_t)$ for a kernel function $f$.   Main contributions of the paper: - The authors propose an online algorithm for linear similarity matching, which can be interpreted as a neural network with Hebbian and anti-Hebbian connections. - The authors provide experiments to demonstrate that the proposed method can aid unsupervised learning tasks.",0.2375,0.1625,0.175,0.20388349514563106,0.1650485436893204,0.16666666666666666,0.18446601941747573,0.11403508771929824,0.17073170731707318,0.18421052631578946,0.2073170731707317,0.23170731707317074,0.20765027322404372,0.13402061855670103,0.17283950617283952,0.19354838709677416,0.1837837837837838,0.19387755102040818
971,SP:da4ef2c8073ac1461514c1f97070bb8317352a2b,"The paper proposes a neural network architecture, inspired from factorization machines, for classification task from tabular data. In particular, the proposed factorized neural networks (F2NN) leverage the capabilities of shallow networks for learning factorized low-level representations that can approximate (the typically) high-dimensional representations of each field. This,in turn, together with different choices of activation factions, provides a generalization over different factorization approaches, and can lead to good (albeit not always better) performance on different classification tasks. Results from several (16) experiments on binary and multi-class classification tasks and two CTR tasks, comparing F2NN to different benchmark algorithms, show that it has comparable performance to some of standard (SOTA) methods. ","The paper describes an approach for tabular data classification. In particular, authors proposed the Fieldwise Factorized neural network to tackle the problem defining a general framework in which it is possible to express other tabular classification methods. The proposed approach permits to manage categorical and numerical fields and seems to outperform other methods (on average). Moreover the experiments seem to demonstrate the importance of per-field network and the improvement in the results using GELU instead of the ReLU of Quadratic activations.","- The authors proposed a novel deep learning model for tabular data classification. - The authors utilize the field-wise factorized networks to extract the meaningful information from both categorical and numerical features of the tabular data. Then, using those embeddings to solve the classification problem. - The experimental results showed that the proposed method achieved comparable performances in comparison to SOTA deep learning models for tabular data (SAINT).","This paper presents a general neural network framework for tabular data classification, that can recover a range of popular classification methods. The main components are per-field representation by factorized network and an activation function on aggregated representations of all fields. Experiments on general tabular classification and CTR prediction demonstrate its effectiveness, although performance gain is marginal.   ",0.168141592920354,0.1592920353982301,0.168141592920354,0.23170731707317074,0.17073170731707318,0.18181818181818182,0.23170731707317074,0.2727272727272727,0.3333333333333333,0.2878787878787879,0.24561403508771928,0.21052631578947367,0.19487179487179487,0.20111731843575417,0.2235294117647059,0.25675675675675674,0.2014388489208633,0.1951219512195122
972,SP:dac8f289249a1f62b38338d2d81f8cf6d12a861d,"Motivated by the fact that in practice models are applied to text generated later than their training data, this paper shows that splitting the datasets in the common time-agnostic way results in overestimating their performance compared to using time-stratified datasets. This work also conducts in-depth analyses of the performance degradation as the time span from a training set to a (later) test set increases and draws several conclusions: 1) proper nouns, numbers, words related to politics/sports, emerging new words are affected the most; 2) increasing model size improves overall performance but does not solve this performance degradation issue; 3) this phenomenon affects some end-user applications. To mitigate this issue, this work proposes to use dynamic evaluation which updates the model in an online fashion. Based on these findings, this work recommends the wide adoption of time-stratified dataset splits to encourage more realistic evaluation and the development of more adaptive language models.","The paper studies the problem of temporal generalization in pre-trained language models, that is, the ability of the models to generalize well to future data beyond their training period. The paper argues that temporal generalization is necessary for pre-trained language models as they increasingly form part of dynamic and non-stationary environments that may require updated knowledge to perform better. To benchmark temporal generalization of the current models, the paper introduces three language modeling datasets - arXiv, WMT, CustomNews with timestamp information. On experimenting with the existing model, Transformer-XL, on the time-stratified setup, the paper finds that the performance deteriorates as the evaluation data moves further away from the training period. An in-depth analysis of the model predictions leads to the intuitive conclusion that the model performance suffers from the novel and emerging words/ concepts present in the future data. Next, the paper considers time-stratified closed-book QA tasks to evaluate the temporal performance of language models on downstream tasks. This paper convincingly shows that the performance on QA tasks deteriorates as the pre-trained language model becomes increasingly outdated with time, thus motivating the temporal generalization problem. Naively increasing the capacity of the model (from Transformer-XL) does not solve the problem. Therefore, the paper builds on the existing dynamic evaluation methods and proposes continuous updating of the language model with the incoming stream of the data. Such a method adapts the language model to temporal trends from incoming data, thus reducing the speed of the model from becoming outdated. Although dynamic evaluation results in improved performance, the paper shows that temporal generalization in pre-trained language models is far from solved and still poses a challenging problem for the community.","This paper studies temporal generalization of language models -- how well they do on text data ""from the future"" (ie after they are trained). It performs experiments in the domains of arxiv abstracts and news domains, where the date of each document is given.   The paper trains language models on data up to and including December 2018, and evaluates perplexity on the months afterwards -- perplexity goes up as models have to extrapolate more, much more than if documents were split iid. The paper finds similar reslults when the task is changed to QA, or when a larger model is pretrained.   The paper introduces analysis for what new linguistic patterns cause perplexity to increase (e.g. new words or words that change in context.)    The paper tries one simple way of addressing this -- dynamic evaluation -- but also argues that it's not enough.","This paper evaluates language models on time-stratified datasets, i.e., they split datasets so that the test set is newer in time than the training set. It shows LMs are worse in terms of perplexity and closed-book question answering ability on the test data, some times by surprisingly large amounts. They analyze where the largest source of errors occurs (proper nouns, named entities, etc.) and also show that online finetuning at test time can mitigate but not solve these temporal generalization issues.",0.25477707006369427,0.12101910828025478,0.10191082802547771,0.12195121951219512,0.08013937282229965,0.12056737588652482,0.13937282229965156,0.1347517730496454,0.19047619047619047,0.24822695035460993,0.27380952380952384,0.20238095238095238,0.18018018018018017,0.12751677852348992,0.1327800829875519,0.16355140186915887,0.12398921832884095,0.1511111111111111
973,SP:db15839ac53280a4ef57ee472565c137d975f52e,"- Paper focuses on 3D dog reconstruction from a single monocular image - A coarse 3D dog estimate is generated using an existing pipeline (WLDO, SMALST), although with Tversky loss added to prevent oversized estimates (a known problem in prior work).  - The paper introduces an encoder-decoder graph convolutional network which incorporates vertex-level local image features for applying free-form refinements to the the vertices of this coarse estimate. - These are supervised using only weak 2D supervision and are shown effective for modelling dog shapes (which are very diverse). - Ablation study + results on StanfordExtra, BADJA and AnimalPose show state-of-the-art performance. ","This paper studies the problem of joint animal pose and shape reconstruction from a single image. This work is largely based on the literature [Ref 2] but adopts a coarse-to-fine mechanism for joint pose and shape learning.  The method extracts global embedding as before in the first stage. In the second stage, this work leverages a local feature extractor to provide pixel-aligned local features for mesh refinement. Specifically, the extracted local features are given as input to the U-Net to refine meshes at a different resolution. Experimental comparisons with existing work [Ref 2, 3, 32] have been conducted on the StanfordExtra dataset and BADJA dataset.","This paper presents a coarse-to-fine pipeline to recover animal pose and shape. They first predict SMAL parameters and then use an MRGCN to refine surface vertices to achieve better image alignment. Their MRGCN follows an encoder-decoder structure, which is the main difference against previous GCN based methods. Their method achieves state of the art results on StanfordExtra, Animal Pose and BADJA datasets. ",This paper presents a method for estimating the pose and shape of animals from images. The paper addresses limitations of previous approaches that cause misalignments and 2D keypoint locations. The main idea is to initialize using a parametric animal model and subsequently use a graph convolution network (GCN) to optimize for vertex positions to better fit shape segmentation. Experimental results how improved performance.,0.18627450980392157,0.14705882352941177,0.10784313725490197,0.1559633027522936,0.13761467889908258,0.2,0.1743119266055046,0.23076923076923078,0.1746031746031746,0.26153846153846155,0.23809523809523808,0.20634920634920634,0.18009478672985785,0.17964071856287428,0.13333333333333333,0.19540229885057472,0.1744186046511628,0.203125
974,SP:db47529dd9563a3e773f5d5f0ec1e49736526467,"This paper tackles the well known problem of mixture proportion estimation and Positive-Unlabelled (PU) learning. The authors propose a a (new) estimator for MPE based on the assumption of a PU classifier having a ""pure"" top bin. The authors then develop a novel  iterative outlier-removal-like algorithm for learning a classifier separating the positives and negatives from PU data assuming access to the true MPE. Finally, they combine these two approaches to create a novel alternating algorithm that estimates MPE based on a current PvN classifier, and use the MPE estimate to learn a better PvN classifier from PU data.","The paper proposes two methods for PU learning. First, a method for estimating the fraction of positives among unlabeled examples, named Best Bin Estimation (BBE). Secondly, an approach for PU learning called Conditional Value Under Optimism (CVuO). The BBE works on top of the decision score of a given pretrained classifier. The score is used to find a region of the instance space with the minimal ratio of the portion of unlabeled and the portion of positive examples. The ratio is an upper bound on the fraction of positives being estimated. In case there exists a region populated only by positive examples (the condition called pure positive bin property), the bound is tight. The authors provide a finite sample bound for their estimator. The CVuO turns any supervised method to PU learning algorithm. The approach assumes the fraction of positive examples to be known. The idea is to rank the unlabeled examples by a score of a pretrained classifier and remove it the fraction of positive examples. The remaining examples are taken as true negatives and then used with the true positives to re-train the classifier. The procedure is repeated until convergence. Besides, the authors propose an algorithm, called $TED^n$, combining BBE and CVuO. The methods are empirically shown to outperform several recent methods on semisynthetic problems created from CIFAR, MNIST, and IMDB datasets. ","This paper proposes a unified framework for learning from positive-unlabeled (PU) data, including BBE for mixture proportion estimation (MPE)  and CVuO for PU learning. In the literature there are important and elegant works exists that analyze the consistency of the mixture proportion without the irreducibility assumption. They formalize their analyses based on an assumption that the percentage of a blackbox classifier's output for positive samples greater than a fixed value is higher than that for negative samples. I suggest a brief introduction of the conditions that this assumption satisfies, maybe the inductive bias or something, while intuitively such an assumption is quite reasonable. They provide finite sample guarantee for BBE and also empirically verify it. In the PU learning stage, the authors use an iterative manner to progressively identify and remove all the positive points from the unlabeled data during training. This is again quite a reasonable way of thinking because of the memorization of neural networks. The combination of BBE and CVuO is called (TED)^n. The authors provide a lot of supportive experiments to verify their methods.","This paper targets the problems of mixture proportion estimation and PU learning. The authors propose two simple but effective methods to address two problems. Also, the combination of two methods is proposed to train an accurate binary classifier under mild conditions. A series of experiments are conducted to verify the effectiveness of the proposed method. ",0.3137254901960784,0.2647058823529412,0.19607843137254902,0.21238938053097345,0.084070796460177,0.11602209944751381,0.1415929203539823,0.14917127071823205,0.36363636363636365,0.26519337016574585,0.34545454545454546,0.38181818181818183,0.1951219512195122,0.19081272084805653,0.25477707006369427,0.23587223587223585,0.13523131672597866,0.17796610169491525
975,SP:db99c8710baffcb2083a1e7f078969f270d28357,"This paper studies the effect of learning rules on the algorithms learned by recurrent networks on context-dependent integration tasks. A number of continuous time vanilla RNNs are trained on integration tasks with different learning rules: backprop, genetic algorithms, hebbian learning, and full-force. The paper finds that across these learning rules, all networks learn consistent attractor topologies in order to solve the task. As the complexity of the task is increased (increasing number of contexts), the representations start to differ, but the attractor structure is preserved. The paper finds that using line attractors to solve integration seems to be a universal property of these networks, independent of learning rule. Finally, the paper shows how properties of the learned attractors (both the topology and whether the attractors are continuous manifolds or discrete points) vary with task input noise.","A popular paradigm for modeling the brain using task-optimized deep learning models identifies three key features governing the solutions learned by neural networks: architecture, task, and learning rule. Building on work by Maheswaranathan et al. [1], which showed that architecture influences the representations learned by trained recurrent neural networks (RNNs), but not their attractor structure, the authors of this work extend the analysis to study the effect of learning rule and task on the representations and attractor structure of trained RNNs — with broadly consistent results. The authors show that the attractor structure is largely invariant to the choice of learning rule (at least on two simple tasks: random dot motion and context-dependent integration, with large input noise), but the learned representations are not invariant, especially as the complexity of the task (the number of individual contexts to integrate) increases. However, the authors also exhibit a setting where the attractor structure is *not* invariant to the choice of learning rule: when the input noise is low in either task. They further show that the learned attractor structure depends sensitively on the amount of noise injected into the network, with a transition between one solution in the low-noise regime and another in the high-noise regime. While these results may seem contradictory, they may be consistent under the hypothesis that the attractor structure of trained RNNs exhibits universality across architectures and learning rules *when the task is sufficiently difficult*.","The authors extend the recent work of Maheswaranathan et al to explore how learning rule and task complexity affect the solutions obtained by recurrent neural networks trained on neuroscience-inspired tasks. Specifically, four different learning rules are used to train networks on a suite of similar tasks with systematically increasing complexity. The authors show that learning rule has some effect on representation geometry, and this effect grows with task complexity. Attractor topology, however, remains universal for almost all tasks and learning rules. The authors also show some instances in which network dynamics are influenced by input noise. This sensitivity, however, decreased as task complexity increased. ","This paper aims to characterize the impact of design choices in RNN-based experiments on learning dynamics and representation geometries, both of which are important data sources when using RNNs to model biological intelligence. It begins by setting up the problem of investigating RNNs’ fitness to model cognitive and neural systems, and drawing a distinction from the most similar work (Maheshwaranathan et al). It then describes a standard continuous-time RNN architecture, three tasks, one of which is a complexity-modular version of another, and four learning rules of which I believe one is local. Along with the model setup, it details evaluation and visualization methods. The bulk of the paper appropriately focuses on results. This goes through the following conclusions: dynamics do not vary with learning rule (implicitly, for more complex tasks), representations vary with task complexity, dynamics do not vary with task complexity, dynamics do vary with learning rule under low input noise with simpler tasks, and finally dynamics are confirmed to not vary with learning rule under low input noise with more complex tasks (suggested explanation being that more complex tasks have a more constrained solution space). The paper ends with a summary of these conclusions and their implications for the use of RNNs in computational neuroscience.",0.30434782608695654,0.18840579710144928,0.1956521739130435,0.1375,0.15833333333333333,0.22857142857142856,0.175,0.24761904761904763,0.12857142857142856,0.3142857142857143,0.18095238095238095,0.11428571428571428,0.2222222222222222,0.2139917695473251,0.15517241379310345,0.19130434782608696,0.16888888888888887,0.15238095238095237
976,SP:dbb5f6426a0733ddb7fbc69b3f32020a4ffacef3,"The paper has four main contributions:  1. introduces the _set norm_ normalization layer in neural network models for set data, as opposed to feature normalization (aka. batch-norm) and layer normalization layers; 2. provides intuition behind a ""cleaner"" implementation of residual connections; 3. implements set norm and the cleaner residual connections in modified versions of existing Deep Sets and Set Transformers models; 4. introduces a new dataset called Flow-RBC with over 100,000 examples. Each ""example"" consists of a $(X,y)$ pair, where $X=\\{x_1, \dotsc, x_{1000}\\}$ is a set of measurements on 1000 red-blood cells from a patient, and $y$ is a hematocrit level label. Each $x_i$ consists of both volume and hemoglobin mass measurements. This dataset is significantly larger than similar existing datasets.","This paper presents improved versions of two standard premutation-invariant networks: Deep Sets++ and Set Transformer++. They make two critical design decisions: set normalization and clean residual paths. They also introduce Flow-RBC, a new benchmark for permutation-invariant prediction. Finally, the paper evaluates several variants on several benchmarks, including Flow-RBC.","The paper aims to improve training of deep neural networks applied to sets. The authors investigate permutation equivariant normalizations and residual connections in DeepSet and SetTransformer models, and propose a modification of layer norm which calculates standardization statistics over both features and all set elements. They also propose a modification to DeepSet and SetTransformer models inspired from pre-norm ResNet and pre-norm Transformer. The modifications are experimentally tested on a pointcloud classification dataset, synthetic variance regression, and a new dataset for set prediction contributed by the authors.","The paper tackles the problem of designing deep networks that operate on sets. The challenge is addressed by normalization layers and skip connections. The authors formulate some design choices and advocate the choice of set normalization and clean residual connections. The layers are incorporated with two different choices of architectures and are evaluated on several sets learning benchmarks. In addition, a new benchmark is introduced. ",0.08461538461538462,0.13846153846153847,0.11538461538461539,0.25,0.28846153846153844,0.20454545454545456,0.21153846153846154,0.20454545454545456,0.23076923076923078,0.14772727272727273,0.23076923076923078,0.27692307692307694,0.12087912087912087,0.16513761467889912,0.15384615384615388,0.18571428571428572,0.25641025641025644,0.23529411764705882
977,SP:dbc8337ff4aa629a9784a899cad2bf7ea513165c,"A problem of fair sparse linear regression is addressed in this paper, where bias depends upon a hidden binary random variable. This hidden r.v. can be thought of as a clustering assignment combining sparse regression. An optimization problem is formulated by the authors as a mixed-integer linear programming originally, then, a continuous relaxation is used to reformulate it into an invex optimization problem. The theoretical results mainly focus on the solution's uniqueness of the invex problem by verifying Karush-Kuhn-Tucker conditions and primal-dual certificates. In the experiments, the proposed method shows the ability to correctly recover the support of the regression parameter vector, and the correct clusters. ","This paper studies the inverse problem of fair sparse regression. The corresponding optimization problem contains a continuous variable and discrete variable, and it is combinatorial. The authors consider a novel invex relaxation of it. They prove that there exists a unique solution to the invex problem，and the true variables of the model can be recovered exactly. ","The paper considers sparse regression with a hidden binary attribute modeling unknown bias (as in lack of fairness).  This is in contrast to most other work on fairness where the protected attribute is known, and used to impose fairness constraints.  The paper presents an invex relaxation (bi-convex generalization of convexity without local minima), proves its correctness, and makes claims that it can exactly identify hidden bias, and does so at no cost to prediction accuracy.  ","The paper considers the task of sparse regression on a biased dataset where the bias depends on a hidden binary attribute. A new optimization problem is formulated which is then relaxed into an invex optimization problem. The authors then prove that their invex fair lasso formulation correctly recovers the hidden attributes and finds a regression parameter vector with the same support as the true parameter vector. This is done by constructing a pair of primal and dual variables that have the desired properties and fulfill the KKT conditions. In the experimental section, the theoretical results are validated on synthetic data. Moreover, the method is evaluated on two real world datasets where the hidden attributes are recovered in both cases.",0.1875,0.14285714285714285,0.2857142857142857,0.21052631578947367,0.3508771929824561,0.23684210526315788,0.3684210526315789,0.21052631578947367,0.2689075630252101,0.15789473684210525,0.16806722689075632,0.15126050420168066,0.24852071005917156,0.1702127659574468,0.277056277056277,0.18045112781954886,0.22727272727272727,0.1846153846153846
978,SP:dbe68005bef5c6a249fade7825a1f34e60d555cc,"In this paper, the authors introduce an internal-external learning framework for style transfer. In the proposed method, the classical VGG-based style loss, which encourages the stylized result to capture the low-level statistics of the style image, is combined with a GAN loss designed to capture the style priors from the style database. In addition, contrastive losses are designed to encourage the results to better capture the stylization-to-stylization relation. Experiments in the paper demonstrate the proposed method can generate visually more appealing results compared to previous style transfer methods.","The paper proposes two new additional losses for style transfer: 1. A contrastive loss to incorporate to bring the style of the stylized images with the same style image closer to each other, while pushing the stylized images from different style images further from each other. The same for content. 2. An adversarial loss to learn the general aesthetics of stylized images over a larger dataset. The authors demonstrate that a combination of these losses plus the regular style-content loss can generate high quality, and surprisingly consistent stylized images, both qualitative and quantitatively as well as user studies.",The paper describes a method for feed-forward image stylization that combines several losses. The novel losses are a contrastive loss that optimizes the style embedding of the training image to match the embedding of a target style and content and to be distinct from other style and contents.  Other losses used during training include feature mean/variances and a GAN loss. ,"The paper proposes a neural style transfer algorithm, for transferring the artistic style of one image to another.   It is basically SANet [32] with some incremental novelties.  SANet is an encoder-decoder transformer architecture that accepts two images (content/desired style) and outputs a stylized version of the content by optimizing a Gatys-like loss function.  Two novelties are claimed over SANet.    1) the use of ‘human external information’ by which the authors mean a corpus of real artworks in the desired style, to train a GAN discriminator added to the SANet.  It is argued that adding knowledge of multiple real artworks of the desired style in this way helps produce more plausible stylizations.  2) the use of contrastive learning to train their architecture.  The idea here is to ensure pair-wise consistency between style descriptors of generated artwork and the aforementioned corpus of artwork sharing that desired style. ",0.24731182795698925,0.15053763440860216,0.25806451612903225,0.1919191919191919,0.24242424242424243,0.3064516129032258,0.23232323232323232,0.22580645161290322,0.1610738255033557,0.3064516129032258,0.1610738255033557,0.12751677852348994,0.23958333333333331,0.1806451612903226,0.19834710743801653,0.2360248447204969,0.1935483870967742,0.18009478672985785
979,SP:dc4c480012629bb7fabe6c1492b804f4e40b3078,"This work introduces a new model class combining elements from autoregressive and discrete diffusion models. The new approach is named Autoregressive Diffusion Models (ARDMs). The authors show good performance compared to alternative generative models, in particular in lossless image compression. The authors demonstrate that the new model has particular benefits compared to the autoregressive and discrete diffusion models in terms implementation efficiency, scalability and parallelization.  ","This is a very dense paper that proposes a new autoregressive modeling (ARM) approach, which enjoys several properties. The proposed approach builds on two main ideas: 1) order-agnostic ARM which they exploit to randomize over the generation of variables according to a random order picked uniformly from all possible permutations of orders, 2) denoising diffusion probabilistic models extended to work on discrete variables. Then, one contribution of this paper is to improve the computational cost of order-agnostic ARM. This stems from rewriting the log likelihood component at a given timestep $t$, and noticing that this single term can be optimized for all datapoints in a minibatch simultaneously. The parametrization of the neural network proposed in this work is also an improvement over a traditional approach to generation in an autoregressive diffusion model. Furthermore, eq (3) is key for the realization that the un-ordered generation of $k$ tokens can happen independently, because this would contribute to $k$ times the log likelihood at a given timestep $t$. Using ideas from a related work, in this paper the authors show how to exploit this fact and a dynamic programming approach to parallelize autoregressive diffusion models for a given budget.  An additional contribution is to consider the application of the proposed model to the problem of upscaling. This is modeled as a (discrete) diffusion process where the forward process is destructive (downscaling) and the backward process is constructive (upscaling). As discussed originally in the work used by the authors as a basis for discrete diffusion processes, upscaling can be defined using simple transition matrices, which define the transitions in the Markov chain supporting the process from a downscaled version of the data to an upscaled version. The authors present two variants of a parametrization approach to learn the backward distributions.  An extensive experimental campaign complements the ideas presented in this paper, with application ranging from the image to the text domains, including upscaling and lossless compression.","Autoregressive Diffusion Models (ARDM) are discrete diffusion models that extend D3PMs and generalize order agnostic auto-regressive models (OA-ARM) . Similarly to other diffusion models, training OA-ARDMs only require evaluating a single transition step (i.e $p(x_{\sigma(t:t+k)} | x_{\sigma(<t}))$, where $\sigma$ is a random permutation). In contrast to traditional ARMs, OA-ARMs are trained to generate k steps $p(x_{\sigma(t:t+k)})$ at each transition.  Furthermore, the authors supplement the OA-ARM with two additional contributions: - factoring the generative model across the variable *depth* (named *depth upscaling*), which allows unfolding the generative process into an even greater number of steps ($S \times T$ steps instead of $T$) - an algorithm to parallelize generation using dynamic programming (allows trading quality for generation speed)  The authors demonstrate the effectiveness of OA-ARM on character-level text modelling (text8), image modelling/compression (CIFAR-10).","The paper proposes Autoregressive Diffusion Models (ARDMs), which is a combination of two concepts: autoregressive models and (discrete) diffusion models. The proposed ARDMs generalize order-agnostic ARMs [1] and discrete diffusion models [2]. ARDMs are efficient in several ways: they do not require the causal dependency so that the can be trained in efficient way, and they provide the parallel generation. The authors provide proper experiments that back up their work.  [1] Uria et al., A deep and tractable density estimator, 2014. [2] Austin et al., Structured denoising diffusion models in discrete state-spaces, 2021.",0.38461538461538464,0.2153846153846154,0.2923076923076923,0.10153846153846154,0.08307692307692308,0.12751677852348994,0.07692307692307693,0.09395973154362416,0.2,0.2214765100671141,0.28421052631578947,0.2,0.1282051282051282,0.13084112149532712,0.23750000000000002,0.13924050632911394,0.1285714285714286,0.1557377049180328
980,SP:dc581119d780cdbc9a57c19f06e6fb7d0fc89f9e,"This work proposes to apply flatness-aware regularizations to learning to optimize methods. Specifically, the approximate hessian spectrum and entropy based regularizers are added to the L2O objective. They performed a theoretical analysis of the regularized L2O method and evaluated different variants of the regularizer on simple CNN and MLP models trained on CIFAR and MNIST. Similar to the effect of flatness-aware regularization on the ordinary training, the regularizer improved the learned optimizers to find minima that generalizes better.  ","This paper aims to solve the poor generalization performance of Learning to Optimize (L2O) methods by introducing flatness-aware regularizers. The idea is to add a regularization term to the meta-training objective that encourages the final iterate obtained by applying the learned optimizer, to lie in a flat region (measured by the spectral norm of the Hessian, or the local entropy function). The authors prove a bound for the generalization error, and show empirically that adding flatness-aware regularizers to existing L2O methods, can improve generalization or both the learned optimizer (across unseen tasks) and the solution (across dataset splits).","The paper proposes to use meta-learning to learn an optimizer that automatically seeks wide local minima without the need to explicitly compute the sharpness measure on the fly. When training the optimizer, the parameters of the optimizer are updated in such a way that the training loss and the sharpness measure at the end of the updated trajectory are minimized. Theoretical results were provided for the generalization guarantees of the learned optimizer under some technical conditions. Finally, the experimental results demonstrate that the sharpness-aware optimizer outperforms baselines on simple models.",The paper aims at improving the generalization of the learned optimizers. Some regularizers to induce the flatness of local minimas including Hessian spectrum and the entropy function are proposed. Both theoretical analysis and empirical results are presented to demonstrate that flatness-aware regularizers can enhance the generalization ability of optimizees.,0.2375,0.225,0.175,0.2376237623762376,0.2079207920792079,0.15217391304347827,0.18811881188118812,0.1956521739130435,0.28,0.2608695652173913,0.42,0.28,0.20994475138121546,0.20930232558139536,0.2153846153846154,0.24870466321243523,0.2781456953642384,0.1971830985915493
981,SP:dc89bf2325c7d9a281e9b029f0054d3e4bf8f1d0,"1) The main contribution of this paper is a new knowledge distillation (KD) framework called ""skeptical student"" that can circumvent the ""immunity (undistillability)"" provided by a ""nasty (defensive/secretive)"" teacher.   The concept of ""nasty"" teacher was proposed in [A] to prevent stealing of a teacher model by a malicious student. This paper proposes a countermeasure that borrows ideas from self-distillation, where knowledge is transferred to shallow subsections of the student model, each with its own auxiliary classifier (AC). The proposed skeptical student performs knowledge distillation both to the shallow sub-sections and to the full student model to overcome the impact of a nasty teacher.  [A] Ma et al., ""Undistillable: Making a nasty teacher that cannot teach students"", ICLR 2021  2) The paper also includes an algorithm for training skeptical students in a data-free KD scenario (the original data used to train the teacher model is not available).   3) Experimental results on three datasets indicate that the proposed skeptical student can learn well both from the normal and nasty teachers. ","Knowledge distillation (KD) can be used by an adversary to obtain sensitive information from a teacher model presenting privacy risks. Nasty teachers were proposed to handle this problem where students were unable to learn or distill knowledge from nasty teachers. In this paper, the authors show that skeptical students using a combination of KD to intermediate shallow subsections of their model together with Self Distillation (SD) can extract information from a nasty teacher and perform well on classification tasks.","The authors investigate the knowledge distillation immunity of Nasty teachers. A nasty teacher is a model that maintains good prediction performances but significantly degrades the performances of student models that distill knowledge from it. In this paper, the authors proposed a hybrid distillation that neutralizes the protection of nasty teachers, allowing the training of accurate student models.","The paper introduces a skeptical student distillation framework that diminishes the effect of a nasty teacher. By placing auxiliary classifiers at different depths of the student model to form a shallow subsection and introducing a new self-distillation term, the resulting skeptical student clearly outperforms a vanilla student in all data-available, data-limited, and data-free settings when learning from a nasty teacher. The proposed KD method (especially the ensembled outcome) also provides similar performance as a vanilla student when learning from a normal teacher, thus being applicable in most cases. Quantitative analysis and visualizations are provided to show that the method works as expected.  ",0.14534883720930233,0.12209302325581395,0.18023255813953487,0.20253164556962025,0.17721518987341772,0.2631578947368421,0.31645569620253167,0.3684210526315789,0.29245283018867924,0.2807017543859649,0.1320754716981132,0.14150943396226415,0.199203187250996,0.1834061135371179,0.22302158273381295,0.23529411764705882,0.15135135135135133,0.18404907975460122
982,SP:dcc5fa4e998a956ecccb598063e936bcfad21b9a,"The paper considers the functional relationship between (time-varying) learning rate & batch size and the generalization error. The contributions are summarized below: * Assuming quadratic risk function and the SDE limit, the authors derive a novel theoretical generalization bound when the (learning rate / batch size) ratio monotonically decreases. * Inspired by the generalization bound, the authors propose a heuristic functional form of the generalization error w.r.t. the (learning rate / batch size) ratio. * The authors conduct experiments on CIFAR10 and CIFAR100 datasets, and show that the heuristic generalization error model can fit the actual generalization curve relatively well. * The authors propose a novel kernel function for Bayesian optimization in hyperparameter optimization. The kernel function is inspired by the heuristic generalization error model. Empirical results show that the proposed kernel can match or outperform existing Bayes optimization baselines. ",This paper approximates the learning dynamics by an Ornstein-Uhlenbeck process and uses some PAC-Bayes results to obtain a functional form for the generalisation behaviour as a function of the ration of the learning rate to the batch size.  It obtains results for both constant and time-varying parameters.  It then uses these to develop a kernel function for hyper-parameter optimisation using Gaussian Processes modelling.,"The authors study the dependence of generalization error of trained neural networks on the batch size and the learning rate used in SGD. The primary motivation is to extend the previous works to the case where batch size and learning rate change over the trajectory. The authors present a functional form using PAC-based generalization bounds to model the desired dependence. They conduct extensive experimentation to show that the functional form approximates the generalization error well. In addition, the authors show that a hyperparameter search based on the proposed model outperforms existing hyperparameter optimization libraries like Hyperopt and Optuna.","The authors study the learning rate over batch size dependence of generalization bounds and try to use them for hyperparameter tuning.  In section 4, the authors study generalization bounds for convex optimization with the motivation that this helps understand neural networks. In section 5, the authors use the bounds to propose a functional approximation for the generalization based as a function of the n=eta/|S| scaling of the fluctuations around the minimum, including for the possibility that n is time dependent. The generalization error is roughly a sum of integer powers of sqrt(n). In section 6, the authors the authors fit a set of points to the generalization formula they found. In section 7, the authors use this expression to look for optimal hyperparameters and compare them with other hyperparameter tuning methods.",0.13970588235294118,0.25,0.23529411764705882,0.208955223880597,0.29850746268656714,0.2727272727272727,0.2835820895522388,0.3434343434343434,0.23880597014925373,0.1414141414141414,0.14925373134328357,0.20149253731343283,0.18719211822660098,0.28936170212765955,0.23703703703703705,0.16867469879518074,0.1990049751243781,0.23175965665236048
983,SP:dcca662eced6f8a737a12fed540c02e203569408,The paper uses a method inspired by column generation to continuously add temporal logic rules to their model to maximize the likelihood of the data. The goal is to generate a model that is highly interpretable. They compare against state of the art baselines and get good improvements.,"This paper proposed a model for learning temporal logical rules for the temporal point process. Specifically, the key idea is to formulate the rule learning as a restricted master problem and gradually optimize the objective function, which can significantly lower down the search space. The authors conduct extensive experiments on both synthetic data and real data.  ","This paper builds upon the work of [1] and proposes an algorithm to simultaneously estimate the logic rules and the model parameters.  The estimation problem is formulated as the contained optimization problem (6), with both continuous and discrete parameters to optimize over. The proposed TELLER algorithm divided the original problem into a sequence of Restricted master problems and gradually expand the search space for a global optimum.  Subjective constraints and expert opinions can be incorporated into the search algorithm as well.   [1] Shuang Li, Lu Wang, Ruizhi Zhang, Xiaofu Chang, Xuqin Liu, Yao Xie, Yuan Qi, and Le Song. Temporal logic point processes. In International Conference on Machine Learning, pp. 5990–6000. PMLR, 2020.","This paper introduces TELLER, a framework with novel algorithms based on the temporal point process model to discover interpretable temporal logic rules. To this end, the authors designed a “rule generation - evaluation” two-stage way to solve the problem efficiently. The proposed method is evaluated using one synthetic and two real data sets. Experts have also verified that the learned temporal logic rules are interpretable and reflect many critical principles in real practices.",0.22916666666666666,0.25,0.2916666666666667,0.32142857142857145,0.2857142857142857,0.14035087719298245,0.19642857142857142,0.10526315789473684,0.1917808219178082,0.15789473684210525,0.2191780821917808,0.2191780821917808,0.21153846153846154,0.14814814814814814,0.23140495867768596,0.21176470588235294,0.24806201550387597,0.1711229946524064
984,SP:ddadd261284967ba675b4a6a3352c4e02698f773,"The authors in this paper study the problem of estimating the trace of a matrix that is changing dynamically over several iterations. Under the assumption that the diagonal entries of the matrix are not directly accessible, but multiplication with a vector $Av$ (or the quadratic form $v^{\top}Av$), the Hutchinston estimator is the baseline algorithm for comparison. The Hutchinston estimator takes the average of $\epsilon^{-2} \log 1/\delta,$ estimates $g_i^{\top} A g_i$ where $g_i$ is a random vector with iid gaussian variables, or Rademacher ($\pm 1$) variables, and is correct with probability $1-\delta.$  The model is that we have a sequence of matrices $A_1, \ldots, A_m$ each with $\|| A_i \|| \le 1$ and $\||A_{i+1} - A_{i} \|| \le \alpha,$ (where the matrix norm used is the Frobenius norm) and we wish to output estimates of $tr(A_1),\ldots,tr(A_m)$ such that each estimate is accurate to $\pm \epsilon.$ Using a Hutchinston estimator at each iteration will require $m \epsilon^{-2} \log m/\delta$  where all the answers are accurate with probability $1-\delta.$  The paper presents an algorithm that is able to maintain accurate estimates for such a dynamically changing $A$ using only $(m \alpha + 1 )\epsilon^{-2} \log m/\delta,$ which is essentially better by a factor of $\alpha.$ The authors also show a lower bound claiming to show that this method cannot be improved unless the original Hutchinston method is improved.  The algorithms extend their algorithm to the case where you have a stronger nuclear norm bound assumption, and present numerical simulations demonstrating their advantages.","This paper studies dynamic setting of trace estimation problem; compute the trace of a sequence of matrices efficiently. The authors consider the differences of consecutive matrices are small and utilize the linearity of trace and Hutchinson estimator, which returns trace values with only matrix-vector multiplications. In addition, in order to reduce the variance of trace estimations, they adopt a damping strategy; it controls the scale of the input matrix to estimate and reduces its variance appropriately. They propose a closed form of the optimal damping factor. Experimental results show the superiority of the proposed algorithm with various problems of trace estimation under both synthetic and real-world datasets.","The authors proposed a method for estimating the trace for a sequence of matrices that only changes for a small amount at each step. The method originates from the well-known stochastic trace estimator, but bring it to a new practical setting (e.g. Hessian at learning iterations, evolving network, etc). By an interesting treatment of the iterative process (i.e. damping), the resulting algorithm could cut the computational cost by a factor of the size of the tolerable error. In addition, the authors included the recent development/improvement on the original Hutchinson estimator, to get better versions of their algorithm under more assumptions.","This work considers the problem of dynamic trace estimation. Specifically, if there is a sequence of matrices $A_1, \dots, A_n$, and the norm of $A_i - A_{i-1}$ is small relative to the norm of $A_i$, we can estimate the trace of each matrix efficiently. The paper gives an efficient algorithm, the convergence analysis and conducts relavant experiments. ",0.1111111111111111,0.1111111111111111,0.11481481481481481,0.2018348623853211,0.1926605504587156,0.15384615384615385,0.27522935779816515,0.28846153846153844,0.5,0.21153846153846154,0.3387096774193548,0.25806451612903225,0.15831134564643798,0.16042780748663102,0.18674698795180722,0.20657276995305163,0.2456140350877193,0.1927710843373494
985,SP:ddc3de66ff4a6af7825fd96678da46af7a5f00b1,"In this paper, the authors study the minimax problem for finding the mixed Nash equilibrium for mean-field two-player zero-sum games. This is an inf-sup problem in the Wasserstein space. In this paper, the authors consider the case where the sup problem is solved and apply the gradient descent flow for the result inf sup functional. Some theoretical analyses are conducted on these dynamics. They show its convergence to the mixed Nash equilibrium under some conditions. Some numerical schemes for the quasistatic Langevin gradient descent method with inner-outer iterations are presented. They show that the method works on training the mixture of GANs. ","This paper studies a minimax problem arising from finding the mixed Nash equilibrium for mean-field two player games. A quasi-static Wasserstein gradient flow is proposed to solve such optimization problems. By discretizing this flow, a simple while practical Langevin gradient descent algorithm is proposed.","The paper under review studies the QuasiStatic Wasserstein Gradient Flow and its application to solve two-player zero-sum games. The corresponding Langevin SDE is derived for the Fokker-Planck equation, and the discretized algorithm (GD) is proposed in this new setting.  Example of the min-max problem on 1-d torus and spheres are provided. ","This paper focuses on the methodology of finding mixed Nash equilibrium. The authors first introduces the corresponding quasi static Wasserstein gradient flow dynamics for solving the problem, which is a limiting dynamics with $q$ seen as infinitely faster than $p$. Then the continuous dynamics is proved to be convergent under mild assumptions. Furthermore, by discretizing the above gradient flow, the authors proposes a practical algorithm for solving min-max optimization problems with GAN as a special example. Experimental results shows the superiority over the vanilla LGDA algorithm especially with large $\beta$’s. ",0.205607476635514,0.14018691588785046,0.205607476635514,0.2608695652173913,0.41304347826086957,0.21428571428571427,0.4782608695652174,0.26785714285714285,0.2391304347826087,0.21428571428571427,0.20652173913043478,0.13043478260869565,0.2875816993464052,0.18404907975460122,0.22110552763819097,0.23529411764705882,0.2753623188405797,0.16216216216216214
986,SP:dddd55812f5134b3282fbba34c7548743a929b8b,"The authors propose a knowledge distillation algorithm for out-of-domain data, where local similarity between different domains is leveraged to transfer knowledge across models. In particular, the authors first present a basic method based on distributionally robust optimization and discuss the its problem in OOD settings. Then the authors improve the basic method by introducing a local loss and an aligning loss. The proposed method, named MosaicKD aims at synthesizing locally-authentic and globally-legitimate samples for KD. The authors demonstrate that MosaicKD can be applied to various OOD data and outperforms several data-free and data-driven methods (e.g. ZSKT, DFQ and CRD)","This paper considers a new task, knowledge distillation using out-of-domain data, and proposed a novel method, MosaicKD. Leveraging the shared common local patterns, MoscaiKD is able to effectively learn from the OOD data.  The paper is well organized and presented, and experimental results demonstrated the effectiveness of the proposed solution. Even I have some questions about the experiments, the quality of the paper is high.","This paper proposes a method for knowledge distillation with out-of-domain data (OOD-KD). The key idea is to dismantle out-of-domain images into patches and assemble in-domain data by training a generative adversarial network. It is based on observation that different domain may still share common local patterns from which new (in-domain) categories can be re-assembled. The proposed method is developed undert the framework of the so called Distributionally Robust Optimization (DRO), restricted by a ball space centered at the OOD patch distribution.","This paper studies knowledge distillation in out-of-domain settings and proposes a generative method called MosaicKD to handle the agnostic domain gap between OOD transfer set and original training data. The core idea of MosaicKD lies in the observation that different domain may still share some local patterns, which can be re-assembled for cross-domain synthesis. The author introduces a four-player adversarial training method to learn a locally-authentic and globally-legitimate distribution for knowledge distillation.",0.16037735849056603,0.19811320754716982,0.18867924528301888,0.31343283582089554,0.29850746268656714,0.33707865168539325,0.2537313432835821,0.23595505617977527,0.25316455696202533,0.23595505617977527,0.25316455696202533,0.379746835443038,0.19653179190751444,0.2153846153846154,0.21621621621621626,0.2692307692307692,0.273972602739726,0.35714285714285715
987,SP:de6042132614d46342646d95996d24eeaf793fb3,"By pointing out the recent transformer-based models are mostly designed for visual-language data, this paper introduce TriBERT which specifically targeting on audio-visual modalities to address such limitation. Inspired by ViLBERT, TriBERT is a transformer based model which enables contextual feature learning across three modalities: vision, pose and audio with the use of flexible co-attention. Besides, this paper introduce a learned visual tokenization scheme based on spatial attention and leverage weak-supervision to allow granular cross-modal interactions for visual and pose modalities. To demonstrate the proposed approach, TriBERT is pretrained on MUSIC221 and show improved performance in audio-visual sound source separation and other audio-visual-pose retrieval. ","The goal of this work is to learn human-centric audio-visual representations. The authors claim to be first in using transformer models in the context of granular audio-visual detection or segmentation tasks (e.g. sound source separation). The authors propose TriBERT, which is a three-stream model with tri-modal attention to generate generic representaitons for AV tasks. The three modalities are audio, video and pose (keypoints). The model is trained with weak supervision (video level labels). The model gives state-of-the-art results on sound source separation.","This work proposes TriBERT to handle the modalities of RGB image, poses and audios for tackling the task of sound source separation. The BERT model is used to extract features following ViLBERT. Experiments show appealing results on sound separation metrics compared with the author-reproduced previous SOTA methods.","The paper presents TriBERT a BERT-like model for learning multi-model representations of three modalities simultaneously, namely: vision, pose and audio. The architecture of TriBERT takes inspiration from the visual-linguistic model VilBERT (notably known for the applications in VQA) in terms of its Transformer architecture and of its multi-modal attention (the latter is extended in the paper to 3 modalities instead of 2).  TriBERT training comprises four losses to optimize. In particular, after the projection of the 3 unimodal feature sequences into the joint feature space by the multimodal transformer, there are 3 sequences of features, namely: <h_v>, <h_p> and <h_a> for respectively enriched visual, pose and audio features. Then, the <h_v> and <h_p> features are (independently) trained to localize and predict the sound source (the musical instrument(s) which is (are) played in the recording) using 2 (independent) BCE losses. It should be noted that the localization part is learnt in a weakly-supervised manner as only global instrument classification annotations are available in the training data. The <h_a> audio features are trained to classify the sound source (the 3rd BCE loss).  Finally, the 4th supervision task is the multi-modal sound source separation one. The original audio spectrogram is mixed with other audio spectrogram(s) and the task consists in predicting a spectrogram mask which would separate the original audio from the others (the 4th BCE loss). Following [37], for this last task, a U-Net neural architecture is used. U-Net takes the mixed audio spectrogram at its input and the fusion of <h_v>, <h_p> and <h_a> at its bottleneck representation to predict the desired spectrogram mask.  Once trained, TriBERT is evaluated on the MUSIC21 dataset for the sound separation task, and it also fine-tuned and evaluated for the multi-modal retrieval downstream task. ",0.1875,0.10714285714285714,0.2857142857142857,0.17582417582417584,0.31868131868131866,0.3958333333333333,0.23076923076923078,0.25,0.1032258064516129,0.3333333333333333,0.0935483870967742,0.06129032258064516,0.20689655172413793,0.15,0.15165876777251186,0.2302158273381295,0.14463840399002495,0.10614525139664804
988,SP:de7ad01e46c62de79db5beab872ecf46bdca79d7,"This paper presents a novel method for automatically decaying the learning rate when training deep neural networks. The core idea relies on the observation that the weight norms often times bounce up during training, and that decaying the learning rate after the bouncing is beneficial. The authors conducted extensive experiments in different settings, highlighting the effectiveness of the proposed LR decay schedule, as well as the special cases where weight bouncing doesn't happen and simple decay schedules work competitively.  Overall, I think this paper shows intriguing findings, and opens potential new directions for further understanding the training and generalization of deep neural networks. However, there are also questions that I hope the authors can answer in the rebuttal period, in order to further improve this paper. ","This work deals with the optimization of learning rate schedules for deep learning. It offers three main contributions   1. An equation relating the change in weight norm to the gradient norm, learning rate, weight decay, and the weight norm--- for weights that go into batch-norm. This equation provides intuition for the behaviour of this weight norm.   2. An algorithm and code for adjusting the learning rate by reducing it by a factor once the norm of the weights reach a maximum, then reducing it again by the same factor after about 85% of the total training time. The algorithm is benchmarked on about 14 learning tasks, within vision, RL, and NLP and shows similar results when compared to other complex learning rate schedulers algorithms. Being adaptive, their algorithm has at least one less hyperparameter compared to the competition and appears more robust.   3. Empirical tests of a conjecture that complex learning rates are only beneficial in the presence of extremums in the overall weight norms.    ","The paper proposes a simple method to decay the learning rate automatically by tracking the weight norm of deep neural networks. A key observation from this work is that: there is a connection between weight norm bouncing, weight decay and optimizer convergence. The authors indicate that complex learning only benefits the training when weight norm bouncing exists, which further relies the presence of $l_2$ regularization on the weight. They also indicate that for those experiments without weight norm bouncing, like experiments in NLP and RL, the learning rate schedules could be simplified with no loss in performance.  Based on that, the paper proposes a new learning rate scheduler named ABEL. Basically, ABEL will drop the learning rate when the weight norm starts to bouncing. More specifically, when the weight norm growth slows down, the learning rate will be dropped by a pre-defined dropping factor. ","This paper presents an empirical study of the dynamics of the total norm of the weights of neural network models in relation to the learning rate scheduling, focusing on what the authors call the ""bouncing effect"".   Based on empirical findings, they propose a method called ABEL to automatically decay the learning rate in a step-wise fashion. Essentially ABEL decays the learning rate after registering two changes in the direction in which the weight norm is progressing. The authors assume that the first change occurs at a local minimum, while the subsequent one indicates that the noise prevails the signal.  ABEL further decays the learning rate one last time at ~85% of the total training epochs. They show that the cosine annealing and (manually) tuned step-wise schedules are beneficial when the bouncing effect happens, while in the absence of it, they claim that using a constant learning rate for most of the training epochs with a final decay is sufficient to obtain good performances. The paper concludes with a section dedicated to understanding the bouncing effect which recalls some recent works that study the weight dynamics of neural nets.",0.2283464566929134,0.2283464566929134,0.2440944881889764,0.18072289156626506,0.21686746987951808,0.2465753424657534,0.1746987951807229,0.19863013698630136,0.1631578947368421,0.2054794520547945,0.18947368421052632,0.18947368421052632,0.19795221843003416,0.21245421245421245,0.19558359621451105,0.19230769230769232,0.20224719101123598,0.21428571428571427
989,SP:e08c089f17d0bf90c67d0fa4aafd3fccdb4545ff,"Proposed a method for disentangling a set of partially labeled factors while also capturing all unlabelled factors. As an application, CLIP is used to annotate a subset of attributes in natural images. Disentanglement is evaluated using the proxy task of image manipulation. ","The paper investigates the task of disentanglement under partially labelled data. The setting is well-motivated as new general-purpose models like CLIP emerge, which can be used to provide partial labelling.  The proposed formulation is intuitive, utilizes supervised training on the labelled attributes, an entropy regularizer on the unlabelled ones, and a reconstruction loss. Experiment results on standard datasets (e.g., Shapes3D, dSprites) confirm the effectiveness of the method. The authors extend the method to a zero-shot setting, where partial labels are obtained via CLIP latent similarity matching. Qualitative results on real-image datasets (e.g., FFHQ, AFHQ, cars) show the advantage over various baselines, including StyleGAN-based methods.","The paper presents a new approach for disentangling attributes from images in a semi-supervised setting, where attributes are only available for a subset of images, and only a subset of attributes are observed. The basic idea is to use as a latent space output from a classifier trained on images, along with a residual latent space and use both of them to reconstruct an image. Inference is done either via direct optimization with respect to the learnt generator or by fitting a second-stage inference network using the learnt generator. The empirical studies appear to show that the proposed method improves over previous methods [29] and [14]. ","This paper proposes an encoder-decoder based disentanglement method that applies to the semi-supervised and weakly-supervised case (i.e., only a subset of attributes are partially annotated). To adapt the proposed method to real images where the partial labels are unavailable in general, this paper proposes a method of using CLIP to annotate the unlabeled real images, and the resulting image manipulation method is termed ZeroDIM.   Experiments are conducted on synthetic datasets (Shapes3D, Cars3D, dSprites and SmallNORB) to show better performance on disentanglement learning, and experiments are conducted on real datasets (FFHQ, AFHQ and Cars) to show better image manipulation results.  ",0.23809523809523808,0.30952380952380953,0.2619047619047619,0.16216216216216217,0.2072072072072072,0.19444444444444445,0.09009009009009009,0.12037037037037036,0.10679611650485436,0.16666666666666666,0.22330097087378642,0.20388349514563106,0.130718954248366,0.17333333333333334,0.15172413793103448,0.16438356164383564,0.21495327102803738,0.1990521327014218
990,SP:e09ed39fb96867d0f6c6e239eb0f7bdcc87874db,"This paper develops a randomized primal-dual algorithm for solving linearly constrained problems with a nonsmooth nonconvex objective and convex constraint set. Specifically, the objective is composed of a smooth nonconvex term and a weakly convex and hence nonsmooth nonconvex term. The authors show that the limit points of the algorithm are stationary points. The authors then prove almost sure asymptotic rate of $1O(/\sqrt{k})$ and complexity of $O(\epsilon^{-2})$ for obtaining an $\epsilon$-stationary point, in expectation. Some preliminary empirical results are included. ","The paper proposes a randomized primal-dual method for solving the nonconvex nonsmooth optimization problem with linear constraints. The main analysis relies on the idea of using auxiliary problems and the Lyapunov function proposed in  (Zhang & Luo, 2020b).  The paper shows that any cluster point of the generated sequence is almost surely a stationary point of the nonconvex nonsmooth problem and also provides an almost sure asymptotic convergence rate together with the expected iteration complexity of the proposed method.  ","This paper studies a particular type of nonconvex nonsmooth problem. The authors propose to use the Bertsekas' convexification procedure on the nonconvex problem, and then use randomized coordinate method to solve the corresponding augmented Lagrangian. The paper also proposes a so-called surrogate stationarity measure to establish the convergence rate for the proposed method.","This paper proposes a randomized primal-dual coordinate method for solving linearly constrained nonsmooth nonconvex optimization problems. At each iteration, this method only selects a primal variable to update randomly. The proposed algorithm can solve large-scale problems since the computational complexity in each iteration is very cheap compared with the full gradient methods. The authors prove that any cluster point of the sequence of iterates is almost surely a stationary point. Some experiments on the non-PSD kernel SVM problem and the linearly constrained Lasso problem show that it is practical.    ",0.26744186046511625,0.1511627906976744,0.27906976744186046,0.24050632911392406,0.4177215189873418,0.24074074074074073,0.2911392405063291,0.24074074074074073,0.2608695652173913,0.35185185185185186,0.358695652173913,0.14130434782608695,0.27878787878787875,0.18571428571428572,0.2696629213483146,0.28571428571428575,0.38596491228070173,0.1780821917808219
991,SP:e17c793ac8378edb8a964c39b6e5ff024eeb3f83,"The authors consider label differential privacy (DP) where the goal is to satisfy DP with respect to the labels of the examples. This model was formally introduced by Chaudhuri and Hsu (COLT 2011).  They propose two models for label DP:  1) Based on ALIBI (Additive Laplace noIse coupled with Bayesian Inference)  2) Based on PATE (Private Aggregation of Teacher Ensembles)  A prominent example where label DP is quite useful is in online advertising where the goal is to, given a user's profile, predict the advert conversion probability (the label). If we assume that the training is done by the advertising network (e.g., a tech company) who has the data (the features), the goal is to make sure the advertiser (some company who wishes to advertise on a tech platform) cannot infer the labels.  They estimate the *empirical* privacy loss of the trained models via the lens of black-box membership inference attacks.  The novelty of this work, it seems, lies in the (previously unexplored) connections between PATE and label DP. Ghazi et al. also proposed randomized response mechanisms for label privacy.  They experimentally evaluate the performance of their algorithms on CIFAR-10, CIFAR-100 where they show medium to high accuracy results (e.g., 93.7% on $\epsilon = 1.6$).  "," The paper considers the label DP setting and proposes ALIBI and PATE-FM  to achieve Label DP. Their ALIBI approach combines Laplace noise and Bayesian inference and improves over the prior work. Moreover, they introduce a memorization attack in the Label DP scenario.  ","This paper focuses on deep learning with level differential privacy (DP), in which the privacy of only the label is preserved. The authors propose two new algorithms: a variation of PATE using a recent semi-supervised learning approach and the fact that features are public in label DP to train better teacher models and use fewer student model queries which consume DP budget ; and a new approach based on local DP noise for the labels, and a Bayesian post-processing that uses the current version of the model to refine guesses about the true underlying label. Finally, the authors run a label memorization attack to empirically assess the privacy. ","The paper studies the problem of supervised learning under label privacy, which only imposes differential privacy constraints on the labels of training samples. The paper proposes two algorithms based on different privatization techniques. (1) PATE-FM: The first algorithm divides samples into $K$ equal subsets and trains $K$ non-private teacher classifiers using FixMatch, a semi-supervised learning technique with the unlabeled features from all $n$ samples and labels of samples from the corresponding subset. Then the algorithm trains a private student network using the PATE framework, which is based on private processing of predictions from teacher classifiers. (2) ALIBI: The algorithm randomly perturbs the label of each sample by adding Laplace noise onto the one-hot encoding of the labels. After normalizing the noisy vector, the private soft labels can be then used for subsequent training.  The paper then performs experiments to evaluate the privacy/accuracy tradeoff for both methods and the LP-2ST algorithm proposed in reference [17]. The paper also performs label inference attacks to estimate the empirical privacy for both methods.",0.08018867924528301,0.11320754716981132,0.1650943396226415,0.3488372093023256,0.3488372093023256,0.3119266055045872,0.3953488372093023,0.22018348623853212,0.2,0.13761467889908258,0.08571428571428572,0.19428571428571428,0.13333333333333333,0.14953271028037385,0.18087855297157623,0.19736842105263158,0.13761467889908258,0.23943661971830985
992,SP:e191b0d1e365f2e9bc1da30983dc2bb4f81aae96,"Given samples from two distirbutions, how can we estimate the KL divergence between them? More generally, how can we estimate the ratio of the two densities at given points (e.g, on the samples from one of the dists)? The authors propose a novel method for density ratio estimation (DRE). DRE is challenging when the two distributions have different supports or the density ratio is very high. The authors propose a solution that builds on the previous work, namely Rhodes et al..,  which instead of computing the ratio directly, it writes it as multiplication of a series of density ratios between some intermediate distributions (telescoping).   The authors improve over Rhodes et al. by accounting for the distribution shift in a systematic way and also slightly changing the way the intermediate distributions are constructed. The empirical results are promising and show the benefit of the approach compared to the baselines. ","This paper proposed to estimate the (log) density ratio, $\log [p/q]$ by introducing an intermediate density $m$, and rewrite it as $\log [p/q] = \log [p/m] - \log [q/m]$. The authors then estimate each ratio by a generative classifier (logistic regression). Some simulations were conducted to argue the benefit of the proposed method.","This work estimates the ratio of two densities using intermediate densities that have sufficient overlap with the target densities. The proposed method is based on the scaled-Bregman Divergence and it can be equivalently formulated as a multi-class logistic regression problem. By combining the idea of telescoping, the proposed approach mitigate the issue of FOD/HOD density-chasm and show superior performance on a variety of datasets.","This paper addresses one of the essential tasks in machine learning, the problem of density ratio estimation (DRE). In this paper, the authors consider applying the binary classification method to DRE. When the samples are well separated, this problem becomes difficult. To tackle this problem, this paper proposes a framework called scaled Bregman divergence. Under this framework, the density ratio can be estimated successfully. The authors also present some applications in experiments.",0.10067114093959731,0.14093959731543623,0.12080536912751678,0.2545454545454545,0.18181818181818182,0.19117647058823528,0.2727272727272727,0.3088235294117647,0.25,0.20588235294117646,0.1388888888888889,0.18055555555555555,0.14705882352941177,0.1935483870967742,0.16289592760180996,0.22764227642276422,0.15748031496062995,0.1857142857142857
993,SP:e1a61b5e4f525a5218a8ac80875f24fce1fc62e3,"The paper introduces UnO, a framework for estimating confidence bounds for many parameters of the reward in an off-policy setting and with finite state-action spaces. In this work, first an estimator for the CDF of the return is provided, which makes use of an importance sampling correction. From this estimator, it is easy to derive estimators for many useful parameters. Although these estimators are in general biased, it is possible to derive high confidence bounds for any of these parameters.","Follow-up: I check the paper again and now I am persuaded that the proof can be extended to the reward with continuous distribution. There seems no fundamental technical barrier to me. So I deem this issue as resolved and add 1 point. However, my concern regarding the significance of the paper remains unchanged. The estimator proposed in the paper is based on the importance ratio and estimated CDF which is not novel. I also do NOT think the universality of such an estimator is a big contribution. (It is quite obvious that universality can follow from such estimators.) Many significant previous works on OPE have been focused on proposing estimators with important properties, such as doubly robustness (e.g. https://arxiv.org/abs/1511.03722), free of curse of horizon (e.g. https://arxiv.org/pdf/1810.12429.pdf), etc. So there are many other properties that is more meaningful and applicable than a mere universality. An estimator that is only universal but does not come with any other desired and important properties is not an interesting estimator and will be of limited use in practice.  Therefore, I think the universality claimed by this paper is not interesting and significant enough. I feel this paper's significance is a little overstated.   ============================ This paper studied the OPE problem for sequential decision-making and proposed an estimator that estimates and bounds the entire distribution of returns. The method is model-free and can obtain high confidence bounds for the parameters of the distribution. The method works under different settings of MDP: fully observable MDP vs POMDP; Markovian vs non-Markovian. ","The paper proposes an algorithm for estimating any parameter of the distribution of returns in an off-policy manner, and derives confidence intervals for the quantities of interest. The paper also shows how the proposed algorithm can be extended to deal with many issues commonly faced when applying RL to real world problems (nonstationarity, non-markovian observations, etc.).","This paper proposes UnO, which is an off-policy evaluation method that estimates any distributional parameter (mean, variance, etc.) simultaneously by estimating the entire distribution of returns and it provides high-confidence bounds for these parameters. Moreover, UnO can be used in settings with partial-observability, non-Markovianity, non-stationarity, and distribution shifts.",0.32926829268292684,0.21951219512195122,0.17073170731707318,0.06691449814126393,0.07806691449814127,0.27586206896551724,0.10037174721189591,0.3103448275862069,0.2641509433962264,0.3103448275862069,0.39622641509433965,0.3018867924528302,0.15384615384615385,0.2571428571428571,0.20740740740740743,0.11009174311926605,0.13043478260869565,0.28828828828828823
994,SP:e1ca7e6a8ef1a6841efee9d177dc3b2e0c30a95c,"Paper presents empirical analysis Multi-headed Self-Attention (MSA) as part of Vision Transformer (ViT) and its variants. It identifies the following properties of MSA: (1) ViT with MSA improves both accuracy and generalization by flattening the loss landscape; (2) ViT with MSA behaves differently from Convolutional Neural Nets (CNNs). Specifically, ViT with MSA behaves as a low-pass and CNNs as high-pass filters. And (3) multi-stage neural nets behave like small independent models connected in series. Based on these observations, paper proposes a new architecture -- AlterNet, where CNN block is combined with MSA block and this ""base"" structure is replicated to produce deeper models. AlterNet is shown to outperform pure CNNs in both large and small data regimes. ","The authors show that the robustness and better performance of ViTs is attributed to their property of flattening the loss surface. Insights are taken from a parallel ICLR submission, where spatial smoothing has been shown to smooth loss landscape, and help improve robustness. The paper shows that the properties of spatial smoothing are exhibited by MSA as well.  ",This paper attempts to explain the behaviour of multi-headed self-attention in vision transformers using a series of empirical observation. Some main observation are that the poor performance of ViT in the small data regime is not due to overfitting but to issues induced into the optimization procedure and that the behaviour of MSA is somewhat complementary to convolutions. Based on the observations a new model is proposed that combines convolution and MSAs. The experiments show that the new architecture improves results on smaller dataset.,"This paper analyzes how vision transformers (and in contrast convolutional architectures) work from an optimization standpoints. The paper shows experimentally that contrary to popular belief ViTs do not directly benefit from their more expressive underlying representation, but instead smooth the loss landscape which aids training. The paper then presents a hybrid CNN-ViT architecture that combines some of the benefits of ViTs with advantages of CNNs.",0.09090909090909091,0.15702479338842976,0.09917355371900827,0.2413793103448276,0.20689655172413793,0.16279069767441862,0.1896551724137931,0.22093023255813954,0.18181818181818182,0.16279069767441862,0.18181818181818182,0.21212121212121213,0.1229050279329609,0.18357487922705315,0.12834224598930483,0.19444444444444448,0.1935483870967742,0.1842105263157895
995,SP:e1f0dfa05e32423b80306017aa6219a0bda27b7b,"The authors present a generator for constrained CAD sketches that can also be used to convert hand-drawn line drawings to constrained CAD sketches or to auto-complete partial sketches. Two transformer models are used, the first model generates CAD primitives, optionally conditioned on a line drawing image or on a partial CAD sketch, and the second model generates constraints conditioned on the generated primitives. The authors show good qualitative results and quantitatively improved CAD generation performance over two simple baselines.","The work builds on the SketchGraphs dataset that contains CAD sketches, consisting of primitives and constraints. This dataset opened many opportunities to tackle research problems that could not be easily be tackled before. One important question is how to define a generative model that can unconditionally sample from the distribution. Another important question is how to sample conditioned on some input, e.g. a hand drawn sketch. This paper looks at both of these questions. ","This paper proposes a new deep generative model for 2D CAD sketches. The transformer-based model is able to generate both the primitives as well as the primitive constraints that make up a sketch, and its output can be then imported into standard CAD software. In addition to unconstrained sketch generation, the authors demonstrate experiments where they use the model to automatically complete partial sketches as well as produce sketches that resemble a rough human-drawn sketch that is given as input.","This paper presents a model capable of generating CAD sketches with constraints. Such sketches are typically formed as a sequence of 2D primitives (points, lines, arcs, circles) related by geometric constraints (parallelism, orthogonality, coincidence, etc). The model generates these sequences in an auto-regressive fashion, one primitive at a time, and then one constraint at a time. The model is trained on a large dataset of CAD sketches [Seff et al. 2020] that contains the necessary ordered sequences of CAD primitives and constraints.  The model is implemented as two Transformers, one to generate the sequence of primitive, and one to generate the sequence of constraints conditioned on the primitives. For the second part, a PointerNetwork mechanism is used to identify the primitives referenced by each constraint.   The model is used either to generate random CAD sketches that exhibit similar statistics as the training data (as measured by the number of primitives and the number of constraints used), or to auto-complete a partial CAD sketch by predicting missing primitives and constraints. In addition, an image-conditioned version of the model is also described, which can take as input an approximate bitmap drawing and turn it into a constrained CAD sketch.",0.13580246913580246,0.1728395061728395,0.2962962962962963,0.16,0.28,0.35365853658536583,0.14666666666666667,0.17073170731707318,0.12,0.14634146341463414,0.105,0.145,0.14102564102564102,0.17177914110429446,0.1708185053380783,0.15286624203821655,0.15272727272727274,0.20567375886524822
996,SP:e24cb974888eded52d8b6f3f295438d75c5a9643,"Within the now common pre-train/fine-tune training paradigm, this paper provide an empirical examination of the influence of pre-training on out-of-distribution accuracy *after* fine-tuning, specifically for computer vision. The authors showcase evidence suggesting that the choice of the pre-training dataset, the size of the model and the fine-tuning learning rate directly influence downstream OOD accuracy throughout a series of tasks. In addition, they provide some analysis of the effect that various training techniques (eg. label smoothing, augmentation etc..) have on OOD accuracy when they are used at fine-tuning time.","This paper provides a thorough and relatively in-depth analysis of how pre-trained models affect OOD generalization performance. Specifically, the authors dissect this issue and perform extensive ablations on learning rates, model architecture, dataset size, model size, etc. By studying large-scale pretraining on vision tasks, the paper claims that 1) pre-trained models typically have better OOD generalization performance (which prior works have somewhat discussed as well); 2) small learning rates during the fine-tuning phase tend to create more robust models; and 3) some of the previous understandings (e.g., of how ID and OOD accuracies are related; or of the effect of training data size) might need to be revisited.","This paper empirically studies simple techniques to improve out-of-domain (OOD) generalization. First, they show that fine-tuning a pretrained model significantly improves OOD accuracy over the non-fine-tuned baseline. Then, they show that fine-tuning with smaller learning rates outperforms larger ones on OOD data, even though both get approximately identical in-domain (ID) accuracy. Finally, the conclude that larger models, larger datasets, or (preferably) both improve OOD generalization.","This paper empirically studies pre-trained model on OOD generalization. Specifically, the authors consider four datasets in this paper: PACS, Office-Home, VLCS and TerraIncognita. Based on the experimental results, the authors concludes three main observations . First,  larger models and larger datasets need to be simultaneously leveraged to improve OOD performance. Second, using smaller learning rates during fine-tuning is critical to achieving good results. Third, the strategies that improve in-distribution accuracy may lead to poor OOD performance. ",0.1836734693877551,0.16326530612244897,0.15306122448979592,0.12280701754385964,0.21052631578947367,0.20833333333333334,0.15789473684210525,0.2222222222222222,0.189873417721519,0.19444444444444445,0.3037974683544304,0.189873417721519,0.16981132075471697,0.18823529411764706,0.1694915254237288,0.15053763440860213,0.2487046632124352,0.1986754966887417
997,SP:e2d3d274733aa1b1d28499f7392b1392c7a60365,"The authors proposed an algorithm called impression learning as a mechanism for the brain to learn the generative model of sensory inputs and meanwhile performs probabilistic inference of this model. The authors proposed a local synaptic learning rule to learn the generative model, and considered sampling-based inference. It is a valuable contribution to our understanding the general principle of neural coding.","The authors are interested in understanding how brain circuits are able to perform unsupervised learning. The work is an extension of the wake-sleep algorithm, where the algorithm is asked to perform two functions: i) infer latent variables from observations and ii) build a generative model of the received observations. They start by defining a loss function to be minimized for their system to perform both functions and then interpret this minimization as a plasticity rule between the weights of a network of neurons. Their learning algorithm is compared to other existing one in the literature, and its efficiency is probed on a real-world task.  ","In this paper the authors introduce a framework for approximate online Bayesian inference in an unsupervised setting. This framework generalises the wake-sleep algorithm and enables the derivation of a local learning rule. The generative and inference components of the model are mapped onto distal and basal dendrites, respectively. A number of experiments are conducted to demonstrate the model behaviour using both artificial (toy) and naturalistic tasks.","This work combines probabilistic learning in the tradition of the Helmholtz machine with the recent hypothetical ideas about apical dendrites carrying top-down learning signals (expectation) to implement BP using local learning rule promoted earlier by numerous authors such as Larkum,  Lillicrap and Richards.  There are innovations. The impression learning mechanism proposed does sleep-wake in real-time.  It avoids the offline sleep phase in learning by using a global switching signal that can briefly substitute real incoming data with generative samples to enable learning continuously in a way that minimally perturbs natural neural trajectories.  This presents another new approach that allows the brain to learn generative models through local synaptic plasticity while concurrently performing sampling-based approximate inference wrt these models.  Incidentally, this work has been presented in COSYNE 2021 by the authors.  ",0.3064516129032258,0.2903225806451613,0.25806451612903225,0.1792452830188679,0.1509433962264151,0.1791044776119403,0.1792452830188679,0.26865671641791045,0.11940298507462686,0.2835820895522388,0.11940298507462686,0.08955223880597014,0.22619047619047616,0.27906976744186046,0.16326530612244897,0.21965317919075142,0.13333333333333333,0.11940298507462686
998,SP:e2d78e6eba2bc0e6273a6ce65549866bc3a29fe7,"This paper considers the problem of learning how to control restless bandits.  When all parameters of the system are known, Whittle index policy usually offers a good performance. The main contribution of the paper is to propose an algorithm, NeurWIN, that uses a neural network architecture to learn the Whittle indices. Most of the paper is devoted to the description and the derivation of the algorithm. At the end of the paper, the authors present four illustrations of how the algorithm works. The algorithm is compared to an index-based policy and an (old?) RL algorithm REINFORCE for the small systems. The learning behavior of NeurWIN is very good in all tested cases.",The authors develop a reinforcement learning based method for computing the Whittle Index heuristic policy for solving Restless Bandit problems. This is a novel methodological contribution in the space of restless bandits. Several experimental results are provided demonstrating the good performance and general applicability of the method.," Whittle index are used to construct a powerful heuristics for restless bandit problem.  This paper proposes to estimate Whittle Index in discounted Restless bandit problem via a deep reinforcement learning algorithm. The authors define a notion of strong indexability that they use to construct a deep RL algorithm that learns Whittle indices. The authors argue that when a problem is strongly indexability and the neural network is precise enough, the deep RL algorithm will learn Whittle index (although their theoretical result is not fully convincing to me).  The authors present extensive experimental results that show that their algorithm compute Whittle indices and provide a very good performance compared to existing work. ","The paper proposes a method to automatically learn the Whittle Indices for a Restless Multi-Armed Bandit (RMAB) problem.   Contributions:  The Algorithm: The algorithm tries to learn a neural network that takes as input the state of a given arm and provides as output the whittle index of that state. It does this by showing a connection between the whittle indices for an arm and the indices of an optimal index policy for a family of MDPs (Env(𝝀)) based on that arm’s MDP (Env). It then learns a good index policy for this family of MDPs using a REINFORCE-type method.  Theoretical Justification: Towards substantiating the claim that learning a good policy for Env(𝝀) is equivalent to learning a good whittle index, the paper provides a proof about an epsilon-delta relationship between learning a good policy and a good whittle index.  Empirical Justification: The paper shows that the proposed algorithm works well on 3 previously published RMAB instances, when compared with 1 similar ‘learning a whittle index’-type baseline and 2 reinforcement learning baselines. ",0.12389380530973451,0.23008849557522124,0.25663716814159293,0.2765957446808511,0.3404255319148936,0.25225225225225223,0.2978723404255319,0.23423423423423423,0.16477272727272727,0.11711711711711711,0.09090909090909091,0.1590909090909091,0.17500000000000002,0.23214285714285715,0.20069204152249134,0.16455696202531647,0.14349775784753366,0.1951219512195122
999,SP:e3991e75c1ce364cad00807cf1e45e1e40b57933,"This paper explores the difference between the one- and two-stage based HOI detection methods and a corresponding method to conduct the human-object interactive pair detection and interaction classification separately, based on the Transformer structure and previous work QPIC. Moreover, two distinct methods named dynamic re-weighting and PNMS are also proposed. In experiments, the proposed model achieves impressive performance and outperforms its predecessor, QPIC.  Even the performance is good, comparing to QPIC, the proposed method is limited in design novelty. Moreover, some experiment details, especially the core separated pair detection and interaction classification performance contribution, are not verified.   Overall, I think this work is interesting but still has a gap to achieve the acceptance bar of NeurIPS. Detailed comments are given below, and looking forward to the authors' response.","This paper explored the HOI detection problem and aimed to mine the benefits of two-stage and one-stage HOI detection methods. To this end, this paper firstly gave a detailed analysis of conventional one-stage and two-stage HOI detection methods and concluded the advantages and disadvantages of the conventional methods. Then, based on their analysis, this paper proposed a Cascade Disentangling Network (CDN), which is a one-stage framework with disentangling human-object detection and interaction classification in a cascade manner. Finally, this paper conducted experiments on HICO-Det and V-COCO datasets and achieved state-of-the-art performance.","Inspired by its success on various vision and NLP tasks, Transformer based architectures have recently been adopted and achieved impressive performance on the task of HOI detection [3, 27, 38]. This paper builds upon these work and further innovates on the decoder architecture. The idea is that instead of having a single stage decoder which generates human and object bounding boxes and object and interaction class labels altogether, the decoding process is divided into two stages, where a first decoder is used to generate interactive human-object pairs, followed by a second decoder that predicts the interaction class. This resembles the traditional two stage HOI detectors which first detect individual objects followed by classifying the category of each pair. However, thanks to the development of Transformer based methods [3, 27, 28], one can directly predict human-object pairs in one go rather than relying on individual object detection and pairing. Meanwhile, the proposed framework can still build upon the same intuition which is to decouple interaction detection and classification so the network can just focus on one task at once. Besides this main architectural change, the proposed approach also features some new techniques on training (Sec. 3.4 ""dynamic re-weighting"") and post processing (Sec. 3.5 ""pair-wise non-maximal suppression""). The approach is evaluated on standard datasets (HICO-DET and V-COCO) and has achieved notable gains over SOTAs in detection mAP.","The paper proposes a novel HOI detection pipeline which includes a human-object pair decoder and interaction decoder once visual feature is extracted. Both the human-object pair decoder and interaction decoder are modeled by a transformer architecture composed of multiple layers of attention. It also presents some techniques for further improvement such as dynamic re-weighting and pair-wise NMS (PNMS). The proposed method presents superior detection results on common benchmarks, namely HICO-Det and V-COCO.",0.1984732824427481,0.20610687022900764,0.16030534351145037,0.3235294117647059,0.17647058823529413,0.14102564102564102,0.2549019607843137,0.11538461538461539,0.2692307692307692,0.14102564102564102,0.23076923076923078,0.4230769230769231,0.22317596566523604,0.14794520547945206,0.20095693779904306,0.19642857142857145,0.20000000000000004,0.21153846153846154
1000,SP:e3a0b2cb1a7e2ed24eb413cbd4545cfcddc30a69,"The paper proposes variational relational learning by learning relations between two inputs via variational inference on a probabilistic graphical model (PGM). The PGM that they use factors as p(a)p(z)p(b|a,z) where a,b are the two inputs and z is the supposed relationship between them. The example shown in the experiments is rotational mnist, where b is a rotated version of a, and z should encode the degree of rotation. The paper learns both the forward network and in inference network in a VAE-like approach; the elbo derivations appear correct to me.","This paper proposes a model to infer the relationship between multiple instances in a dataset by inferring a latent variable. The authors accomplish this by defining an optimization problem that optimizes the ELBO of the proposed graphical model. The paper presents a nice solution to some of the identification issues that can arise when inferring the latent variable, in particular the so called “information shortcut” when the model overfits to only learning the “absolute” property of the dataset, rather than inferring the shared latent traits. ","This paper proposes a relational learning method based on variational Bayes. The main idea is to learn relations among objects independently of each object's own properties. The model theory is discusses where authors define the problem and discuss about solutions for some limitations of the model when a relation between objects A and B can be found by only looking at one of the objects own properties. The paper is quite well written. Algorithms are provided, but not the code. As far as I understood, results are obtained on ""benchmark"" datasets, where some synthetic versions of these datasets are used to create the relations between the instances of the original dataset and of the synthetic.","This paper introduces a variational method for relational learning. It first introduces relational learning as learning based on relational property instead of absolute property, and introduces conditions (Eq. 1). Then it proposes VRL-PGM with a variational lower bound. To eliminate the information short-cut, it introduces relation-preserving data augmentation (RPDA). Experiments show that the method is able to perform relation discrimination and relation mapping, on a variant of MNIST and Yale face datasets.",0.18181818181818182,0.21212121212121213,0.1414141414141414,0.25882352941176473,0.12941176470588237,0.13793103448275862,0.21176470588235294,0.1810344827586207,0.18666666666666668,0.1896551724137931,0.14666666666666667,0.21333333333333335,0.19565217391304351,0.19534883720930232,0.16091954022988506,0.21890547263681592,0.13749999999999998,0.16753926701570682
1001,SP:e3afbbb77e0f749b13f656d2a77741ac3c1c4ffa,"This paper proposes two new techniques: Cross-scale Embedding Layer (CEL) and Long Short Distance Attention (LSDA). Cross-scale Embedding Layer samples patches with convolution kernels of different sizes and concatenates them as one embedding, so that patch embeddings contain information of multiple scales. Long Short Distance Attention includes a short attention module and a long attention module. The short attention is computed in a local region. The long one covers the whole image but is computed sparsely. These two new designs enable the network to learn cross-scale features. Also, a dynamic position bias is designed to make the network suitable for inputs of different resolutions.","The paper proposed a novel multi-stage vision transformer architecture, named CrossFormer, which aimed at building interaction among features from different scales. The proposed architecture follows the layout of Swin transformer but replaces the shifted windows transformer block with the proposed short-distance attention and long-distance attention. A cross-scale module is proposed to replace the patch embedding or patch merging layer in the Swin transformer. Comprehensive experiments are conducted on image classification, objection detection, and segmentation tasks. Results shows the effectiveness of the proposed architecture. ","The paper describes a pyramidal vision transformer that introduces cross-scale patch embeddings, dynamic relative position biases and ""Long-Short Attention"". The model shows superior performance on imagenet classification and especially on dense prediction tasks. Ablations on classification are provided hinting that the multi-scale embeddings are the most important factor contributing to the overall superior results to similar, previous architectures.","This paper proposes a novel vision transformer architecture called CrossFormer which focuses on the cross-scale ability in the attention module. Specifically, the proposed CrossFormer introduce Cross-scale Embedding Layer (CEL) and Long Short Distance Attention (LSDA) to model cross-scale interactions. CEL has the capability to merge multi-scale features with various kernel sizes, while LSDA enables the self-attention to capture feature interactions of both short distance and long distance. Experiments demonstrate that the proposed CrossFormer achieves performance improvement on image classification, object detection, instance segmentation and semantic segmentation tasks.",0.14953271028037382,0.1308411214953271,0.2336448598130841,0.1724137931034483,0.3218390804597701,0.2786885245901639,0.1839080459770115,0.22950819672131148,0.2717391304347826,0.2459016393442623,0.30434782608695654,0.18478260869565216,0.1649484536082474,0.16666666666666666,0.25125628140703515,0.20270270270270271,0.31284916201117313,0.2222222222222222
1002,SP:e3b590c9017a5d261f48441a21851f3256144729,"This paper proposes a novel adversarial attack against concept-based explanation methods. Technically, it utilizes a widely used attack method on DNN - PGD to generate perturbations. Adding these perturbations to concepts will mislead the target explanation methods to pinpoint meaningless parts in the input as important features.  Evaluation on a CNN model trained on the ImageNet dataset demonstrates the effectiveness of the proposed method. ","The authors derive a new adversarial attack, token pushing attack, that targets concept-based explanations. The adversarial perturbations are added to the concepts, resulting in a shift of the internal representation of a concept. The authors provide experiments that show that the adversarial attack changes the concept vectors and their visualization.","The paper describes the threat model for concept-based interpretability methods and mainly investigates the vulnerability of TCAV and faceted feature visualization. It introduces Token Pushing (TP) attacks, which learn small perturbations for the token of concept leading to different output for the interpretability method. The proposed attack is evaluated on pre-trained ImageNet models on the Describabke Texture Dataset for concept tokens.",The paper proposes an adversarial attack against concept based interpretability methods. The key idea is to change the hidden layer representation at which the concept-based method is being applied in an adversarial manner. The results show that the proposed method is indeed able to fool TCAV and FFV methods.,0.21875,0.21875,0.25,0.2549019607843137,0.29411764705882354,0.2222222222222222,0.27450980392156865,0.2222222222222222,0.32,0.20634920634920634,0.3,0.28,0.24347826086956523,0.22047244094488186,0.2807017543859649,0.22807017543859648,0.29702970297029707,0.24778761061946905
1003,SP:e3eedd50aad07b0439742a48a42191bb96eea846,"The paper builds upon the DIAYN idea (Eysenback 2018) that an agent could develop skills in an unsupervised environment by finding a set of skills that collectively visits the whole state space but encourages each skill to cover a different subspace and later use one of these skills to simplify the learning of a downstream task. The paper points out that prior work maximizes the distribution of latent states, but does not necessarily maximize world state differences, because latent states are the result of a learned projection from world states. The paper introduces an explicit Lipschitz constraint that forces latent state differences to be larger than observed state differences and relaxes a prior constraint of equality so that the latent states only need to move in the same direction as observed states but not in a fixed ratio. The paper shows this can be implemented as an efficient per step intrinsic reward for unsupervised learning.  Since the latent state aligns with observed state, one can use simple planner on downstream tasks to choose the right z to achieve a goal in observed space without any additional learning or fine tuning (zero shot skill exploitation). The paper evaluates LSD on 2D skills in the widely used AI GYM benchmark. Qualitatively, the LSD model discovers challenging dynamic behaviors in the difficult humanoid environment that competing unsupervised models fail to discover. Quantitatively, expected reward is higher for LSD and LSD zero shot than competing models. Direct evaluation of state space coverage shows LSD has significantly larger coverage than competing models supporting the paper hypothesis. LSD is also demonstrated on robotic tasks and is able to learn to pick objects in using unsupervised learning. ","This paper proposes a new objective (LSD) for unsupervised skill discovery, building on prior skill discovery work using mutual information criteria. As in previous works, skills are parameterized by a latent variable. The main ingredients are changes to the MI-based reward that (1) maximize alignment of learned state representation and latent variable, and (2) ensuring that state space differences are larger in the actual state space than in the learned representation. These modifications act as a prior so that the skill variable contains semantics (as it encodes a direction in representation space) and skills are encouraged to significantly change the agent's state."," The paper proposes a new skill discovery objective based on lipschitz-constrained dynamics skills. Specifically, for a state encoding \phi, <\phi_{t+1} - \phi_t, z> is maximized with a DIAYN-style training, and spectral normalization on \phi. The connection to DIAYN with Gaussian skills is shown, which is an unconstrained version of the proposed objective. A version of the method that defines a prior distribution of skills with finite support is also presented, which can be used similarly to discrete skill learning methods. Empirically, the method achieves significant improvements over prior work, being able to tackle ant and humanoid domains, where all prior methods fail.  ","The paper introduces Lipschitz-constraint skill discovery (LSD), a method for unsupervised skill discovery. The method addresses one limitation of the line of unsupervised skill discovery works which use mutual information between states produced by a skill and latent skill representations as the training objective, which is that they often discover overly simple and static skills, and require some additional domain information to be provided to learn more dynamic skills. LSD addresses this problem by reformulating a Lipschitz-constrained objective to learn continuous or discrete skills, and then demonstrates the effectiveness of the learned skills for downstream tasks.  ",0.10357142857142858,0.08214285714285714,0.08571428571428572,0.18269230769230768,0.20192307692307693,0.1509433962264151,0.27884615384615385,0.2169811320754717,0.24489795918367346,0.1792452830188679,0.21428571428571427,0.16326530612244897,0.15104166666666666,0.11917098445595856,0.126984126984127,0.18095238095238095,0.20792079207920794,0.15686274509803919
1004,SP:e44c358c66a59b84ce5cd56995aa99e6456a1ca5,"The paper proposed a new adversarial perturbation by stacking imperceptible adversarial patches. Each adversarial patch could be regarded as a disturbance rectangle mask and contains learnable coordinates, scaling, and shifting factors. By applying an adversarial patch into the input sample (e.g., image), the values of input inside the mask will be scaled and shifted. These adversarial patches are stacked and generated by solving an adversarial program problem with zero-order optimization and numerical optimization.","This paper studies multi-patch adversarial attacks, where each perturbation is generated via a program. Since the program includes both discrete variables and continuous parameters, they design a gradient estimation scheme for optimizing the parameters, and a program search algorithm to search for programs with increasing lengths. On several image classification benchmarks, they demonstrate that their approach achieves decent attack success rates and low L2 distortion, which are comparable to the C&W attack. Meanwhile, their attack requires much lower L0 distortion.",This paper focuses on crafting imperceptible patches to fool image classifiers. The method leverages program synthesis and numerical optimization  and produces perturbations with a lot less perturbed pixels compared to baselines. The experiments on various image datasets demonstrate the effectiveness of the method.,"This paper proposes APSyn to generate a sequence of imperceptible adversarial patches for a physical and imperceptible attack against image classification models. This attack is a combination of physical patch attacks and conventional imperceptible perturbation attacks. In APSyn, the authors formulate the attack problem as adversarial program synthesis and use gradient-based optimization to search for the optimal solution (a method for estimating the gradient for discrete values is also proposed). The experiments on small subsets of the MNIST, Fashion-MNIST, CIFAR-10, and ImageNet demonstrate that the proposed approach can achieve smaller L_0 distortion and larger L_2 distortion compared to the C&W attack.",0.13333333333333333,0.13333333333333333,0.21333333333333335,0.14634146341463414,0.2926829268292683,0.3953488372093023,0.12195121951219512,0.23255813953488372,0.14953271028037382,0.27906976744186046,0.22429906542056074,0.1588785046728972,0.12738853503184713,0.1694915254237288,0.1758241758241758,0.19199999999999998,0.253968253968254,0.22666666666666666
1005,SP:e4dcee650631bed12637ab241c4ae4dad2d33971,"The paper proposes a new model called Option-Controller Network with requisite inductive biases to model temporal hierarchy. This model is used to learn temporal abstractions and the control policy in the space of options using demonstration data via imitation learning. The performance on discrete action (Craft) and continuous action (Dial) environments are promising compared to existing baselines such as OMPN, CompILE and MoE. "," This work targets the issues in learning from demonstration, in order to achieve the ability of skill reuse/ transferability and improve the sample efficiency. To address the issues, the authors propose an option-control network (OCN), including a high-level controller and a pool of low-level options, which is an ""imitation-finetune"" paradigm as well. Finally, the proposed method is evaluated on two domains, Craft (discrete action space) and Dial (continuous action space), with results demonstrating its effectiveness.","This paper proposes to extract options from a dataset using an options framework with a recurrent controller and multiple recurrent option networks. During pre-training (i.e. skill extraction), the actions from the framework are computed by the weighted sum over options, which can be end-to-end differentiable, and trained using behavioral cloning. Once all options are extracted from the dataset, a new controller can be trained to solve a new task with the learned and frozen options via RL. The experimental results on the discrete CRAFT environment are impressive, outperforming all baselines. The results on the continuous DIAL environment show improved results over the baseline methods but the difference is marginal.",The paper proposes a specific neural network architecture for option learning with recurrency on both option and high-level controller. The final action outputs are determined from a mixture of experts of all options. The approach learns options offline from demonstrations (behavioural cloning) and combines them online by learning a new high-level controller via RL (PPO) while using frozen options. The submission uses variations of 2 domains from prior work for evaluation and shows good performance. ,0.234375,0.25,0.203125,0.20253164556962025,0.16455696202531644,0.1415929203539823,0.189873417721519,0.1415929203539823,0.16883116883116883,0.1415929203539823,0.16883116883116883,0.2077922077922078,0.2097902097902098,0.1807909604519774,0.18439716312056736,0.16666666666666669,0.16666666666666666,0.16842105263157897
1006,SP:e4e92a65093200eadb265a3dfe825a129abc2dc2,"This paper introduces Contrastive Attraction and Contrastive Repulsion (CACR) loss for self-supervised learning with a doubly contrastive strategy. It constructs two conditional distributions to model the importance of positive samples and negative samples according to their distances to the query. The importance, which is reflected by weights, guides the query to not only more strongly pull more distant positive samples, but also more strongly push away closer negative samples. Theoretical analysis and empirical results show that CACR generalizes the conventional CL loss and is more robust in more general cases. Superior performances are achieved on both curated and uncurated datasets, and on various downstream tasks.","This paper aims to improve the conventional contrastive learning (CL) by introducing a CACR loss, which takes the distance-based weights into the consideration for CL loss computation. Theoretical analysis is provided on the robustness property of the representation learned by CACR. Experiments are also presented to evaluate the performance of CACR compared with MoCo V2, SimCLR and other recent CL methods. ","In this paper, the authors present a new contrastive representation learning method. Specifically, a doubly contrastive learning (CL) approach is proposed, in which it contrasts positive and negative samples within themselves separately. The proposed method is termed ""contrastive attraction and contrastive repulsion (CACR)"". Extensive experimental, as well as theoretical analyses, show the effectiveness and robustness of the proposed method. The main contributions of this work are the proposed CACR strategy and the corresponding experimental analysis.","This paper proposes a new contrastive attraction and contrastive repulsion (CACR) loss for self-supervised representation learning. By formulating two conditional distributions, CACR considers different importance of positive/negative samples according to their distances to the query sample. Experimental results show superior performance of CACR against traditional contrastive loss and state-of-the-art methods.",0.12264150943396226,0.18867924528301888,0.3113207547169811,0.27419354838709675,0.25806451612903225,0.22666666666666666,0.20967741935483872,0.26666666666666666,0.6,0.22666666666666666,0.2909090909090909,0.3090909090909091,0.15476190476190477,0.22099447513812157,0.40993788819875776,0.2481751824817518,0.27350427350427353,0.26153846153846155
1007,SP:e53886e380fa0f08ca75da9ae6a042e0fd8b8f09,"This paper introduces a language-guided adaptive convolution layer into neural module networks (NMN) to address the limited generalizability of NMN to unseen concepts. The proposed method is able to achieve on-par performance to state-of-the-art models on CLEVR VQA task while significantly outperforms state-of-the-art models on CLEVR-Ref+ and CopsRef.  The model is also able to generalize better to novel compositions of objects and relations in CLOSURE.   In addition, a new challenging out-of-distribution test split for REF task is introduced, to evaluate on model generalizability to adversarial perturbations and unseen combinations of known concepts. The proposed method has shown a clear advantage on this new test split. ","The paper presents an incremental NMNs with a language-guided adaptive convolution layer into vanilla NMN. The goal o LG-Conv is to have a module that can adaptively co-attend over potential objects of interest from the visual input and textual input by altering the convolution.    To achieve this, they present biSAtt and CoSAtt networks to learn the so-called guidance kernel G from textual and visual inputs.   Experiments are done on multiple widely accepted visual reasoning benchmarking datasets, as well as a new benchmark that is designed to explicitly test the model's ability to generalize to adversarial perturbations.  ","The authors introduce three new tools to improve compositionality in visual reasoning tasks: 1. They apply pixel-adaptive conv nets [Su et. al., 2019] to the modules of Neural Module Networks. The guidance features, which weight the convolution computation, are computed via attention over the full image and text query. They present two different attention architectures to compute this feature: one separates the attention computation over vision and language and the other co-attends. 2. A new CLEVR test set that is constructed from the S-ref split of CLEVR. The new test set consists of (a) object, spatial relationships not seen during training and (b) contrast sets [Gardner et. al., 2020]. 3. They parameterize modules by argument (e.g., `blue`) to reduce the number of modules to be learned. Their model achieves competitive performance on CLEVR and improves over all baselines on the S-Ref and F-Ref splits. On their new split (C3-Ref+) they achieve the lowest drop in accuracy and maintain performance as the number of comparison relationships stays high.  Pixel-Adaptive Convolutional Neural Networks. Su et. al., 2019. Evaluating Models’ Local Decision Boundaries via Contrast Sets. Gardner et. al., 2020.","This paper proposes a novel extension to the neural module network (NMN) to tackle vision-language tasks. The proposed method applies a language-guided convolution operator to the neural modules, which improves information exchange and fusion between the visual and language modes. The proposed framework is applied to various benchmarks in the visual question answering (VQA) and referring expression recognition (REF) domains. Experiment results demonstrate a superior performance of the proposed method.",0.23275862068965517,0.1896551724137931,0.19827586206896552,0.2079207920792079,0.1485148514851485,0.10256410256410256,0.26732673267326734,0.11282051282051282,0.3194444444444444,0.1076923076923077,0.20833333333333334,0.2777777777777778,0.2488479262672811,0.1414790996784566,0.24468085106382978,0.14189189189189189,0.17341040462427745,0.149812734082397
1008,SP:e5ca6db6db5266d1c39b2654d63ab6e298030aee,"In this paper new generalization error bounds based on Wasserstein distance are derived. As shown in the paper, these bounds are tighter than existing bounds based on mutual information. Also, the authors briefly discuss how their bounds could be extended to deal with backward channels (instead of forward channels), and how they could lead to bounds based on general f-divergences.","This work presents several expected generalization error bounds in terms of the Wasserstein distance. Several settings (full-dataset, single-letter, and random-subset) are considered in the paper. Under the assumption that the loss function is bounded and using Hamming distance in the Wasserstein distance, the proposed bounds are shown to be tighter than the existing mutual information (or KL divergence) based generalization error bound. The author also claims that serval new bounds based on a variety of information measures can be obtained using these Wasserstein distance-based bounds.  ","This work provides novel generalization bounds, which limit (in expectation) the gap between a classifier’s empirical risk, i.e. how well it performs on the training data, and population risk, i.e. how well it performs on the true distribution. Generalization bounds can be in terms of many parameters of the problem, and this paper focuses on Wasserstein distances. Specifically, it provides bounds in terms of the Wasserstein distance between the hypothesis distribution before and after a single sample (“single-letter”), before and after a random subset of the samples (“random-subset”), and before and after the full set of samples (“full-dataset”). These Wasserstein distances were studied in previous work as well, but these bounds are tighter than the predecessor bounds. ","This paper considers the problem of obtaining generalization bounds using the statistical measure of dependence between the input and output of a learning algorithm. Tha main contribution of this paper is to provide a generalization bound using the Wasserstein distance between the posterior distribution and P(W). This result is further improved by considering the single sample case where the generalization bounds are based on the Wasserstein distance P(W|Z_i) and P(W). Also the authors extend their results to the setup of Steinke and Zakynthinou ‘20. The authors specifically follow the literature of information-theoretic generalization bounds, and compare their bounds to the existing bounds in the literature. In all cases, the authors show that the bounds using Wasserstein distance are tighter than the bound using mutual information (or KL).  Also, they show for a toy example of estimating the mean of gaussian distribution their bound provides “order-wise” improvement over the mutual information bounds where the mutual information based bounds gives upper bounds on the generalization that decays as 1/sqrt(n) but the Wasserstein distance based bound gives a bound that decays as 1/n.  The authors also use the joint-range technique to provide generalization bounds based on f-divergences.  ",0.3442622950819672,0.22950819672131148,0.4262295081967213,0.2808988764044944,0.38202247191011235,0.2601626016260163,0.23595505617977527,0.11382113821138211,0.1262135922330097,0.2032520325203252,0.1650485436893204,0.1553398058252427,0.27999999999999997,0.15217391304347827,0.1947565543071161,0.2358490566037736,0.23050847457627122,0.1945288753799392
1009,SP:e6225bb2da53af6af258a2bb45a4a54f9a9dcd17,"The paper presents an approach for calibration of neural networks. The proposed approach is based on the analysis of the softmax logits and decomposing it into two components that depends on the norm of the feature vector (norm component), and the other that depends on the similarity (similarity component) with the weight vector corresponding to a class. The paper then derives learnable parameters that could control each of the components and thus can be used for calibration of the predicted probabilities. The paper claims that for out-of-distribution (OOD) scenarios, this calibration scheme outperforms previously proposed approaches. The empirical results are reported for in-distribution (IND) on CIFAR-10 and CIFAR-100 and their corrupted versions of these for OOD, for various different models. ",This paper studies the problem of model calibration in the presence of distribution shifts for vision classification tasks. The authors propose a sensitivity decomposition on the last softmax layer of the model. The resulting calibration approach demonstrates improved performance compared to existing calibration methods under distribution shifts.,"The paper proposes Geometric Sensitivity Decomposition (GSD) as the new calibration method in the face of out-of-distribution (OOD) data. The paper identifies that the insensitive norm is responsible for bad calibration under distribution shift. Based on GSD, the paper proposes a simple and quick training and inference scheme. The paper achieves SOTA in calibration metrics in the face of corruptions while having arguably the simplest calibration method to implement.","This paper studies the problem of neural calibration from a novel geometric perspective and finds that the insensitive norm is responsible for bad calibration under distribution shift. Based on this insight, this paper proposes Geometric Sensitivity Decomposition (GSD), which decomposes the norm and angular similarity into an instance-dependent and instance-independent component. Subsequently, this paper proposes a training and inference scheme, to encourage the norm to reflect distribution changes. The proposed method is easy to implement and achieves SOTA results in calibration metrics in the face of corruptions.",0.104,0.144,0.16,0.3191489361702128,0.3404255319148936,0.5211267605633803,0.2765957446808511,0.2535211267605634,0.2247191011235955,0.2112676056338028,0.1797752808988764,0.4157303370786517,0.15116279069767444,0.18367346938775508,0.18691588785046728,0.2542372881355932,0.2352941176470588,0.4625
1010,SP:e649825f577c0e3735921809683f85045279a748," This work proposes TransGAN by replacing the CNN-based structure of both the generator and the discriminator in GANs with Transformer-based structure. To reduce the computing load, grid self-attention is proposed. Meanwhile, a relative position encoding scheme is introduced to the attention module to make the model better aware of the position information. Experimental results suggest that TransGAN achieves comparable performance to state-of-the-art CNN-based GANs.","This paper has presented TransGAN: a purely Transformer-based architecture for Generator and Discriminator in GANs. To solve the huge computation complexity of self-attention in high-resolution, this paper introduces grid-self-attention. Also, to solve the unstable training process of GANs, this paper introduces several techniques such as multi-scale Discriminator, and modified normalization. The experiments are conducted on various datasets.","This paper proposes to build a GAN model using only transformer blocks, and completely free of convolutions. Several architecture designs are proposed and a unique training recipe is also introduced for stably training the proposed TransGAN. TransGAN  can scale up to high resolution (e.g. 256 × 256), and delivers state-of-the-art results on multiple benchmarks. ","This work successfully takes an important step in bringing transformer architecture to the family of GAN models, so far dominated by convolutional architectures. Another contribution is the localised attention ('Grid Self-Attention') that addresses the problem of increased memory requirements of attention at higher-resolution feature map. The empirical evaluation shows competitive results on several benchmarks, including state-of-the-art scores on STL-10; however, it is missing an important benchmark - ImageNet.   Despite such limitation and missing references and comparisons to some relevant prior work (which hopefully could be addressed in rebuttal), this work seems to meet the quality and novelty requirements for NeurIPS. I am willing to improve the rating further if my concerns are addressed.",0.23943661971830985,0.19718309859154928,0.23943661971830985,0.19047619047619047,0.2698412698412698,0.19298245614035087,0.2698412698412698,0.24561403508771928,0.1440677966101695,0.21052631578947367,0.1440677966101695,0.09322033898305085,0.25373134328358204,0.21875,0.1798941798941799,0.2,0.1878453038674033,0.12571428571428572
1011,SP:e66364cdffd1d602ff2ef3fa47d8153657be5882,"The paper presents a way to generate bias-conflicting samples at feature level, which help de-bias the model. For this, the authors first propose learning two separate (disentangled) representations: a) representation of implicit attributes (i.e., necessary to predict the target) learned through the normal cross-entropy loss and b) representation of biased attributes learned through generalized cross entropy loss (also used by LfF for bias amplification). The empirical results demonstrate that generating such samples outperforms oversampling/reweighting.","Dataset bias significantly degrades the generalization performance of deep neural networks. To alleviate this problem, the authors propose a novel approach for developing the debiased representation, where the model learns the disentangled representation and synthesizes diverse bias-conflicting samples via feature-level data augmentation. They demonstrate its effectiveness through extensive experiments on the image classification task. ","This paper proposed a feature-level augmentation method to disentangle the bias and intrinsic features for debiasing image classification. This paper pointed out that the diversity of augmented bias-conflicting (i.e., few-shot, tailed) data is crucial for debiasing. The proposed framework consists of two encoders that encode the image into disentangled intrinsic and bias features. (1) To achieve the disentanglement, the modules for intrinsic and bias features are trained using cross-entropy (CE) loss and generalized cross-entropy (GCE) loss, respectively. (2) To generate augmented data, the intrinsic and bias features are swapped in each mini-batch. Experiments conducted on two synthetic datasets Colored MNIST and Corrupted CIFAR-10) and two real-world datasets (BAR and Biased FFHQ) demonstrate that the proposed method achieves the best performance compared to those that do not require a bias type. The quantitative evaluation further illustrates the disentanglement.","The paper proposes a novel feature-level data augmentation technique to train image classifiers in a debiased manner. The approach consists of disentangling “intrinsic” and “bias” attributes from training set images and then synthesizing new “data points” by combining intrinsic and bias features from different images. Since, a lot of image datasets contain a large amount of bias-aligned data points (high correlation between intrinsic and bias attributes), the proposed feature-swapping approach’s aim is to generate bias-conflicting data points. The approach results in better classification performance on 2 synthetic and 2 real-world datasets.",0.16455696202531644,0.31645569620253167,0.17721518987341772,0.23214285714285715,0.19642857142857142,0.2054794520547945,0.23214285714285715,0.17123287671232876,0.14432989690721648,0.08904109589041095,0.1134020618556701,0.30927835051546393,0.1925925925925926,0.22222222222222218,0.1590909090909091,0.12871287128712872,0.1437908496732026,0.24691358024691357
1012,SP:e76181efc206b60a3e679def4f82caabd41c458a,"The work shows a scenario that the classical MDS algorithm fails. ""When the derived matrix has a significant number of negative eigenvalues,"" the classical MDS algorithm tends to fail when the number of dimensions increases sufficiently large. Then the work designs an algorithm with improved performance bound and shows the potential on practical applications.","A new decomposition of the classical multidimensional scaling (cMDS) error term is generated and used to get additional insights of the method.  As I first came across the draft, I naively thought; “Well, cMDS is extremely well known, how can one innovate on such topic?”. Moreover, the departing justification stating cMDS does not excel in non-linear mappings wasn’t that great; I mean cMDS ASSUMES Euclidean space and linearity so it was no wonder that used out of its natural scope it may be suboptimal. But I am very glad to say that the authors proved my naivety wrong very quickly! There is a very clear contribution, the paper is strongly supported, and reading enjoyable.  STRENGTHS + The unusual “late comer”. Innovating in a method which is so well known is difficult.  WEAKNESSES + There are some unclear decisions and justifications but it is somewhat contradictory that I’m criticizing this when, I in the place of the authors, I would very likely have proceed equally… Notwithstanding, I’m indicating this below, in case a second thought can be given + There is an unconventional mixture of theoretical claims with experimental testing whereby some “theoretical proofs” are complemented with heuristics verified experimentally, but actually not theoretically proven. ","The paper considers the classic embedding method known as multidimensional scaling and focus on the so called STRAIN  and SSTRESS objectives, both designed for embedding a given squared distance matrix into a lower dimensional space, often for visualization, but potentially also for reducing problem size for downstream tasks.   The idea is that the SSTRESS is the better objective to optimize as it captures how well the distances of the embedded data points approximate the input distances, but computationally hard, so maybe it is enough to just use the solution for the simpler STRAIN objective, that boils down to Singular Value Decomposition and that may work fine.  The paper analyzes the solution of STRAIN in terms of the more relevant SSTRESS objective, and shows a decomposition of the error of the STRAIN solution as measured with the STRESS objective and analyzes this objective to understand what happens. The punchline is that if the input distance matrix is not Euclidian (no data set with these pairwise distances exist in any dimension), then the SSTRESS error of the embedding achieved by optimizing STRAIN can  actually increase with increased embedding dimension. A property that seems counterintuitive as a higher dimension should allow improved embeddings. The paper shows  experimentally that this phenomena actually occurs in practice as well and that the performance is nicely explained by the error decomposition proved and discussed in the paper. In particular the experiments show that the classic Isomap embedding has this issue.  Furthermore, if these STRAIN embeddings are used for a simple downstream task like standard classification, the results become increasingly worse as the embedding size is increased. ","The authors study the problem of multidimensional scaling (MDS) for embedding data into Euclidean space and perform a theoretical error analysis of the classical muldimensional scaling (cMDS) algorithm. Their main result decomposes the error of cMDS into 3 components, which they use to explain the behaviour and degradation of the cMDS error with respect to the embedding dimension. Then, they propose an algorithm, based on their theoretical findings, and show that it outperforms cMDS on standard image classification datasets.",0.2222222222222222,0.3148148148148148,0.2962962962962963,0.15121951219512195,0.07804878048780488,0.10408921933085502,0.05853658536585366,0.06319702602230483,0.20253164556962025,0.11524163568773234,0.20253164556962025,0.35443037974683544,0.09266409266409267,0.10526315789473684,0.2406015037593985,0.13080168776371306,0.11267605633802816,0.16091954022988506
1013,SP:e7f42951d12240dda9db52b0af836559f019aaed,"The paper proposes DP-Sinkhorn, an optimal-transport based generative model for privacy-preserving data generation based on differential privacy.  Optimal transport based generative models minimize variants of the Wasserstein distance, in particular an entropy-regularized version. The Sinkhorn divergence adds an autocorrelation term which cancels out the entropic bias completely when the distance is zero. [36] showed that the gradients of the empirical Sinkhorn loss are biased and proposes using independently drawn samples from the generator, the ""debiased"" Sinkhorn loss.  This paper proposes the ""semi-debiased"" Sinkhorn loss which, instead of using independently drawn samples, splits the sample along the batch dimension.   Differential privacy is added by perturbing gradients with the privacy level determined using the RDP accountant.   The method of using the semi-biased Sinkhorn loss with DP is termed DP-Sinkhorn and the paper argues that the lack of adversarial training, as opposed to GANs, avoids training instability and the need for early stopping.   Experimental results indicate better performance on MNIST, Fashion-MNIST and Celeb-A compared to other DP generative models. ","The paper presents a DP model based on Sinkhorn GAN. Different from DP GAN models that are based on adversarial loss, they use an optimal transport (OT) distance similar to Wasserstein distance called Sinkhorn divergence. The idea is simple and incremental, it is an extension of Sinkhorn GAN. Experimental results looks promising but some important results are missing.   ","This paper proposes a new technique for generating differentially-private synthetic data. The approach avoids adversarial training altogether, which has historically given poor model accuracy due to its instability and sensitivity to noise. Instead, the authors minimize Sinkhorn divergence, which is a computationally-tractable approximation to the optimal transport distance between the true distribution and the generated one. The authors demonstrate empirical gains over existing techniques for DP generative models. ","The paper considers the problem of generating private synthetic data using generative adversarial networks trained with differential privacy. While this has been considered, the authors propose an improvement by optimizing Sinkhorn divergence, which has been shown to be a successful strategy for nonprivate GANs. Due to challenges which do not arise in nonprivate GAN training (a difficult bias variance tradeoff), they use a weakening of the unbiased Sinkhorn divergence. They evaluate their strategy on multiple datasets, measuring both FID and a classifier's performance on synthetic data.",0.11428571428571428,0.11428571428571428,0.09714285714285714,0.15517241379310345,0.15517241379310345,0.22857142857142856,0.3448275862068966,0.2857142857142857,0.19540229885057472,0.12857142857142856,0.10344827586206896,0.1839080459770115,0.17167381974248927,0.16326530612244897,0.1297709923664122,0.140625,0.12413793103448276,0.20382165605095545
1014,SP:e88beb0e53311ed787654c504f0ba198233e18fd,"Non-linear GCNs have attracted a lot of attention recently, and many methods are proposed around them. However, this paper finds and investigates that current GCN variants still have not shown significant advantages over a properly designed linear GCN (their proposal). This is significant to the whole field and can help shape a new direction to propose real powerful non-linear GCNs. ","The paper investigated the feature propagation steps of linear GCNs from a perspective of continuous graph diffusion, and analyzed why linear GCNs fail to benefit from more propagation steps. From the perspective of diffusion equation, the authors decoupled the terminal time and the feature propagation steps, and proposed DGC to make it more flexible and capable of exploiting a very large number of feature propagation steps. Theoretical insights and extensive experiments on benchmark and large graph datasets have verified the superiority of DGC. ","Along the line of linear GCNs, this paper dissects the feature propagation steps via continuous graph diffusion. With theoretical analysis on numerical methods to approximate Graph Heat Equation, the paper proposes a linear GCN, named as DGC, that decouples terminal time $T$ and propagation steps $K$. As shown in real-world experiments, DGC enjoys competitive results compared with linear and non-linear GNNs.","This work studies linear GCNs and tries to provide a solution to find the optimal tradeoff between under-smoothing and over-smoothing. It first pointed out the limitations of SGC, then propose the Decoupled Graph Convolution (DGC) model to address the limitations. The key idea is to view the feature propagation step in linear GCN from a perspective of continuous graph diffusion, and dissect the terminal time and the propagation steps to enable choosing a flexible continuous terminal time (and therefore can benefit from more propagation steps).",0.16129032258064516,0.1774193548387097,0.16129032258064516,0.2289156626506024,0.30120481927710846,0.23809523809523808,0.12048192771084337,0.1746031746031746,0.11494252873563218,0.30158730158730157,0.28735632183908044,0.1724137931034483,0.13793103448275862,0.176,0.1342281879194631,0.26027397260273966,0.29411764705882354,0.19999999999999998
1015,SP:e8c0267924796d83c9be0258bc187ab0218ffcfa,"This paper studies some of the optimization challenges with training large graph neural networks for reinforcement learning tasks, in particular locomotion control tasks. The paper proposes that one of the larger challenges in optimizing graph neural networks is the significant variance between updates across the network graph. The method proposes to randomly freeze nodes and components of nodes in the graph network. The freezing of these different blocks inside of the graph Network helps reduce the effects of large changes in the output of the network making the overall optimization less noisy and more stable resulting in better performance.  Pros - The analysis in the paper does appear to be rather thorough. Especially going into the specifics about the difficulties in training neural networks for this particular type of problem. Also at the same time showing some of the limitations of training graph neural networks for example nerve net and its difficulty to perform well on larger agent morphologies. - The paper then proposes a random perimeter freezing algorithm that increases the overall learning performance and final policy quality of the nerve net like design. - Along with the performance gain the method is still able to show significant transfer capabilities across some different morphologies.  Cons - One of the more subtle challenges in the contribution of this paper is that the method overall is somewhat simplistic. It could be thought of as performing a type of block dropout while training the network in order to reduce the amount of parameter change when making policy updates. - One of the other more nuanced difficulties in the contribution of the paper is that it focuses on training graph neural networks for locomotion tasks on reinforcement learning. This could be seen as a somewhat specific area of research that may not have a more broad impact on the community. - Related to the last point it would be helpful to compare this method to the amorphous work. That work calls out the challenges with training and the optimization of graph neural networks that may limit their overall application. In this new work proposed by the authors shows they've now been able to overcome some of the difficulties in training such that it may perform better than the amorphous algorithm as well. ","* The authors study a previously published method for learning locomotion in 3D agents, called NerveNet, which models the agent as a graph of joints linked by limbs and uses a Graph Neural Network (GNN) mechanism to compute joint forces. Previous work has shown that this method transfers better than MLPs across morphologies, but also scales poorly to larger agents.    * The authors identify a problem that they suggest causes this poor scaling - namely, instability of policy updates, even at performance-optimal choices of hyperparameters. They choose to call this ""overfitting"" - a contentious choice no doubt, but why not. They show that a L2 regularization penalty on weights alleviates the problem a bit, but not enough.   * Crucially, they also discover that reducing learning rates to zero for the encoding and decoding components of the model (i.e. using random frozen embeddings) considerably improves performance.   * Somehow they decide to give a name to the fact of using frozen random embeddings, calling it ""Snowflake"".   * They compare this method to standard NerveNet and MLPs on various locomotion tasks. They show that ""Snowflake"", unlike vanilla NerveNet, performs about as well as MLPs even on large morphologies (after enough training). Importantly, they also show that ""Snowflake"" trained on a given morphology transfers even better to a smaller morphology than NerveNet (and that MLPs simply can't do that at all).  * In short, the contribution of this paper (as I understand it) is an identification of a source for poor NerveNet performance, a solution to this problem (just use random frozen embeddings), and a measurement of the improved performance (matches MLPs in performance, considerably beats them in transfer).","The authors propose a framework to address the challenge of scaling the application of graph neural network (GNN) based policies to locomotion challenges in reinforcement learning. The authors' hypothesize that the scaling challenges of GNN policies relate to overfitting of internal networks within the GNN architecture during RL training and unstable policy updates during on-policy RL training using PPO. The authors then introduce their framework, named Snowflake, to address this challenge which freezes distinct parts of the GNN during training to prevent overfitting and unstable policy updates. The authors demonstrate their framework by applying NerveNet, a GNN based policy network, to the centipede environment in the Gym + Mujoco suite. In their experiments and subsequent analysis, the authors show supporting evidence for their claim that NerveNet doesn't scale to large locomotion challenges and that the policy updates are unstable as suggested by large KL divergence between the updates when compared to regular MLP policy updates. Subsequently, the authors show how common overfitting mitigations fall short of significantly improving the performance of NerveNet in the centipede setting and provide supporting evidence that the Snowflake increases the performance of NerveNet to match regular MLP policy performance in a variety of Mujoco tasks, including various sizes in the centipede environment. The authors also claim, and support that claim, that GNN based policies, such as NerveNet, provide better generalization abilities compared to MLP based policies.","The paper considers training RL agents by using graph neural networks (GNNs) as the function approximator, therefore inducing a useful inductive bias. Training GNNs is not without its hurdles and thus far previous work has focused on relatively small agents, putting into question scalability. This work attempts to address this issue by propposing parameter freezing in parts of the GNN. The hypothesis for this is that policy updates become unstable with GNNs, to which authors provide empirical evidence. The experiments also show better performance in more complex agents and zero-shot transfer than the NerveNet baseline.",0.13101604278074866,0.13636363636363635,0.06951871657754011,0.15925925925925927,0.08518518518518518,0.12931034482758622,0.1814814814814815,0.21982758620689655,0.2708333333333333,0.1853448275862069,0.23958333333333334,0.3125,0.15217391304347827,0.1683168316831683,0.11063829787234042,0.1713147410358566,0.12568306010928962,0.1829268292682927
1016,SP:e93d22a06a5aff40d57d37767fa2c9b3e5ec7b40,"This paper design four quantitative indicators of attack failures (IoAFs) that might help to systemically automatically check the correctness of robustness evaluations. Besides, the authors also provide several mitigation methods for these IoAFs. According to these indicators (IoAFs) as well as corresponded mitigation methods, the authors show the effectiveness of indicators (IoAFs) applied on several existing defense approaches and also proposed mitigation methods.","This paper proposes a systematic method for understanding the failures of adversarial attacks as means towards thorough evaluation of robustness defenses in the research community. The devised framework relies of three connected aspects: (i) analysis of the main causes why gradient-based adversarial attacks fail, (ii) metrics for identifying their reason of failure and (iii) practical mitigations to ensure attack efficiency when evaluating defenses. All these aspects are illustrated and confirmed on four existing defenses that were initially subjected to limited evaluation.","This paper reasons about complications that may cause authors to not recognized that their newly proposed defense is ineffective. The authors identify failure motivations, propose 5 indicators (measurable) that are indicative of an attack failure, and also propose appropriate mitigations. The paper very methodically and experimentally presents this findings, and offers a more concrete support towards adaptive attacks evaluations whenever new defenses are proposed. ","The paper, similar to [1], proposes a systematic way to debug problems that may arise when implementing adversarial attacks to evaluate a proposed defense. They categorize the implementation problems into 4 categories, propose 5 indicator metrics that can identify what problem exists in an implementation, and highlight 5 mitigation strategies that can be used when a particular indicator is triggered. In the context of 4 existing defenses, they show that original Projected Gradient Descent attack implementation overestimates the security of these defenses. By leveraging the proposed indicator values and using their suggested mitigations to properly generate PGD adversarial examples, they are able to show that the attack is much more lethal and can invalidate the existing defenses. They compare the efficacy of their manual approach for debugging with AutoPGD, which tunes hyperparameters automatically to improve the PGD attacks chances of success.  [1] Carlini, N., Athalye, A., Papernot, N., Brendel, W., Rauber, J., Tsipras, D., ... & Kurakin, A. (2019). On evaluating adversarial robustness. arXiv preprint arXiv:1902.06705.",0.1746031746031746,0.1746031746031746,0.19047619047619047,0.10975609756097561,0.23170731707317074,0.265625,0.13414634146341464,0.171875,0.07228915662650602,0.140625,0.1144578313253012,0.10240963855421686,0.15172413793103448,0.1732283464566929,0.10480349344978165,0.12328767123287672,0.1532258064516129,0.14782608695652172
1017,SP:e99227744d39960db6bf3d332bec5f7172f9d8af,"This paper proposes a new sampling schedule for the experience replay buffer often used by reinforcement learning algorithms. The proposed approach is to perform value backups via bread-first search, starting from the set of terminal states and then moving backwards. The paper proposes a way to building such graph from sampled transitions and it shows, empirically, that the proposed approach (TER) performs better than other methods in a specific class of problems. ","The authors propose a topological experience replay (TER) method to perform Q value updates in a reverse sweep style. A variety of empirical results demonstrate that TER improves the Q-learning convergence speed significantly. The paper is well-written and well-organized, and the motivation is clear. ","This paper proposes a novel experience replay technique motivated by the topology of the collected data. The technique changes the order in which states are sampled for a minibatch Q-function update, sampling states backwards in a breadth-first way through a trajectory graph constructed from the replay buffer. This means that terminal states, which are more likely (by assumption) to receive reward, and don't require bootstrapping for value estimation, are updated first allowing reward from these states to propagate backwards to earlier states quicker. This method shows improved performance on several MiniGrid and Sokoban tasks, outperforming other baseline experience replay methods (PER, EBU, DisCor). They perform extensive ablations on the difference between TER (their method) and EBU, showing why TER outperforms EBU.","The paper proposes a new sampling strategy to improve the sample efficiency of Q-learning based methods. Bootstrapping being a fundamental characteristic of TD methods, the accuracy of the next state's value is critical in updating the current state. Previous work preform a similar observation, leading to methods such as backward value iteration. However, performing reverse sweeps is challenging in high dimensional tasks. As such, the authors propose to iteratively construct a graph representing the underlying MDP on which reverse sampling is executed. The authors report considerable improvement on grid-like tasks when compared to baselines that propose sampling schemes. The authors also provide numerous additional experiments attempting to elucidate the merits and limitations of the approach.",0.1506849315068493,0.2602739726027397,0.2328767123287671,0.23404255319148937,0.2978723404255319,0.14516129032258066,0.23404255319148937,0.1532258064516129,0.1440677966101695,0.08870967741935484,0.11864406779661017,0.15254237288135594,0.18333333333333332,0.1928934010152284,0.1780104712041885,0.1286549707602339,0.1696969696969697,0.1487603305785124
1018,SP:ea10f7e7ec515123044829902e19ef0b80979fd3,"This paper introduces a graph-based model for salient region discovery in videos, called Spatio-Temporal Graph Neural Networks (DyReg-GNN). The key idea is to capture latent interactions between various entities by graph; moreover, RNN is used to capture inter-flame information. Generally, the idea is reasonable and the experiments show the effectiveness of the proposed approach. ","The paper proposes to improve spatio-temporal graph neural networks by adaptively computing the input node features from spatially localized attention regions. The attention regions are dynamically predicted from each input frame, which is in contrast to previous methods that use fixed regions or pretrained object detectors. It is shown that by only optimizing a video classification loss, the model can learn to attend to objects, thereby extracting object-centric node features and facilitating interaction modeling. On the Something-Something-V1&V2 datasets, the model achieves comparable or slightly better classification accuracy than previous work, and can consistently boost performance when combined with multiple backbones.",This paper proposes a graph based network for spatio-temporal understanding. The core idea of the paper is to learn to select visual features for node representations in graph dynamically using convolutional encoders. The paper shows competitive results in the Smth-Smth v1 and v2 datasets and MultiSync MNIST. The contributions of the paper are demonstrating a method about how to learn to select dynamic regions and making it work in the context of graph neural networks.,"The paper proposes a novel approach for generating dynamic visual nodes, ideally capturing salient visual regions, which can then be fed to a spatio-temporal Graph Neural Network in order to model intra-region relationships. In contrast to prior work, which uses feature map columns in a grid or object detector bounding boxes as the input visual nodes to a GNN, the proposed DyReg-GNN learns to localize a fixed number of visual nodes from a video, without using object detectors or assuming spatial annotations.  To achieve this, the authors propose using node-specific trainable neural networks, which output region locations and sizes based on the input spatio-temporal feature map. Then spatio-temporal features are differentiably-pooled from each one of these regions, are fed to a standard GNN, are refined based on interactions, and finally are mapped back to their original spatio-temporal positions to yield a refined spatial feature map. Therefore, the DyReg-GNN can be inserted at any intermediate level in a standard convolutional model.  Results show that the proposed approach significantly outperforms GNNs that are applied on a fixed grid on a synthetic dataset (MultiSync MNIST), is much more efficient than methods that use object detectors, and the generated regions are qualitatively shown to correlate with objects, change size over time, and even retain object identities over time. The method is also evaluated on Sth-Sth V1, where it leads to a competitive performance, although the improvement over the grid baseline is much smaller.",0.2413793103448276,0.3103448275862069,0.3448275862068966,0.1619047619047619,0.2761904761904762,0.2987012987012987,0.13333333333333333,0.23376623376623376,0.08032128514056225,0.22077922077922077,0.11646586345381527,0.09236947791164658,0.17177914110429449,0.26666666666666666,0.13029315960912052,0.18681318681318682,0.1638418079096045,0.1411042944785276
1019,SP:ea3a3f61ff11ab7a8f21b99750f5feccfeed63cf,"The paper proposes a novel learning framework, which seems to me to be a cross between prediction with expert advice and bandit framework ported to stochastic off-line settings.  The key features are:  1. On every round, the learner chooses a limited number of experts to observe. 2. After T rounds, the learner outputs a function on experts' predictions that to be used on the next round. 3. The experts and the outcome are stochastic.   The framework seems to naturally fit a scenario where obtaining an expert prediction (e.g., by running a computationally expensive program) is costly.  The upper and lower bounds on the expectation of the learner's loss on the (T+1)th round are provided (lower bounds without proofs). The cases where m, the number of experts we are allowed to observe, is equal to the total and less than the total are treated separately.","The setting in the paper is a prediction with expert advice in the stochastic setting, while the learner observes a limited number of experts each round (denoted by m), and is limited on how many experts to choose in prediction (denoted by p). The main result is providing an algorithm with fast rates convergence if 1/T, as long as the learner is allowed to see two experts per round and to use two experts for prediction. Moreover, for m=p=1 the authors provide a lower bound of 1/sqrt{T}. ","UPDATE: The authors have addressed my questions in their response, and confirmed my initial positive view of the paper. Their plan for improving the presentation of the introduction seems satisfactory.  ---  The paper considers aggregation of a finite set of K predictors (experts) in the (batch) statistical learning setting, when the loss is Lipschitz and strongly convex. It presents learning algorithms that perform one sequential pass through the training data, and satisfy one of the following constraints: * no constraint: full information * budget constraint: algorithms can query at most C experts in total   during their run * query and output constraint: algorithms can query at most m experts   per round, and output a convex combination of at most p experts. The   following specific cases are considered:   - m=p=2 (and extension m>2,p=2)   - m=1 and p=2   - m=2 and p=1.  In all cases (near-)optimal bounds are obtained in probability. ","In this paper, the authors consider the problem of prediction with limited expert advice, with the goal of minimizing the excess generalization error, in the stochastic regime. They study a setting where they assume that the loss function is L-Lipschitz and \rho strongly convex.   They show that if the learner is only allowed to either make one observation per round in the prediction phase or query the advice of only one expert in the test phase, then there is a probability bounded by a constant that the worst case excess risk has a slow convergence.  To circumvent this issue, they study the impact of changing the number of observations and the number of expert queries, showing that in particular using at least two observations and two expert queries instead of one is sufficient to achieve a fast rate convergence of the excess generalization error. They consider a budgeted setting, where the learner has global budget constraint, and a more classic limited expert's predictions setting, which fixes the number of observed expert advices at each round.   ",0.18791946308724833,0.14093959731543623,0.2080536912751678,0.2391304347826087,0.2826086956521739,0.19078947368421054,0.30434782608695654,0.13815789473684212,0.1751412429378531,0.14473684210526316,0.14689265536723164,0.1638418079096045,0.2323651452282158,0.13953488372093023,0.1901840490797546,0.180327868852459,0.1933085501858736,0.1762917933130699
1020,SP:ea7c4aaeccbfe7b280062d63b5d8f2798b993bd9,"The paper investigates a realistic situation of open-set SSL, which includes the class distribution mismatch and the feature distribution mismatch. Moreover, it assumes the class relationship between labeled data and unlabeled data is unknown. The proposed method utilizes two scores to detect the data that belong to the common classes, and conduct domain adaptation to solve the feature mismatch problem.","The paper proposes an approach for universal semi-supervised setting that handles both class distribution and feature distribution mismatch. The key idea of the CAFA approach lies in introducing a scoring mechanism that identifies shared classes in the labeled and unlabeled data. The objective function in CAFA consists of supervised cross-entropy loss on labeled data, adversarial loss for feature adaptation between labeled and unlabeled data and class sharing data exploration term on pseudo-labels that are calibrated by weighted softmax function where weights are computed for each class. The approach is validated on three datasets CIFAR-10, Office-31 and VisDA on three tasks: (1) only class distribution mismatch,(2) only feature distribution mismatch, and (3) both class and feature distribution mismatch.","This paper studies the universal semi-supervised learning, which combines traditional SSL and domain adaptation with the open-set problem. Under this setting, two issues are raised as the technical challenge, i.e., the class distribution shift and feature distribution shift. Accordingly, the CAFA is proposed in this paper, which identifies the class shared data among training and testing domain first, and then employs domain adaptation on these data for semi-supervised learning. Experiment results demonstrate the appilcability of the proposed method on several benchmark datasets.",The paper targets the universal semi-supervised learning problem where the class distribution and the feature distribution both mismatch between the labeled data and the unlabeled data.  The paper proposed a new framework Class-shAring data detection and Feature Adaptation (CAFA) to address the universal semi-supervised learning problem. The key insight is to design a scoring mechanism integrating domain similarity and label prediction shift to identify both the labeled and unlabeled data from the shared classes.  The paper conducts experiments on several datasets for semi-supervised learning and domain adaptation and shows competitive results on the semi-supervised learning setting with both class distribution mismatch and feature distribution mismatch. ,0.32786885245901637,0.29508196721311475,0.47540983606557374,0.18699186991869918,0.2926829268292683,0.313953488372093,0.16260162601626016,0.20930232558139536,0.2636363636363636,0.26744186046511625,0.32727272727272727,0.24545454545454545,0.21739130434782608,0.24489795918367346,0.33918128654970764,0.2200956937799043,0.3090128755364807,0.2755102040816326
1021,SP:eaf2bf87c4bf25440c7bd8462c80b87e913361c2,"This work aims to alleviate the performance degradation problem of the student model during knowledge distillation. The authors argue that the degradation problem may come from the sharpness gap of the model outputs.  They first demonstrate the relation of the sharpness gap (between teacher and student) and the degradation degree, then introduce the realsoftmax function to measure the sharpness of the model output, and finally propose ATKD to adaptively change the temperatures of the teacher and the student for reducing the sharpness gap.  Besides, the authors also analyze previous methods (e.g., Early Stop and TS) from the perspective of the sharpness of models. ","This paper is to improve the vanilla knowledge distillation method. Based on the observation in Cho&Hariharan that student degrades by oversized teachers, this paper proposed an adaptive temperature solution. A new metric called sharpness is introduced to quantify the teacher-student gap. Experiments are done on CIFAR100 and ImageNet.",This work explores how the sharpness gap between a teacher model and a student model affects knowledge distillation performance. The paper argues that a large sharpness gap harms KD and proposes an adaptive temperature strategy to speed up the decrease of the sharpness gap in training. The experiments demonstrate a high correlation between the sharpness gap and student performance. It also shows that the proposed adaptive temperature strategy can achieve SOTA results on the image classification tasks.,"The paper proposes to mitigate the performance degradation by controlling the sharpness gap between a large teacher and a student model. The sharpness is defined as the real softmax function (the logarithm of the sum of the exponentials of the logits). During the training, the temperature is set according to the sharpness of the logits. For the teacher model which has a sharp output, the temperature is larger so that the output becomes more smooth.  ",0.14423076923076922,0.23076923076923078,0.25,0.28,0.2,0.2857142857142857,0.3,0.3116883116883117,0.3466666666666667,0.18181818181818182,0.13333333333333333,0.29333333333333333,0.1948051948051948,0.2651933701657459,0.2905027932960894,0.22047244094488191,0.16,0.2894736842105263
1022,SP:eb0328f825919b21b6d75bef755371a62ef30398,"This paper investigates the theoretical performance of a number of variables selection methods when the columns of the design matrix are correlated. They focus specifically on the case where the Gram matrix for the design is block-wise diagonal (2 x 2 blocks) with the off-diagonals within each block given by a correlation parameter and diagonals set to 1. The quality of each estimator is evaluated by using its expected Hamming distance. Extending ideas presented in _UPS delivers optimal phase diagram in high-dimensional variable selection [Ji and Jin]_ and others, phase diagrams are constructed with phase curves (derived in the appendices) establishing the boundary between each region: exact recovery, almost full recovery, and no recovery. A discussion of each variable selection method provides insight and a comparison between them is included. ","The goal of the paper is to theoretically compare several variable selection schemes related to Lasso. To that end, the authors propose a comprehensive and visual methodology: phase diagrams. Under an idealised setting, they show that, depending on the strength and sparsity of the signal, variants of Lasso fall in one of the three following regimes, Full recovery, Almost Full Recovery, No Recovery. Their main results consist in the exact and non-asymptotic characterisation of those regimes. Once the phase diagrams are computed, the efficiency of algorithms can easily be compared. Most demonstrations rely on the fact that, under the “blockwise correlation” setting, methods can be  reduced to bivariate (Lasso-type) sub-problems.","This paper performs theoretical analyses of several variable selection approaches such as Lasso, Elastic net, SCAD, thresholded Lasso, forward selection, and forward backward selection. While the previous studies focused on the model selection consistency, this paper focused on the impassibleness of the model selection consistency based on Hamming errors. Its theoretical analyses show that SCAD has better performance than Lasso, Elastic net is worse than Lasso, and other approaches are superior to Lasso.",The paper compares several variable selection methods over block-wise diagonal designs. The performance is theoretically evaluated by the expected Hamming error. Some simulations are implemented to justify the empirical performance of the methods in the comparison.,0.14285714285714285,0.09774436090225563,0.12030075187969924,0.13274336283185842,0.10619469026548672,0.1232876712328767,0.168141592920354,0.1780821917808219,0.43243243243243246,0.2054794520547945,0.32432432432432434,0.24324324324324326,0.15447154471544716,0.1262135922330097,0.18823529411764706,0.16129032258064516,0.16,0.16363636363636364
1023,SP:eb248aa78d7f5dc36e303f679e92eee2099bbf77,"Authors analyze the role of global pretraining in few-shot classification, establishing classification loss over all classes as an upper bound on the classification losses from particular tasks. Authors then consider the case where task-based labels are available but are not easily collated into a global set of labels. They introduce a task-aware clustering-based method for assigning global labels to embeddings from a task-pretrained feature extractor, allowing them to achieve near-oracle performance in settings where global labels are absent. The method improves upon task-based pretraining alone by a substantial margin. ","This paper tackles few-shot image classification problem which has two main categories of solution, meta-learning based methods and global classification based methods. The authors provide analysis to show that the few-shot result for global classification based method is the result upper bound for meta-learning based method. Due to the unavailability of global labels in global classification in real applications, the paper proposes one algorithm to convert local labels to global labels for global classification. Results show the effectiveness of the proposed method on miniIMAGENET and tieredIMAGENET datasets.","Stemming from recent observations that standard supervised training (called ""global classification"") empirically works better than local meta-learners, authors propose to infer labels from a trained meta-learner in order to fine-tune the feature-extractor. leveraging the benefits of global classification without requiring absolute labels.  In more details, authors first draw a connection between global classification risk and local classification risk (Proposition 1). This proposition essentially formalises the simple idea that solving a global classification problem (i.e over the full set of classes) is harder than solving a local one (i.e with a strictly smaller set of classes).  Second, authors propose an algorithm to infer global labels for each sample in a meta-training set by leveraging the features produced by a trained local meta-learner. The method relies on alternating between clustering extracted features and pruning the centroids with low number of assigned points. During the clustering step, local constraints force samples with the same local label to share the same global assignment. This assignment is decided by a majority vote (i.e the centroid that minimises the average squared distance to all samples with same local label). Once assignments have been made, centroids are updated following standard incremental rules.     Finally, authors compare their method on popular benchmarks mini-ImageNet and Tiered-Imagenet against recent meta-learners, and perform some ablation studies on the clustering hyper-parameters and quality. ","This paper aims to reveal the effectiveness of using global labels in FSL from a theoretical perspective, and proposes a meta label learning (MeLa) method to infer the global labels by using a clustering algorithm. Overall, the investigated problem, i.e., whether to use global labels, is interesting and important. Also, the proposed theoretical analysis will be of interest to the community. However, the proposed clustering algorithm and the experimental part are not very surprising.",0.19791666666666666,0.21875,0.15625,0.2857142857142857,0.23076923076923078,0.1072961373390558,0.2087912087912088,0.09012875536480687,0.2,0.11158798283261803,0.28,0.3333333333333333,0.20320855614973263,0.1276595744680851,0.17543859649122806,0.16049382716049382,0.2530120481927711,0.16233766233766234
1024,SP:ec3caea4933ae5ebfe002c01aacac44e642fc38b,"This work introduces a method for doing generative modeling, and conditional generative modeling (e.g., image restoration) using the optimal transport map between two distributions.  The paper first discusses the different uses of Optimal Transport (OT) on generative models, and shows that most of the works are on using the OT cost and not the map/plan itself. The paper also discusses some of the work that does show the OT map but only into latent spaces due to the intrinsic complexity of computing the OT map on the original space.  The proposed method uses the Wasserstein-2  distance as a cost measure and assumes the existence of a unique OT deterministic plan, implying the existence of a map.  The formulation of the OT plan estimation problem (Lemma 4.1) ends up addressing a saddle point problem (max-min).  The analysis requires that the dimensions of input and output spaces be the same. The method also proposes to use a deterministic and non-learnable mapping to embed the input signals on a space that has the same dimension of the output space, so the framework can be applied. This embedding needs to be manually defined depending on the application.  The paper presents experiments on image generation and image restoration on classical  datasets: MNIST, CIFRAR, CelebA (64x64). Regarding image restoration three different applications are analyzed: denoising, colorization inpainting.  The method compares similarly to other generative modeling models (in particular similar to WGANs). ","The paper proposes to learn the optimal transport map from the latent distribution to the data distribution directly by optimizing the W2 distance. To find the OT map, the authors replace $y$ with $T(x)$ in the Kantorovich dual problem. Experiments show that the proposed method works well and learns the image distribution successfully.  ","In this paper, a novel optimal transport (OT) map based generative model is proposed. The OT map is computed as the generator map from noise distribution to data distribution. Different from previous works that mainly focus on manipulating distributions in latent space, the proposed method directly computes the OT map in the data/ambient space. One of the benefits of this approach is that the FID is controlled by the W-2 loss in the data space (i.e., theorem 4.3). Experiment results show that the proposed method has comparable or better performance than competitive methods on various tasks. ","This paper proposes a min-max optimization algorithm to apply OT maps directly in ambient space. And that extends the method the case when the input and output distributions are located in the spaces of different dimensions and derive error bounds for the computed OT map. Contributions of this paper have three aspects: 1) the end-to-end algorithm is given to fit OT maps for Q-embedded quadratic cost, i.e., if two distributions located on the spaces of equal dimensions, the identity embedding Q(x)≡x (Wasserstein-2 distance); if not, choosing Q-embedded quadratic cost. 2) The theoretical analysis the error bounds is proved. 3) this paper demonstrates large-scale applications of OT maps in CV tasks, image generation and image restoration. ",0.0995850622406639,0.12863070539419086,0.13692946058091288,0.3888888888888889,0.2962962962962963,0.22,0.4444444444444444,0.31,0.264,0.21,0.128,0.176,0.16271186440677965,0.1818181818181818,0.18032786885245905,0.2727272727272727,0.1787709497206704,0.19555555555555554
1025,SP:ec81fa20670c378ca7c25fc9314127d04ce787a3,"The authors propose a theoretically-grounded technique to improve neural network generalization. The loss is augmented with a term that encourages units to have a more diversity in its activations. More specifically, the authors use a radial basis function to measure pairwise similarity between activations and proposed loss terms based on either the sum or the determinant of the similarity matrix.  Based on experiments in several classification models on CIFAR10, CIFAR100, and ImageNet, models trained with this additional loss term achieve higher test accuracy, and also reduce the generalization gap between train and test. ","The paper proposes an interesting angle on the generalization of neural networks. Through proving a new generalization bound that has an internal distance term, the authors argue the neural networks’ generalization performance is related to the internal diversity of the neurons within each layer. As such, some regularization terms are introduced to encourage the internal ``diversity” within each layer. The effectiveness is verified empirically through experiments on a series of data sets including CIFAR and ImageNet.  ","The authors prove an upper bound on the estimation error that depends on pairwise distances between hidden unit values per layer. The bound itself is quite likely very loose in standard deep learning settings due to the dependence on various norms of the weights. Loosely inspired by the bound, the authors then propose an optimization algorithm with a particular regularizer that encourages within-layer activation value diversity. The empirical results suggest that the proposed algorithm boosts the performance in most regimes studied (CIFAR 10, CIFAR100 trained on ResNets, Dense nets). The boost is minimal for standard datasets, but it increases when the labels are randomized. Compared to standard empirical risk minimization, a small boost in test accuracy on transfer learning tasks is also reported.","In this paper, the authors propose a regularization technique encourage the ""activation diversity"". Specifically, they design a within-layer loss that add penalty to the similar neurons with the same activation pattern. They also showed that encouraging the within-layer diversity can be used to control the Rademacher complexity of model. ",0.1595744680851064,0.19148936170212766,0.14893617021276595,0.2236842105263158,0.17105263157894737,0.0967741935483871,0.19736842105263158,0.14516129032258066,0.27450980392156865,0.13709677419354838,0.2549019607843137,0.23529411764705882,0.1764705882352941,0.1651376146788991,0.19310344827586207,0.16999999999999998,0.20472440944881892,0.13714285714285715
1026,SP:ecb947944a43b9108e8018595ac33e790910e568,"This paper mainly focuses on the architectural sizing of the neural network accelerators. Optimizing accelerator parameters is expensive due to the time-consuming simulation and numerous infeasible design candidates. This work presents a data-driven offline optimization framework, PRIME, that reuses the previous simulation results to produce new designs without additional active queries to an explicit silicon or simulator. The key idea is to train a conservative surrogate model, that predicts the cost (e.g. latency) of an architecture candidate, by minimizing the prediction error of feasible designs and penalizing the infeasible designs for avoiding optimism. Experiments show that PRIME offers 1.20x - 1.54x speedup on average over manual design and 1.26x speedup for unseen applications.","The authors first introduce two practices for application-specific hardware accelerators: 1) Designers need to spend considerable manual effort and perform large number of time-consuming simulations to find accelerators that can accelerate multiple target applications while obeying design constraints. Moreover, such a simulation-driven approach must be re-run from scratch every time the set of target applications or design constraints change. 2) An alternative paradigm is to use a data-driven, offline approach that utilizes logged simulation data, to architect hardware accelerators, without needing any form of simulations. Such an approach not only alleviates the need to run time-consuming simulation, but also enables data reuse and applies even when set of target applications changes.  As such, they develop such a data-driven offline optimization method for designing hardware accelerators, dubbed PRIME, which learns a conservative estimate of the desired cost function, utilizes infeasible points, and optimizes the design against this estimate without any additional simulator queries during optimization. The authors evaluate PRIME architects accelerators---tailored towards both single- and multi-applications as well as unseen applications in a zero-shot setting.","The paper proposes an approach to optimizing parameters of hardware architectures to design architectures that are more efficiently able to execute neural networks. To accomplish this, the paper proposes Prime, an offline optimization algorithm that constructs a surrogate of the design space then optimizes against that design space. Specifically, Prime is trained to be robust to both unseen points in the design space (rather than underestimating their cost) and to infeasible points. For each of these goals, Prima adds a term to the loss function, penalizing adversarial examples and infeasible training points. The authors evaluate Prime by showing that given the same number of evaluations of the ground-truth data, Prime results in lower cost results than online optimization.",The authors present an offline performance model for optimizing neural net hardware accelerators. They introduce a mechanism for dealing with non-viable hardware design points and for specializing towards specific applications from a single surrogate. They present results versus online methods on a variety of axes.,0.23728813559322035,0.17796610169491525,0.07627118644067797,0.13043478260869565,0.08152173913043478,0.10084033613445378,0.15217391304347827,0.17647058823529413,0.1956521739130435,0.20168067226890757,0.32608695652173914,0.2608695652173913,0.18543046357615892,0.17721518987341772,0.10975609756097562,0.15841584158415845,0.13043478260869565,0.14545454545454545
1027,SP:ecfb3abbdf86c6123a0f56fcefa2a04581e57d04,"This paper presents a method for capturing the 3D shape and appearance of objects using an implicit model for the shape, and an aggregation of reprojected CNN features for the appearance. The authors highlight two problems with existing approaches that they try to address - slow training and rendering times. The key contribution of the paper is the use of meta-learning, in the form of a learned initialization, to improve the efficiency of the training process. The results show that the proposed method achieves a marginal improvement in training efficiency.","This paper presents MetaNLR++, a novel method for representing and rendering novel views of object-centric scenes. The scene representation combines a CNN-based feature extractor which extracts deep features from input views and a Neural SDF module which serves as a proxy geometry surface for transforming the extracted features to the novel views. The transformed features are then aggregated and processed by a CNN-based decoder to synthesize the output image. This use of image-based and surface-based scene representation enables faster rendering time compared to volume-based representation such as NeRF. A meta-learning technique based on learned initialization was leveraged to enable fast fitting of the model to new testing scenes. Experiments on two novel-view synthesis benchmarks demonstrate that the proposed method performs favorably compared to existing methods while requiring significantly less time for convergence.","This paper presents an approach to improving the training performance and overall quality of the recently introduced Neural Lumigraph Rendering (NLR) framework for novel view synthesis for objects, using a meta-learning framework that allows for quickly computing a representation of new scenes that can be rendered in real time.  The framework encodes the object surface using a neural signed distance function (SDF) representation of the surface, which can then be efficiently decoded using sphere tracing techniques. The framework encodes features from the 2D input images, which are aggregated from images that are visible in the target and decoded into the reconstructed target view. View-dependent effects are achieved using an MLP that acts as a learned weight-mapping function for the features aggregated from each input view and processed by the decoder. This decoder, in processing these feature maps, can fix artifacts caused by imperfect geometry reconstruction in the surface aggregation function. The Reptile algorithm is used as part of a meta-learning framework to learn a good parameter initialization from multi-view datasets that allows for quickly converging to a representation for previously unseen objects. Evaluations suggest that this approach achieves a good balance between fast convergence on unseen objects, final image quality and fast rendering speed compared to recent alternatives including the original NLR.  Overall my impression is that the main contributions of this work are in the approach described to use more sophisticated shape and appearance representation compared to the original NLR, using image encoders and decoders and a more sophisticated weight-mapping function to achieve superior results while allowing for comparable inference speeds after training. My impression is that the meta-learning framework as described, which is largely based on the Reptile algorithm, is not as substantial a contribution.","The paper finds a set of weight values that is a good initialization to train a recent novel-view synthesis (NVS) method, Neural Lumigraph Rendering (NLR). A meta-learning library, Reptile, is used to optimize this initial weight vector across a corpus, the DTU multiview dataset. The paper shows, that with such an initialization, optimization proceeds much quicker and to similar quality. Additionally, it is suggested to change NLR to aggregate features from the input images instead of pixel values directly, a change that appears to be orthogonal to the meta-learning aspect. Results show that after much fewer optimization steps, a good result is achieved. The method is fast, but that is a property of NLR.",0.3333333333333333,0.36666666666666664,0.23333333333333334,0.29285714285714287,0.14285714285714285,0.1054421768707483,0.21428571428571427,0.11224489795918367,0.1794871794871795,0.13945578231292516,0.17094017094017094,0.26495726495726496,0.2608695652173913,0.171875,0.20289855072463772,0.1889400921658986,0.1556420233463035,0.15085158150851583
1028,SP:ed16f04c3ee178d5e3890073ff61c0c2ee37e1f5,"The authors propose to sandwich the EBM log-likelihood with newly introduced upper and lower bounds, with which a minimax objective is formed to train EBMs.  Specifically, based on the variational lower bound from Grathwohl et al. (2021), the authors approximate the entropy term therein to form the presented lower bound, and then adding a gradient-penalty term mimicking WGAN to form the new upper bound.  The lower and upper bounds are optimized wrt different groups of parameters, as, intuitively, both minimizing and maximizing a lower bound wrt the same group of parameters is potentially unstable (this is the main motivation for this submission).","This work proposes a new method for density estimation using Energy-based model. Classical variational bounds replace the negative log-likelihood by a computable lower bound, and phrase the maximum likelihood problem as a minimax game on this lower bound. A possible algorithm to solve the problem is then to alternate between minimization and maximization of this lower bound (with respect to different variables). Although this approach has shown good performance, the authors claim that the concept of minimizing such a lower bound can lead to instabilities. In particular, they claim that the WGAN objective tends to be unstable to optimize, and that while clipping the gradient in this case solves the instability issue, the origin of the instability lies in minimizing an upper bound. To circumvent this issue, the current work constructs both an upper and a lower bound to the negative log-likelihood, and alternate between minimizing the upper bound and maximizing the lower bound. The upper bound is inspired by what is done in WGAN, i.e., by adding a gradient penalty. The algorithm is tested on several benchmarks, and is shown to be rather competitive compared to existing algorithms.","This paper proposes a method to train EBMs with lower and upper variational bounds. Previous approaches used only one side of the bound. This causes significant training instabilities as the training objective involves alternately maximizing and minimizing the same quantity.  The lower bound is estimated by lower-bounding the log determinant of the jacobian multiplied with itself with the smallest singular value of the jacobian.   The upper bound is inspired by tricks used to make WGAN and related EBM training approaches stable by regularizing the gradient.   The minimum singular value, needed for computing the lower bound, is computed using an iterative algorithm run for a small number of steps. Automatic differentiation is used cleverly to ensure full Jacobians don’t need to be instantiated.  The upper bound is loosened, and Hutchinson’s trace estimator is employed to efficiently compute.  ","This paper proposes a bidirectional bound on the EBM log-likelihood, such that it can maximize a lower bound and minimize an upper bound when solving the minimax game of EBM. The paper demonstrates the effectiveness of the proposed method on density estimation and sample generation. The major contribution of the current paper is to use the bidirectional bounds to train the EBMs.",0.3076923076923077,0.20192307692307693,0.16346153846153846,0.18652849740932642,0.13471502590673576,0.1510791366906475,0.16580310880829016,0.1510791366906475,0.2698412698412698,0.2589928057553957,0.4126984126984127,0.3333333333333333,0.21548821548821548,0.1728395061728395,0.20359281437125745,0.21686746987951805,0.20312499999999997,0.2079207920792079
1029,SP:ed23ac4ef5cf6242b082c9ca85997cd619418342,"This paper proposes a method, SyncTwin, to estimate treatment effects in Longitudinal and Irregularly sampled data with Point treatment (LIP) setting. The idea is to create a synthetic twin from controls that closely matches a treated target in representation. By using the pretreatment time samples of the twin to estimate temporal trend, SyncTwin can predict the outcome of the target when no treatment is administered. The method is applied to simulated and real data with lower prediction error shown. ","The paper proposes an individual treatment effect estimation method called SyncTwin, which learns synthetic twins of target patients via the use of representation learning for counterfactual predictions. The proposed method is used on longitudinal data such as electronic health records. The paper is concerned with the setting of longitudinal and irregularly sampled data with point treatment. The paper describes the proposed method, offers some theoretical guarantees, and demonstrates the utility of the method on both synthetic and real-world data.","The paper introduces a latent variable model (SyncTwin) for estimating time-varying individualized (point-) treatment effects (ITE) given temporal irregularly sampled pre-treatment covariates and outcomes. Theoretical results justifying the proposed approach include upper bound errors on the learned representations and counterfactual predictions. Experimental results on synthetic and real-world datasets show SyncTwin outperforms baselines per mean absolute error (MAE) on ITE and recovering RCT findings, respectively.","The paper proposes a representation learning approach to model counterfactuals in longitudinal data with point interventions over time. Their approach, SyncTwin is thus suited to situations where an intervention is made on a temporal dataset at an instance of time or as a continuous exposure by treating all points post the intervention as the potential outcomes under treatment.   They compare the performance of their model against several counterfactual estimation baselines originally proposed to model static observational data with interventions.",0.26582278481012656,0.16455696202531644,0.26582278481012656,0.1875,0.175,0.14925373134328357,0.2625,0.19402985074626866,0.26582278481012656,0.22388059701492538,0.17721518987341772,0.12658227848101267,0.2641509433962264,0.1780821917808219,0.26582278481012656,0.20408163265306126,0.1761006289308176,0.136986301369863
1030,SP:ed856969b4bd31d7a1a18b12a5dc91a4187846c1,"This paper considered the DP mean estimation problem with unknown covariance under Mahalanobis norm. In the known covariance setting, previous work can estimate gaussian mean near optimally. But in the unknown covariance setting, previous algorithms either need to pay an additional conditional number factor, or require d^3/2 samples to estimate the covariance which are both unsatisfactory. The key difficulty of the problem is getting Mahalanobis norm guarantee without first estimating the covariance privately.   The authors present two different (exponential time) algorithms for Gaussian and sub-Gaussian settings. The gaussian algorithm is based on an exponential mechanism with Tukey depth score. Although this approach has been used in [45] before, they need to choose the sampling region adaptively based on the data in order to achieve a good Mahalanobis guarantee. This brings additional difficulty in making the algorithm private, and they use the classical propose-test-release framework to obtain privacy guarantees. The second estimator uses the empirically rescaled Gaussian mechanism, where the gaussian noise is scaled by the empirical covariance such that Mahalanobis guarantee is naturally achieved. This estimator can be applied more generally on sub-gaussian distribution, though the sample complexity has a d/\alpha\eps^2 instead of the optimal d/\alpha\eps.  ","The paper studies the problem of private mean/median estimation for Guassian and subGaussian variables. In order to reduce the noise required to satisfy privacy, the algorithms tailor the noise to the (unknown) covariance of the distribution, without directly estimating the covariance matrix. This is an important distinction from some prior methods, as privately estimating the covariance matrix is expensive. The methods used combine techniques including the exponential mechanism with Tukey depth, the Gaussian mechanism, and propose-test-release(PTR). While the authors mention that the privacy analysis uses standard techniques, they give a rigorous proof of the utility of the mechanism which they mention uses novel techniques.  ","This paper investigates mean estimation of high dimensional Gaussian (and sub-Gaussian) distributions under the constraint of differential privacy (DP). More specifically, the authors aim at privately estimating the mean of a high-dimensional Gaussian in the Mahalanobis distance (w.r.t. to the covariance matrix).   One simple approach for this task is to first privately estimating the covariance matrix (say up to a constant factor in spectral distance) after which the problem has known solutions. Unfortunately, this approach is suboptimal as estimating the covariance matrix is harder task. Thus, the authors provide a way to circumvent the issue and construct simple estimators whose sample complexity scales only linearly with the dimension.  The first algorithm is based on combination of the propose-test-release (PTR) framework, exponential mechanism, and the concept of Tukey depth. Roughly speaking, the authors show that by sampling from the distribution defined by the exponential mechanism with a score function based on the Tukey depth restricted to a data dependent set of possible outputs (some points with high Tukey depth), one can get an accurate and private algorithm for mean estimation. In order to maintain privacy while restricting the output to be a data-dependent output set, the authors make use of the PTR framework to test whether the specific dataset is ``safe’’ for use. They then show that when sampled from Gaussian data, a dataset will be safe with high probability, proving that the algorithm will often output an estimate of the mean. Finally, upon success the output will w.h.p. be a point of high Tukey depth, so the estimator is also accurate.   The second algorithm relaxes the requirement of strict Gaussian data to sub-Gaussian data. This algorithm is also based on PTR framework, with a clever application of ""skewed"" Gaussian noise addition. The standard Gaussian mechanism used in DP adds Gaussian noise scaled equally in all directions. The authors propose to use an additive Gaussian mechanism (to the empirical mean) that adds noise proportional to the empirical covariance of the data instead. Intuitively, this preserves the scale of the data (and thus preserves accuracy w.r.t. to the Mahalanobis distance). While this is not private for worst case datasets, the authors circumvent this by using the PTR to first test whether the dataset is ``good’’ (appropriately defined), and if need be, project the dataset to the set of good datasets before running the algorithm. Finally, they show that with high probability, sub-Gaussian data will be good and there will be no projection, thus the algorithm will output the empirical mean (perturbed with skewed Gaussian noise), that will be accurate w.r.t. the Mahalanobis distance.  ","The authors considered the problem of estimating the mean of d-dimensional Gaussian and sub-Gaussian distribution under DP constraint. The error bounds for the mean estimate is given in terms of the Mahalanobis distance, however, interestingly enough, the proposed method does not estimate the covariance matrix. The tool proposed by the authors is a sampling mechanism which is based on Tukey depth: the probability of choosing a point is exponentially proportional to the negative Tukey depth. Then one can run a mean estimator on the sampled data. This alone is not enough to accomplish DP, therefore the authors make use of a propose-test-release framework which is basically checks whether the sampled data contains too many points for which the Tukey depth is too small. The proposed methods of order D^{2/3} and in addition, it does not assume anything about the covariance matrix, and does not need a prior estimate for the covariance matrix as previous methods required. The sub-gaussian case basically find the closest gaussian model and applies the same mechanism. ",0.17391304347826086,0.3188405797101449,0.19806763285024154,0.4074074074074074,0.26851851851851855,0.15246636771300448,0.3333333333333333,0.14798206278026907,0.23163841807909605,0.09865470852017937,0.1638418079096045,0.384180790960452,0.22857142857142854,0.20214395099540583,0.21354166666666666,0.15884476534296027,0.20350877192982456,0.21829855537720705
1031,SP:edea1310cd8434f5627305a94208553597d1a97b,"The authors consider the problem of multi-armed bandits with the additional constraint that the learner can only pull arms that belong to the so-called arm-memory. First, the authors look at the regret minimization setting, and provide a worst-case regret lower bound of order $\Omega(T^{3/2})$ that any single-pass algorithm must satisfy, given that arm-memory size is strictly smaller than the number of arms. Single-pass algorithms can only add an arm or remove it from the arm-memory once. Second, the authors investigate the pure exploration problem and propose an algorithm that is $(\epsilon,\delta)$-PAC, only uses $O(\log^\star(n))$ space complexity, and has a sample complexity of order $O(\frac{n}{\epsilon^2} \log(1/\delta))$.","The paper studied the multi-armed bandit problem with bounded arm-memory. In this model, at each time at most $m<n$ arms (and their statistics) may be stored in memory. The paper is focused on a single-pass model, in which after an arm being removed from the memory it can no longer be accessed.  For the regret minimization problem, the authors prove a $\Omega(n^{1/3}T^{2/3}/m^{7/3})$ lower bound, and show a simple $\tilde{O}(n^{1/3}T^{2/3})$ upper bound.  For the best arm identification problem, they show an algorithm with sample complexity of $O(\frac{n}{\epsilon^2}(\log(1/\delta)+ilog^{(m-1)}(n)))$ where $ilog^{(r)}$ is the iterated logarithm of order $r$. Thus showing that $log^*(n)$ memory is sufficient for asymptotically optimal sample complexity. They also point out an error in [1] who previously claimed for showing such an algorithm.  ","This paper studies a variant of stochastic multi-armed bandits in which the learner doesn't have access to all the arms. The learner has a memory of size $m$, which means it can store and access only $m$ arms. The learner can discard some arms to free up space for new arms, which come in a stream. When an arm is discarded, it can never be accessed again.   The standard problems of regret minimization and best-arm identification are studied here; a regret lower bound for the former and an algorithm for the latter are given.  Let $T, n,$ and $m$ denote the time horizon, the number of arms, and the memory size, respectively. When $m=2$, a regret upper bound of $\widetilde{O}(n^{1/3}T^{2/3})$ was shown in previous work. This paper gives a matching lower bound up to logarithmic factors, and gives a lower bound of ${\Omega}(n^{1/3}T^{2/3}/m^{7/3})$ for all $m=1,2,\dots,n-1$. This is in sharp contrast with the $m=n$ case, which achieves $O(\sqrt{nT})$ regret.  For best-arm identification, the paper provides an algorithm with memory size $1 + \log^* (n)$ that pulls $O(n\log(1/\delta)/\varepsilon^2)$ arms and outputs an $\varepsilon$-optimal arm with probability $1-\delta$. This sample complexity is optimal even with memory size $n$. A similar result was shown in a previous work, but this paper claims that that proof had a bug.","This paper studies the classical $T$-period $n$-armed stochastic bandit problem under ""memory"" constraints: the decision maker can only store reward statistics for a certain maximum number $m < n \ll T$ of arms at any time $t\in[T]$. The authors establish an $\Omega\left( T^{2/3} \right)$ fundamental hardness result (modulo dependence on $m,n$) for the cumulative regret minimization problem under ""single-pass"" policies (arms discarded from memory cannot be recalled) with memory sizes up to $n-1$. This is optimal in $T$ since an algorithm with a matching upper bound in $T$ (up to log factors) already exists in literature. The result is also particularly interesting since it is known that the ""holy grail"" of $\Theta\left( \sqrt{T} \right)$ regret (modulo $n$) is achievable with memory size $n$. The authors also study the memory-limited best arm identification problem under the fixed confidence setting, and propose an $\left( \epsilon,\delta\right)$-PAC algorithm (where the parameters are the approximation and confidence factors respectively) with a space complexity of $m = \mathcal{O}\left( \log^\ast n \right)$ and an optimal sample complexity. Notably, a different algorithm that was claimed to have these very properties, already appeared in a STOC'20 paper. The authors show that the claims made in said paper are, in fact, erroneous, by constructing a family of instances on which aforementioned properties are provably violated.",0.2755905511811024,0.30708661417322836,0.28346456692913385,0.3333333333333333,0.2564102564102564,0.2151394422310757,0.22435897435897437,0.1553784860557769,0.15450643776824036,0.20717131474103587,0.17167381974248927,0.2317596566523605,0.24734982332155475,0.20634920634920637,0.2,0.25552825552825553,0.2056555269922879,0.2231404958677686
1032,SP:ee104acdbbe96a304b07b281a6c1b7b0efb0a545,"This paper tackles the problem of sim2real domain adaptation for the bird's-eye-view vehicle segmentation task. The authors trained existing models (Lift Splat and Point Pillar) with carefully sampled synthetic data,  pseudo-labels on real data (eq. 6), and gradient-reversal layers with f-DAL inspired loss. On the nuScenes dataset, the authors found that the these tricks improve models' (Lift Splat and Point Pillar) sim2real generalization accuracy of the vehicle segmentation IOU (Table 1 and Table 2).",The authors propose a technique for sampling data from a self-driving car simulator. They sample in a simulator-agnostic way in an effort to reduce the distance between the label marginals and therefore reduce the sim2real gap.They compare their approach to standard domain adapation techniques. They compare their techniques to standard data adaption frameworks and baselines and show promising results. They also demonstrate that domain adversarial training outperform simple style transfer approaches. ,"The paper proposes a method for both generating data in simulation and training with the generated data for the bird's-eye-view (BEV) vehicle segmentation task. It takes inspiration from recent theoretical advances in domain adaptation (DA) and frames learning from simulation as DA. It also points out special properties of using simulated data for adaptation, i.e., we can control the simulated data (label) distribution. Inspired by risk upperbound derived in prior work, it proposes a synthetic data generation strategy that samples data based on human-designed spatial priors and priors of the target domains (if some target labels are given). Once the data is generated, it employs a training method that extends recent work on adversarial DA to the dense prediction task of BEV vehicle segmentation and aims to learn invariant representation across domains. For experiments, the paper synthesizes data using the CARLA simulator and evaluates the adapted model on the nuScenes dataset. Results show that the method outperforms other data synthesis strategies and training methods such as style transfer-based DA.",This work targets the sim2real domain adaptation problem and approaches the problem from two angles - data collection and transfer method. The authors discuss the importance of how data is generated from the source domain and propose ways to effectively design scenarios (placement of other vehicles) that lead to better transfer. This work also introduces a novel method for domain adaptation that utilizes adversarial-training and learning with pseudo-labels. They measure transfer performance across domains (CARLA to NuScenes) on the bird’s-eye-view vehicle segmentation from multi-view images task.,0.1,0.3125,0.2,0.25675675675675674,0.17567567567567569,0.12571428571428572,0.10810810810810811,0.14285714285714285,0.17582417582417584,0.10857142857142857,0.14285714285714285,0.24175824175824176,0.1038961038961039,0.196078431372549,0.18713450292397665,0.15261044176706826,0.15757575757575756,0.16541353383458648
1033,SP:ee5f7316d51bcb8e09c19ee565498cc13f6349f3,"Conformal prediction is a method that intends to provide set-valued, calibrated prediction, in the sense that they cover the true class with a guaranteed marginal statistical accuracy. The paper mainly proposes to learn the conformal predictor at the same time as the predictive model is used, through an end-to-end procedure.  To this effect, conformity scores and threshold values (using quantiles) are approximated by differentiable surrogates that are directly injected into the learning framework. It is also argued that the framework can offer some solutions to better control class-wise predictions, or class-wise constraints on the conformal sets. ","In split conformal prediction, conformalization is applied using a model that has been trained on a separate training set. However, the training procedure usually optimizes an objective that has little to do with the ultimate goal of producing informative prediction sets. For the problem of classification, the paper proposes an alternative training procedure that simulates conformalization on mini-batches so that the learned model is directly optimized to produce smaller prediction sets. In addition, the proposed alternative training procedure can accommodate other loss functions, which can induce certain structures in the prediction sets. The performance gains are demonstrated through experiments.","**Summary:** * The paper considers putting conformal inference into the process of fitting deep neural networks, i.e., the paper considers ""differentiable conformal prediction"" / back-prop'ing through conformal inference. * The goal is to reduce the size of the prediction sets generated by conformal inference -- and more specifically to reduce the size variably in different parts of the input space. * Some experiments are given to show that the confidence set sizes are indeed smaller w/ the proposal than w/o it. ","Conformal prediction methods generally operate via post-processing on black-box classifiers to produce a confidence set. In this paper, the authors integrate this post-processing step into the training procedure in order to reduce the inefficiency of conformal prediction while providing the same coverage guarantee. The proposed method uses soft thresholding and differentiable sorting to convert two popular conformal post-processing method classess (THR - Sadinle et al. and APS - Romano et al.) into differentiable modules, although they only use THRLP (the method from Sadinle et al. applied to log-probabilities) in the training process for the results (in Table 1).  Additionally, they propose the use of weighted inefficiency regularization (also termed the size loss) and a ""configurable"" classification loss that can yield desirable behavior w.r.t. class-conditional inefficiency and mis-coverage (or coverage confusion) respectively.  The proposed method is shown to outperform a competing method by Belloti on 5 public multi-class datasets (including CIFAR100) and a binary dataset (wineQuality).",0.2079207920792079,0.13861386138613863,0.22772277227722773,0.15,0.22,0.225,0.21,0.175,0.1411042944785276,0.1875,0.13496932515337423,0.11042944785276074,0.208955223880597,0.15469613259668508,0.17424242424242423,0.16666666666666663,0.1673003802281369,0.14814814814814817
1034,SP:ee716fab2e1526de9bebd75417adaa97b439c2d3,"The paper deals with the task of semantic correspondence, i.e., finding correspondence between semantically similar images, e.g., the front left paw of two different dogs.  In a nutshell, the paper explores the use of Transformers for this problem and demonstrate that the built-in global receptive field is very beneficial for this task.  Specifically, the authors improve the cost-aggregation part of a semantic correspondence pipeline, which refines the initial matching costs.  The authors propose to concatenate the correlation between pixels (on a feature map from a neural network) with the corresponding appearance embedding, which helps to disambiguate noisy or ambiguous matches (like the left and right eye of a cat as in Figure 2).  Results are demonstrated on standard benchmarks.","The paper proposes a cost-volume refinement method using transformers to be used for a cost-volume-based semantic correspondence framework. The main innovations of the method are the use of transformers, how the cost volume is aggregated with appearance features, and the serial way in which appearance features are used. The method is tested on SPair-71K, PF-Pascal, and PF-Willow datasets. ","In this paper, the authors propose transformer-based cost aggregation networks for semantic correspondence with challenges of large intra-class appearance and geometric variations. The overall learning pipeline hierarchically aggregates the matching scores computed between disambiguated input features from semantically similar images by combining with swapping self-attention and residual connections. Extensive experimental results on three datasets, SPairk-71k, PF-PASCAL, and PF-WILLOW, show its efficacy on finding dense semantic correspondences.","This paper addresses the problem of finding dense correspondences between semantically similar images. Given a feature extractor, the primary focus of this work is on the design of a matching algorithm to find the optimal matches for each spatial location. The proposed method is a learned transformer based architecture that takes advantage of multi-scale features to resolve ambiguous local matches. Results demonstrate that the performance of the proposed model is comparable to existing methods on multiple datasets. ",0.15447154471544716,0.13821138211382114,0.21138211382113822,0.265625,0.21875,0.1527777777777778,0.296875,0.2361111111111111,0.3333333333333333,0.2361111111111111,0.1794871794871795,0.14102564102564102,0.20320855614973263,0.17435897435897435,0.25870646766169153,0.24999999999999994,0.19718309859154928,0.14666666666666667
1035,SP:ef09ca343401a8ec8486a8744d923dacb5e60525,"Existing edit-based models condition on a single sentence or document. This paper argues that such frameworks do not represent processes well in which the same document undergoes multiple iterations of revisions, for example the edit histories of Wikipedia articles or source code. The proposed system is a two-stage model that first labels each token in the next revision with one of four edit labels, and then employs a seq2seq second stage model to predict the output tokens for generative labels such as INSERT and REPLACE. Both stages do not only condition on the previous revision but on the n latest revisions. Gains in perplexity are shown, as well as improvements of two downstream tasks: ""edit-conditioned generation"" (generating meta information to edits such as comments) and ""edit-conditioned classification"" (1-way semantic intent classification).","This paper considers the task of explicitly modeling the process of editing sequences in an iterative manner, while the prior approaches for sequence editing are usually single-step editing methods. Thus, the proposed method is significantly novel. The proposal is to decompose the editing process into (i) predicting edit operations (insert, substitute, delete, and keep) and (ii) generating spans corresponding to those operations. Both these steps are conditioned on the previous versions of the input sequence and its edit history. The model first predicts edit operations conditioned on previous documents and then revises the document conditioned on edits and previous documents.  In my opinion, the experimental section can be improved significantly. It currently lacks sufficient comparisons with prior work to demonstrate the efficacy of iterative edit modeling over prior methods of single-step editing. The proposed evaluation metrics also seem to be directly aligned with the design of the proposed model, hence comparisons with other models on these metrics might be unfair.","This paper proposes a new task of modeling editing processes to model the whole process of iteratively generating sequences. To tackle the new challenge, the authors propose a conceptual framework to describe the likelihood of multi-step edits and describe neural models that can learn a generative model of sequences based on these multi-step edits. The Experimental results show that modeling editing processes improve performance compared to previous single-step models of edits on related downstream tasks and the proposed task.","The paper presents a new problem of modelling the series editing operations of an article, i.e. $p(x_1 \dots x_n )=\prod_t p(x_t | x_1\dots x_{t-1}) $, where each $x_t$ represents an article after t-th editing. There are three types of operations: insert, delete, and replacement. The paper propose a model based on a modified Transformer to predict both the edit operation and the content to be generated for insertion and replacement. To predict the edit operation type (including additional no-edit action), the model uses a Transformer encoder to encoder the k previous version of history (if $k=1$, it is just based previous version) and a simple autoregressive MLP to predict the type of editing operation for each of the token. For consecutive insertion and replacement edits, the model will take the average contextual representation of the span to generate tokens (together with the span start and end index and edit type embedding). For evaluation, the paper constructs two datasets with editing history: WikiRevisions (from wikipedia editing history with some cleaning) and CodeRevisions (from 700 python repositories on Github with MIT license, over 1000 commits and 500 stars). The paper propose a set of three metrics: operation perplexity, generation perplexity (given operation), and the editing perplexity combining both. On these metrics, the paper shows the proposed EditPro model improves over baseline LEWIS method on WikiRevisions data. It also shows more edit history is helpful in prediction, therefore EditPro based on three previous revision history is better than that based on one immediate previous version.   The paper also propose three downstream tasks for evaluation: editing given the text comment (from the commit message), edit-conditioned generation, and edit-condition classification.  ",0.17647058823529413,0.14705882352941177,0.25,0.16049382716049382,0.2222222222222222,0.3170731707317073,0.14814814814814814,0.24390243902439024,0.11724137931034483,0.3170731707317073,0.12413793103448276,0.0896551724137931,0.1610738255033557,0.18348623853211007,0.1596244131455399,0.21311475409836067,0.1592920353982301,0.13978494623655913
1036,SP:ef1f8f0697d64b160cd56070f7a9dbb88687f917,"This paper proposes an approach to train an image generator using only a short video clip or a single image. The overall framework follows the design of sinGAN that has an unconditional generator that generates new images from random noises, and the generated images and the input real images/frames are fed to discriminators to enforce similar distributions. The key contribution is to disentangle the image into content and layout features, and use different discriminators, which is claimed to reduce the memorization issue.",The paper presents a new GAN model that generates new scene compositions from a single training image or a single video clip.  This is achieved by a two-branches (content-layout) discriminator and a diversity regularization technique. The paper also claims as the first work learning from a single video.,"This paper proposes a single-image/single-vide GAN model that obtains both high quality and high diversity in the generated images, as opposed to previous models that only achieve one of the two. To achieve this, the discriminator consists of two branches that evaluate global layout and quality independently and a novel regularization approach for the generator is introduced. Comparisons to SinGAN, ConSinGAN, and FastGAN show improved performance.","The paper proposed a new method for generating new scene compositions using a single video. The method proposes a two-branch discriminator, one for content discrimination and the other for layout discrimination. It proposes a diversity loss for generator to encourage images generated from different latent codes to be perceptually different. Experiments show the method generates diverse and high-quality scene images.",0.13253012048192772,0.2289156626506024,0.14457831325301204,0.26,0.34,0.17391304347826086,0.22,0.2753623188405797,0.1935483870967742,0.18840579710144928,0.27419354838709675,0.1935483870967742,0.16541353383458648,0.25,0.16551724137931034,0.2184873949579832,0.30357142857142855,0.183206106870229
1037,SP:efa0a93c057725aa83d9413c4a07b5066e4b65ef,"This paper proposes a self-supervised approach for learning the state representation of RL tasks. The main contribution apart from prior works is to involve additional 'retracing' trajectory samples in representation training, which are trained by minimizing the propensity between retraced samples and forward posterior samples. They also proposed an intuitive way to mitigate the irreversibility in RL dynamics.","This paper considers state representation learning problem in deep RL. It exploits the cycle-consistency supervision and develops a “learning via retracing” approach. Such supervision signals are generally inherent in existing data and does not need additional interaction with environment, which leads to better sample efficiency. Learning from predictive supervision from temporally forward and backward directions reveals information from both the future and past to the target state, leading to more accurate latent state inference. In particular, the paper proposes the Cycle-Consistency World Model (CCWM) along with practical considerations (e.g., adaptive truncation to remove irreversible states), under the model-based framework based on generative dynamics modeling (CCWM).","This paper proposes ""learning via retracing"" as an approach to learn state representations, through matching a trajectory both in the forward and in the backward direction. This paper then introduces Cycle-Consistency World Model (CCWM) which is a model-based RL algorithm which learns through retracing. This method is sample efficient and provides good state representations for predicting the future and generalization. Furthermore, it proposes a value-based approach to identify ""irreversible"" transitions. ","The paper investigates a self-supervised approach for learning the state representation in RL tasks. In addition to the predictive (reconstruction) supervision in the forward direction, the authors include a “retraced” transitions for representation/model learning, by enforcing the cycle-consistency constraint between the original and retraced states. The authors claim that this facilitates stronger representation learning and improve upon the sample efﬁciency of learning. As it is not always possible to find such a cycle consistency between two states and for counteracting the negative impacts brought by the “irreversible” transitions, the authors add a novel adaptive “truncation” mechanism.",0.2542372881355932,0.1864406779661017,0.3728813559322034,0.1926605504587156,0.1743119266055046,0.2328767123287671,0.13761467889908258,0.1506849315068493,0.22,0.2876712328767123,0.19,0.17,0.17857142857142858,0.16666666666666666,0.27672955974842767,0.23076923076923075,0.18181818181818182,0.19653179190751446
1038,SP:f05c1f55b73378c219954ca30bd9825dbe51cde9,"The paper aims to break the label independence assumption in existing GNNs. To this end, a combination of GNNs and CRF is proposed, named SMN. SMN improves GNNs by modeling the joint distribution among discrete node labels; SMN is a promising alternative to CRF as it provides efficient learning and inference procedures. Empirically, SMN achieves better node-level and graph-level prediction accuracies than several existing models, with minor additional runtime overhead.","Authors study the problem of node labeling in the inductive case, i.e., at test time the goal is to label all the nodes of a given graph. For that problem, several variants of GNNs and CRFs have been proposed in the past, and the authors propose a Structural Markov Network that uses GNNs to model the potential functions of a CRF with the subtle difference that a proxy optimization problem is solved to make learning more efficient. Experiments are provided to demonstrate the applicability of their method. ","This paper proposes a CRF for classifying nodes in graphs where the CRF has a potential for each node and edge in the graph. GNNs are used for computing the potentials, one GNN for node potentials and one GNN for edge potentials. For general graphs, computing the partition function is intractable, so approximations are used during both learning and inference.  Learning draws from prior work and combines learning pseudomarginals of nodes and edges with GNNs and optionally some steps of ""refinement"" by optimizing a maximin game equivalent to likelihood maximization. Inference uses sum-product or max-product loopy BP (they perform similarly, though max-product is slightly better). The procedure is called a ""Structured Markov Network"" (SMN). Experimental results on several graph node classification tasks show that SMNs outperform several baselines, including both CRFs with standard training (using approximate inference during training) and CRFs trained with pseudolikelihood.  ","The paper targets the task of graph node classification in the inductive setting, taking an input node and edge feature representations, and inferring a categorical label for each node. It focuses on the case where it is not sufficient to classify a node independently of its neighbours, but where information stemming from predictions for neighbouring nodes needs to be taken into account.  It improves over the current approaches by offering a performant and efficient method to combine advantages of CRFs (joint inference) with the representational power afforded by GNNs, while overcoming the computational challenges. In particular, this paper improves on the closest attempts at combining CRF and GNN, Ma+ 2018 and Qu+ 2019, which use pseudolikelihood; Qu+ 2019 is used as baseline in experiments (“GMNN”), while pseudoloikelihood as training objective is investigated in sec5.5.5.  --- update after rebuttals I have read other reviews and the authors' replies, as well as extra experiments. I am maintaining my score.",0.18055555555555555,0.20833333333333334,0.19444444444444445,0.17045454545454544,0.22727272727272727,0.14285714285714285,0.14772727272727273,0.10204081632653061,0.08860759493670886,0.10204081632653061,0.12658227848101267,0.13291139240506328,0.1625,0.136986301369863,0.1217391304347826,0.1276595744680851,0.16260162601626016,0.13770491803278687
1039,SP:f06b9c93fd9b3eb31cdc3b5b0b598b7254806221,"The paper seeks to develop invariance and equivariant graph neural networks.  Based on the assumption that node feautures can be separated into a node coordinate feature vector and coordinate-independent node feature vector, the authors develop a GNN architecture, called ""angle preserving graph network"" (AGN), that is  equivariant to  the Euclidean group and conformal group. The method is evaluated using the ""benchmark"" graph datasets produced in (Dwivedi et al., 2020) from, e.g., MNIST and CIFAR10 datasets.","The authors propose distance preserving Graph Network and Angle Preserving Graph Networks which are equivariant to node permutation as well distance preserving transformations/ angle preserving transformations of the coordinates associated with the nodes. The authors generalize the E(N) GNN work to include group symmetries to the conformal group and to the case when not all nodes in the graph are connected. Experiments on the  polytopes classification dataset show that the model is effective when symmetries are present in the data, and ineffective when they are not (MNIST, CIFAR and TSP).",This paper proposes two graph networks that are equivariant to distance and angle preserving transformations in graph coordinates. It moves one step forward based on GN (Graph Network) by decoupling node coordinates from other node attributes. The experiments on synthetic datasets shows the capabilities of the proposed symmetry-driven graph networks.,"This paper proposes two kinds of equivariant GNNs, including the distance preserving graph network (DGN) that is equivariant to the Euclidean transformations and angle preserving graph network (AGN) that is equivariant to the Conformal group. The proposed frameworks generalize several typical previous architectures. The advantage is experimentally evaluated on a synthetic dataset composed of n-dimensional geometric objects. ",0.23376623376623376,0.16883116883116883,0.2597402597402597,0.16483516483516483,0.1978021978021978,0.3333333333333333,0.1978021978021978,0.2549019607843137,0.3448275862068966,0.29411764705882354,0.3103448275862069,0.29310344827586204,0.2142857142857143,0.203125,0.2962962962962963,0.2112676056338028,0.24161073825503354,0.3119266055045872
1040,SP:f0a61fbf44f88eef53f21ef8433330359ae211f4,This paper uses Gaussian mixture as a prior to address the scenario of imbalanced attributes in GANs latent space clustering. The study derives Stein latent optimization that provides reparameterizable gradient estimations when assuming a Gaussian mixture prior for the latent space. The results show that the proposed model achieves superior performance compared to the baselines. ,"The paper is concerned with unsupervised image generation using GANs. They devise a solution (SLOGAN) for addressing the problems current models have, especially when dealing with datasets with unbalanced features/attributes. In addition, to learn attributes from data and improve the conditional generation, it suggests using a new contrastive loss (U2C).   ","The paper proposed a Stein Latent Optimization for Generative adversarial networks, which can perform conditional generation in an unsupervised manner. The core innovation is the reparameterizable gradient estimations. The proposed unsupervised conditional contrastive loss further ensures the single attribute generation capacity. The idea is somewhat novel and motivated. The authors perform empirical studies on diverse datasets to conclude that the proposed algorithm is effective in learning balanced or imbalanced attributes.","This work proposes a method for performing unsupervised conditional generation from imbalanced attributes. The key components of this work are the following:  * A (architecture agnostic) GAN with a GMM latent space, where we have one component per attribute in the data.   * The parameters of the components $(\mu, \Sigma)$ are learned via implicit reparameterization, and leveraging stein's lemma to derive gradients for $\mu$ and $\Sigma$.   * The components are encouraged to be distinct by introducing a contrastive objective. An encoder is used to obtain a low dimensional vector representation of a generated sample, and this sample is encouraged to be close to the mean vector of the corresponding component in the latent space. ",0.12727272727272726,0.23636363636363636,0.23636363636363636,0.17647058823529413,0.19607843137254902,0.2,0.13725490196078433,0.18571428571428572,0.11607142857142858,0.12857142857142856,0.08928571428571429,0.125,0.1320754716981132,0.20800000000000002,0.15568862275449102,0.14876033057851237,0.1226993865030675,0.15384615384615385
1041,SP:f0ab1df67e9d971948d5ead8d5ad68355e9d8bf6,"This paper analyzes intriguing properties of Vision Transformers (ViTs) in the context of robustness and generalizability compared to CNNs. In particular, they show that (i) ViTs are highly robust to severe occlusions and perturbations, (ii) ViTs are significantly less biased towards texture, (iii) ViTs that encode shape representation provides accurate semantic segmentation without labels, (iv) we can ensemble off-the-shelf tokens from a single ViT model for transfer learning to a wide range of tasks. They conduct their experiments on three transformer families (ViT, DeiT, T2T) across 15 vision datasets. ","The paper systematically studies ViT’s performance on occlusions, domain shifts, spatial permutations, adversarial and natural perturbations. Experiments show that ViT models are relatively more robust to occlusions, perturbations and domain shifts, and less sensitive to local textures. Off-the-shelf features from ViT models yield high accuracy on several downstream classification datasets.","This paper explores a variety of properties of ViTs about occlusions, shapes and texture bias, perturbations, robustness and domain shifts. The authors empirically demonstrate above merits of ViTs compared with CNNs by sufficient experiments on fifteen vision datasets. Besides, an architectural modification to DeiT for shape distillation and a transfer approach utilizing an ensemble of representations with a pre-trained ViTs are proposed as new design choices. ","This paper analyzes several intriguing properties of vision transformers (ViTs): First, it shows vision transformers have strong robustness against severe occlusions for foreground objects, non-salient background regions, and random patch locations, which are significantly better than the CNN counterparts. Second, vision transformers demonstrate better awareness of the shape whereas CNNs make decisions mainly based on texture. A shape distillation strategy is further proposed to improve the ViTs for a balance between classification performance and shape bias. Third, vision transformers achieve better robustness against spatial patch-level permutations, adversarial perturbations, and common natural corruptions. Finally, vision transformers transfer better than CNNs on visual classification and few-shot learning. A training scheme using multi-block class tokens (and/or average patch tokens) as a feature ensemble is applied to further boost the performance.",0.18681318681318682,0.15384615384615385,0.24175824175824176,0.18867924528301888,0.20754716981132076,0.2835820895522388,0.32075471698113206,0.208955223880597,0.16666666666666666,0.14925373134328357,0.08333333333333333,0.14393939393939395,0.23611111111111113,0.17721518987341772,0.19730941704035873,0.16666666666666666,0.11891891891891893,0.19095477386934673
1042,SP:f11e6f0e074e9b5032b396d17ab6b531d5ba0f00,"This work studies random features regression on a setting where data $(x_{i}, y_{i})\in\mathbb{R}^{n_{0}+1}$, $i=1, \cdots,n$ is generated from a noisy linear model $y_{i}=\beta^{\top}x_{i}+\epsilon_{i}$ with correlated covariates $x_{i}\sim\mathcal{N}(0, \Sigma)$, planted weights $\beta\sim\mathcal{N}(0,\Sigma_{\beta})$ and independent Gaussian noise $\epsilon_{i}\sim\mathcal{N}(0,\sigma_{\epsilon}^2)$. Estimation is given by $\hat{y}(x) = \hat{\beta}^{\top}\sigma\left(Wx\right)$ with $\sigma$ a real-valued activation function and $W\in\mathbb{R}^{n_{1}\times n_{0}}$ a Gaussian matrix with iid $\mathcal{N}(0, 1/n_{0})$ entries, and $\hat{\beta}\in\mathbb{R}^{n_{1}}$ minimising the ridge regression risk on the features $f = \sigma\left(Wx\right)\in\mathbb{R}^{n_{1}}$.  Its main theoretical contribution is to provide an asymptotic characterisation of the test MSE, the bias and the variance on the proportional regime $m,n_{0}, n_{1}\to\infty$ with fixed ratios $\phi=n_{1}/m, \psi=n_{0}/n_{1}$, as a function of the asymptotic spectral statistics of $(\Sigma, \Sigma_{\beta})$, the ratios $(\phi,\psi)$, the $\ell_2$ penalty strength $\gamma>0$, the noise variance $\sigma_{\epsilon}^2$ and the constants $\rho = \mathbb{E}\left[z\sigma(z)\right]$, $\eta = \rm{Var}\left[\sigma(z)\right]$, $z\sim\mathcal{N}(0,1)$, characterising the activation $\sigma$.   It provides numerical experiments confirming the validity of their formulas for finite but large dimension, and discusses the implications of the anisotropies $(\Sigma, \Sigma_{\beta})$  on the bias, variance and test error in different regimes. In particular, it shows that alignment between $\beta$ and the leading eigenvectors of $\Sigma$ reduces the test MSE, and that anisotropy can induce double-descent like behaviour in the learning curves.   ","This paper studies nonlinear random feature regression of a linear response in the *anisotropic* setting where both the covariates and parameter have arbitrary covariance matrix. The training error and test error are computed explicitly as the dimension, sample size, and number of features jointly tend to infinity. The proof method relies on linearization and a technique from free probability involving the construction of linear pencils. The asymptotic formulas are used to theoretically describe (i) the beneficial effect of overparameterization on the bias and variance components of the test error and (ii) the number of critical points of the test error as a function of overparameterization. This paper graphs the asymptotic formulas for some specific models to demonstrate parameter-wise and sample-wise multiple descent phenomena. ","The paper studies random feature regression in high dimensional setting, when the data and model weights are assumed to be normal with anisotropic covariances. By an error decomposition argument, the exact formulas for the bias, variance and total error are provided. The results motivates the discussion on the multiple descent effect of the sample-wise test error curve, and the authors also prove that the error against overparametirazation only exhibits double descent. The phase transition is associated with the spectral structure of the approximated kernel matrix. Also, the error is improved when the data covariance aligns more with the model weight covariance.","This paper evaluates the training and generalization errors of the random feature (ridge) regression model, in the regime where the number of samples $m$, the input feature dimension $n_0$, and the hidden layer size $n_1$ tend to infinity at the same rate. More precisely, the authors consider * Gaussian data with zero mean and general covariance $x_i \sim \mathcal{N}(0, \Sigma)$; * label generated by $y(x_i) = \beta^\top x_i/\sqrt{n_0} + \epsilon_i$, for some additive noise $\epsilon_i \sim \mathcal{N}(0, \sigma_\epsilon^2 ) $ and coefficient vector $\beta \sim \mathcal{N} (0, \Sigma_\beta)$; * (random feature) kernel regression on $K(x_i, x_j) = \frac1{n_1} \sigma(W x_i/\sqrt{n_0})^\top \sigma(W x_j/\sqrt{n_0})$;  and evaluate the training and test performance as a function of the dimensionality, the nonlinear function (via a few scalar parameters), as well as the spectral behavior of $\Sigma$ and $\Sigma_\beta$.  As a consequence of this theoretical evaluation, the authors characterize sample-wise multiple descents, steep cliffs, and how they relate to, e.g., the spectrum of the (random feature) kernel matrices and the structure of coefficient vector $\beta$. ",0.11764705882352941,0.09477124183006536,0.20261437908496732,0.224,0.272,0.2549019607843137,0.288,0.28431372549019607,0.31,0.27450980392156865,0.17,0.13,0.16705336426914152,0.14215686274509803,0.24505928853754944,0.24669603524229075,0.20923076923076922,0.17218543046357615
1043,SP:f195188e684d5131c30ffcc6258856b02ee45926,"This paper extended VRX (Ge et al. 2021) with an interface to allow human modify the class-specific structural concept graphs (c-SCG) as well as a procedure to distill the human changes in the c-SCGs back to original task's neural networks. This is an interesting piloting idea. However, the scale of the experiments is not convincing enough to demonstrate the generalizability of the performance to a large number of concepts, classes, datasets and complex scenarios. ",The paper provides a way for explaining the reasoning of a neural network to humans in the form of a class-specific structural concept graph (c-SCG). The c-SCG can be modified by humans. The modified c-SCG can be incorporated in training a new student model. Experiments show that the new model performs better on classes that their corresponding c-SCG have been modified.,"The paper proposes an interactive learning algorithm for image processing tasks based on structural explanations. The paper therefore reuses recent advances in creating structured image explanations for image classification and uses knowledge distillation to update the model after end-users manipulated them.  The approach is evaluated for robustness / predictive improvement and zero-shot learning on known datasets (ILSVRC2012 and iLab-20M, object dataset). The baselines are the non-interactive system and a system with more training data per class - each with the same architecture. The results show that the proposed approach generates better results after human corrections and can be used for novel classes.","The paper proposes a technique for allowing humans to interact with NNs.  It starts by approximating a NN by extracting concepts from an image, using those concepts to form a graph, and then putting that graph through a GNN.  Then, it allows users to edit these ""concept based graphs"" and uses those edited graphs to retrain the GNN.  Experimentally, this is shown to improve performance in three different experiments.   ",0.19230769230769232,0.16666666666666666,0.15384615384615385,0.22727272727272727,0.18181818181818182,0.10576923076923077,0.22727272727272727,0.125,0.17391304347826086,0.14423076923076922,0.17391304347826086,0.15942028985507245,0.20833333333333331,0.14285714285714288,0.16326530612244897,0.1764705882352941,0.17777777777777776,0.12716763005780346
1044,SP:f1d59aab76442bb3e920219bc52bae762dc9a57f,"The authors study regularized regression with a sparsity inducing constraint. (In the appendix also the extension to the low-rankness inducing case is considered.) The authors propose a new non-convex formulation, which is optimized via BFGS. The non-convex formulation is motivated by the well-known IRLS (Iteratively Reweighted Least Squares) algorithm. Extensive numerical simulations are conducted on a wide variety of problems and the proposed algorithm is compared with a number of state-of-the-art approaches. While the proposed algorithm does not always perform best, it is in all cases at least competitive to the best performing approach. Moreover, the authors prove that the induced non-convex function, which is optimized in their approach, is ridable -- meaning that all local minima are global minima and the Hessians of all saddle points have at least one negative eigenvalue. ","The authors propose a bilevel optimization approach for solving regularized empirical risk minimization. The considered minimization problem ($\cal{P}_{\lambda}$) refers to a linear model with parameters $\beta$ that are regularized in terms of a sparsity enforcing term $R(\beta)$. More specifically, the authors treat regularizers $R$ that can be recast in a so-called quadratic variational form. Therewith, a reparametrization of the original problem can be regarded as a bilevel program with smooth inner problem. Subsequent to the introduction, the authors provide a characterization of regularizers that have a quadratic variational form, followed by a theoretical analysis and numerical experiments.","Authors proposed a re-parametrization of the variational form of usually sparsity enforcing penalties (including the Lasso and its variations). The proposed variational re-parametrization leads to a non-convex but smooth bilevel optimization problem, that authors propose to solve using usual L-BFGS. In addition to an extensive benchmark, authors show that the potential saddle points of the new minimized function are not ""pathological"".","This paper proposes a reparametrization of common objective functions in the sparse estimation literature (for linear models). This allows rewriting LASSO, group LASSO, low-rank estimation as a smooth bilevel problem, and then applying fast methods for smooth problems such as L-BFGS. Critically, the gradient to the outer problem has a simple form to compute. The smooth problem is shown to have no spurious saddles, although it is nonconvex. The paper then shows impressive empirical results on multiple problems of interest stemming from this simple reparametrization.",0.1357142857142857,0.12142857142857143,0.10714285714285714,0.13861386138613863,0.1485148514851485,0.2,0.18811881188118812,0.26153846153846155,0.1724137931034483,0.2153846153846154,0.1724137931034483,0.14942528735632185,0.15767634854771784,0.16585365853658537,0.13215859030837004,0.16867469879518074,0.15957446808510636,0.1710526315789474
1045,SP:f1eb66f24a14808d404f9ad9773ef4288efa060e,"The sliced Wasserstein distance (SWD) is a metric on the space of probability measures and serves as a practical alternative to the standard metric arising from the optimal transport (OT) problem, the Wasserstein distance. Indeed, while the Wasserstein distance suffers from computational and statistical limitations on large-scale and high-dimensional settings, SWD can alleviate these issues while offering similar theoretical properties. Hence, SWD has been increasingly popular within the statistical and machine learning (ML) community in recent years.  SWD is defined as an expectation that is intractable in general; in practice, it is then usually approximated with a simple Monte Carlo estimation, but prior studies reported that this method might induce an important approximation error on high-dimensional settings, leading to an underestimation of the ""true"" dissimilarity between two distributions. Therefore, one would need to compute a Monte Carlo approximation based a very large amount of samples to obtain an accurate approximation, which increases the computational complexity of SWD.   Several variants of SWD have recently been introduced to overcome this problem and include the ""generalized sliced Wasserstein distances"" (GSWD): this class of distances aim at offering a better computational efficiency, by comparing distributions from their nonlinear one-dimensional representations (instead of the linear ones used in SWD). This paper focuses on this line of work: the limitations of GSWD are identified to motivate the formulation of a novel class of distances, called ""Augmented Sliced Wasserstein distances"" (ASWD).  Similarly to SWD and GSWD, the definition of ASWD is connected to the Radon transform: the authors introduce a novel type of Radon transform, the ""spatial Radon transform"" (Definition 1), and use it to define the class of augmented sliced Wasserstein distances.  Then, they study the theoretical properties of the spatial Radon transform and ASWD. Their main result states that ASWD verify all metric axioms if and only if the mapping $g$ (that characterizes the underlying spatial Radon transform) (1) is injective (Theorem 1), or (2) maximizes ASWD over the space of bounded and injective functions (Corollary 1.1). The authors then introduce a specific class of mappings (Equation (15)) which meet the conditions of Corollary 1.1 thus guarantee that the resulting ASWD is a metric. Such mappings require training a neural network to make ASWD data-adaptive. Finally, the superior empirical performance of ASWD against existing variants of SWD (including GSWD) is illustrated on various problems, including gradient flows, generative modeling, color transferring and barycenters of measures.  ","The paper develops a computationally efficient metric between two probability distributions, which is called Augmented Sliced Wasserstein Distances (ASWD). The proposed metric applied neural networks to map two distributions from $\mathbb{R}^d$ into high dimensional non-linear manifolds, then the mapped distributions are compared using the sliced Wasserstein distance. It is shown in the paper that ASWD is a well-defined metric, and an algorithm for empirically evaluating ASWD is provided. The empirical applicability of ASWD is demonstrated by numerical experiments including gradient flow, generative modeling, sliced Wasserstein autoencoder, image color transferring and  sliced Wasserstein barycenter.","This paper introduces _Augmented Sliced Wasserstein Distances_ (ASWD) which consist, in a nutshell, in computing the usual SWD between pushed measures: $\mathrm{ASWD}(\mu,\nu; g) = \mathrm{SWD}(g \circ \mu, g \circ \nu)$ (note: the pushforward operator does not render properly, so I will use $\circ$ instead). The idea is then to maximize on $g$ (under constraint/regularization) to obtain a pushforward map $g$ that provides the most discriminative projection directions. Authors provide an exhaustive condition on $g$ (namely, injectivity) to make the ASWD a metric between probability measures. Their approach is then showcased on a various set of benchmark experiments in computational optimal transport (gradient flows, generative models, auto-encoder, barycenters, color transfert...).","The author proposed a variant of sliced Wasserstein distance (SWD) called **augmented sliced Wasserstein distance** (ASWD) which replace the linear projections in SWD with a non-linear one. The construction of ASWD consists of two steps: (1) define spatial Radon transform (SRT) for probability measures (2) replace the Radon transform in the original SWD with this new SRT. The key intuition of SRT is to first transform the function $\pmb{x}$ to high dimensional hyperspace by transformation $g(\pmb{x})$ and then perform Radon transform on functions defined in this hyperspace.   Theoretically, the author proves the conditions required for SRT to be injective. Later, the author uses this result to show the corresponding ASWD is a distance metric. In addition, the author proposes a training objective for finding the transformation $g(\cdot)$.   Empirically, the author conducts both toy and practical experiments to demonstrate the advantages of ASWD in terms of higher projection efficiency and also provides some ablation studies in the appendix to investigate the effects of different $g(\cdot)$ choices.",0.09828009828009827,0.0687960687960688,0.11547911547911548,0.20618556701030927,0.25773195876288657,0.20869565217391303,0.41237113402061853,0.24347826086956523,0.27485380116959063,0.17391304347826086,0.14619883040935672,0.14035087719298245,0.15873015873015872,0.10727969348659006,0.16262975778546712,0.18867924528301888,0.18656716417910446,0.1678321678321678
1046,SP:f1fff3cf410a296e20250c10ac01b86856bb1468,"This paper tackles the high variance issues of the existing sampling methods, which approximate the full-graph training of GNNs. The proposed method is built upon existing work that treats GNN neighbor sampling as a multi-armed bandit problem. The authors design a novel biased reward function to reduce variance and avoid numerical instability. The proposed reward function leads to a more meaningful notion of regret directly connected to sampling approximation error. Based on that, the authors prove that the regret of the proposed algorithm is near-optimal and demonstrate the fast convergence of the sampling approximation error. Empirical results verify that the proposed algorithm, Thanos, has improved variance estimates and performance across benchmarks over BanditSampler and others.","The authors propose Thanos, a bandit-based approach for subsampling graph neighbourhoods, to be used as context for graph neural network predictions. It improves on the prior art (BanditSampler) by proposing trading off increased reward bias for less variance. This allows the authors to reduce numerical optimisation issues, achieve competitive benchmarking performance, as well as explicitly take into account the GNN training dynamics in their theoretical analysis, demonstrating near-optimal regret (with an added ln T factor).","The authors propose a novel bandit sampler, addressing the limitations of the previous work, for graph neural networks. The previous work utilized EXP3 which as known issues with low probabilities (variance can be really high) and leads to small learning rate. The regret bound was given in terms of a static oracle instead of a dynamic one. Also, the plays are considered in the oblivious adversary setting.  These issues are addressed in the current work by rethinking the reward function. Experimental results are shown on a wide variety of datasets as well as GNN architectures to validate the proposed algorithm.  ","This paper proposes an adaptive neighbor sampling method for graph neural networks applied to large graphs. This method is based on Rexp3, an adversarial bandit algorithm. Central to it is a novel reward, which unlike previous work induces a biased sampler, which is shown to have a near optimal regret. Even though it is biased, this sampler makes up for it by having better numerical stability, requiring less unrealistic assumption for the bound, and having provably faster convergence to variance reduction of the estimator.  Empirically, the paper confirms that the behavior of the proposed method matches theoretical predictions and performs better than a more naive bandit approach on a toy graph problem and Cora. Large scale experiments are then conducted on several standard benchmarks where the proposed methods appears to outperform baselines.",0.09322033898305085,0.17796610169491525,0.2033898305084746,0.2077922077922078,0.14285714285714285,0.19,0.14285714285714285,0.21,0.18181818181818182,0.16,0.08333333333333333,0.14393939393939395,0.11282051282051282,0.1926605504587156,0.192,0.1807909604519774,0.10526315789473685,0.16379310344827586
1047,SP:f22d271703c9ef539033e0c54b497d90ab9585cd,"This paper proposes investigates the role of positional and relational embeddings in transformer-based policies for multi-task reinforcement learning (MTRL) across different morphologies. Unlike prior works that completely discard the structural information of the robot in transformer-based policy, this paper proposes to leverage structural information using traversal-based positional embedding and graph-based relational embeddings. Experiments are performed for MTRL as well as transfer learning on several gym environments, which demonstrate the effectiveness of the proposed method against the baselines.",This work introduces a structure-aware transformer for inhomogeneous multi-task reinforcement learning tasks. This work proposes to use traversal-based positional embedding and graph-based relational embedding to encode morphological information. The papers show that their proposed approach outperforms prior state-of-the-art methods on the module multi-task RL benchmarks and transfer learning settings.,"This paper addresses the inhomogeneous multi-task reinforcement learning (MTRL) problem. Following existing works (Huang et al. 2020, Kurin et al. 2021), a policy is represented as a graph. Each node in the graph is an identical modular neural network. The current state-of-the-art approach,  (Kurin et al. 2021),  uses a self-attention mechanism which allows direct communication between nodes. The author argued that the sel-attention mechanism discards the morphological information, which may be critical for agent learning and better perfromance. Therefore, they proposed to encode the morphological information via traversal-based positional embedding and graph-based relational embedding. The proposed approach is simple yet outperforms the state-of-the-art approaches on the MUJOCO MTRL environments.  ","The paper proposes SWAT to incorporate the agent's morphology into transformer-based policy in multi-task RL. Assuming that the agent's limbs are nodes and joints are edges of a graph, two embeddings capture the morphological structure: first, positional embedding (PE) to represent the absolute position of limb nodes by tree-traversal (pre-order, in-order, post-order), and second, relational embedding (RE) to represent the relative distances of limb nodes in terms of their graph connectivity (normalized Laplacian, shortest path distance, personalized page rank).  The hypothesis is SWAT's traversal-based PE and graph-based RE enable effective multi-task policy learning in diverse morphological tasks. In its support, the paper demonstrates that SWAT achieves a higher average return than the baselines GNN-based method (SMP) and morphology-free transformer (AMORPHEUS) in Gym Mujoco locomotion tasks. While SWAT and AMORPHEUS are close in performance on four in-domain locomotion tasks, SWAT shows a significantly higher average return than all baselines in five cross-domain tasks with diverse morphologies. Two ablation experiments analyze the performance impact on excluding PE and RE. The paper also qualitatively examines the behavior patterns for SWAT and AMORPHEUS policies trained in cross-domain (CWHH++) and tested in a single task (Humanoid).",0.2682926829268293,0.3170731707317073,0.2804878048780488,0.45614035087719296,0.2807017543859649,0.19166666666666668,0.38596491228070173,0.21666666666666667,0.11057692307692307,0.21666666666666667,0.07692307692307693,0.11057692307692307,0.3165467625899281,0.2574257425742575,0.1586206896551724,0.2937853107344633,0.12075471698113208,0.14024390243902438
1048,SP:f24a27377b117c6cabaf7434393b370c7c7e10a0,"This paper studies the extreme multi-label classification problem. Specifically, on top of the existing partition-based approaches that partition the label space into mutually exclusive clusters, the paper proposes a solution to relax this assumption by disentangle the multi-modal labels with non-exclusive clustering as an optimization approach. Evaluations are reported on four datasets against the existing baselines. ","The authors propose a new method for partitioning labels space that can be used with a family of currently popular partition-based/label tree-based extreme multi-label classifiers, which organize labels in the form of the tree. The proposed method introduces redundancy of labels in the different label clusters, which are mutually exclusive to the clusters in many methods. The problem of label assignment is formulated as maximization of matcher's (part of classifier predicting clusters for labels) precision, which is optimized by adding new labels to the cluster after initial training with clusters with mutually exclusive labels. The new partitioning method is applied to XL-Linear and X-TRANSFORMERS in the experimental study, resulting in a steady improvement in predictive performance on four popular benchmark datasets.","In extreme Multi-label classification (XML) a large range of methods based their prediction on partitioning whether the input space or label space.  The paper addresses this partitioning phase, proposing a ""disentangled partitioning"", allowing overlapping clustering of labels. The proposed model consists to use two matrices, the first one M corresponding to the classification of the examples to each cluster (the matcher) and the second one C indicating the assignment of the labels to each cluster.  The goal is to optimize those matrices ensuring that  MC is close to the label matrix under constraints to ensure that each label is present up to lambda time in different clusters (the disentanglement factor). After a formal analysis of the problem, the authors propose a simple algorithm to optimize the matrix C, using existing approaches to learn the matcher. To validate the approach authors first propose an experiment merging artificially labels for clustering. The second series of experiments evaluate the performances wrt SOTA algorithms and varying the disentanglement parameter. ","Label partitioning approaches are well-explored in Extreme Classification (XC) literature whose core idea is to cluster semantically similar labels together and then to sample the negative labels only from a small number of relevant clusters. Although this has been shown to significantly speed up the training and prediction routines, most existing partitioners simplistically assume that each label belongs to exactly one cluster. This could degrade prediction accuracies since labels in XC can often be multi-modal with multiple associated semantics. This paper addresses this issue by relaxing the single cluster assumption and thus allowing each label to instead end up in \lambda different clusters. This is achieved though a novel cluster assignment algorithm which is shown to be theoretically well-defined and leads to small but consistent accuracy gains. The proposed algorithm has the added advantage of being plug-and-play, i.e. it can be used with many matching and ranking models based on label partitioning. Enough experiments have been conducted on simulated and real datasets to demonstrate the accuracy gains and discuss the associated increases in costs.",0.31666666666666665,0.3,0.26666666666666666,0.1953125,0.1796875,0.18674698795180722,0.1484375,0.10843373493975904,0.08888888888888889,0.15060240963855423,0.12777777777777777,0.17222222222222222,0.2021276595744681,0.1592920353982301,0.13333333333333333,0.17006802721088438,0.14935064935064934,0.1791907514450867
1049,SP:f24e8f7f4e740873b63bacbac50a38eb82f6a4ac,The authors propose a new method to learn a disentangled representation. The proposed method is Variational Autoencoder (VAE) based. The latent variables consist of nuisance factors and disentangled factors that form the $k$ generative factors per dataset. To help learn the disentangled factors of variation the authors propose to use an additional discriminator and interventions on the disentangled factors. The authors demonstrate the performance of the proposed model on the dSprites and CelebA dataset.,"The paper proposes a new approach for training disentangled generative models. On top of VAE, it separates the latents into two disjoint groups: disentangled and entangled ones, and progressively increases the size of the disentangled group one at a time. To encourage disentanglement, it changes one random dimension of disentangled latent and pushes it to decode and then encode into the same one, and has a GAN loss to make sure that the changed generated distribution matches the ground-truth distribution. The paper shows the performance of the proposed approach on dSprites and CelebA datasets.","This paper presents an unsupervised learning model for disentangled latent representation learning. Specifically, the model works on a combined latent space including both entangled variable and separable variable. The most impressive part is the one-at-a-time (OAT) factor learning approach that iteratively uncovers disentangled dimension and learned from reconstructed samples. The OAT approach generally sounds. ","This study proposes a disentangled representation learning method called ""one at a time"", or OAT factor learning which is a VAE-GAN network to generate high resolution samples and to learn variational factors in an unsupervised manner, without knowing the number of ground-truth factors a priori. The authors formulate the latent space as a union of two disjoint sets, $z_1$ and $z_2$, where $z_1$ corresponds to the variational (disentangled) factors, and $z_2$ denotes either noise or sample-specific factors. To train the proposed network, the authors use a two-step training approach in which they first train the VAE network with only $z_2$ latent factors. Then, they start to learn $z_{1_k}$, one dimension at the time. To enforce $z_1$ to encode the disentangled factors, they use an intervention-based approach, in which the value of one dimension in $z_1$ is changed, while the remaining $z_1$ factors are kept unchanged. The authors reported their experimental results for two datasets, i.e., dSprites and CelebA.",0.32432432432432434,0.13513513513513514,0.3783783783783784,0.16842105263157894,0.2631578947368421,0.2982456140350877,0.25263157894736843,0.17543859649122806,0.16091954022988506,0.2807017543859649,0.14367816091954022,0.09770114942528736,0.28402366863905326,0.15267175572519084,0.22580645161290322,0.2105263157894737,0.1858736059479554,0.1471861471861472
1050,SP:f254d0d36bf67cdd74052f60a68073cb36abfce4,"This paper proposes a model-based meta-RL approach to offline learning over a distribution of MDPs (offline meta-RL). The proposed method is inspired by COMBO and in fact is a direct extension of it to the multi-task setting. The extension involves meta-learning a model-initialisation (or prior) that can be rapidly adapted to new offline RL tasks. They further add a proximal RL policy-improvement operator, where the prior policy is meta-learned over the task distribution. They establish theoretical guarantees for their method and conduct a careful empirical investigation to motivate its design, while establishing that it provides additional benefits compared to relevant baselines. ","Targeting offline meta-reinforcement learning, the work proposes a model-based method called MerPO, with conservative value evaluation and individual policy improvement method with the tradeoff between the meta-policy and the behavior policy influence.  The main algorithm MerPO includes an initialization step and a two-loops meta-learning approach. The initialization step learns the model of the meta-model and dynamics for each task. With the fixed estimations of the models of tasks, the proposed merPO will alternatively update the policies for each task and the meta-policy.  The main contribution claimed in this paper is that the proposed method is a more robust method with the design of a regularization term involving the behavior policy, which improves the meta-learning policy when the behavior policy is actually better than the current meta-policy. Theoretical guarantees are displayed and experiments are conducted to show the performance of MerPO.","This paper considers the challenge in offline meta-RL problems, particularly, the difficulty with balancing a learned policy between exploring OOD state-actions by following the meta-policy and exploiting the offline dataset. To solve the problem, it starts with proximal meta-RL approach in the online setting, points out the problem of a degraded meta-policy in the offline setting due to lack of feedback from online interactions and proposes to regularize the task policy with additional penalty from deviating from the behaviour policy. It also integrates the model-based offline RL method for single tasks (COMBO) and offline meta-learning for task specific dynamics into its final form (MerPO). This paper provides theoretical analysis on its advantage over behaviour policy and the meta policy. Extensive experiments with ablation show the empirical performance meets its expected behaviour.","The authors argue that offline meta-RL methods do not learn from poor data well, by demonstrating that COMBO (a single-task offline RL method) outperforms FOCAL (an offline meta-RL method) only when the training data is good. They use this to motivate a method called MerBO, which involves (1) learning a meta-dynamics model with proximal meta-RL and (2) updating a policy with real and synthetic data using a method called “RAC”, which is equivalent to COMBO but with an added KL regularizer against the behavior and meta-policy, and (3) updating the meta-policy used by RAC. The authors prove that under certain assumptions, the resulting policy will outperform both the meta-policy and the behavior policies. The authors compare the method to existing offline meta-RL methods and demonstrate that (1) the use of a dynamics model is important, (2) the addition of the behavior cloning regularizer is important, and (3) the use of the dynamics model is important to perform well on held-out offline RL tasks.",0.25688073394495414,0.22935779816513763,0.22018348623853212,0.2483221476510067,0.2483221476510067,0.21014492753623187,0.18791946308724833,0.18115942028985507,0.13872832369942195,0.26811594202898553,0.2138728323699422,0.1676300578034682,0.21705426356589147,0.20242914979757087,0.1702127659574468,0.2578397212543554,0.22981366459627328,0.18649517684887457
1051,SP:f28fc6ddfea8a676e0f3b62f084283a47e191c41,"The paper investigates the problem of the susceptibility of neural networks to adversarial attacks. It considers the hypothesis that ""it is not the neural networks failing in learning that causes adversarial vulnerability, but their different perception of the presented data"" (quoted verbatim). In other words (as far as I understand), the networks are like a computer which doesn't do what we want it do, but only what we tell it to do. To test it, the Author produces shows that what is considered in the manuscript to be adversarial perturbations are not random noise, but have some meaning and can be correlated with the training dataset. This is done using MNIST and F-MNIST datasets, and simple fully connected and convolutional networks. However, the attempt to reproduce the results on ImageNet using ResNet-50 and mobileNetV2 models failed.","The submission ""Neural Networks Playing Dough: Investigating Deep Cognition With a Gradient-Based Adversarial Attack"" investigates adversarial perturbations empirically. For small FCNs and CNNs on MNIST/FMNIST, the author finds data points with minimal target loss to for each class by gradient descent based on different initializations. These data points can contain semantic meaning and it is suggested that these patterns closely approximate the perception of these neural networks. ","In this paper, the authors first visualize adversarial patterns generated from three different methods - inputs with zero entries, noise inputs with subtracted perturbations, noise inputs in its average version. The authors then check if the patterns are attack method-agnostic and model-agnostic. Finally, the authors conclude that the patterns approximate their corresponding positive-negative contrast images.","The paper hypothesises that the causes of adversarial vulnerabilities aren't due to the failures of Deep Neural Networks (DNNs) but because of how they perceive data. Hence adversarial examples should be thought of as a signal which opens up black-box DNNs. The author also surmises that the adversarial examples can be abstracted to a coarser level, and these abstractions can serve as a summary of the dataset.",0.08633093525179857,0.07913669064748201,0.16546762589928057,0.11594202898550725,0.14492753623188406,0.12280701754385964,0.17391304347826086,0.19298245614035087,0.3333333333333333,0.14035087719298245,0.14492753623188406,0.10144927536231885,0.11538461538461539,0.11224489795918366,0.22115384615384615,0.12698412698412698,0.14492753623188406,0.11111111111111112
1052,SP:f2be12f73d1c89839bb40826b9b2b2d1cf81ca91,"In this paper, authors proposed a new training strategy which transfer the StyleGAN2-ada to the conditional version gradually by injecting conditional information into the generator and the objective function during the training phase. The proposed method is capable of training on limited data and generating high-quality images. However, the strategy of this method is only applicable to StyleGAN-like architecture and experiments can be improved.","This work studies the problem of training class-conditional GANs in limited data settings. The authors first empirically demonstrate that class-conditioning results in mode collapse in a limited data regime whereas unconditional learning provided satisfactory generative ability. They perform a comprehensive analysis by gradually reducing the size of the training set in two ways, a) reducing the number of classes while keeping a number of 100 training images per class and b) reducing the number of images per class while using 50 classes in all cases. In both cases, the conditional GAN achieves a better FID for larger datasets while performance deteriorates significantly (experiencing mode collapse) when the dataset size is reduced. On the other hand, the unconditional model achieves consistently better FID in limited data settings. Based on this observation, the authors propose a method of injecting the class conditioning by transitioning from unconditional to the conditional case, in an incremental manner. They delineate the proposed architectural changes and training objectives. Authors base their experiments on StyleGAN2 with adaptive data augmentation (ADA) and four datasets. The empirical results suggest significant improvements compared to state-of-the-art methods and established baselines. The authors also perform a comprehensive ablation analysis to understand the contribution of different components.","This work address the limited training data issue for class-conditional GANs. Inspired by the observation that unconditional GANs produce more diverse results under the same setting, the authors propose a learning algorithm that transits the training of unconditional toward class-conditional settings. The authors report FID and KID scores on four datasets to verify the proposed approach.","In this paper, the authors work towards training conditional GANs with limited data. Based on the observation that conditional GAN training suffers worse mode collapse than unconditional training, the authors proposed a training strategy that gradually injects conditional information into unconditional training. In other words, a learning curriculum that gradually replaces unconditional GANs training (losses and structure) with conditional ones is proposed.  Overall, the proposed learning curriculum is reasonable and achieved promising performance on data-efficient cGANs training.",0.29850746268656714,0.16417910447761194,0.26865671641791045,0.12980769230769232,0.12980769230769232,0.3275862068965517,0.09615384615384616,0.1896551724137931,0.23076923076923078,0.46551724137931033,0.34615384615384615,0.24358974358974358,0.14545454545454545,0.17600000000000002,0.2482758620689655,0.20300751879699247,0.18881118881118883,0.27941176470588236
1053,SP:f2da2e26bb83adc280583d4da1c49f57ee3807f8,"This paper proposes a method for scaling the learning rate during training in order to encourage all parameters in a neural net to be fully used. Specifically, the updates for parameters are scaled in inverse proportion to how much they affect the loss (their sensitivity); the scaling factor also depends on how stable the estimate of sensitivity is, so that high-sensitivity parameters whose role changes rapidly aren't down-weighted as much. This technique is shown to improve the performance of Transformer models on several different problems, even when used in combination with other adaptive learning rate methods (Adam, Adamax). Analysis experiments verify that the method has the intended effect: compared to the baseline, more parameters have higher sensitivity, and pruning is less effective. ","This paper focuses on the parameter redundancy issue in large transformer architectures. Instead of pruning redundant parameters, it strengthens training them to make them contribute better performance. To this end, it proposes an adaptive learning rate algorithm SAGE, which automatically scales the learning rate for each parameter based on its sensitivity. The sensitivity is approximated by the dot product between parameters and their gradients. An exponential moving average is used to track the sensitivity scores to reduce uncertainty in mini-batch training. The algorithm is applied to fine-tuning pre-trained transformer models in benchmarks for natural language understanding (NLU), neural machine translation (NMT), and image classification.",The paper proposes the SAGE method. The idea is that neural networks have redundant parameters. Some approaches will prune these parameters which has the effect of not decreasing the performance but to decrease the number of parameters. The paper studies if it is possible to learn better these parameters in order to make them more useful for the network. The paper proposes a method that will allow to train differently the parameters of a network in order to have a better use of the weights. The paper is evaluated in NLP and image classification with transformers. ,"The paper argues that redundant or ""useless"" parameters are not an axiom we should take for granted, but rather a symptom of current optimization settings. The authors propose an adaptive learning-rate schedule that specifically aims to eliminate redundant parameters. Through extensive experiments on fine-tuning transformers, they show that indeed this method (SAGE) does reduce redundancy and also slightly improves results.",0.16,0.168,0.096,0.18691588785046728,0.102803738317757,0.13541666666666666,0.18691588785046728,0.21875,0.1935483870967742,0.20833333333333334,0.1774193548387097,0.20967741935483872,0.17241379310344826,0.1900452488687783,0.1283422459893048,0.19704433497536944,0.1301775147928994,0.16455696202531644
1054,SP:f2e4458d94fc559f5df04abe3f5d8616bd18de64,"This paper is about statistical causal inference. More specifically, this paper is about causal discovery that infers causal graphs. The data used is observational data without intervention. All the variables are continuous.  This paper considers linear causal models with hidden variables. This setup can be written using an overcomplete mixing matrix as in (1). This overcomplete mixing matrix is known to identifiable up to the permutation and signs of the columns if the hidden variables are non-Gaussian and independent [4]. This is known in the field of independent component analysis. When the number o the hidden variables are the same as the observed variables, it is known that changing noise variance assumption is sufficient to make the overcomplete mixing matrix identifiable even when the hidden variables are Gaussian [16]. A contribution of this paper is Theorem 1 that extends the identifiability result [16] to overcomplete cases.   Based on the two identifiability results, they further consider the sufficient and necessary conditions of the linear causal models after the overcomplete mixing matrix has been identified. They aim to identify the underlying causal model beyond the Markov equivalence class.  They further assume the sparseness of the causal adjacency matrix F, i.e., the sparsest causal model is true if multiple causal models give the same overcomplete mixing matrix M. ",The paper provides necessary and sufficient graphical conditions for full identification of linear causal models with latent variables. It introduces the so-called ‘bottleneck’ and ‘strong-redundancy’ condition in combination with bottleneck faithfulness that together ensure identifiability (up to irrelevant permutations/scaling). It also considers likelihood approaches to estimating the unique solution from data that exploit heterogeneity in the independent noise terms over multiple domains. Experimental results are reported to illustrate feasibility of the method.,"This paper provides two graphical conditions for identifying a linear causal model under non-Gaussian error or heterogenous errors across domains when a subset of variables are latent. Specifically, the paper introduces necessary (‘unique minimal bottleneck’ criterion and strong non-redundancy) and sufficient conditions (bottleneck faithfulness) for the identifiability. These conditions indeed form an identifiability condition.","This paper considers one of important problems that under what condition a directed acyclic graphical model, especially linear structural equatioon models, can be inferred without the causal sufficientcy assumption. Hence, the proposed method provide a novel approch that can cope with learning linear structural equation models even when there are several latent variables. The key conditions are (i) sparsest true model, (ii) minimal bottleneck, (iii) strong non-redundancy, (iv) bottleneck faithfulness. It tried to explain a lot of new concepts with examples, and to compare the proposed idea to the existing conditions such as faithfulness. ",0.08755760368663594,0.06451612903225806,0.09216589861751152,0.21333333333333335,0.24,0.32142857142857145,0.25333333333333335,0.25,0.21052631578947367,0.2857142857142857,0.18947368421052632,0.18947368421052632,0.13013698630136986,0.10256410256410256,0.12820512820512822,0.24427480916030533,0.21176470588235294,0.2384105960264901
1055,SP:f30c72431bdde63c8e3429817a2abd7054478f6a,"This paper proposes a new method for out-of-distribution (OOD) detection using deep generative models. In particular, the paper leverages a Bayesian hypothesis testing framework, and is based the so-called „locally most powerful Bayesian test“, which enjoys certain theoretical guarantees (namely that it maximizes the probability of correct OOD detection) under some conditions. It is described how the intractable test can be efficiently implemented in practice (when used in conjunction with deep generative models), leveraging low-rank approximations of the involved Hessian matrix. Empirical evaluations demonstrate that the proposed approach compares favorably to alternative methods in a variety of settings, for both VAE and Glow models.","This paper proposes using a Bayesian hypothesis test for out-of-distribution detection with deep generative models.  The null hypothesis is that the newly observed data is in-sample and the alternative is that the new data is out-of-sample.  The test statistic is a likelihood ratio between the max likelihood model and the model fit to the given test input.  As this statistic is impractical to compute, the paper uses a perturbation of the MLE to find the parameters for the single sample test point.  The paper shows (Prop 1) that this test is the locally most powerful Bayesian test if the training set is sufficiently large.  Yet as the test requires computation of the inverse Hessian matrix, a low-rank approximation is necessary in practice.  Experiments are reported on benchmark image data sets (FashionMNIST, CIFAR, SVHN, CelebA, etc).  The proposed method performs well in terms of AUROC when compared to likelihood ratios and input complexity.  An ablation study of the rank of the Hessian approximation is also performs and shows the method is relatively insensitive to the rank.","The paper proposes a new Bayesian hypothesis test for out-of-distribution detection (OOD) based on a trained generative model. The problem is formulated as a hypothesis testing on the parameter of the model where the null hypothesis $H_0$ posits that the model parameter is $\theta_0$ and the alterternative $H_1$ states that the parameter is $\theta_1$. The test statistic is the ratio of the posterior probabilities of the two (Bayes factor. See (5)). This way to formulate the problem is common and $\theta_0$ is commonly set to the maximum likelihood estimate (MLE) on the training data (in-distribution samples).  The main contribution of this paper is in setting $\theta_1$. The paper proposes to set it by MLE but on data constructed by combining the training data and the test point to test (i.e., so one more point added to the training data. See (7)). The paper shows that doing this leads to a test that satisfies a new notion called “locally most powerful Bayesian test” (LMPBT) given in Def 3.1. Since fully estimating the MLE on the augmented training data for each point to test is expensive, the paper further proposes a way to scale up, based on the use of the upweighting method from [9], and Hessian approximation (see (8) and Sec 4.1). The resulting MLE solution on the augmented data can be written as an additive perturbation (involving a low-rank Hessian) of the MLE estimate from the original training data. In this way, the MLE on the training data can be estimated only once, and reused for each OOD test point. In experiments, the new Bayesian test is shown to outperform competing approaches on standard benchmark datasets such as CIFAR10, CIFAR100, SVHN, MNIST, and Fashion MNIST. ","**Summary** This paper presents a novel OOD detection method for deep generative models. The key idea is to formulate OOD detection as a Bayesian hypothesis test, where the OOD score is the log of likelihood ratio under the original network parameters $\theta_0$ and new parameters $\theta_1$ when adding the test sample $\mathbf{x}_t$. The overall testing framework is similar to previous works on likelihood ratio (Ren et al. 2019) and likelihood regret (Xiao et al. 2020). The novelty lies in the estimation of $\theta_1$ under the alternative hypothesis. Specifically, the authors estimate new network parameters using the second-order Hessian information. The authors also propose strategies to reduce the computational cost. Evaluations on both CIFAR-10 and CIFAR-100 demonstrate superior performance. ",0.3333333333333333,0.3425925925925926,0.24074074074074073,0.39226519337016574,0.18232044198895028,0.12751677852348994,0.19889502762430938,0.12416107382550336,0.20634920634920634,0.23825503355704697,0.2619047619047619,0.30158730158730157,0.24913494809688583,0.18226600985221675,0.2222222222222222,0.29645093945720247,0.21498371335504887,0.17924528301886794
1056,SP:f3f55247de1b7bbd551e72ee2c032392df65aab1,"This paper introduces Newcomblike Decision Problems (NDPs), an analog of MDPs in which the environment (specifically the transitions and reward) may depend on the agent’s _policy_ in addition to the state and action. They theoretically study the performance of standard RL algorithms when applied as is to continuous NDPs. Key results include:  1. Any RL algorithm that eventually finds accurate Q-values for its policy, and is greedy with respect to those Q-values, will converge to a strongly ratifiable policy, or will not converge at all. A strongly ratifiable policy is one that never “wishes” it could switch its action while keeping its “identity” the same. (The informal description is mine; the paper presents a formal statement.) SARSA, Expected SARSA, and Q-learning all satisfy the conditions for this theorem. 2. In any continuous NDP there exists a “strongly ratifiable” policy, but it may not be the optimal policy (for example, in the classic Newcomb problem). 3. Nonetheless, there exist continuous NDPs where typical RL algorithms almost surely do not converge to the strongly ratifiable policy. 4. When there is not infinite exploration of states and actions, RL algorithms instead satisfy weak ratifiability, where the policy never “wishes” it could switch its action to some other action that is explored infinitely often (but may “wish” to switch actions to an action that isn’t explored infinitely often). 5. In the special case of bandit NDPs, similar results can be proven for algorithms with limiting _action frequencies_ (even if there is no limiting policy).","The paper begins by defining a Newcomblike decision process (NDP), a natural extension of MDP in which both the reward and/or dynamics are allowed to depend on the learner's policy. The main theoretical claims of the paper are largely three-fold: 1. define two notions of ratifiability (strong and weak). strong ratifiability can be seen as a policy that always selects actions among those that are optimal w.r.t. the q-values of the policy, while weak can be seen as q-values for a policy being constant among those actions sampled by the policy 2. show that q-values, when converge, must converge to a strong ratifiable policy; this can relaxed to weak ratifiability if q-values are accurate only for state-action pairs sampled infinitely often 3. with the help of several bandit examples, show that standard RL such as QL and SARSA is not required to converge at all.","The paper formalizes Newcomblike decision processes (NDPs), a generalization of Markov decision processes in which reward and transitions may depend on the agent's policy. This formalism better reflects how e.g. self-driving car policies (aggressive vs meek) affect how humans drive (back off vs take advantage of the docile AI). The authors prove a range of theoretical results about when model-free value-based RL algorithms converge to a policy, what that policy looks like, etc. The authors run simple experiments to validate their claims. ","This paper introduces Newcomblike decision processes (NDPs), a generalization of MDPs where the transition probabilities depend on the agent's policy, representing scenarios like Newcomb's problem. The authors use an autonomous driving example to illustrate how Newcomblike dynamics may arise in the real world. They define ratifiability for policies and show that a value-based RL agent cannot converge to a non-ratifiable policy.  They introduce some examples (Repellor Problem and LARPS) of settings where this implies that the algorithm cannot converge to any policy, and investigate the limit behavior of non-converging algorithms (convergence to a specific action frequency).  ",0.16470588235294117,0.10980392156862745,0.1411764705882353,0.16774193548387098,0.18064516129032257,0.3333333333333333,0.2709677419354839,0.3218390804597701,0.3564356435643564,0.2988505747126437,0.27722772277227725,0.2871287128712871,0.20487804878048782,0.16374269005847952,0.20224719101123595,0.21487603305785125,0.21874999999999997,0.30851063829787234
1057,SP:f40ea351a0a79450806c49f21813c20119a5b3fc,"This paper introduces a novel knowledge distillation method for object detection, named Feature-Richness Score(FRS), which makes good use of aggregated classification score of the feature map as the mask of FRS. This pixel-wise FRS mask would be used as the weighted matrix to guide the distillation from teacher to student within selected layers, such as FPN and head. Plenty of well-designed experiments could show the effectiveness and availability of their method on serval mainstream detection framework, also prove it surpasses other recent methods.","One of the challenges in object detection is to train models with reduced computational and memory complexity for deployment on the resource-limited devices. One of the central methods is knowledge distillation which transfers knowledge from large teacher detector into small one with almost preserving the quality of the teacher model. However, existing methods have several issues: i) they ignore feature representation outside bounding boxes which can be helpful (e.g. similar but absent from the class categories objects which detection can improve target class detection)  ii) they use some detrimental representations within bounding boxes (ignore teacher model mistakes). To overcome these issues the paper proposes a novel method (FRS: Feature-Richness Score) to choose important features which can improve detection during distillation process. With experiments on anchor-based/anchor-free and one-stage/two-stage detectors authors demonstrate significant improvement in detection with FRS: FRS outperformes state-of-the-art methods on all distillation-based detection frameworks, Faster-RCNN, Retinanet, GFL and FCOS.","In this paper, the authors proposed feature-richness score to choose important features for knowledge distillation. Specifically, the proposed method uses the classification score from teacher model as the feature-richness masks. The feature-richness masks are used as indicators that show where the import feature is and what the importance of this feature. Based on these masks, the proposed method can focus on distilling import features in FPN and classification head.",This paper proposes to perform knowledge distillation on object detectors with feature richness. They use the feature-richness score to decide how to choose the important features. Experiments and qualitative analysis on COCO have been provided to evaluate their method.,0.1839080459770115,0.20689655172413793,0.1839080459770115,0.10975609756097561,0.10975609756097561,0.19444444444444445,0.0975609756097561,0.25,0.4,0.25,0.45,0.35,0.12749003984063748,0.22641509433962265,0.25196850393700787,0.15254237288135594,0.1764705882352941,0.25
1058,SP:f41ca1ac828221deaf1a7d7d0d6e0393f9746328,"The paper proposes a methodology for learning models when some of the prediction tasks are deferred to human experts (i.e., triage).  In the paper, the authors: (i) first derive the optimal level of triage for a given a model; (ii) then, they provide conditions under which models learnt under full automation will achieve lower predictive performance in presence of triage; (iii) then, they propose an iterative procedure to learn model and optimal level of triage jointly; (iv) lastly, they empirically verify the proposed algorith via a simulation and two case studies of real-world datasets.  The key contributions of this paper are: (1) theorem 3, i.e., the analytical derivation of the optimal triage level; (2) the proposed methodology to learn the model under triage. ","This paper considers the setting of ""algorithmic triage"", where a predictive model can choose to route some data points to a human expert for manual review.  The authors formally define the settings under which triage is useful, and show a motivating result that training a model for non-triage settings may be sub-optimal for learning under triage.  They then derive the optimal triage policy as a deterministic threshold rule, show a gradient-based algorithm for this, and demonstrate strong empirical performance on several synthetic and real datasets.","This paper studies the problem of algorithmic triage, where a predictive model and human experts together generate predictions. The authors introduced a theoretical analysis to model the interplay between the prediction accuracy of the model and the human experts. The authors later introduced a gradient based algorithm to find a sequence of predictive models and triage politics of increasing performance.  Extensive experiments on both synthetic dataset and two real world datasets showed improvement of proposed method over baseline methods. ","This paper presents a method triaging  / deferring examples to human labellers in machine learning systems. Their approach is conceptually simple: begin with a model trained on the full dataset and then iteratively improve the triaged model by updating it on a subset of the data that excludes examples where the previous iteration was worst compared to the human labellers. The approach is justified through a series of theorems showing that, as long as there exist parts of the input space for which the human labelers outperform the model, then triaging offers the possibility of improving the (combined model & human) performance; and further, that gradient descent can improve the model on the restricted space implied by the thresholding approach.  This is not my area, but I generally found the paper to be clear and well written (aside from style issues discussed below). ",0.16666666666666666,0.15873015873015872,0.16666666666666666,0.3068181818181818,0.19318181818181818,0.24050632911392406,0.23863636363636365,0.25316455696202533,0.14893617021276595,0.34177215189873417,0.12056737588652482,0.1347517730496454,0.19626168224299068,0.19512195121951217,0.15730337078651685,0.32335329341317365,0.148471615720524,0.1727272727272727
1059,SP:f41d52bc49e6cdf5356d31abf4c1d1d181fadfc0,"This paper is about distributed density estimation of an unknown probability distribution $p$, where the samples from that distribution are held by different users. In the model considered, there is a central server that collects messages from the users who hold the samples from $p$. Apart from the number of users $n$ and the domain size $k$, the problem is parametrized by the number of samples $m$ that each user holds and the maximum message size of $\ell$ bits. The goal is to minimize the total communication complexity (i.e the number of bits sent to the server). Interestingly, the authors show a phase transition of the error of the estimation with regard to $m$ happening at $m=k/2^\ell$. Specifically, it is shown that the error scales with $\frac{1}{\sqrt{\ 2^\ell}}$ for $m<k/2^\ell$ and with $\frac{1}{\sqrt{\ell}}$ otherwise. The existence of this phase transition is supported by both upper and lower bounds, although they are not shown for the entire parameter regime. However, both bounds are shown for sufficiently large $n$ and they are also tight up to logarithmic factors. The algorithm uses the estimation of a cointoss ($k=2$) bernoulli distribution as a primitive and recursively estimates the mass of certain parts of the domain in this way.   ","This work considers distributed density estimation. The problem setting is as follows: n users each have a dataset of m independent observations from an unknown discrete probability distribution and a central server aims to estimate the unknown distribution. When each user is subject to a bandwidth constraint such that each user can only send \ell bits to the center, the authors seek to understand tradeoffs between communication, sample size, and estimation error in the problem. In particular, for an unknown $k$-ary distribution p, this works characterizes minimax rates of estimating p in L1 distance. ",The paper introduces new algorithms for low bandwidth distributed estimation problems which could be useful in different applications including distribution estimation via multiple sensor readings. The assumption that each node or user can have multiple samples makes this paper interesting and makes it different from the previous works. Another important assumption that makes the paper interesting is the low bandwidth restriction. The authors prove phase transition properties as the bandwidth increases. ,"The paper studies the problem of distributed learning of a discrete distribution.  Formally, there are $n$ participants each of whom receives $m$ samples from a discrete distribution $\mathcal{D}$  on $[k]$.  The participants, after seeing their own samples, each send at most $l$ bits of information to a centralized server.  The goal is for the centralized server to output a distribution $\mathcal{D'}$ that is close to $\mathcal{D}$ in TV distance.  The main results of the paper are matching upper and lower bounds in various regimes of $n,m,k, l$, generalizing previous works that had only studied the case of $m = 1$.  Furthermore, the upper bounds are obtained using non-interactive algorithms with no shared randomness while the lower bound holds even against sequential algorithms that use shared randomness.  The main takeaway is a supposed phase transition when $m \sim k/2^l$ where the ""value"" of additional bits goes from exponential when $m < k/ 2^l$ to polynomial when $m > k/2^l$.  ",0.14220183486238533,0.0871559633027523,0.1743119266055046,0.12631578947368421,0.25263157894736843,0.23943661971830985,0.3263157894736842,0.2676056338028169,0.2289156626506024,0.16901408450704225,0.14457831325301204,0.10240963855421686,0.19808306709265175,0.1314878892733564,0.19791666666666663,0.14457831325301204,0.1839080459770115,0.14345991561181434
1060,SP:f4302bfe8f4c6fc82fb66e4cfd765e9a5a94fb5a,"Using Tweedie's formula, the paper proposes a $n$-th order denoising score matching: a parametric model learns $n$-th order gradient of the log density of perturbed data. More specifically, the paper considers a data $x \sim p(x)$ and a perturbed random variable $\tilde{x} \sim p(\tilde{x})$, which is a marginal distribution of $\tilde{x}$ for a given a conditional distribution $p(\tilde{x}|x)$ and $p(x)$.  Tweedie's formula states that when $p(\tilde{x}|x)$ is an exponential family distribution, the expected value of the $n$-th order moment of the posterior distribution of $x$ given $\tilde{x}$ can be represented as a polynomial function of $\tilde{x}$ and from-first-to-$n$th order gradients of the log marginal $\log p(\tilde{x})$. Nothing that the optimal solution of the least-squares regression to $x$'s moments for a given $\tilde{x}$ is the expected value of the posterior moments, the paper proposes to learn $n$-th order score models by performing the regression. The authors emphasize that for guaranteeing optimality of $n$-th order denoising score matching, ground-truth scores should be accessible up to $n$-1-th order.  Nevertheless, the paper proposes to jointly train the first and second-order score models for empirical studies. In particular, the authors suggest a memory-efficient parameterization of the second-order score and a variance reduction method for the training. For the application of the joint training, the authors first demonstrate that the learned second-order scores provide information about the uncertainty of denoising. Second, the paper proposes to plug in the learned second-order gradients into the Ozaki discretization of Langevin dynamics, which uses the second-order gradients to improve the mixing speed of the Langevin dynamics. The experiments on synthetic data and natural images demonstrate that the second-order score model improves sampling quality for the same discretization steps compared to the Euler-Maruyama discretization (the first order).","The paper generalizes denoising score matching, which estimates the score, i.e. the gradient of the log density, of perturbed data. In particular, it shows how higher-order derivates of perturbed data densities can be estimated and learnt. Much of this is derived via interesting connections of denoising score matching to Tweedie's formula, a technique for denoising noisy data. While the paper theoretically shows how to learn derivatives of the log data density of arbitrary order, its experiments focus on second-order derivatives, corresponding to the Hessian of the log density. The learnt second-order score is leveraged for improved image (and toy data) denoising, where the second-order score allows to parametrize a covariance matrix, that captures the uncertainty in the denoising operation. Furthermore, it uses the second-order score for sampling applications where it is leveraged to design more efficient samplers via preconditioning. It is also shown that directly learning the second-order score as proposed is more accurate and faster than computing it via auto-differentiation from first-order scores at test time.","This paper utilizes Tweedie's formula to generalize denoising score matching to higher order derivatives. The proposed method can approximate second order derivatives more efficiently and accurately than differentiating first order derivatives. Experiments on synthetic data and small image datasets also demonstrates its applications to uncertainty estimation, and Langevin dynamics sampling.",- The author show that denoising score matching with Gaussian noise can be derived from Tweedie's formula through the lens of least square regressions. - This provides a new interpretation of the first score (gradient of log density). - They use the generalization of Tweedie's formula to higher orders to derive a similar estimation of higher order scores. - This is accompanied by a thorough experimental validation for the second order score. ,0.18042813455657492,0.0672782874617737,0.0856269113149847,0.096045197740113,0.14689265536723164,0.21568627450980393,0.3333333333333333,0.43137254901960786,0.4,0.3333333333333333,0.37142857142857144,0.15714285714285714,0.2341269841269841,0.11640211640211641,0.14105793450881612,0.14912280701754388,0.2105263157894737,0.18181818181818185
1061,SP:f461eae616f95bc5d428a1b4c68b124a13b1bb6d,"This paper explores ways to improve the generalization performance and fairness guarantees, by increasing a certain class of instances' label noise to balance the rates between different classes. Theorem suggests that when the gap between noise rates is small and when we don't have high confidence in estimating the noise rates, it may be beneficial to increase noise rates. The paper further proposes a method to insert label noise without knowing the ground truth training labels. Experiments show that the rate balancing procedure can be beneficial to increase test performance.","This submission studies the paradigm of injecting label-noise from the perspective of fairness and robustness. The motivation stems from the fact that balancing the label proportions yields an easier learning problem and improves fairness guarantees. The submission derives a bound between the corrected loss with label noise and the population risk, which through the use of standard concentration inequalities, involves the Rademacher complexity. In order to fix one, they propose a method that detects the right choice of labels that suffer from noise, which then is turned into an algorithm for robust and fair label correction.","This paper considers classification in the case of noisy training data, where one subset of points suffers from higher label noise then the complementary subset. This paper claims that by increasing the label noise rate on the less noisy subset, it is possible to obtain more accurate models since the learning problem is easier.   Additionally, in the context of fairness, increasing the noise rate of the advantaged group improves fairness guarantees in the sense that when we are fair on the noisy data, we are also fair on the clean data.   Lastly, this paper contributes a method to detect which subset suffers from higher noise rates (and how much more is suffers) without access to the ground truth.","In this work, the authors aim to show that adding label noise in some cases could be beneficial to the test performance and fairness of models. Firstly, they find that increasing-to-balancing can result in an easier learning problem and improves fairness guarantees against label bias. Then they propose a method by inserting label noise to the group of labels with lower noise rates. To estimate the noise rates of various groups, the authors propose a detection method by checking the agreements of noisy labels among local neighbors. Experiments are conducted on three tiny datasets and CIFAR-10 dataset across different levels of noise rates to verify the effectiveness of the proposed algorithm. ",0.1978021978021978,0.26373626373626374,0.26373626373626374,0.2268041237113402,0.31958762886597936,0.2033898305084746,0.18556701030927836,0.2033898305084746,0.21052631578947367,0.1864406779661017,0.2719298245614035,0.21052631578947367,0.1914893617021277,0.22966507177033496,0.23414634146341465,0.20465116279069764,0.29383886255924174,0.20689655172413796
1062,SP:f4b66f979597608e9c66069f631877bf71c52525,"This paper proposes a method for self-supervised pretraining on video:  Given a batch of video clips, dense per-frame features are extracted from a backbone architecture like ResNet. The objective is to apply a standard contrastive penalty where a softmax loss picks out a positive pair of feature embeddings amidst a large set of negatives. Rather than compare every feature vector to every other vector in the batch (across samples, time, and space) a subset of representative vectors are chosen for comparison. Starting from BxTxHxW features, we choose from each sample a single frame and a subsampled grid of values to get BxNxN ""anchors"" to compare to. The full set of BxTxHxW features will each be compared to all of the sampled BxNxN anchors.  To apply the loss, for each feature we need one of the comparison anchors to serve as a positive target. This is done by finding the closest match to features from an augmented version of the same clip. That is, another inference pass of the backbone takes in a randomly resized/cropped/flipped version of the clip. Those features are compared to the anchors to find the argmax closest match. This creates a pseudo-ground truth label used to supervise the original features. Importantly, the gradient is stopped so nothing is backpropped through the pseudo-label generation.  Pretraining is done with the proposed set up and evaluated on the downstream setting of unsupervised mask propagation on DAVIS and YT-VOS.","The paper proposes a self-supervised representation learning method to learn dense spatial features for videos. The goal for the representation is to be able to perform correspondences and label propagation across frames. The proposed method involves an objective that encourages features to be distinguishable on inter-frame and intra-frame levels. Through experiments on two datasets, it is shown that these features are effective for label propagation, outperforming previous methods trained with the same data and demonstrating comparable performance to methods leveraging significantly larger amounts of training data. ","The paper presents a method for unsupervised video object segmentation. The method is based on enforcing equivariance between video batch and the same but transformed batch. The paper utilizes  anchor sampling strategy in order to reduce computational requirements, and compute the affinity between features and anchors.","This paper aims at unsupervised feature learning from videos. The learned representation is tested in a video segmentation/tracking setting on the DAVIS and the Youtube-VOS datasets. Although the paper describes many principles for the learning strategy, I could not find any of them to be new and also the results do not indicate that something fundamental was added. ",0.09795918367346938,0.06938775510204082,0.0653061224489796,0.14606741573033707,0.1348314606741573,0.2826086956521739,0.2696629213483146,0.3695652173913043,0.26666666666666666,0.2826086956521739,0.2,0.21666666666666667,0.1437125748502994,0.11683848797250859,0.10491803278688526,0.19259259259259262,0.1610738255033557,0.24528301886792453
1063,SP:f4d0e821de6830722a3458fd40d8d6793a107827,"Paper studies the problem of selecting a criterion for ranking based structured pruning. Particularly, authors observe that norm based criteria show quite similar behavior. Authors proposed a similarity and an applicability properties of 8 considered criteria. Additionally paper proposes a Convolutional Weight Distribution Assumption. Observations made in the paper will be helpful to the future work on structured pruning.","This paper studies 1) the similarity between different pruning criteria for channel pruning, 2) the applicability of different pruning criteria, where applicability refers to the confidence in the importance ranking of filters.  The authors empirically show that for some common pruning strategies such as l1 and l2 pruning and other norm-based pruning, the importance ranking of the filters is almost identical. Moreover, they claim with theoretical and experimental support that the importance score of different filters is very close to each other under certain conditions.  The main contribution of this paper is to analyze these two phenomena for better pruning criteria in the future. ","The authors propose and verify an assumption about the distribution of convolutional weight called CWDA, which reveals that the well-trained convolutional filters in each layer approximately follow a Gaussian-alike distribution. Based on this assumption, the authors analyze the similarity and the applicability problem among different pruning criteria. The point of this analysis is that norm-based metrics, particularly L1 and L2, behave quite similarly. Experimental results on various network structures, tasks, and datasets demonstrate the proposed analysis. The resulting conclusions are interesting. However, there are some issues in the paper. Detailed comments are as follows.","The paper attempts to study and compare different pruning ""criteria"". The paper addresses an interesting question, trying to evaluate if there is a need for a more complex pruning criteria. However the paper can be better organized. And some of the concerts are hard to follow. ",0.2711864406779661,0.22033898305084745,0.1694915254237288,0.18095238095238095,0.12380952380952381,0.1134020618556701,0.1523809523809524,0.13402061855670103,0.21739130434782608,0.1958762886597938,0.2826086956521739,0.2391304347826087,0.1951219512195122,0.16666666666666666,0.19047619047619047,0.18811881188118812,0.17218543046357615,0.15384615384615385
1064,SP:f4df95524c79a22f136495307959b50b2f13274b,"The authors study fairness relaxations, especially in deep learning settings. They propose several conditions under which prior fairness surrogates may degenerate. Three new surrogates are proposed further to avoid degeneracy and the performance and stability of the new surrogates are proved by real-world experiments.","The paper discusses several relaxations for the hard-indicator based fairness measures and compares their fairness-accuracy tradeoffs.   Detailed discussion on the necessary conditions for the failure case of certain relaxations (like linear or hinge) is provided. When the said conditions are satisfied, it is shown that the method converges to degenerate solutions where improving the fairness measure results in very high loss values of the prediction function.   Three surrogates are proposed, namely, the logistic sum, the logistic difference, and the sigmoid relaxation. They are shown to be more stable than the linear and hinge relaxations especially in non-convex models.   Experimental results are provided on tabular, vision and NLP datasets.  ","A popular approach in fair machine learning in-processing methods is to replace the original fairness constraints, which are non-convex and non-differentiable, with surrogates that are easier to handle. This paper studies several of these relaxations. On the one hand, it shows some of their limits by proving sufficient conditions under which existing surrogates may lead to optimization problems where the Lagrangian has no stationary points. On the other hand, it proposes three relaxations that are less prone to this undesirable behavior.","The authors study several surrogates for fairness constraints. They show that simple surrogates such as linear can lead to degeneracy. They show that when the derivative of the surrogate does not go to zero as the prediction goes to infinite, we should deal with degeneracy. They show that such degeneracy is not only theoretical and happens on real-world datasets as well for linear surrogates. ",0.28888888888888886,0.26666666666666666,0.28888888888888886,0.13513513513513514,0.13513513513513514,0.11904761904761904,0.11711711711711711,0.14285714285714285,0.2,0.17857142857142858,0.23076923076923078,0.15384615384615385,0.16666666666666666,0.18604651162790697,0.23636363636363636,0.15384615384615385,0.17045454545454544,0.1342281879194631
1065,SP:f4f2596d5e2c8d964f43d893c8ab91573c01e293,This paper studies the experimental design for the contextual linear bandits. The goal of the problem is to collect a dataset for identifying a good policy (an estimation of the reward vector). This paper provides algorithms for their goal and theoretically analyzes the sample complexity. Experimental results are also provided.,"The authors propose a new setting for the stochastic contextual linear bandit problem: using past contexts only, design a non-reactive policy for future online data. They argue that this setting arises in practice since often it may be logistically too complicated to deploy an online machine learning algorithm. They measure the quality of the dataset by the performance of the greedy policy that uses the least-square predictor on the collected data in the online phase. They develop a new two-phase algorithm (a planner and a sampler) for this problem and provide sample complexity guarantees. They also demonstrate the performance of their algorithm empirically. ","[summary]  This paper considers the problem of efficient nonadaptive sampling to find the best bandit policy, given an offline context data is available. The authors propose an algorithm and provide its sample complexity analysis. Empirical results show the efficicay of the proposed method. ","This paper studies the setting where it is not possible (e.g. not allowed or difficult) to update a policy while it interacts with a system. The authors suggest to build, from a batch of historical contexts, an explorative policy, non-reactive, that is not updated online as it interacts with the system, to gather informative data that are finally used to build a near-optimal policy.  The explorative policy (Planner) consists in a mixture of policies built offline from the historical contexts so as to minimize the uncertainty. The main idea is that using this mixture of policies on the real system will also gather data so as to reduce the uncertainty.  Sample complexity bounds and bounds on the performance of the final policy are derived. The algorithm is also compared with other strategies in numerical experiments.   ",0.32,0.24,0.36,0.1320754716981132,0.24528301886792453,0.3488372093023256,0.1509433962264151,0.27906976744186046,0.13043478260869565,0.32558139534883723,0.18840579710144928,0.10869565217391304,0.2051282051282051,0.2580645161290322,0.19148936170212763,0.18791946308724833,0.21311475409836067,0.16574585635359115
1066,SP:f512d8669ad091afc22eda921a6ea4a94910fbf1,"This work is built upon the idea of independent causal mechanisms in reinforcement learning environments where an agent only has limited inference over other entities in a given situation. Based on this premise, the authors introduce a measure called situation-dependent causal influence (CAI), which is conditional mutual information. The authors show how to estimate CAI and how to use it in exploration and RL policy training.  ","Summary. In reinforcement learning problems, many entities often act independently and only have influence on a small set of other entities. The paper aims to use causality to discover which components of state space are affected by agent’s action and leverage this fact for a more efficient reinforcement learning. To detect cause influence between state component and action, the paper uses mutual information between that component and action (conditioned on current state). Finally, it leverages the detected causal influence to (i) help with exploration by using it as exploration bonus, (ii) take actions with maximum causal influence and (iii) prioritizing experiences with causal influence for training policy. Overall, the paper shows improvement over baseline algorithms in state based robotic tasks.   My main concern is the applicability of the method to image-based settings. I do think latent variable models will cause issues and I am not sure about the extent of the problem. To be fair, authors mention this issue as well in the discussion section. It would be interesting to see how this method performs if state variables are replaced by latent variables (coming from some pretrained generative model; authors can use some expert data for pretraining the generative model for a start to get good features)   ","The paper introduces a state-dependent measure of “causal action influence” (CAI) to detect to what extent the agent can affect the next state. The high-level conceptual contribution (as stated by the authors at the beginning of Section 2) is that the measure should in fact be state-dependent. After introducing the measure and a tractable approximation for it, the authors demonstrate various ways in which it can be used to improve the efficiency of learning: 1. Guiding exploration by rewarding the agent for visiting states where it is determined to have more influence 2. Guiding exploration by selecting actions which are determined to have the most influence (among those generated by the policy) 3. A prioritized experience replay sampling scheme which upweights trajectories where the agent visited more states where it has high influence These approaches are also complementary, and combining the three leads to better results than any alone.","The authors study how to determine what factors in an environment can be controlled by the agent using a causal setup. One of the contributions is a perspective that causal influence is dependent on situation. They propose to use the conditional mutual information to measure if a future state variable is independent of action conditioned on the current state. Of course, estimating this value is difficult. There are some simplifying assumptions made: that the per state variable transition distribution is Gaussian and various quantities can be predicted by neural networks. After an empirical experiment validating that their Causal Influence Detection approach outperforms other attention-based methods, the authors propose 3 ways in which this detector can be used to improve sample efficiency in RL: 1) better state exploration, 2) causal action exploration and 3) prioritized replay with causal influence.",0.2537313432835821,0.208955223880597,0.2835820895522388,0.12440191387559808,0.11961722488038277,0.2236842105263158,0.08133971291866028,0.09210526315789473,0.1366906474820144,0.17105263157894737,0.17985611510791366,0.2446043165467626,0.12318840579710144,0.12785388127853878,0.18446601941747573,0.14404432132963987,0.14367816091954022,0.23367697594501718
1067,SP:f52598148a2f59f592100573d91b2b730affbacf,"    Authors present a continuous formulation of data augmentation for     images. The formulation is used to define a Bayesian model for augmentation     parameters. The Bayesian model does not rely on a bi-level optimization     problem as commonly used for optimizing data augmentation. Instead, only a     single gradient step approximates a full optimization of a network with a     given set of augmentation parameters. Gradient step is taken starting from     pretrained weights determined through initial training using random     augmentation. Samples from the posterior distribution are gathered using     Langevin dynamics MCMC with random batches. Samples are used to train     the ultimate networks. Experiments show that the proposed model outperforms     existing techniques in terms of accuracy of the final models at a fraction     of computation time. ","The paper presents an automated data augmentation technique that optimizes a distribution over a continuous augmentation space. Augmentation is formulated as learning a continuous mapping from an augmentation vector in R^d to an image transformation that composes three general families/modes of image transformations (color adjustment, image filtering, and image warping).   The proposed method uses a bilevel optimization approach where the primary learning task is trained on an augmented training set and the augmentation learning task is trained on the validation set. To make the optimization tractable, the inner level training is replaced by a one-step gradient update on an augmented mini-batch with model parameters being shared across the bilevel optimization iterations, i.e., incrementally updated during the bilevel optimization. A comprehensive set of experiments, including ablation experiments and comparisons to SOTA methods for automated data augmentation, demonstrate the efficacy and efficiency of the proposed method. ",The authors pose the problem of learning data augmentation as a continuous optimization problem and introduces a Bayesian approach for learning and sampling augmentations. The method allows for optimizing over a space of infinitely many possible transformations in contrast to the majority of automated data-augmentation approaches (ADA) which are limited by discretization of the solution space. The values of the proposed approach over prior works in terms of prediction accuracy and speed have been demonstrated on a range of benchmarks. ,"This contribution is aimed at automated data augmentation. A continuous search space is defined using simple image operators, and thus enables learning of augmentation parameters. A randomised algorithm is proposed for parameter learning.",0.1652892561983471,0.19834710743801653,0.0743801652892562,0.1476510067114094,0.09395973154362416,0.1111111111111111,0.1342281879194631,0.2962962962962963,0.2727272727272727,0.2716049382716049,0.42424242424242425,0.2727272727272727,0.14814814814814814,0.2376237623762376,0.11688311688311688,0.19130434782608696,0.15384615384615383,0.15789473684210525
1068,SP:f528e738d53488fd62c65bfd5cc236792b13623b,"The authors introduce a method for learning in a multi-agent setting that centers around predicting what other agents will do in the given context. Crucially, global information is not assumed (at execution), and so the techniques introduced have a focus on agent perspective in the information used to drive behaviour.  The methods are evaluated on three domains requiring multi-agent consideration (coordination or combativeness), and the results demonstrate the merit in taking the local perspective for these partial-information domains.","This paper introduces the Local Information Agent Modelling (LIAM), which learns a latent representation of the other agents. By using an encoder-decoder structure, the latent inference could be performed only based on the ego agent's local information, without knowing the observations and actions of other agents during the test time. The learned latent state is then used to augment the observation of the ego agent for reinforcement learning.  The experiment results show the advantage of LIAM compared with other baselines tested.","This paper proposes an encoder-decoder type neural network for agent modelling in partially observable domains. The method seeks to learn a useful encoding of a controlled agent's action-observation histories to a latent vector. By useful, it is meant that the learned representation used as an input to a downstream RL method allows finding ""best response"" policies to a set of fixed opponent/other agent policies. At training time, decoders are trained to predict the observations and actions of the other agents. At test time, the decoder is discarded, and the encoder part is used to output latent vectors. The proposed approach is demonstrated in three MARL domains, showing improvement over baselines such as simply classifying other agents' policies, or discarding the agent information completely. Ablation studies confirm the effectiveness of the design choices for the proposed method.","The authors propose ""local information agent modeling"" (LIAM), i.e. using a supervised auxiliary predictive task to help learn a representation of the history which is useful to predict other agent's behaviors, and which is then used as a basis for decision making.  The task encoded by LIAM involves predicting the modeled agents' trajectories based on the controlled agent's trajectory.  LIAM is presented both as a general idea, and as a specific implementation which uses encoder-decoder models (controlled agent trajectory -> latent representation -> modeled agent trajectory).  The auxiliary learning task can be performed with supervision in an offline learning framework, where the privileged information of all agents is available.  During execution, the fully trained encoder can be used without requiring the privileged information.  The experiment section involves 3 multi-agent environments, and 5 baselines closely related to LIAM.  On top of the standard performance-based RL results, the authors include a) a model evaluation section where they analyze the learned latent representations and their ability to encode important information, and b) an ablation study which shows the empirical importance of multiple individual components of their specific LIAM implementation.",0.19753086419753085,0.25925925925925924,0.30864197530864196,0.3373493975903614,0.37349397590361444,0.2,0.1927710843373494,0.15,0.13157894736842105,0.2,0.1631578947368421,0.14736842105263157,0.1951219512195122,0.19004524886877827,0.18450184501845018,0.2511210762331839,0.22710622710622708,0.1696969696969697
1069,SP:f53353ecb730a9c9995e84e466e9037826a68c9b,"**Update after author response and discussion**: I am happy to see most of my assumptions positively confirmed and clarified by the authors, as well as the criticism by other reviewers being sufficiently addressed. Some of my improvements remain valid, but I do not consider them crucial for publication. I therefore stick with my original score but raise my confidence - I am in favor of accepting the paper.  **High-level summary**  The paper addresses the question of learning to act well in finite-horizon sequential tasks in unknown environments under computational limitations. This requires solving two problems. (1) In principle, full knowledge about the environment allows agents to act optimally. However, actions that lead to large information gain about the environment often come at the expense of incurring low immediate rewards - trading off these two considerations is non-trivial and has been formalized previously via information directed sampling (IDS), which aims to find policies that minimize the ratio between squared expected regret (of the policy compared to the optimal policy) and information gain (ideally leading to small regret and large information gain). (2) Reducing all (epistemic) uncertainty about the environment can incur a considerable amount of exploratory actions, particularly in complex environments. Often, a less informative (lossy) compression of the environment allows for learning policies that perform very well with considerably lower “exploration cost” (which is particularly relevant for computationally bounded agents). Such a compression has been previously introduced under the name ‘learning target’ which is computed via a (stochastic) mapping from the environment to the learning target random variable. A good learning target requires considerably less information (fewer bits) to be extracted from the environment (compared to reducing all uncertainty about the full environment) while at the same time ensuring that policies trained to maximize the learning target achieve good returns in the actual environments. This trade-off between low reduction in performance and high reduction in sample complexity of exploration has recently been formalized via a rate distortion theory, where the distortion is the expected regret between the policy trained on the learning target and the optimal policy (given an observation history), and the rate is the information that needs to be extracted from the environment to reduce uncertainty about the learning target variable sufficiently.  **Concrete summary**  Previous work (BLASTS) derived an improved Thompson sampling scheme based on learning targets obtained by solving the rate-distortion trade-off via Blahut-Arimoto iterations. While BLASTS provides a principled solution to extract good learning targets, the resulting policies are understood to perform poorly in terms of exploration (increasing informativity of the learning target). The current paper improves upon this by combining IDS (which explores well by taking into account potential information gain and overall performance improvements) with automatically learned targets via Blahut-Arimoto iterations (which solve the rate-distortion trade-off). This leads to a method with a hyper-parameter that allows the designer to put more or less emphasis on keeping the required information acquisition sample complexity low (thus potentially incorporating a range of computational budgets) - the latter of course comes at the cost of potential reductions in performance (returns) of the learned policy compared to an optimal policy (in terms of cumulative regret over the course of learning, but potentially also for the converged solutions if the computational budget is too low). Results are shown for two 50-armed bandits (Bernoulli and Gaussian) where the method compares favorably with other methods (including BLASTS), and on par with IDS on Gaussian bandits.  **Contributions:**  1) Combination of IDS with Blahut-Arimoto learning targets. Both parts are theoretically well motivated and the combination makes sense and leads to a principled theoretical method. To obtain a practical implementation an approximation is needed - the approximation is sensible, though perhaps a bit coarse, but it is theoretically well backed up). Significance: results on the Bernoulli bandit look good, but performance gains compared to IDS don’t carry over to the Gaussian case. Since it remains unclear why, the potential impact remains limited (does the method sound good in theory but easily runs into practical problems, or is there a good reason why we would not expect to see a difference in the Gaussian case?).  2) Solid theoretical foundations - given a constraint on the distortion (corresponding to a value of the hyper-parameter \beta) there is a theoretical guarantee that the learning target is optimal in the sense that there exists no other learning signal target that achieves the same distortion but requires lower rate (“bits extracted from the environment”) - or dually, given a computational budget (set by inverse \beta) no other learning target can lead to higher returns. Significance: given the limited empirical results it remains somewhat unclear to which degree the theoretical results carry over in practice - it cannot be ruled out that (slightly) sub-optimal learning targets could lead to better results in particular cases (e.g. leading to more stable learning, easier to find corresponding policies, etc.). This remains to be seen in the future, but having a solid theoretical background is important - even heuristics might benefit from the theoretical insight presented here.  3) Review of (continuous) rate-distortion theory and the necessary background for the method for a general ML/RL audience (non info-theory experts). Significance: important contribution in itself, though the material is still a bit dense and might need further unpacking / illustrations of the key intuitions for e.g. a general RL practitioner audience. Also, the latter target audience might not be convinced to invest time since the method is currently limited to (non-contextual) bandits.","In reinforcement learning, the ideal thing to do would be to learn all of the information there is about the environment, and then use that to execute the optimal policy. However, in practice, it is impossible to learn _all_ of the necessary information in very complicated environments. Thus, we instead would like to define a _learning target_ that only incentivizes learning a relevant subset of information that is useful for acquiring reward. The learning target must be chosen such that (a) it does not require too much information to learn, and (b) the optimal policy given only the information in the learning target has low regret (relative to the optimal policy given arbitrary information).  So far learning targets have been coded by hand. Ideally we would instead select an appropriate learning target automatically, that outperforms targets coded by hand. The core insight is that constraint (a) is simply lossy compression, where constraint (b) can be thought of as a constraint on the _distortion_ on regret from throwing away some information in the lossy compression. We can then apply standard approaches from rate distortion theory to choose a target that best fits both constraints.  Once we have selected an appropriate target, we must also choose actions that provide us information about the target. This can be done through _information-directed sampling_, in which we choose actions that minimize the ratio between (expected) regret incurred and information gained about the learning target.  The authors instantiate this framework in the case of multiarmed bandits, where the learning targets are environment-conditional policies $p(a \mid \mathcal{E})$. In the case of zero distortion, the learning target is the optimal environment-conditional policy (which obtains zero regret). As distortion increases, the learning target becomes more and more “smeared” (e.g. in environments where the best and second-best arms are very close in reward, the learning target might put similar probabilities on both arms), making it easier to learn. They show that the resulting algorithm outperforms other alternatives that don’t use automatically generated learning targets and/or information-directed sampling.","This papers studies the fundamental problem in sequential decision making problems related to exploration. Previous work have used Thomson sampling in these cases however, they are suboptimal. This work uses rate distortion theory and in generalizes previous Blahut-Aritomoto and Information-directed sampling  algorithms with a novel BLAIDS approach. I have some suggestions in terms of presentation to improve readability and clarity.","This paper proposes the use of Blahut-Arimoto algorithm to learn an encoder from the source to the target, where the source is the environment and the target the policy to be recovered under a budget of distortion / suboptimality (this budget can be seen as a bounded-rationality). The output of BA algorithm would be a satisficing (good enough) policy, satisfying the budget constraints but also minimizing the mutual information between the policy and the environment. With this paper, the authors propose an improvement over Blahut-Arimoto Thompson Sampling method by using Information Directed Sampling in combination with Blahut-Arimoto algorithm (to compute target action of corresponding source environment). Finally, they provide an additional version based on the variance of the reward as a tractable approximation of the mutual information denominator.",0.10668103448275862,0.023706896551724137,0.052801724137931036,0.04610951008645533,0.11239193083573487,0.1935483870967742,0.28530259365994237,0.3548387096774194,0.37404580152671757,0.25806451612903225,0.29770992366412213,0.0916030534351145,0.15529411764705883,0.04444444444444444,0.09254013220018885,0.07823960880195599,0.16317991631799164,0.12435233160621761
1070,SP:f553e8b43018acb3ce695a88dc3b7d361ac5786a,"The authors present a resource-efficient approach for approximating kernels using random feature transformations. The random feature approach was introduced in a seminal paper by Rahimi and Recht, where each random feature is extracted from the input $x$ as $\sigma(\langle w, x\rangle )$, for a random (appropriately distributed) vector $w$ and an activation function (e.g. sinusiod) $\sigma$. In this paper, the goal is to save memory and time by restricting $w$ and $\sigma$ to take values in {$-1,0,1$}, meaning that $\sigma(\langle w, x\rangle )$:  1. can be evaluated only with additions/subtractions (no multiplications) and 2. only takes $1$ bit of storage.   The authors prove that under some conditions, their kernel estimate approaches the true kernel at the limit where the number of features and the number of data points are comparable and approach infinity. The assumptions are roughly: 1. The data follows a Gaussian mixture model, 2. the input features are approximately pairwise orthogonal, 3. the entries of $w$ are chosen i.i.d. and have bounded fourth moment, and 4. some boundedness assumptions on the generalized Gaussian moments of $\sigma$.  The theoretical result is accompanied by numerical experiments on various tasks like kernel ridge regression and SVM on both real and synthetic datasets. The results demonstrate a significant advantage of the authors' approach compared to previous quantized random features approches, both in terms of runtime and memory, without a significant drop in accuracy.","The paper proposes a way to approximate common kernel matrices in a way that is (1) Sparse (2) Low Bit-Complexity (i.e. the nonzero entries have few bits) (3) efficient to compute.  The theoretical justification from this model comes from a random matrix theory (RMT) analysis of kernel matrices for datasets of vectors drawn iid from a fairly general gaussian mixture model. The RMT analysis shows that the true kernel matrix is asymptotically distributed as the approximation described above. No non-asymptotic results are given.  The introduction mentions deep networks, but this paper really has no strong connection to deep learning. View it as a kernel paper.  A good amount of supporting experiments are given to demonstrate that the approximate kernel matrix has the same statistical performance as (eg) a Random Fourier Features approximation.","With Gaussian mixture assumption for input data, this paper proposed a sparse random features approach where the approximated kernel is independent of the iid weights and depends on Gaussian moments of the activation functions. This paper provided a theoretical guarantee for the asymptotical equivalence with the centered kernel. And then, the authors derived the computationally efficient random features and devised a simple algorithm. Finally, they validate the accuracy and efficiency of the proposed random features by several experiments.","This paper studies random features method, which randomly projects the data onto a low-dimensional space, while at the mean time approximates the original kernel structure.  Some contributions of this paper: - The authors show that, under the Gaussian mixture model, in the high-dimensional limit, in the spectrum sense, the random features-type kernel (defined in equation (1) of the paper) only depends on the first two generalized Gaussian moments of the activation function (with the assumption that the random projection weights are i.i.d. from some mean 0 and var 1 distribution). - Based on the above result, the authors propose a special choice of the random weights and the activation function, that could be efficiently computed and needs less memory to store. - The authors provide empirical evidence showing that the proposed method performs as well as other random features methods, and with advantage in computation and storage.",0.11666666666666667,0.10416666666666667,0.1625,0.14074074074074075,0.18518518518518517,0.3717948717948718,0.2074074074074074,0.32051282051282054,0.26174496644295303,0.24358974358974358,0.16778523489932887,0.19463087248322147,0.14933333333333335,0.15723270440251574,0.20051413881748073,0.1784037558685446,0.176056338028169,0.2555066079295154
1071,SP:f58b873aabbd4eac6d38d027516fd563d633cd4d,"In this paper, the authors propose a continuous-time formulation for the inner-loop adaptation in a MAML-like setup. Moreover, the authors propose an efficient forward-mode algorithm to learn the parameters of this inner-loop adaptation that is independent of the number of adaptation steps used. The authors show that this works well for a variety of meta-learning problems.","This paper proposes using a continuous-time formulation along with forward-mode differentiation to perform meta-learning. The setup follows that of ANIL: the weights of the last layer is part of the inner loop optimization while the initialization of these last-layer weights and the rest of the weights are treated as meta-parameters. The proposed method relies on this setup in order to efficiently compute derivatives, which the authors note have a low-rank structure that can be exploited. Furthermore, the weights do not need to be solved over time, and instead a much smaller vector can be solved instead, which is pretty neat. ","The paper describes an algorithm called COMLN for few-shot meta-learning.  In COMLN, the base level loop is modelled as a continuous-time autonomous ODE that is the gradient vector field of the inner loss.  The authors restrict their discussion to a meta-learning setting where the only base-level parameters are the parameters of a linear classifier. The meta-parameters are the parameters of a neural network that acts as feature extractor, the initialization point for adaptation of the classifier and the adaptation time $T$.  The authors point out that, crucially, it is possible to treat $T$ as a learnable meta-parameter thanks to the continuous reformulation of the base level. Meta-training is carried out by gradient descent on the meta-parameter, where the gradients are computed with an efficient implementation (in few-shot classification, with a small number of classes) forward mode differentiation that exploits the loss structure. The authors present experiments on standard benchmark datasets, showing that COMLN achieves better performances than other comparable gradient-based methods. ","The authors propose COMLN -- a gradient-based meta-learning algorithm with continuous-time inner loop adaptation. The authors discusses possible options for computing the meta-gradient (backpropagation through time, solve ODE backwards in time, and forward-mode differentiation), and identifies the forward-mode differentiation as the only method that is both numerically stable and has a memory cost that is constant w.r.t. the inner-loop length. The authors then derive a way to compute the vector-Jacobian products without explicitly constructing the Jacobians in forward-mode differentiation (specifically for the few-shot learning setting), which further reduces the memory cost. Also, by using a continuous formulation for the inner-optimization, the authors are able to compute meta-gradient w.r.t. horizon T and adapt that as well. Finally, the authors conduct experiments on standard few-shot image classification benchmarks, and show that COMLN outperforms other gradient-based meta-optimization methods in accuracy with a lot less memory cost.",0.3225806451612903,0.3709677419354839,0.3709677419354839,0.24528301886792453,0.2169811320754717,0.22674418604651161,0.18867924528301888,0.13372093023255813,0.14285714285714285,0.1511627906976744,0.14285714285714285,0.2422360248447205,0.2380952380952381,0.19658119658119655,0.20627802690582958,0.1870503597122302,0.17228464419475656,0.23423423423423426
1072,SP:f5aec49d99e36274ef9b9c4a6b1feb564f36fd73,"The paper proposes a framework for combining RL with logical specifications. In particular, the task is specified using local statements, which are then converted into a graph and high-level planning (using Dijkstra's algorithm) is used to attain the goal while adhering to any safety constraints. Within this graph, nodes are sets of low-level states, while edges represent the probability of a policy (learned using model-free RL) transitioning between nodes. While there exist other approaches that combine logical (e.g. LTL) specifications with RL, experiments in two continuous control domains demonstrate that the method outperforms these existing approaches, and scales only linearly with the complexity of the specification.  ","This paper studies the problem of using logical specifications of a task to derive an abstract graph, come up with a plan, and train low-level policies more effectively. Given a logical specification written in SPECTRL syntax, it is first converted into an abstract MDP. Dijkstra's algorithm is executed on this abstract MDP, assigning probabilities to edges when they are encountered for the first time. A policy is trained for each edge (S_0, S_1) in the abstract MDP where a random state in S_0 is the initial state and a random state in S_1 is the goal state. The success probability of the policy is used to assign the probability of the corresponding edge. The model is evaluated on two rooms environments and a fetch environment from OpenAI gym. The model is shown to outperform previous models.","This paper addresses problems where the task to be accomplished is given by a specification language (Spectrl). Tasks given in this specification language can be represented as reachability problems on an abstracted graph, where nodes in the abstracted graph represent regions of the underlying state space of the original problem. To find a high-level plan to satisfy the task specification, Dijkstra's algorithm is used. To learn low-level controllers to complete each of the subtasks, reinforcement learning is used. The probability of a subtask being completed by a low-level controller is evaluated empirically, and used to update costs in the high level graph so that the lowest cost path in the high-level graph corresponds to the path maximising the probability of completing the task.   I think the paper is well written, and addresses an interesting problem. To my understanding, the general idea of combining high-level model-based planning with RL for low-level control has been addressed in existing works. However, this is the first work to do so for more complex task specifications. ","This paper addresses the problem of synthesizing (using reinforcement learning) a policy that satisfies a logical specification. The key insight in this, and related works, is that the implicit structure of the specification can be used to decompose the learning problem into several, ideally simpler, learning problems.  Technically, this work operates by first syntactically restricting the space of (infinite horizon) tasks. This class is fairly expressive and has the important property that tasks can be abstractly monitored using a *finite* DAG - with (abstract) satisfaction now being a path finding problem on the DAG abstraction. The conversion is automatic and the complexity grows linearly in the syntactic structure.  The graph structure then admits a nature mechanism to split the task into sub-tasks, corresponding to traversing an edge in the DAG. Thus, by leveraging classic shortest path finding algorithms, where the cost is determined by the traversal probability - which is itself determined by training a policy in the workspace and empirical evaluation - one obtains a hierarchical policy. This defines an iterative algorithm which the paper shows can be used to increase sample efficiency compared to various baselines.  ",0.2072072072072072,0.24324324324324326,0.18018018018018017,0.2605633802816901,0.23943661971830985,0.20670391061452514,0.1619718309859155,0.15083798882681565,0.10752688172043011,0.20670391061452514,0.1827956989247312,0.1989247311827957,0.18181818181818182,0.18620689655172415,0.13468013468013468,0.23052959501557632,0.20731707317073172,0.20273972602739726
1073,SP:f6271e63239ef14defebca9e9ccf68d972f410ad,"This paper proposes a framework, SALKG, for commonsense question answering (i.e., Multiple choice). It employs saliency explanations (i.e., coarse and fine) to regularize the attention mechanism and provide direct supervision for the task. The proposed framework is shown to achieve state-of-the-art or competitive results on three QA benchmarks, namely CSQA, OBQA, and CODAH. The proposed work has highlighted the right research questions and answered quantitatively and to some extent qualitatively. Overall, the paper is well motivated and proposes a technically solid framework for multiple-choice commonsense question answering tasks. I have some concerns regarding the details on using the knowledge graph that I have highlighted in the weaknesses section.","Motivated by that the knowledge graph used in KG-augmented models is useless sometimes, the authors propose two ensemble models by using coarse/fine saliency explanations. Specifically, the authors first get saliency explanations through F_KG, F_NO-KG and saliency methods. Then, the authors propose oracle model using coarse/fine/hybrid explanations as extra inputs, and achieves superb performance than its baselines. Finally, based on oracle models, the authors develop the SALKG framework that uses explanations as supervision for distillation learning. SALKG achieves SOTA performance on CommonsenseQA and competitive performance on OBQA. It is the first to supervise KG-augmented models with KG explanations.","This paper explores how saliency explanations can be used to improve the performance of knowledge-graph-augmented commonsense reasoning (CSR) models, by investigating coarse to fine KB components (an entire graph to its nodes and paths). The paper first explores how KG influences the models in making correct predictions using oracle models, which have access to true saliency explanations. Base on that, the paper proposes SALKG, a framework for KG-augmented CSR that learns from coarse and/or fine saliency explanations that are automatically created. The proposed models achieve better performance compared to the baselines used in the paper.","The paper explores how saliency explanations can be used to improve the performance of KG-augmented models. The authors compared two types of saliency explanations: coarse (whether the KG is useful) and fine (which nodes or paths in the KG are useful) They first showed that using oracle KG saliency explanations as inputs improves model performance (section 4).  They then introduced a model that jointly learns to predict the saliency explanations, and solves the task by attending to corresponding KG features highlighted by the predicted explanations (SALKG). The results showed that SALKG models yield large performance gains in three popular commonsense QA benchmarks (CSQA, OBQA, CODAH).",0.11403508771929824,0.19298245614035087,0.17543859649122806,0.20952380952380953,0.24761904761904763,0.3434343434343434,0.12380952380952381,0.2222222222222222,0.18867924528301888,0.2222222222222222,0.24528301886792453,0.32075471698113206,0.1187214611872146,0.20657276995305165,0.18181818181818182,0.2156862745098039,0.24644549763033177,0.33170731707317075
1074,SP:f670fffc0b49e36fb5b2101bfdd5daf221febc0a,"This paper studies an option-based setting where each option is associated with an _intent_, a distribution over desired trajectories following the option execution. Intents induce _affordances_,  which are starting states that are likely to lead to the intents for each option. Intents and affordances induce a partial option model, where the usual transition model over options is replaced with the intents, and the model is partial over the affordances. The main theoretical questions concern the extent to which planning in this partial option model will lead to good performance in the original semi-MDP. Experiments are in a standard taxi domain with given options and intents.",This paper addresses the problem of learning and planning with options framework in RL. The authors extend an existing notations of affordness and intent in the context of options and perform a mathematical analysis of the bound and the value loss. The ideas are demonstrated on a taxi benchmark with predefined set of options to illustrates the benfit of the approach.   ,"This paper presents a formalism for semi-Markov decision processes that defines ""intention"" (desirable outcomes of behavior) and ""affordances"" (behaviors that achieve an intention). The main idea is that the agent may have many options, but may have a more limited set of intentions, and affordances can constrain the problem of selecting a good option. Theoretical results are also presented, bounding the sub-optimality caused by operating in intention/affordance/option space, rather than in primitive actions. Finally an illustrative empirical experiment gives a proof of concept for learning affordances, given fixed options and intentions.","This paper describes options models as partial models, meaning that the options model predict only the outcome of a subset of state-action pairs (both the state transition and the reward). Options enable temporal abstraction by allowing an agent to select an option at a particular decision point and follow it until it satisfies the termination condition. The primary contribution of the paper is the development of theory which quantifies the loss incurred by using temporal abstraction (options) relative to single step decision making.   ",0.1588785046728972,0.19626168224299065,0.14953271028037382,0.2459016393442623,0.2786885245901639,0.14736842105263157,0.2786885245901639,0.22105263157894736,0.19047619047619047,0.15789473684210525,0.20238095238095238,0.16666666666666666,0.20238095238095233,0.2079207920792079,0.1675392670157068,0.19230769230769232,0.23448275862068962,0.1564245810055866
1075,SP:f691ca0c356da2ec1fe91ffcb493ee1ff9176d6c,"This paper presents an empirical study on offline deep reinforcement learning in atari environments. The authors use tandem learning -- where an agent B learns from experience collected from another agent A. The authors consider 3 hypotheses as to why agent B tends to perform worse in this setting, and provide many ablation-type experiments to support/reject these hypotheses.","This paper provides detailed empirical analysis to study the challenge of offline reinforcement learning. The authors propose to use the Tandem RL setup as the analytic tool, where one active agent performs usual online training loop, while the passive agent can only learn from the data generated by the action learner. The results provide some insights for revealing the difficulty of offline RL.  ","The paper addresses the difficulties that standard RL methods have in the offline setting. It presents a new analysis technique designed to better understand the causes of the difficulties. The technique, called Tandem RL setting, and forked tandem, extend the setup, introduced by Fujimoto, Meger, and Precup in the BCQ-paper ""Off-Policy Deep Reinforcement Learning without Exploration"" and enable systematic studies of causes for some kind of problems with offline RL. The technique is used for extensive studies, in several benchmarks, and cautious conclusions are drawn.","This paper studies the situation of passive reinforcement learning, simulating the famous Held and Hein twin cats paper where an active agent collects data to learn while the passive agent experiences the same data (but does not choose actions). The result, running Double-DQN on Atari, is similar - that the passive learner is often much worse. The paper then attempts to study why by evaluating three hypotheses: bootstrapping, data distribution, and function approximation, through a sequence of experiments. The conclusion is mixed, but mostly supports that data distribution is the key problem, with good evidence that bootstrapping is relatively less important than previously emphasized. Overall, the paper is a clever and thorough study of passive learning beyond the existing literature on offline RL.",0.2711864406779661,0.13559322033898305,0.23728813559322035,0.25396825396825395,0.36507936507936506,0.1839080459770115,0.25396825396825395,0.09195402298850575,0.11382113821138211,0.1839080459770115,0.18699186991869918,0.13008130081300814,0.26229508196721313,0.1095890410958904,0.15384615384615385,0.21333333333333332,0.24731182795698922,0.1523809523809524
1076,SP:f6bbae01ea68344ac99567340db89c9f83c679c8,"By reformulating Iterative Machine Teaching’s (IMT) optimization problem to an adaptive sampling problem, the papers proposes a Locality Sensitive Teaching (LST) algorithm. LST achieves exponential teachability and reduces the time complexity of each iteration from O(N), where N is the number of teaching examples, to few O(1) lookups in hash tables. They also attach the IoT perspective to the problem.","The paper proposes locality sensitive teaching (LST) which can be applied to IoT and online personalized education. In iterative machine teaching (IMT), the teacher selects example in each iteration for the student to learn, based on a greedy search algorithm which is expensive. The authors reformulate the optimization into a maximum inner product search problem, and use locality sensitive hashing (LSH) in the searching process to reduce the iteration-wise cost from $O(n)$ to $O(1)$. Theoretical analysis and practical hardware design are provided. Experiments validate the advantage of LST.",This paper considers an iterative machine learning task of how an omniscient teacher choosing learning data optimally such that the student can learn fast from the data via gradient descent. It proposes an adaptive inner product sampling (AIPS) algorithm that uses locality-sensitive hashing-based sampling to make the search much more efficient than greedy approaches. The authors show the exponential teachability of their approach and provide empirical studies.,"The paper applies locality sensitive hashing to online teaching, where the machine uses a student’s information to retrieve a data sample from hash tables. The paper claims several contributions. (1) Propose a novel teaching framework, called Locality Sensitive Teaching (LST) for the retrieval. (2) LST has provable near-constant time complexity and is exponentially better than existing baseline (i.e. linear scan) (3) LST achieves 425.12x speedup over the linear scan baseline.  ",0.2857142857142857,0.15873015873015872,0.2222222222222222,0.18681318681318682,0.1978021978021978,0.14492753623188406,0.1978021978021978,0.14492753623188406,0.1891891891891892,0.2463768115942029,0.24324324324324326,0.13513513513513514,0.23376623376623376,0.15151515151515152,0.20437956204379565,0.21250000000000002,0.21818181818181817,0.13986013986013987
1077,SP:f7926c98aecc18ff1c698c163c5233ac6a3d6e57,This work draws inspiration from the invariant risk minimization (IRM) framework to improve the generalization capability of inverse reinforcement learning (IRL) methods. The paper's contributions are deriving a new IRL objective from IRM. This new algorithm includes a penalty regularization term to the standard objective which helps the agent from overfitting to the peculiarities of the expert demonstrations. ,"This paper investigates a specific technique for Inverse reinforcement learning to avoid the detrimental effect of ""spurious correlations in the data by the learning model"". The objective is to avoid ""behavioural overfitting to the expert data set"". The paper uses an invariant risk minimization principle as a regularization approach for the maximum entropy inverse RL problem.","The paper studies the Inverse Reinforcement Learning (IRL) problem, addressing the issue of avoiding overfitting when having access to a finite set of expert demonstrations. The paper proposes an approach based on Invariant Risk Minimization (IRM) as a regularization approach for maximum entropy IRL. After having presented the method, an experimental evaluation on both finite and continuous environments, showing the advantages of the presented approach.","The authors' propose a regularization loss for inverse reinforcement learning (IRL) based on invariant risk minimisation (IRM). They argue that existing IRL methods suffer from overfitting due to the sparsity of expert trajectories, and that this is analogous to the dataset level overfitting tackled by IRM. This approach is validated by comparing against unregularized and L2 regularized maximum entropy IRL on a grid-world, and is used in a scalable adversarial IRL algorithm when dealing with more complex environments. They close by evaluating the reward function learnt by this regularized adversarial IRL in terms of its utility in training a new policy from scratch.",0.2033898305084746,0.2542372881355932,0.22033898305084745,0.39285714285714285,0.25,0.3076923076923077,0.21428571428571427,0.23076923076923078,0.125,0.3384615384615385,0.1346153846153846,0.19230769230769232,0.20869565217391306,0.24193548387096775,0.15950920245398775,0.3636363636363636,0.17500000000000002,0.2366863905325444
1078,SP:f797316fad2c2c4384594493348aed8b5fefbb89,"The paper generalizes an existing problem into a multi-step one. The original problem proposed by Kleinberg and Raghavan is formulated as a Stackelberg game played between a principal and an agent. The principal wants the agent to invest efforts in some given acitivities and uses an assessment policy to score the agent's performance. Once the assessment rule is announced, the agent best responds to the rule by choosing the optimal amount of effort to invest on each activity, aiming to maximize the score it receives. The goal of is to find an assessment rule that will incentivize the agent to invest more efforts. In this paper, the agent's effort investment is a multi-step process. Effort investment in a step will have consequences to the scores in subsequent steps. Hence, both the agent and the principal adopt a long-term vision over their strategies. The paper presented an efficient algorithm to compute the best assessment rule for the principal and also analyzed the time it needs to incentivize a desired effort profile.","The paper discusses the problem of stateful strategic regression, where the basic idea is to model the interaction between a principal and a strategic agent over multiple time steps. Specifically, the principal announces a sequence of assessment policies, and the agent invests effort to play a best response to this sequence, defined as the effort policy that maximizes the cumulative score the agent receives. An important aspect of the model is that the agent’s effort accumulates over time through an “internal state”. For example, studying for an exam today can help a student’s performance in the following days.","The paper studies the problem of strategic regression when the designer’s goal is to incentivize the agents’ effort in the long term. The model follows the one in [Kleinberg and Raghavan, 2020] but considers multiple time steps during which an agent’s effort can accumulate over time in the form of an internal state. The paper characterizes and computes the Stackelberg equilibrium. They also make the following observations: (1) the class of linear policies is as strong as the larger class of monotonic policies; (2) the multi-step policies are more effective than the one-step policies.","This paper studies a theoretical model of a multi-round assessment game between a principal and an agent. This model is an extension of Kleinberg and Raghavan (2020) to the multi-time-step setting where the efforts of an agent can accumulate benefits from one time step to the next. It studies the equilibrium computation problem under simplifying linearity assumption. Its main finding is that in the multi-step case, the principal can incentivize a wider range of effort profiles than in the single time step case.",0.18857142857142858,0.14857142857142858,0.17142857142857143,0.27,0.21,0.24489795918367346,0.33,0.2653061224489796,0.3448275862068966,0.2755102040816326,0.2413793103448276,0.27586206896551724,0.24000000000000005,0.1904761904761905,0.22900763358778625,0.27272727272727276,0.2245989304812834,0.25945945945945953
1079,SP:f7a90b48984a0f335771273c725087e4acba0d72,"This paper addresses an extended reinforcement learning setting with general utilities, where the agent's objective can be any convex function of the state-action distribution. Especially, it recasts the problem as a min-max game between a policy player and a cost player, and it describes a principled methodology to solve the game. Finally, it provides ways to instantiate the methodology with known algorithms, and a brief experimental evaluation in simple domains.","The paper discusses the problem of minimizing a convex function of the stationary distribution of a system with Markov dynamics, subject to the (convex) Bellman flow constraint and optionally subject to other convex constraints. The paper provides a family of algorithms designed to tackle this problem. It also proves results about sample complexity. The proposed framework is a significant innovation since it unifies several apprenticeship learning and imitation learning algorithms, which have been proved to be useful.","The paper studies the convex RL problem, which is defined as finding policy $\pi$ minimizing $f(d_\pi)$ where $f$ is a convex function and $d_\pi$ is the occupancy measure of policy $\pi$. Using Fenchel duality, this formulation can be written as a min-max and be solved using the well-known framework of repeated game playing first introduced by Freund and Schapire (1999). The authors show that their framework unifies some well-known approaches in RL by appropriately choosing the no-regret algorithm for the policy and cost player.","The authors propose to consider a convex optimization problem over the polytope formed by the state-visitation frequency of an MDP. They constructed algorithms based on existing tools of sub-gradient descents, such as Frank-Wolfe, FTL and OMD, in conjunction with an oracle that (approximate)-solves an MDP with scalar rewards. Altogether, they provide a meta-algorithm that translates a scalar reward MDP oracle to an algorithm for the convex MDP. The authors also highlight several applications, such as the Apprenticeship Learning problem and the constrained MDP problem, of their formulation. ",0.2054794520547945,0.2054794520547945,0.1643835616438356,0.23376623376623376,0.19480519480519481,0.13186813186813187,0.19480519480519481,0.16483516483516483,0.13043478260869565,0.1978021978021978,0.16304347826086957,0.13043478260869565,0.19999999999999998,0.18292682926829268,0.14545454545454545,0.2142857142857143,0.17751479289940827,0.13114754098360656
1080,SP:f7b3d454013b4092e27d27d989b694d22f7936de,"This paper performs empirical analyses of applying the CLIP to various vision-and-language tasks. It demonstrates the potentials of the model in generalizing to different downstream applications, and provides some suggestions for model deployment. Experimental results show that CLIP pretraining leads to competitive performance and further combining it with V&L pretraining can outperform existing methods.","This paper explores how features from CLIP (Contrastive Language-Image Pre-training, Radford et al., 2021) affect the performance of vision-and-language models across a series of tasks. The authors explore using CLIP as a visual encoder in two settings, plugging its features directly into task-specific fine-tuning; and combining CLIP with intermediary vision-and-language pre-training before fine-tuning on downstream tasks. The experiments suggest that the simple change to CLIP offers significant benefits over commonly used encoders such as BottomUp-TopDown, providing very strong results across a wide range of vision-and-language tasks.","The paper utilizes recently proposed CLIP-ResNet/ViT visual encoders instead of the standard backbones to conduct a large-scale empirical study for several vision and language tasks. The experimental setup aims to answer the question in the title of this paper by conducting 1) the task-specific fine-tuning of existing V&L models with CLIP-based visual backbone and 2) performing full-scale V&L pre-training. The resulting performance in VQA, Visual Entailment, V&L Navigation suggests that CLIP is a viable alternative to the existing visual representations (e.g., ResNet-based models pre-trained on ImageNet).","This paper examines how well CLIP's visual encoders are transferred on vision-and-language (VL) tasks.   The paper conducted three without-VLP tasks: VQA, image captioning, and vision-and-language navigation, and three with-VLP tasks: VQA, SNLI-VE, and GQA to show CLIP's visual encoders' transferability,    CLIP's resnet-based visual encoders consistently outperformed their Imagenet pre-trained counterpart.   The authors found that CLIP's ViT-based visual encoder performed far worse than the resnets.   From Grad-CAM and detection fine-tuning experiments, the authors speculated that ViT's features lack localization information.  ",0.24561403508771928,0.2807017543859649,0.22807017543859648,0.21212121212121213,0.20202020202020202,0.19,0.1414141414141414,0.16,0.13541666666666666,0.21,0.20833333333333334,0.19791666666666666,0.17948717948717946,0.2038216560509554,0.1699346405228758,0.21105527638190957,0.20512820512820515,0.19387755102040816
1081,SP:f8239f3c44b7d93152811cc2538f5bb9e28df8a8,"The paper introduces Memory Tower Augmentation (MeTA) --- an adaptive data augmentation approach for temporal graph networks (TGNs). The proposal aims to tackle overfitting and improve learning in TGNs. MeTA applies a hierarchical message-passing scheme to update TGNs' memory states. In addition, MeTA considers three different data augmentation strategies: i) perturb time, ii) remove edges, and iii) add edges with perturbed time.  The experiments show that MeTA achieves higher accuracy than existing TGNs (without data augmentation) on future link prediction and node classification tasks. ",This paper proposes MeTA: multi-level module that processes the augmented graphs of different magnitudes on separate levels. Three DA strategies augmenting are adopted (perturbing the edge time to simulate time shifts; removing edges; adding edges). Experiments are conducted on edge prediction and node classification tasks.,"This paper proposes an adaptive data augmentation framework for dynamic graphs that improves the performances of two well-known baseline models on node classification and edge prediction on four datasets. Three data augmentations were used with the purpose of modifying temporal and topological features. These strategies inject the inductive bias that closer events in the dynamic graphs, whether in time or topology, are more informative. ","The paper introduces Data Augmentation (DA) to the temporal graphs in order to offset the increased variance induced by complex GNNs. Three DA methods: perturb time, remove edges and add edges with perturbed time are considered. To make sure the DA effectively works for edges that are less informative, the author proposes a novel and efficient adaptive DA method based on recursion, which is named Memory Tower Augmentation (MeTA). The proposed MeTA can be added on top of RNN based temporal graph implementation. Experiments results on edge prediction and node classification tasks using public temporal graph data set suggest the effectiveness of the method. ",0.17857142857142858,0.15476190476190477,0.3333333333333333,0.2608695652173913,0.34782608695652173,0.2153846153846154,0.32608695652173914,0.2,0.2692307692307692,0.18461538461538463,0.15384615384615385,0.1346153846153846,0.23076923076923075,0.174496644295302,0.2978723404255319,0.2162162162162162,0.21333333333333332,0.16568047337278108
1082,SP:f8b032dd0b25f65a51e0bb23429fff5b5f5d0d30,"This paper studies a best-of-all-worlds algorithm for online learning with feedback graphs by incorporating a novel regularizer. The proposed algorithm simultaneously achieves near-optimal regret w.r.t. $T$ in the adversarial setting, stochastic setting and corrupted stochastic setting.","This paper introduces a novel bi-level regularization, namely the Tsallis-Shannon entropy, which is a combination of the two types of entropy with a linear shift. The paper proves the lower bound for the Hessian of the new regularization which is a diagonal matrix with positive entries, and thus convexity. By applying the regularization in the ""Follow the Regularized Leader"" framework, the paper derives an algorithm for online learning problems with undirected graph feedback, which is a generalization of the Multi-Armed Bandits (MAB) problem and Prediction with Expert Advice (PEA) problem. The algorithm achieves near optimal regret bounds for both bandits and full feedback problems with adversarially-corrupted stochastic loss setting. The paper also proves a bound for online learning problems with general undirected graph feedback, which is a big step towards a best-of-all-worlds guarantee for this setting.","The authors consider the best-of-both-worlds problem in bandits with graph feedback. Similar to [27], they consider stochastic bandits with C adversarial corruption that interpolates between the stochastic setting and the adversarial setting . As a result, they get $\text{polylog}( T)\sum_{k\in K}1/\Delta_{\min,k}$ regret in the stochastic setting and $\sqrt{|K|T}$ in the adversarial setting, where K is a clique cover of G. ","This paper considers the problem of online learning with feedback graphs. Feedback graphs tell which losses the learner gets to observe each time an action is chosen.  In particular, special cases include the bandit setting when the graph has no edges, and the full information setting when the feedback graph is a clique.  The authors derive bounds that scale in terms of the clique covering number of the graph. This clique covering number is the number of cliques (fully connected subgraphs) that are appearing in the graph. This number is useful because each time one arm is picked, the learner can observe the losses of all the other arms that are in the same clique.  Reasoning about the problem in terms of cliques is interesting because the exploration vs exploitation trade-off is not the same everywhere on the graph. If we consider each clique as one entity, we have a standard multi-armed bandits exploration vs exploitation trade-off. But then, when considering which action to play within the clique, this is a full information problem as we will get the feedback for all actions within the clique either way.  The authors then provide a FTRL algorithm that combines the Tsallis entropy (optimal for MAB) and the Shannon entropy (optimal for Full info).  They also mix in a log barrier regularizer to help with the stability of the algorithm.   The authors derive a single bound that covers simultaneously the adversarial, the stochastic, and the corrupted setting. That bound is near-optimal because of the consideration of the covering clique number rather than the independence number (which basically means that the edges in the feedback graph that are not part of a clique are not taken advantage of), and because the extra log barrier regularization term leads to extra log factors.",0.5,0.30952380952380953,0.4523809523809524,0.11888111888111888,0.26573426573426573,0.3333333333333333,0.14685314685314685,0.18055555555555555,0.06312292358803986,0.2361111111111111,0.12624584717607973,0.07973421926910298,0.227027027027027,0.22807017543859648,0.1107871720116618,0.15813953488372093,0.17117117117117117,0.12868632707774796
1083,SP:f91c7f521e80ec6f284cd6c68af6f50e78f98a67,"This work tackles the problem of active learning for non-parametric classifiers and streaming data, where data points arrive sequentially and, after providing its prediction, the active learning strategy must decide whether to observe the true label or not. Performance of active learning strategies under this setting is evaluated using the difference between the cumulative sum of expected loss incurred by the strategy and the cumulative sum of expected loss incurred by a Bayesian-optimal classifier, which relates to the pseudo-regret in stochastic multi-armed bandits. The authors propose the first computationally efficient algorithm for active learning using Deep Neural Networks (DNNs) (contribution 1), which is achieved using over-parameterized DNNs. By relying on the theory of Neural Tangent Kernel (NTK) approximation, they provide a theoretical analysis of the proposed strategy (contribution 2) and show that one can leverage recent works in model selection to design a novel model selection algorithm (contribution 3). More specifically, they provide high-probability upper-bounds on the pseudo-regret and on the amount of data required for the bound to hold.","Using a combination of the Neural Tangent Kernel (NTK) approximation and contextual bandit approaches, the paper first derives a neural active learning algorithm for the streaming setting where the data points come sequentially and the algorithm has to decide whether to query for the label of each data point. A second similar algorithm is developed for model selection so that the model complexity does not have to be known beforehand. Both algorithms have counterparts which train the neural networks with stochastic gradient descent. The goal of the algorithms is to balance the regret and the number of queried data points. Using the mentioned theoretical frameworks, joint bounds on the regret and the number of queries are derived.","This paper presents algorithms for active learning in the non-parametric regimes, through NTK approximation. The resulting algorithm matches the minimax rates for active learning in certain regimes. It also studies the model selection problem with respect to unknown parameters, which is missed in previous analysis with NTK approximation.",NB: The original submission contains an error. The supplementary material contains the fixed version of the submission.   This paper produces the first NTK based active learning method with provable guarantees. ,0.20224719101123595,0.08426966292134831,0.05056179775280899,0.1111111111111111,0.05982905982905983,0.14285714285714285,0.3076923076923077,0.30612244897959184,0.3,0.2653061224489796,0.23333333333333334,0.23333333333333334,0.24406779661016947,0.13215859030837004,0.08653846153846152,0.1566265060240964,0.09523809523809523,0.17721518987341772
1084,SP:f9893bcbda99867c1f536846eacf9ccef968cc44,"The paper proposes a substitute of the vanilla self-attention module, which claims a linear complexity. Like Performer and many other previous works, it introduces kernel functions and change the computation order among QKV in the self-attention module. Essentially, it replaces softx with ReLU and a cosine operation. Experiments are done on several benchmarks.","This work introduces a simple and elegant way to incorporate recency bias for kernelized linear attention mechanism using cos-based re-weighting mechanism.  This cosine re-weighting can be effectively implemented via query and key matrix transformations for linear attention mechanisms. The authors experiment with LRA, and both language modeling with autoregressive and bidirectional setups to show the efficacy of proposed method.","The paper proposes a variant of the transformer network. The authors base their work on their analysis of softmax attention. They argue that softmax works well because of two reasons (i) it stabilizes training due to reweighing the attention as well as associated connections within the network, and (ii) it forces non negative values in the attention matrix. Utilizing these observations, they propose two modifications (i) a linear projection kernel i.e. ReLU to compute similarity, and (ii) a cosine function for reweighing the attention values. With exhaustive experiments on 5 benchmarks, authors demonstrate that the proposed modifications lead to state of the art performance with reduced computational compelxity.   ","This paper presents cosFormer which presents a linear operator to replace the softmax operator in calculating self-attention for transformer architectures, maintaining competitive predication accuracy while enjoying linear space and time complexity instead of quadratic costs compared to the vanilla baseline. Under this context, the authors emperically show that two factors are critical to improve the efficiency of transformers: (1) the self-attention matrix should have non-negative elements and (2) aggregating negatively-correlated information needs the non-linear re-weighting strategy. Based on the insightful analysis, the proposed cosFormer adopts ReLU and the Ptolemy’s theorem (cosine re-weighting) in designing a linear replacement of the softmax operator. The performance of cosFormer is validated with the language modeling (autogressive in both causal and non-causal cases) on WikiText-103 dataset,  the finetuning on a lot of downstream datasets such as GLUE, IMDB and AMAZON, and the comparison with state of the art methods on the long-range-arena benchmark.  ",0.10909090909090909,0.36363636363636365,0.2909090909090909,0.16129032258064516,0.3064516129032258,0.23853211009174313,0.0967741935483871,0.1834862385321101,0.1,0.09174311926605505,0.11875,0.1625,0.10256410256410256,0.24390243902439027,0.1488372093023256,0.1169590643274854,0.1711711711711712,0.19330855018587362
1085,SP:f9d7c4d7e3550c5c384b4949f7cb50c708824b85,"This work addresses a limitation of CAMs (Class Activation Mappings) in the context of weakly supervised semantic segmentation (W3S) taking the information bottleneck perspective. The reason for the CAMs being highly selective, it argues, lies in the use of double-sided non-linearities (e.g. sigmoid) in the final layer of the classification network. The work removes this non-linearity and develops a strategy of obtaining high-recall attention maps through a one-shot adaptation process. Used as initial masks in a SotA W3S pipeline, these masks impressively increase the final segmentation accuracy achieving new state-of-the-art on established benchmarks.",This paper is about semantic image segmentation training with image-level labels. The method is using an auxiliary loss to help to find a larger localization area for pixel-wise training. Experiments are done on Pascal VOC2021 and MSCOCO.,"This paper presents a new method for computing class activation maps (CAMs), which has been used as crucial evidence for object detection and segmentation in a weakly supervised learning setting. A well known drawback of CAM and its variants is that they often highlight only small discriminative regions of an object. This paper interprets such a phenomenon through “information bottleneck” and proposes a way to alleviate it accordingly. In short, the proposed method finetune a classification network for generating the original CAM with a different classification loss and a different global pooling layer that allow more features to contribute to classification. The method enhances the quality of the original CAM noticeably and consequently improves performance of a weakly supervised semantic segmentation method relying on CAMs.  The main idea of this work is novel, and useful to alleviate the well-known limitation of the CAM and its variants. Moreover, the efficacy of the proposed method has been demonstrated by extensive experiments. However, the overall pipeline for semantic segmentation depends heavily on an existing method (IRN [2]), and it demands a much larger number of hyper-parameters compared to the original CAM.  ","This paper focuses on the problem of weakly-supervised semantic segmentation, where the goal is to conduct segmentation by mainly using image-level annotation as such weak form of annotations can be collected more efficiently.   Specifically, authors analyze the issue of Information Bottleneck and propose a simple method RIB to reduce it to obtain more complete CAM localization maps. A e a global non-discriminative region pooling (GNDRP) is further applied to enhance the effects of RIB.   Experiments on PASCAL VOC  and MS COCO datasets show the effectiveness of the proposed method.",0.058823529411764705,0.23529411764705882,0.18627450980392157,0.3076923076923077,0.38461538461538464,0.14210526315789473,0.15384615384615385,0.12631578947368421,0.20652173913043478,0.06315789473684211,0.16304347826086957,0.29347826086956524,0.0851063829787234,0.16438356164383564,0.19587628865979378,0.10480349344978167,0.22900763358778625,0.19148936170212766
1086,SP:f9dc584dc82d8deed928e5b1a9c81347abc05534,"The paper proposes TOME, a Transformer that additionally performs cross-attention over contextual mention encodings (in 1 or 2 layers - TOME-1/TOME-2) along with the usual self-attention. It can be viewed as extending EaE (Fevry et al., 2020) from entities to mentions (aka., virtual KB - Dhingra et al., 2020). TOME is pretrained on 150m mentions extracted from Wikipedia by masked language modeling and other auxiliary objectives. It significantly outperforms EaE on fact checking (HoVer, FEVER) and entity-only QA (TriviaQA, CWQ), but lags behind explicit retrieve-and-read models like RAG, REALM, FiD.","This paper proposes an external knowledge-based pre-trained model that can leverage entity-wise knowledge embeddings. The mention encoder model encodes entity mentions occurred in an external corpus (i.e. Wikipedia) into embeddings as the memory. The TOME model retrieves most possible entity mentions from this memory and performs attention on them to generate aggregated embeddings, which will be integrated into the output representations of the current layer. To make training efficient, a two-stage training strategy is used where mention encoder is trained first and TOME is trained secondly. Evaluations are performed FEVER, HoVer, TriviaQA and ComplexWebQuestions, comparing with several baselines covering BERT, EaE, RAG and REALM, where TOME achieved consistent improvements on most of them.","The authors propose to extend the standard Transformer architecture by allowing it to attend over factual information, represented as a large memory of dense representations of entity mentions. The resulting architecture is made up of two parts: a mention encoder, which is used to build up the ""mention memory""; and the transformer model augmented with attention over the memories. These are pre-trained in two stages for efficiency reasons. The model is evaluated on two claim verification datasets and two QA datasets, showing convincing performance against comparable methods.","__Method:__ This paper proposes a new approach to integrate knowledge sources into Transformer-based models. Concretely, entity mentions found in English Wikipedia (approx. 150M -- these mentions are linked to Wikipedia entities) are encoded into __Mention Memory__, which consists of key vectors and value vectors.  The knowledge representations in a __Mention Memory__ are accessed from a __TOME__ block, which is a stack of transformer blocks with a __memory attention__ layer. In this layer, a mention in the input sequence is converted into a “knowledge-injected” vector representation which is given by a weighted sum of the mention vectors (i.e., value vectors). The entire model (TOME) is pretrained in two stages. First, the Mention Encoder is pretrained, and the Mention Memory is generated. Here, the Mention Encoder is trained with a small-scale TOME architecture (Batch-TOME) using the masked LM loss and the coreference loss. Next, the Mention Memory is fixed, and other parameters in the TOME model are updated. The TOME model is trained with a masked LM task as well as an entity prediction task. This output layer allows us to use the TOME model for downstream tasks such as TriviaQA and Complex WebQA.   __Evaluation:__ This approach is evaluated on two tasks: claim verification (HoVer, FEVER) and QA (TriviaQA, CWQ) and compared with various baselines (EaE, RAG, REALM etc.). In the claim verification experiments, the TOME models (both 1 and 2 blocks) outperform all baselines on HoVer, which require reasoning using multiple sources. On FEVER, the TOME models outperform baselines except RAG. In the QA experiments, the TOME model with 2 blocks outperforms all baselines on CWQ. On TriviaQA, the TOME models outperform similar models such as EaE and much larger generative LMs (T5) but underperform the retriever-based models. Additionally, the authors perform analysis on retrieved passages, memory sizes, and performance on unseen entities in a QA task.   ",0.16666666666666666,0.11458333333333333,0.2604166666666667,0.11864406779661017,0.3220338983050847,0.375,0.13559322033898305,0.125,0.08038585209003216,0.1590909090909091,0.12218649517684887,0.10610932475884244,0.14953271028037385,0.11956521739130435,0.12285012285012284,0.13592233009708737,0.17715617715617715,0.16541353383458646
1087,SP:f9f335370a62a3640762894afd8d683c1b0ed1c9,"This paper discusses a novel approach to piano transcription which involves predicting notes as events rather than frame-level activity. This approach aligns well with the eventual goal of music transcription, i.e., transcribing musical audio to some form of music notation, e.g.: MIDI or staff which are both based on note events. The method proposed utilizes semi-Markov CRFs which have been utilized to directly predict note/pedal events as non-overlapping continuous intervals in the audio. Utilizing standard metrics for piano transcription, the authors show that their method outperforms current state-of-the-art methods which perform frame-wise transcription.","This paper presents an event-based polyphonic piano transcription model using neural semi-CRFs. Unlike the state-of-the-art ""onset and frame"" model and its variations, which rely on frame-level note state predictions, this work is based on segmenting a long sequence into a set of intervals which correspond to notes and silences. They also propose a forward-backward learning algorithm for computing the score functions of the intervals and, as a result, it enables to compute the partition function. This new approach achieved comparable performance to previous state-of-the-arts and it has a great potential to audio event detection tasks.  ","The authors take a well studied network architecture and task (MAESTRO piano transcription) and propose a straightforward extension of previous work to incorporate a CRF output layer that can directly predict temporal dependencies among the output labels. This contrasts with previous SOTA techniques that use multiple output heads (for separate parts of notes) and heuristics to infer the notes from network outputs. The CRF loss function is used to train a network similar to previous work, and shows improvements in classification accuracy, most notably in Onset+Offset F1 score.","The paper proposes to use semi-CRF for piano transcription. Since piano transcripts include asynchronous events and vanilla semi-CRF is not able to cope with these, the paper assumes that the events are independent from each other. Additional features for predicting the velocity, onset, offset refinements are included. Experiments show that the semi-CRF approach can be on par with the state of the art.",0.1650485436893204,0.10679611650485436,0.1650485436893204,0.1523809523809524,0.12380952380952381,0.1348314606741573,0.1619047619047619,0.12359550561797752,0.25757575757575757,0.1797752808988764,0.19696969696969696,0.18181818181818182,0.16346153846153846,0.11458333333333333,0.20118343195266272,0.16494845360824742,0.15204678362573099,0.15483870967741936
1088,SP:fa192a4227a481ee334dac421af47abced3f80c0,"The paper studies a federated learning scheme for training machine learning models using gradients provided by a set of strategic users. At each iteration, each user gets a mini-batch of samples, computes her gradient, and decides whether to report it to the fusion center (FC) or to report a random signal.  The authors propose a strategy for the FC which incentivizes users to truthfully report their gradients and ensure convergence guarantees. Results are validated experimentally on publicly available datasets.","This paper studies the incentives in distributed learning, where the users can choose to join the cooperation or not. The paper formalizes this problem as a two-player game between the fusion center and the user, designs a strategy to learn and provides a convergence guarantee. The contributions include: 1. formalizing of a reward mechanism between FC and users; 2. proposing a strategy for the FC to make users join the cooperation, where FC cannot observe users' actions; 3. providing theoretical guarantee on the convergence rate.",This paper proposed an incentive mechanism for a distributed learning setting. The users are encouraged but not obliged to provide the gradient. The results on convergence and an upper bound on the estimation bias of the gradient were shown. ,"The paper formulates the interaction between the fusion center (FC) and users in distributed learning (or federated learning) from the perspective of game theory. In order to encourage the users to provide effective information to the FC for model update, the FC adopt zero-determinant (ZD) strategy, associated with which there are some practical issues addressed by the authors. Theoretical analysis on the convergence of the proposed algorithm is presented. ",0.225,0.1125,0.2,0.1511627906976744,0.23255813953488372,0.3333333333333333,0.20930232558139536,0.23076923076923078,0.22857142857142856,0.3333333333333333,0.2857142857142857,0.18571428571428572,0.21686746987951808,0.1512605042016807,0.21333333333333332,0.208,0.2564102564102564,0.23853211009174316
1089,SP:fa4ec2e9987fc7ebdc3a546edd4529e5b92653d4,"The paper proposes DEAL, an RGB patch descriptor that has a learned robustness against non-rigid deformations. Given a SIFT keypoint, surrounding ResNet features are used to estimate a non-rigid thin-plate spline deformation of the patch. The RGB patch, after being rectified by the thin-plate spline, is passed to a CNN that regresses a 128-dim descriptor. The network is trained via a hard triplet loss. The training data is simulated and uses groundtruth correspondences/matches, but results are shown on both real and synthetic data. Experiments compare to a number of classic and learned baselines. DEAL outperforms other methods in the main experiment and shows best or second-best performance on a couple of applications.","This paper presents a learnt local keypoint descriptor for deformable objects. It is trained in a supervised fashion using the HardNet network and loss on the SIFT keypoints of a synthetic dataset (succinctly presented in the paper, based on existing SfM datasets). The novelty is the addition of a spatial transformer network (STN) to normalize the patches, inline with recent local descriptor learning approaches. As far as I understand the main conceptual difference with existing works is that the (STN) is not only affine but uses a TPS on a log-polar grid. My understanding of the paper is that it argues this part is crucial for matching deformable objects.","The paper introduces a new deep learning pipeline for the computation of local image features that are robust to non-rigid deformations. The proposed method is end-to-end, and works by simulating non-rigid transformations on a synthetic dataset to jointly learn how to rectify the deformed patches, while extracting discriminative and invariant features at the same time. Comparisons with several state of the art pipelines on multiple datasets confirm the effectiveness of the proposed approach.","This paper proposes to learn deformation-aware local feature descriptors by rectifying the local region with carefully designed spatial transformer networks (STNs). Instead of using 3D training data, the paper proposes to learn from deformed 2D samples. The feature extraction network utilizes two STNs to rectify the local region and HardNet to extract the final feature. Among the two STNs, the second STN is specially designed with the thin-plate-spline transform and a polar grid so that the resulting region flexibly deform from a circular shape. The experimental results show that better or competitive results with much less processing time.",0.15966386554621848,0.15966386554621848,0.15126050420168066,0.16363636363636364,0.19090909090909092,0.19480519480519481,0.17272727272727273,0.24675324675324675,0.1782178217821782,0.23376623376623376,0.2079207920792079,0.1485148514851485,0.16593886462882096,0.19387755102040818,0.16363636363636364,0.19251336898395718,0.1990521327014218,0.16853932584269665
1090,SP:fac7ab0695779480fedacd6a4f6e6d64416efdfc,"The paper suggests a construction of Covariance functions of multi-output Gaussian processes for vector fields on manifolds. The construction is independent of coordinate choices, but does depend on a Nash embedding of the manifold. In addition, the paper describes a clear mathematical framework of coordinate independent constructions of Gaussian processes. The approach is extended to variational approximations of such Gaussian processes, where the independence of the approach w.r.t. coordinate choices is again taken into consideration.","The paper proposes a natural extension of vector-valued GPs to data residing on Riemannian manifolds. The construction is straight-forward (replace matrix products with bilinear forms), which I consider a good thing. This also imply that it is relatively easy to apply the developed theory in practice. Toy demonstrations are provided to show that this is indeed the case. The paper is exceptionally well-written.",The authors propose a class of vector Gaussian processes (GP) on a class of Riemannian manifolds.  An important feature of the proposed GP is its ability to model vector fields on the Riemannian manifold. This is a broad application in physical sciences. One of the main novelties of the work is the development of gauge-equivariant kernels operators. Variation inference algorithms are extended for posterior inference.  Numerical examples demonstrate the practical performance of the proposed GP.,"This paper proposed a way to define vector-valued Gaussian processes on Riemannian manifolds. A computational framework is provided to facilitate statistical inference using these models. The main contribution is an explicit methodology for characterizing vector-valued Gaussian processes through extrinsic kernels satisfying a gauge constraint, thus bypassing abstract mechanisms often employed to define stochastic processes on Riemannian manifolds. Two examples are provided to illustrate the power of the proposed models.",0.21794871794871795,0.2692307692307692,0.19230769230769232,0.24242424242424243,0.24242424242424243,0.23684210526315788,0.25757575757575757,0.27631578947368424,0.2112676056338028,0.21052631578947367,0.22535211267605634,0.2535211267605634,0.2361111111111111,0.27272727272727276,0.20134228187919462,0.22535211267605634,0.23357664233576642,0.2448979591836735
1091,SP:fad01a92a789fef8b661bb10483ff389d97b02b7,"The authors present a method for ultra-high resolution image segmentation. It follows the learn to zoom approach to non-uniform downsample the original high resolution image. Furthermore, it adds edge-based loss to guide the downsample process. The upsampling step is also included in the training stage. The proposed method is evaluated on several public datasets and achieves promising results. Effectiveness of components are verified in the experiments. ","Semantic segmentation using deep learning techniques becomes challenging if the input images are of high resolution. The image resolution can be lowered and the network learned, but downsampling based on uniform grid pattern on the high resolution image can lead to network not being able to learn semantic information in high frequency regions, i.e. the loss of information is uniform over the image. In order to alleviate this problem, the downsampling can be done based on a non-uniform grid on the high resolution image which is content adaptive where more samples are selected at the high frequency/important regions while low frequency regions are sampled sparsely. Previous work has computed this non-uniform downsampling in a deep learning framework as an independent task. This paper proposes to jointly learn the downsampling grid estimation task and the particular task of semantic segmentation. They show improved IoU metric over semantic segmentation results on standard benchmarks.","This paper aims to improve the performance of segmenting ultra high-resolution images by replacing the commonly used downsampling functions with the learned deformation sampling module. The biggest difference of the proposed method is, instead of sampling all areas uniformly, the sampling density is estimated based on the contents. The experiment results on the Cityscape, DeepGlobe and PCa-Histo datasets show some improvements on mIoU and cost-performance trade-offs compared with uniform downsampling.","This paper address the semantic segmentation problem on high-resolution images. Existing methods uniformly downsample the original image to a small version to meet the memory requirement, while the uniform downsampling is suboptimal. The authors propose a deformed downsampling method in this paper and mainly compare it with the previous edge-based downsampling, showing better performance. Although the proposed method is somewhat similar to one previous method, while directly applying which on segmentation task does not improve the performance. This demonstrates the importance of the modifications proposed by the authors in this paper.  ",0.2898550724637681,0.2028985507246377,0.2898550724637681,0.12258064516129032,0.14193548387096774,0.24324324324324326,0.12903225806451613,0.1891891891891892,0.21505376344086022,0.25675675675675674,0.23655913978494625,0.1935483870967742,0.17857142857142858,0.1958041958041958,0.24691358024691362,0.16593886462882093,0.17741935483870966,0.2155688622754491
1092,SP:fad2e552f31504231abea25efee7e65a93f6c620,"### POST-REBUTTAL ### I would like to thank the authors for their reply. I am pretty satisfied with the justification for setting $c(k,d)=1$. The comparison between the different related works also looks great. I am not raising my score further to Accept as I still see the presence of the Lipschitz constant in the bound as a weakness but this is common to many works and hopefully its estimation will see some advances in the nearest future.  ### POST-REBUTTAL ###  This paper presents margin-based generalization bounds for deep neural networks based on the k-variance. This latter is a new way of measuring the variance in the observed data (or the probability distribution generating it) based on optimal transportation (OT) theory and the Wasserstein distance related to it. The obtained generalization bounds are claimed to have several important advantages as k-variance: 1) can be computed from the available data contrary to other complexity measures such as the popular Rademacher complexity; 2) portrays more truthfully the underlying generalization capacity of the considered model. In addition to k-variance, the obtained bound also relies on the Lipschitz constant of the margin function with respect to (wrt) the encoder and the desired threshold on the margin used to define the empirical risk. Experimental results on large-scale datasets suggest the usefulness of the introduced complexity measure in capturing the generalization of deep neural networks. ","This paper develops generalization bounds based on class margins and the optimal transport cost (in Wasserstein-1 distance) between two independent random subsets sampled from the training distribution. The authors argue that the optimal transport cost term explains the correlation between clustered representations and good generalization. Empirically, they show that mixup+OT cost improves over mixup+DBI in predicting generalization error.","Firstly, the authors proposed new margin bounds based on k-variance which can better capture the structural properties of the data distribution than other margin based bounds. Secondly, they further developed a variant called gradient normalized margin bounds which perform well empirically on the large scale PGDL challenge data. Moreover, they informally showed the k-variance bounds' ability to capture the low-dimensional structure of data distribution. Last but not least, they showed that maximizing the margin will give rise to the feature separation. The main contribution of the paper is to introduce the idea of k-variance generalization bounds.","This paper proposes margin-based generalization bounds based on the k-variance, a generalization of the variance based on ideas from optimal transport. The k-variance, like the Rademacher complexity, is a data-dependent term, and captures certain properties of the data such as concentration and separation of features. Unlike the Rademacher complexity, the k-variance is easily empirically estimated.   The authors proceed to empirically evaluate how well their bounds can predict generalization, using the framework of recent work in the literature, namely the PGDL competition. The authors show competitive empirical results on these tasks. Although there are existing generalization measures which perform as well or better, this work is notable as it's bounds are theoretically supported. ",0.07692307692307693,0.13247863247863248,0.1752136752136752,0.19672131147540983,0.2786885245901639,0.25,0.29508196721311475,0.31,0.3474576271186441,0.12,0.1440677966101695,0.211864406779661,0.12203389830508475,0.18562874251497002,0.23295454545454544,0.14906832298136646,0.18994413407821228,0.2293577981651376
1093,SP:fafaa9d1e39c489c68fa73f87e54a2a2d48df16b,"The paper works on self-supervised audio-visual representation learning. It shows uncurated movies and TV shows are good sources for learning audio-visual representation. The proposed model targets on short video clips like 3-second clips, and learned representation is evaluated on action classification and audio classification. ","The task is to learn audio-visual representations from uncurated data such as movies and TV videos. Unlike previous works that show learning from uncurated data is ineffective, the authors show good performance from learning from these videos. They propose hierachical sampling policy, with negatives being sampled from both within and across different sources. The model is trained on the dataset that the authors collected, which consists of thousands of hours. The performance on various downstream tasks (action recognition, sound classification) is on par compared to using much larger uncurated datasets such as IG-Random.","The paper proposes a method to learn self-supervised audio-visual representations from long (order of half to couple hours) uncurated videos namely from movies and TV shows. A new sampling approach is presented to ensure batches are created while taking into consideration the effect of re-occurring audio-visual patterns (main character or background scenes). Picking multiple clips from the same long video that are spaced by some minimum distance is found to be effective as compared against random sampling. Pretrained models are evaluated on standard action recognition tasks: UCF101 and HMDB, and audio classification on ESC50.","This paper performs self-supervised, audio-visual pretraining of deep CNNs on movies and TV shows. The downstream tasks investigated are action classification in videos (HMDB51 and UCF101 datasets) and environmental audio classification (ESC50 dataset). The paper shows that pre-training on 0.7-1.4 years worth of TV and movies results in downstream task performance that is competitive with, but lags slightly behind, the current state-of-the-art approaches that pre-train on datasets like HowTo100M, AudioSet, and IG-Kinetics65M. ",0.3125,0.4791666666666667,0.3541666666666667,0.22105263157894736,0.17894736842105263,0.1836734693877551,0.15789473684210525,0.23469387755102042,0.20481927710843373,0.21428571428571427,0.20481927710843373,0.21686746987951808,0.2097902097902098,0.31506849315068497,0.25954198473282447,0.21761658031088082,0.19101123595505617,0.1988950276243094
1094,SP:fb4ad7e4d5644623f8f01670518e726fd3f0aa4d,"The paper studies the problem of online reinforcement learning, while making use existing offline data that was previously logged by some behavior policy, where there is unobserved confounding in the logged data. They provide some conditions under which the RL model can be identified from the confounded logged data, and under these conditions propose a variant of optimistic value iteration for online learning that incorporates this offline data as a regularization term, which has an intuitive Bayesian interpretation based on conditioning on this data. Finally, they provide regret bounds for their algorithm, which has a leading term incorporating the effect of the logged data, which decays to zero as the amount of logged data grows to infinity.","This paper proposed an algorithm to learn optimal policy reinforcement learning in the presence of confounded observational data. It proposes to remove confounders from the data and improve sample efficiency in online settings. It addresses two scenarios, partially observed and unobserved confounder, by using two techniques, the frontdoor, and backdoor criterion. It further theoretically analyzes the regret bound in these settings. ",This is a technical paper presenting how to incorporate offline observational data to improve the sample efficiency in the online reinforcement learning setting. The issue is the potential presence of unobserved confounders in the observational data which impact the transition dynamics and the rewards and how to adjust the exploration bonus used in the online setting. The authors suggest an algorithm (DOVI) which adjusts for the confounding bias (where the coufounders are partially observed or unobserved). They then derive a bound on the regret when using linear function approximation which shows that the regret is smaller than the optimal online regret thanks to the use of offline observational data if they are informative.,"In the main paper, the authors study the problem of performing value iteration using confounded observational data where the confounders are partially observable. Applying for backdoor adjustment to correct the observational data, the authors propose de-confounded optimistic value iteration (DOVI) in this setting. Finally, a sublinear regret bound of DOVI is provided for linear confounded MDPs and cases where the observed subset of the confounders satisfy the backdoor criterion.",0.1111111111111111,0.21367521367521367,0.18803418803418803,0.3442622950819672,0.22950819672131148,0.168141592920354,0.21311475409836064,0.22123893805309736,0.3142857142857143,0.18584070796460178,0.2,0.2714285714285714,0.14606741573033707,0.21739130434782608,0.23529411764705882,0.24137931034482757,0.2137404580152672,0.20765027322404372
1095,SP:fbee52276f84c84d98036dcb9020171a0d4adae3,"This paper evaluates existing attacks and defenses to gradient inversion attack, where eavesdropper of the protocol could steal private data from the clients. It re-evaluates attacks under relaxed assumptions, the effectiveness of the defenses, and proposes actionable ways of combining defenses under realistic assumptions to have more robust FL. The paper clarifies the state of the art in gradient inversion attacks and sheds light on best practices for good defense deployment","This paper has empirical study of federated learning with the following contributions: 1. It evaluates the state-of-the-art gradient attack algorithms under weaker assumptions. For example, attackers do not have access to batch norm statistics and some private labels. Under these weaker assumptions, these attack algorithms perform significantly worse. 2. It experiments various defense algorithms. For example, gradient pruning, MixUp, Weak-InstaHide, with the current state-of-the-art Carlini et al. 2020 attack algorithm. Further, it designs and evaluates a new method which combines various defense algorithms. Through empirical evaluations, it is shown that when attack batch size is 1, W-InstaHide is highly useful, and when attack batch size increases, the combined gradient pruning+W-InstaHide gives the best result.","The authors of this paper provide a systematical analysis for the robustness of federated learning. They first show that current strongest gradient inversion attacks cannot perform well under relaxed assumptions and then demonstrate a new defense that can make the attacks less effective, even under their original assumptions. Sufficient experiments provide strong support for their conclusions. ","The paper presents an empirical investigation of existing gradient inversion attacks and defenses in federated learning setting. The authors point out that current attacks make strong assumptions such as knowing batch norm stats or private labels, whose absence would weaken the attacks. The authors also conclude with a list of best practices of doing conducting safe federated learning via comprehensive review and experiments on current defense methods.",0.2638888888888889,0.16666666666666666,0.20833333333333334,0.11290322580645161,0.12096774193548387,0.21428571428571427,0.1532258064516129,0.21428571428571427,0.22388059701492538,0.25,0.22388059701492538,0.1791044776119403,0.19387755102040816,0.1875,0.21582733812949642,0.15555555555555556,0.15706806282722516,0.19512195121951217
1096,SP:fbfad5ed0c2325a6543eb8749db85c0fc650cdbf,"The authors consider the problem of recovering the support of multiple sparse vectors from a noisy mixture of linear measurements (i.e., mixed linear regression), and from a noisy mixture of signed measurements (i.e., mixed linear classification). That is, each measurement corresponds to one of the unknown sparse vectors, but which vector is unknown.  For these problems, they propose three algorithms, each of which can be applied to either mixed regression or mixed classification. Furthermore, they provide guarantees on the number of measurements that needs to be collected to recover the support with high probability.  The guarantees for their first algorithm hold when the support matrix is p-identifiable, but worst-case guarantees that do not require p-identifiability are also given. Stronger guarantees hold for their second algorithm in the case that the unknown supports are flip-independent. Finally, guarantees hold for the third algorithm when the support matrix has small Kruksal rank. "," The paper addresses a generalized problem in 1-bit compressed sensing, considered in several papers [9, 19, 27, 29]. The problem is to recovery the supports of ALL of l unknown k-sparse vectors from 1-bit noisy measurements based on a sufficient number of query vectors (See Sec. 1.1). The problem is addressed for both mixture of linear classifiers (MLC) and mixture of linear regressions (MLR).   Corollary 1 provides sufficient conditions for the number of queries to recover the supports with asymptotically probability 1. The corollary follows immediately from Theorems 1 and 2 based on the notion of an identifiable set of l sparse vectors in Definition 1. Theorem 3 postulates a flip-independent set of l sparse vectors in Definition 2 to use Jenerich’s algorithm for the canonical polyadic (CP) decomposition of a tensor. Theorem 4 postulates that the set of sparse vectors has a known Kruskal rank r to guarantee the uniqueness of the CP decomposition. Theorem 3 provides a tighter sufficient condition than Corollary 1 for l>4 while Theorem 4 is tighter than Theorem 2 for (2l-1)<(r-1)(log l + 2), which holds for small l.   ","This work studies support recovery of a collection of multiple, unknown sparse vectors using a small number of noisy linear or 1-bit observations where the observations are constructed from random mixtures of the unknown sparse vectors. In other words, for each measurement, one of the multiple, unknown sparse vectors is chosen uniformly at random to generate the response and we aim to learn the support of all of these unknown vectors from the mixed measurements using the least amount of measurements as possible. In particular, the work considers sample complexity of recovery in both models and provides efficient algorithms to perform the recovery. Previous work in the area considered either approximate recovery under restrictive assumptions or support recovery only for the case where there are \ell = 2 unknown sparse vectors (of course, the \ell = 1 case has been studied extensively). This work considers support recovery for \ell >= 2 unknown sparse vectors.","This paper considers a support-recovery problem for a mixture of sparse linear classifier and regressions with $l \ge 2$ number of components in the active-learning setting, i.e., where we can design input-vectors (or queries, as few as possible). For sparse linear classifiers, unlike in previous work [19] where supports of any two unknown vectors are disjoint, the authors consider a few milder assumptions on the support (Definition 1 and 2). For sparse linear regressions, they do not impose target vectors to be on some scaled integer lattice [27, 44] (but instead we need minimum magnitude $\delta > 0$ for any non-zero entry, and some high SNR condition).   In order to recover all supports, three assumptions are considered for the support-matrix: (1) p-identifiable, if some size $p$ subset of support locations uniquely decides an unknown vector, then the authors can devise some combinatorial approach extending [19], (2) flip-independent, which essentially is required for some 3rd-order (support) tensor uniquely decomposable, (3) more general case where we deal with higher-order (support) tensor decomposition which cannot be done in polynomial-time in general but for full-rank 3rd order tensors.   The heart of the proposed algorithms for all three cases is to compute $|occ(C,a) := \{v \in \mathcal{V} | \ supp(v)|_C = a \}|$ for all possible combination of $C$ and $a$ of a fixed length $s \ge 1$ (Lemma 2). It seems quite non-trivial to compute all these $\Omega(n^s)$ quantities using only $O(log n)$-queries. Equipped with this powerful lemma, subsequent procedures seem to be nice combinatorial arguments adjusted to different assumptions. Overall algorithms and proofs are quite non-trivial. ",0.23225806451612904,0.21935483870967742,0.2064516129032258,0.17435897435897435,0.18974358974358974,0.2236842105263158,0.18461538461538463,0.2236842105263158,0.11428571428571428,0.2236842105263158,0.13214285714285715,0.12142857142857143,0.2057142857142857,0.22149837133550487,0.1471264367816092,0.19596541786743518,0.15578947368421053,0.1574074074074074
1097,SP:fc4cc42b4c1bc8f916cb0664e690341989d1c616,"The paper suggests a new representation of the 4D light field using a neural scene representation that predicts the light field from the 6D Plucker coordinates. The new scene representation enables the pixel radiance estimation using a single network query, whereas previous works needed to follow ray-marching or volume rendering procedures that require multiple queries along the ray. The presented scene representation is then utilized to learn multiple shapes or scenes using hypernetworks, in order to learn a prior of multiview consistency. Novel view synthesis results are presented for the learned scenes from the dataset, as well as generalizations from only single view supervision. Moreover, the authors demonstrate how sparse depth maps can be extracted from the learned light field in the case of Lambertian scenes.","The paper presents a new neural scene representation based on the idea of light fields. Rather than predicting properties (e.g. occupancies, colors) for points in space, the paper proposes to predict such entities for all rays in a scene using simple MLP networks. For the network input the authors propose to use Plucker coordinates to canonically parametrize the viewing rays independently of a point offset.  To encourage multi-view consistency of the network predictions, the authors propose a meta-learning approach allows to decouple the rendering from the latent code optimization.  In contrast to volumetric neural scene representations that require expensive sampling with multiple network predictions per ray for rendering a novel view, the proposed method only requires a single network evaluation per ray while achieving state-of-the-art rendering results.  ","The authors propose a novel neural scene representation called Light Field Networks (LFNs), where geometry and appearance of the considered scene are represented in a 360-degree, 4D light field that is parameterized via a neural implicit representation. In contrast to other ray-marching-based or volumetric-rendering-based techniques that rely on hundreds of evaluations per ray in similar tasks, the proposed LFN only requires a single evaluation per ray, thereby significantly improving efficiency and enabling real-time rendering at low memory requirements. Key aspects to achieve this are the parametrization of the space of light rays based on Plücker coordinates. In addition, the authors embed LFNs in a meta-learning framework to allow novel view synthesis from solely sparse 2D image supervision.    The overall approach seems novel and interesting. While the complexity benefits have been demonstrated by the authors, the current approach seems to be limited to simple scenes.  ","Light Field Networks encode the light field of a 3D scene, which comes with certain restrictions on where an observer can be placed. LFNs are coordinate-based MLPs that use the Plucker parameterization of directed rays in 3D space to represent rays. Thus they can output the final color of a pixel with a single network evaluation, massively speeding up rendering relative to NeRF but sacrificing ""hard"" multi-view consistency, which now needs to be learned. LFNs here are trained via a hypernetwork and scene conditioning happens via an auto-decoded latent code for the hypernetwork. While light fields only represent appearance directly, it is possible to use gradients to extract depth along appearance edges. Results are presented on ShapeNet and very simple synthetic rooms.",0.2283464566929134,0.2440944881889764,0.16535433070866143,0.21052631578947367,0.18045112781954886,0.15789473684210525,0.21804511278195488,0.20394736842105263,0.168,0.18421052631578946,0.192,0.192,0.2230769230769231,0.2222222222222222,0.16666666666666669,0.1964912280701754,0.18604651162790697,0.17328519855595667
1098,SP:fc6d9fc8824e1c209d905b7842ee0f9c59122b34,"This paper proposes a joint learning method to solve the placement and routing problems together. Such problems used to be studied separately.  For the placement, this paper is also solving the placement of macro and standard cells together. Moreover, both CNN and GNN are adopted to provide different embedding model.  Finally, the experimental shows that the proposed jointly method has notable performance margin than traditional separate pipeline. ","This paper proposed a joint learning method for the placement of macros and standard cells, by the integration of reinforcement learning with a gradient based optimization. The paper also proposed to jointly solve placement and routing via reinforcement learning, which has been the first time attempted in literature. The designed policy network incorporates both CNN and GNN to provide a multi-view embedding with a global embedding and a node level embedding. The paper also proposed a random network distillation to encourage exploration. Empirically, the method surpasses the separate placement and routing pipeline by a notable margin.",The paper introduces a joint placement and routing system based on deep reinforcement learning. They use two RL agents to do placement and routing and train them using a single reward function. This design and the reward structure enables them to reach closer to the global optimum. The results show that the learned policies are effective and beats the baselines they consider.,The paper proposes a deep reinforcement learning method to solve the chip placement and routing problem. They first introduce an RL based method combined with a gradient-based method to approximate the wirelength. They then expand their RL agent to also support routing task as well.   ,0.3880597014925373,0.208955223880597,0.208955223880597,0.1958762886597938,0.16494845360824742,0.22580645161290322,0.26804123711340205,0.22580645161290322,0.30434782608695654,0.3064516129032258,0.34782608695652173,0.30434782608695654,0.3170731707317074,0.21705426356589147,0.24778761061946908,0.2389937106918239,0.22377622377622378,0.25925925925925924
1099,SP:fcb8c7104e308da121569a585a99ad18fa91167b,"This paper studies online convex optimization in the strongly-adaptive regret setting. Essentially, this paper generalizes MetaGrad, an OCO algorithm which was able to adapt to the type of convexity of the losses, to the strongly adaptive regret setting.  This algorithm is build using the metagrad meta-regret structure, geometric covering, a new surrogate loss function, and a generalization of the sleeping experts analysis. By deriving second-order regret bounds on the sub-intervals, the authors are able to show strongly-adaptive regret guarantees. The proposed algorithms were also shown to have favorable performance in experiments.  ","In this paper, the authors develop a universal algorithm, namely UMA, that is able to minimize the adaptive regret of general convex, exp-concave and strongly convex functions simultaneously. The key idea is to modify MetaGrad to support sleeping experts and introduce additional surrogate losses for strongly convex functions and general convex functions. In this way, UMA can minimize the adaptive regret of multiple types of convex functions automatically, and thus is universal. Some experiments are done to evaluate the effectiveness of UMA.","This paper considers the problem of minimizing the adaptive regret together with being adaptive to the regularity of the loss function (exp-concavity, strong convexity, convexity,...). On one hand, algorithms and techniques to minimize the adaptive regret already exist but existing results are specific to the curvature of the losses. On the other hand, there exist universal algorithms that are adaptive to broad classes of loss functions (Metagrad and extensions). This paper combines both techniques and provides two algorithms (PAE and UMA) that get both guarantees simultaneously. In particular, it is adaptive to changes in the regularity of the losses. Finally, the authors provide some experiments to illustrate their results.","In this papers, the authors tackle the problem of online optimization with unknown (or even changing) convex function types, which can be plain convex, exp-concave, or strongly convex. Their main goal is to design an algorithm that suffers asymptotically optimal regret on any time interval.   To this end they extend the MetaGrad algorithm, a method that guarantees low static regret against plain convex or exp-concave functions, to also handle strongly convex functions and in a way that guarantees low adaptive regret, not only static regret. Their main technical contribution is showing that this can be done by incorporating sleeping experts in the meta algorithm of MetaGrad.",0.1875,0.25,0.17708333333333334,0.26506024096385544,0.3253012048192771,0.19090909090909092,0.21686746987951808,0.21818181818181817,0.1574074074074074,0.2,0.25,0.19444444444444445,0.20111731843575417,0.23300970873786409,0.16666666666666669,0.22797927461139897,0.2827225130890052,0.1926605504587156
1100,SP:fcc60c8d3ffabe17685cc068ed8f4f67dcee921a,This paper proposes a method named MABCQ to utilize offline data in the MARL environment via (1) value deviation and (2) transition normalization. These two techniques are used to correct the bias in the individual observation of the transition probability and improve the discovery of optimal policy. The authors showed the convergence of the Q-learning under the non-stationary transition probabilities after modification.  ,The authors propose a scheme to modify the transition probabilities in the learning process in a fully decentralised MARL setting. The authors show that the method is still converging to the optimal solution. They also provide some experiments on synthetic environments.,"The paper studies an offline MARL setting, with a focus on the decentralized case, and discovers that the difference between transitions in the dataset and other agents can be very large. Authors propose two techniques, value deviation and transition normalization, to modify the transition probabilities. Experimental results validate its performance improvement over BCQ on multi-agent MuJoCo tasks. ","This paper studies a MARL offline setting, where each agent is trained independently. value deviation and transition normalization techniques are proposed to modify the transition probabilities. Experimental results on four benchmarks show MABCQ's performance improvement over BCQ.",0.15625,0.203125,0.234375,0.24390243902439024,0.17073170731707318,0.3448275862068966,0.24390243902439024,0.22413793103448276,0.39473684210526316,0.1724137931034483,0.18421052631578946,0.5263157894736842,0.19047619047619047,0.21311475409836064,0.29411764705882354,0.20202020202020202,0.17721518987341772,0.4166666666666667
1101,SP:fcde8566eb9547021c949e5ebdb843556231fe7a,"This paper studies impacts of data augmentation. Authors propose new measures for evaluating data interference and occlusion robustness. Through analyzing changes in accuracy induced by data distortion, authors argue that current practice about using and treating data augmentation has led to biased model analysis and interpretation methods. ","This paper delves into the assumption that data modification is detrimental to training, while negligible when analysing models. It aims to encourage better practice when dealing with data distortions rather than elimination. With experiments and analyses, the authors show that current shape biased identification methods and occlusion robustness measures are biased. The authors then propose a fairer alternative to measure occlusion robustness. A series of experiments are conducted to disprove common assumptions and put forward the argument that not preserving the data distribution can lead to learning better representations. ","This paper studies empirically the effect of artefacts and biases in model analysis and training methods relying on data manipulation. They argue that common metrics used for analysing shape bias and occlusion robustness also (undesirably) include the effect of artefacts (such as edge sharpness) introduced by the corresponding image manipulations, and propose a new metric, iOcclusion, which is more robust to these effects.  Further, it is argued that these artefacts also manifest themselves in training schemes based on mixed sample data augmentation (MSDA), and can be beneficial by introducing a systematic bias towards complexity.","In this paper, the authors examine the effects of distorting data and its effects on  biased results. They emphasize on image modifications, and they propose an alternative measure of occlusion robustness (called the interplay occlusion based on the interplay between seen and unseen data.) The authors also focus on questions about mixed sample data augmentation. They mention the need for arguing about bias from distortion and its role in trusting models and injecting informed knowledge as well.",0.23404255319148937,0.2978723404255319,0.2765957446808511,0.19101123595505617,0.16853932584269662,0.2127659574468085,0.12359550561797752,0.14893617021276595,0.16883116883116883,0.18085106382978725,0.19480519480519481,0.2597402597402597,0.16176470588235295,0.19858156028368792,0.20967741935483872,0.18579234972677594,0.18072289156626506,0.23391812865497075
1102,SP:fd472a8273d57b0846777b078af42125e9c904d2,"This paper studies adversarial attack and defense of GNNs at scale. The authors investigate the relationship between surrogate losses and accuracy, and scalable attack and defense algorithms. Some theoretical intuitions and extensive empirical evaluation are provided to support the claims.","This paper studies the robustness of graph neural networks on a massive scale compared to the existing robustness work that focuses on smaller graphs, in terms of three perspectives: surrogate loss, attack, and defense.  - The firstly proposed surrogate loss can be used to enhance the strength of adversarial attacks. - The authors propose the scalable attack method for large graphs, which is implemented with sparse operations for reducing space complexity by considering a subset of entire adjacency elements when optimizing networks. - Lastly, the authors propose the scalable yet robust function for aggregating neighboring information on graph neural networks, which is adopted from the previous Soft Medoid method for the scalable purpose.","This paper addresses the problem of adversarial robustness for GNN regarding attack and defense on large graphs up to 100 million nodes. In terms of attack, they show that existing attack methods are weak for large graphs. To address this, they introduce the Masked Cross Entropy (MCE) which only considers correctly classified nodes on the attack. In addition, they also tackle the issue of handling large graphs which should be scaled with quadratic space complexity of adjacency matrix, and introduce the Projected Randomized Block Coordinate Descent (PR-BCD) based on Bernoulli sampling. In terms of defense, the authors propose the Soft median with differentiable relaxation of the Median. ","The paper studies adversarial attacks against graph neural networks as well as adversarial robustness at scale.  The authors first propose two first-order optimization attacks, i.e., PR-BCD and GR-BCD. The authors then propose an empirical scalable defense based on soft median aggregation. Both attacks and defenses are evaluated on benchmark datasets and show their effectiveness.   Strengths +The studied problem is important +Propose a novel attack loss function +Evaluation is extensive  Weaknesses -The paper studies white-box attacks -Algorithm details are unclear -The proposed defense is empirical",0.325,0.325,0.35,0.2636363636363636,0.16363636363636364,0.12037037037037036,0.11818181818181818,0.12037037037037036,0.15730337078651685,0.26851851851851855,0.20224719101123595,0.14606741573033707,0.1733333333333333,0.17567567567567566,0.21705426356589144,0.26605504587155965,0.1809045226130653,0.1319796954314721
1103,SP:fd94eba6f88231cd682cc42099a440a3823d7b5e,"The paper presents a technique for parameter reduction in neural networks. It follows the motivation of parameter redundancy in over-parameterized networks and aims to parameterize large networks with a smaller and shared set of weights. More specifically, a set of parameters based on a predetermined reduction factor is randomly assigned to various layers of a network and trained jointly. It is a simple technique that can be applied to any network architecture as the parameter generation is decoupled from the underlying architecture. The proposed method is evaluated in several tasks from classification to segmentation by using different network architectures. The experiments show improved or competitive performance when trained with similar or fewer parameters compared to the baselines."," This paper uses parameter generation to generate parameters for different hidden layers in neural networks. There are several significant strengths and weaknesses in this paper. Especially, the practicability of the proposed method is questionable. It would be good to see my detailed comments below. ","The paper proposes recurrent parameter generator (RPG) that is able to generate (ideally) arbitrarily large model based on a fixed set of inputs $\mathbf{W}$. Unlike common pruning or compressing techniques, the authors argue that RPG decouples the expressivity with the degree of freedom of a model, and that we can dynamically generate model parameters on the fly (while taking advantage of the pseudo-random seed) with a simple mechanism. Experiments show that this new way of generating model parameters is able to perform on par with, or better than, many existing compressive or pruning approaches.","In order to reduce size of deep models, this work propose one parameter sharing method for different convolutional layers. With the help of one sharing set of parameters, all convolutional kernels can be generated from the sharing parameters. They show the effeciveness of this method in classification, pose estimation tasks.",0.1016949152542373,0.1694915254237288,0.13559322033898305,0.22727272727272727,0.20454545454545456,0.11458333333333333,0.2727272727272727,0.20833333333333334,0.32,0.10416666666666667,0.18,0.22,0.14814814814814817,0.18691588785046728,0.19047619047619047,0.14285714285714288,0.1914893617021277,0.1506849315068493
1104,SP:fd974c9dc7ce5252092f1e0a117e3f70a7f5439a,"The paper presents a new way of molecular optimization that is based on chemical fragments. The authors employ VQ-VAE to encode a fragment library in a coherent latent space. They argue that this method produces more diverse rationales than a Gaussian VAE. The proposed generative method uses reinforcement learning (PPO) to select modifications of the chemical structure: a fragment decoded by VQ-VAE is added at a chosen attachment point, or a part of the compound is removed. The model retains only modifications that fulfill initial constraints regarding the novelty, diversity, and chemical properties of a generated molecule measured by formerly trained predictive models. The experiments show significantly better performance of the proposed FaST model compared to a diverse set of baseline models.",This paper proposes to perform molecular optimization by sequential translation. A reinforcement learning policy is employed to increase the diversity of the generated molecules. The proposed method is shown to be able to generate high-quality molecules on single-property and multi-property optimization tasks. The paper is well written and easy to follow.,"This paper proposes a new reinforcement learning method for molecular optimization. They train VQ-VAE to learn the vocabulary of molecular fragments which enables the optimizer to efficiently search the chemical space. The optimizer consisted of general reinforcement learning and MPNN, and devised sampling schemes to improve performance. By integrating evaluation criteria as a reward function, the proposed model can additionally improve the performance in terms of those criteria. The experiments show not only its superior performance but shorter optimization time. ","The present paper aims to discover novel molecules with desirable properties. One of the main contributions is to compose a molecule fragment-by-fragment, where each fragment is drawn not from a fixed fragment dictionary but from a NN-based pre-trained fragment generator. Other contributions are related to the optimization module using RL techniques; introducing fragment deletion as an action, sequential translation, novelty- and similarity-aware reward design, and initial state distribution. The proposed method is evaluated by ligand generation tasks, where generated molecules are evaluated in terms of success rate, novelty, and diversity. ",0.11290322580645161,0.18548387096774194,0.13709677419354838,0.25925925925925924,0.2222222222222222,0.16049382716049382,0.25925925925925924,0.2839506172839506,0.17894736842105263,0.1728395061728395,0.12631578947368421,0.1368421052631579,0.15730337078651685,0.22439024390243903,0.1552511415525114,0.2074074074074074,0.16107382550335572,0.1477272727272727
1105,SP:fe3e06675a95deaee45689c5a85a61deaf2aee5b,"This paper shows theoretically and empirically that self-supervised objectives can be used to extract useful information about the posterior of the topic proportion vector given a document, regardless of the underlying models. It extends the findings of Tosh et al.2020 about contrastive objectives to reconstruction-based objectives. It is interesting to see that one simple reconstruction objective recover the posteriors generated from the different probabilistic topic models on the synthetic dataset.","The main strength of this paper lies in the thorough theoretical analysis and corresponding proofs. The authors not only prove that a new reconstruction-based objective can also extract posterior topic information for a general topic model, but also strengthen the guarantee for contrastive objective in Tosh et al. (2020) by removing some of their assumptions and the necessity of landmark documents. ","This paper considers the problem of performing posterior inference for probabilistic topic models. A ""general"" topic model consists of two key parts:  - $A$ : the topic-word probability matrix - $\Delta$ : the prior distribution over doc-topic probability vector w  Given these parts and a new document x, the goal is to infer the posterior over w: $p(w | x)$.   It is well-known that this posterior can be estimated in several ways using standard approximate Bayesian inference methods: MCMC, variational, etc. The detailed steps of these methods are usually customized to the choice of prior.  This paper suggests a new possibility: that a learned transformation -- denoted f(x) -- trained using a so-called *reconstruction* loss (see Eq 1), can be then translated into the desired posterior via a linear function. Surprisingly, the paper suggests that learning f(x) does not depend on the prior \Delta, so the big idea is that this representation is robust to misspecification of the prior.  Concretely, the contributions of the paper seem to be:  1) Theorem 3, which claims that there exist linear weights \theta that, when applied to the vector produced by the ideal learned transformation f(x) that minimizes the reconstruction objective (Eq 1), can exactly equal the expected value of any polynomial summary function of the doc-topic probability vector w under the posterior.  2) Theorem 4, which claims that even if f is not an exact minimizer, if it is within an additive tolerance \epsilon of the ideal loss, then there exist again weights \theta such that the squared error between the true posterior summary and the linear function has bounded error for all documents.  3) Theorem 5, which looks at the contrastive objective in Eq 2, and shows that there exists a linear function that can recover the expected polynomical summaries of random variable w under the posterior.  Experiments in Sec. 5 look at toy data, with results in Fig 2 and Table 1. Essentially, these results show that the proposed SSL method produces lower error posterior estimates (in terms of  total-variation distance from ground truth) than using Bayesian inference with the wrong prior.   Experiments in Sec. 6 look at real data (the AG news dataset), comparing the representations learned by SSL at a downstream classification task to baselines of bag-of-words, word2vec, and the contrastive method of Tosh et al. Fig. 3 suggests that the proposed method offers a few percentage points absolute gain in accuracy over word2vec, and is similar to the previous NCE method (Tosh et al) in accuracy.","This paper introduces a new method for performing inference on topic models, based on self supervised learning. Their approach is generalizable to multiple types of topic models. Specifically, the authors show that the expectation of some polynomial function of the topic posterior will be a linear function of the output of function that optimizes the self-supervised learning objective. The authors explore 2 types of self-supervised learning objectives, one based on reconstruction and one based on contrastive learning that was explored in prior work. The authors provide proofs for their main result as well a result robust to approximate minimization of the self-supervised objective. In their experiments the authors show that their approach outperforms inference with a misspecified model. The authors also run semi-supervised learning experiments on their approach. ",0.2054794520547945,0.3835616438356164,0.3013698630136986,0.3709677419354839,0.24193548387096775,0.09456264775413711,0.24193548387096775,0.06619385342789598,0.16666666666666666,0.054373522458628844,0.11363636363636363,0.30303030303030304,0.2222222222222222,0.11290322580645161,0.21463414634146344,0.09484536082474226,0.15463917525773194,0.14414414414414414
1106,SP:fec6a06582972b6f7996df9e6759f80d1f77627e,"The authors proposed a simple and straightforward method to address an obvious but critical and long-lasting issue in semi-supervised learning. They showed a number of analyses and demonstrated superior performance over state-of-the-art methods (mostly) consistently on five datasets. The proposed method is simple and easy to use without extra computation needed. Most importantly, I do see the possibilities of adapting this method widely across semi-supervised learning or self-training methods that use pseudo-labeling. I would recommend acceptance at this time.  ","This work extends the popular FixMatch framework with dynamic, class-specific thresholds. Specifically, a multiplier is designed to scale the fixed threshold for each class, which is computed based on the proportion of pseudo-labeled instances with confidence values passing the threshold—the motivation is to lower the threshold for classes that have fewer pseudo-labeled instances. The effectiveness of the proposed approach, named FlexMatch, is demonstrated on CIFAR, STL-10 and ImageNet where clear improvements over FixMatch are observed.","The paper proposes to incorporate ideas from curriculum learning into semi-supervised learning. Semi-supervised learning consists on training models with only a subset of labeled samples, and a typically larger set of unlabeled samples. The paper proposes to improve on ideas from pseudo-labeling whereas fixed thresholds are defined to decide the pseudo-labels for unlabeled samples as the training process goes. In this approach, the idea is to modulate the thresholds used during the training process as a function of time (iterations) and the individual classes being classified. The motivation is that some classes might be harder to learn. In order to deal with initial conditions, the paper proposes to use a warming up strategy whereby an additional class (undecided) is considered for samples that have not reached a confidence to be assigned to any class. The proposed method in combination with other methods based on consistency regularization such as FixMatch lead to improved accuracies.","This paper introduces a curriculum learning strategy for semi-supervised learning. Their motivation is that most existing semi-supervised learning utilizes a fixed threshold to compute pseudo-labeling loss, but the fixed threshold should be adjusted according to the state of the model. Then, they propose to adjust the threshold based on the prediction of the model and design a class-specific confidence threshold. Their threshold is designed so that the class with a larger number of confident samples has a higher value of the threshold. Thus, the classes hard to learn will have a small threshold. Their way of designing the threshold is computationally efficient and will not cause much training time increase.    The experimental results show that their method is very effective in challenging situations such as when the number of labeled samples is limited or when the target dataset is challenging. ",0.12643678160919541,0.21839080459770116,0.1839080459770115,0.275,0.2875,0.2229299363057325,0.1375,0.12101910828025478,0.1111111111111111,0.14012738853503184,0.1597222222222222,0.24305555555555555,0.1317365269461078,0.1557377049180328,0.13852813852813853,0.1856540084388186,0.20535714285714282,0.23255813953488372
1107,SP:fee6a81e4dbc9d8abc9d55d7136b6de8006ba1f8,"This paper proposed Introspective Distillation (IntroD) to achieve good OOD generalizability without sacrificing ID performance. Their training paradigm has three key components (1) factual reasoning ID teader model and counterfactual reasoning OOD model to capture ID and OOD inductive bias, (2) intersection between two inductive biases by comparing the predictions between ID teacher and OOD teader, and (3) knowledge distillation for a strong student model. They conduct experiments on visual QA and extractive QA tasks. On VQA-CP v2 and VQA v2, built upon four counterfactual teacher models (i.e., RUBi, LMH, CSS and CF-VQA0, their introD have shown large ID improvements, leading to considerably large improvements in harmonic mean of ID and OOD accuracies. They also conduct detailed ablation studies to identify the best introspection and distillation strategy. For Extractive QA experiments, they follow the experimental setting from prior work (Ko et al., 2020) where the dataset is divided into subsets based on the position of answers. IntroD has shown its effectiveness on the extractive QA experiments as well.  ","This paper studies learning models for visual and text-only question answering (VQA and SQUAD) that do well on both in-distribution test sets, as well as out-of-distribution ones.  The paper takes a knowledge distillation approach for this and designs two ""teacher"" modules: ""ID-teacher"" that captures in-domain bias, and ""OOD-teacher"" in which the in-domain bias is reduced using [23] (Niu et al 2021)'s method. The models are then ensembled and the aggregate probabilities are used to supervise training of a student model.   The paper evaluates this approach on VQA and SQUAD, with a variety of different methods as teachers. The method seems to increase performance on both OOD and ID sets (which is perhaps surprising, since one might hypothesize that the ID teacher ought to do best by itself on ID data, and likewise for the OOD teacher).","This paper propose a novel method on training VQA system that can achieve competitive performance both in-distribution and out-of-distribution. Specifically, the proposed system leverage a recent causality-based QA model to estimate the OOD distribution (via counterfactual reasoning), thus building two sub-modules to handle ID and OOD settings respectively. The two modules are then blended via knowledge distillation to train a single student model that can better handle the inductive bias. The authors perform extensive experiments on VQA and text-only question answering, with multiple baseline models and show the proposed training paradigm can consistently improve the overall performance (measured by harmonic mean over ID and OOD settings).  ========================================================================================  Thanks for the response from the authors. I've updated my rating after reading the rebuttal as well as the comments from other reviewers.","The paper claims to provide a better balance of in-domain and out-of-distribution settings. Normally, algorithms optimized for one hurt the other. The paper proposes to use a weighting mechanism to to balance a model's reliance on in-domain world (modeled as causal factual world) and out-of distribution world (modeled as causal counterfactual world). Experiments on VQA-CP and SQUAD datasets show that the proposed method does indeed help improve ID performance of several algorithms optimized of OOD accuracy while maintaining/slightly improving its OOD performance. ",0.14035087719298245,0.2046783625730994,0.0935672514619883,0.2482758620689655,0.14482758620689656,0.16911764705882354,0.16551724137931034,0.25735294117647056,0.17777777777777778,0.2647058823529412,0.23333333333333334,0.25555555555555554,0.1518987341772152,0.2280130293159609,0.12260536398467431,0.25622775800711745,0.17872340425531916,0.20353982300884957
1108,SP:ff321c62ff012f2a3c4fb02f9ba95daee33636f0,"In this work, the authors propose a new feature learning limit for infinite-width neural networks that resolves some of the computational issues with the previous $\mu$-limit. This new limit, called the $\pi$-limit, is based on projected gradient descent, and crucially allows the authors to evaluate the model on deep networks, which could not be done before. The authors show that it outperforms both finite-width and previous infinite-width networks on CIFAR10 and Omniglot.","The paper proposes an infinite-width parameterization ($\pi$-parameterization) that is similar to $\mu$-parameterization of Yang and Hu, with two important tweaks: the initialization involves a random projection of a certain form and the gradient update invokes the same projection. This results in an infinite-with description that is more amenable to computation and analysis. The paper demonstrates experimentally that this method achieves feature learning and better performance than NTK.","The authors study a certain variant of an MLP trained using a projected gradient descent-inspired update rule in the infinite width limit. This infinite width model admits closed-form rules for computing the predictor at any training iteration (as long as the V-transform or neural network kernel being known, which is the case for e.g. the arc-cosine kernel). The authors evaluate their model on Omiglot and CIFAR10 against NTK and finite width baselines.","The paper introduces an approach (named pi-limit) to compute explicitly the infinite-width limit performance of a multilayer perceptron (MLP). The approach is based on projecting the gradient of the loss at each epoch in a fixed direction, chosen upon initialization. If one does so,  the expectation value of the function learned by the network at infinite training time can be computed explicitly by the iterative procedure described in Theorem 3.5. The asymptotic accuracy on a network trained on CIFAR10 is 61.5 %, slightly (0.2 %) better than the accuracy obtained by the nu-net procedure, which does not allow estimating explicitly the limit, and almost 2 % better than the the accuracy of a Neural Tangent Kernel, which, as well known, is feature agnostic. Based on these tests, it is concluded that the pi-limit is the first procedure which allows computing exact expectation values in a MLP and that, at the same time, allows learning features.",0.18181818181818182,0.22077922077922077,0.24675324675324675,0.18309859154929578,0.2676056338028169,0.22077922077922077,0.19718309859154928,0.22077922077922077,0.11949685534591195,0.16883116883116883,0.11949685534591195,0.1069182389937107,0.1891891891891892,0.22077922077922077,0.1610169491525424,0.17567567567567566,0.16521739130434784,0.1440677966101695
1109,SP:ff608359d72b2fd9207c2c8d86282ace1d8b619b,"This paper proposes a defense method against poisoning attacks in offline RL. The authors propose a framework named COPA to certify the number of tolerable poisoned trajectories. The proposed defense includes a partitioning stage which separate the (possibly poisoned trajectories), a training stage where $u$ sub-policies are trained with $u$ partitions,  and an aggregation stage where the sub-policies are aggregated to produce the final policy. Three aggregation protocols are proposed with corresponding certification results.","This paper studies the problem of certifying a policy learned via an offline RL algorithm is robust to poisoning attacks. The authors propose two certification criteria: per-state action stability and lower bound of cumulative reward. Three different protocols are proposed, although they all based on ensembling / aggregation of sub-policy decisions. For the certification of cumulative reward, a tree-based search approach is given that evaluates all possible actions within a set. Experiments on Freeway and Breakout for a few different offline RL algorithms are given, showing Freeway can be certified to be robust to poisoning attacks for a large poisoning threshold, while Breakout cannot.  ","This paper studied how to certificate a policy when the training dataset is partially corrupted in the offline reinforcement learning scenario. Two criteria including per-state action stability certificate and cumulative reward lower bound certificate are proposed. Based on these two criteria, the paper designed the COPA, a general certificate framework, which provably achieved certain level of certificate. The COPA went through two phases during training, where in the first phase, the trajectories are split into subsets of equal size, then sub-policies are trained on each subset, and finally the sub-policies are aggregated to form a single learned policy. Experiments demonstrate the effectiveness of the proposed COPA certification framework. ","This paper proposes a certification method against poisoning attacks in offline reinforcement learning, where attackers can manipulate a subset of the training trajectories. It presents two certification criteria: per-state action stability and cumulative reward bound. It also introduces different aggregation protocols to train the policies and provides some bounds regarding the certification. In addition to theoretical results, it also has ablation studies to identify the implications of different parameters. ",0.19736842105263158,0.34210526315789475,0.27631578947368424,0.25471698113207547,0.20754716981132076,0.21621621621621623,0.14150943396226415,0.23423423423423423,0.3,0.24324324324324326,0.3142857142857143,0.34285714285714286,0.16483516483516483,0.27807486631016043,0.28767123287671237,0.2488479262672811,0.25000000000000006,0.2651933701657459
1110,SP:ff641ae83dfd806ab9770e37bd824e928c2b06a6,"This paper has the following contributions: * Propose Non-Parametric Transformers (NPT), a trained transformer applying attention across training data points to make predictions. * Demonstrate that attention across datapoints is crucial for NPTs. * Display good results of transformers on tabular datasets. * Apply a self-supervised objective to tabular data by masking some features.","This paper introduces a neural network architecture termed Non-Parametric Transformer (NPT) that uses self-attention (i) between data points and (ii) between features within each data point. The core conceptual contribution is to treat supervised deep learning problems as tasks where the model receives the entire dataset (incl. both labeled and unlabeled data points) as input — in contrast to the common approach of providing data samples independently to a deep learning model. The proposed NPT model is evaluated on tabular datasets where the authors report competitive results compared to boosted decision trees (which represent the state of the art). Applied on MNIST and CIFAR10, the NPT model performs reasonably well. The authors further perform a series of experiments to analyze the learned attention patterns and the importance of self-attention between data points for generalization performance.","This paper proposes to use self-attention between data points, in order to model the relation between the samples in the dataset. The big claim of the paper is that by doing so, they are going beyond individual input-output pairs in deep learning. While true, this is the main paradigm of entire fields like person re-identification and metric learning, so this claim is massively overstated.  The paper presents good results in tabular datasets, reaching the best results in 4 out of 10 datasets, which is very good considering that different versions of boosting dominate the field.","Deep parametric models have demonstrated tremendous success in NLP, computer vision, and many other settings. These models typically take as input an instance and output a prediction / label(s) for that instance. In this paper, the authors describe an approach that stands in contrast, the input to the model is the _entire_ dataset. Predictions are made collectively via attention between (1) data points and (2) attributes. Unlike non-parametric models that solely interpolate between instances, the proposed Non-Parametric Transformers (NPTs) learn attention mechanisms between data points and between attributes.   The authors describe how the NPT can be trained with a masking approach analogous to masked language-models as well as an approach analogous to 'standard' classification settings.   The authors provide extensive experiments that not only demonstrate the empirical effectiveness on established benchmarks tasks, but also to better understand the behavior and capabilities of each component of the NPT model. ",0.3076923076923077,0.23076923076923078,0.25,0.17518248175182483,0.1897810218978102,0.1836734693877551,0.11678832116788321,0.12244897959183673,0.08666666666666667,0.24489795918367346,0.17333333333333334,0.12,0.16931216931216933,0.16,0.12871287128712872,0.2042553191489362,0.18118466898954702,0.14516129032258066
1111,SP:ffb273a8ad8895be2fcfa2af3cb2624617304de9,"  In this paper, the authors propose a novel SSL method (i.e., LaGraph) for graph representation learning based on latent graph prediction. The overall presentation is good with sound theoretical analysis. There is also a theoretical comparison between LaGraph and various related work with different categories. Extensive experiments were conducted on various datasets for both node-level and graphs-level tasks, where the proposed method has competitive performance to a set of baselines. However, there are some details regarding the problem definitions, technical content, and experiment settings that need further clarification.","A method called LaGraph is proposed for semi-supervised graph representation learning. In particular, a new task named latent graph prediction is introduced, and the corresponding objective requiring no negativing samples is also derived. It is impressive that the proposed method requires no negative samples and works with a small batch size. ","The paper proposes a new self-supervised learning framework for learning latent representations of graphs. Like recent SSL methods (BGRL,  Barlow twins), LaGraph does not rely on negative samples but only compares the embeddings of the graph to the embeddings of an augmented version of the graph. To avoid representational collapse, a reconstruction term (similar to denoising autoencoders) is used. They provide a theoretical analysis of the method which provides some connections to other approaches.",The authors propose a self-supervised learning (SSL) framework for graph neural networks by predicting an unobserved latent graph. The proposed self-supervised learning loss is derived from upper bounds to the original objective functions for predicting latent graphs. The proposed method is robust to small sample size and small batch size.,0.18681318681318682,0.14285714285714285,0.1978021978021978,0.17307692307692307,0.28846153846153844,0.18666666666666668,0.3269230769230769,0.17333333333333334,0.34615384615384615,0.12,0.28846153846153844,0.2692307692307692,0.2377622377622378,0.1566265060240964,0.2517482517482518,0.14173228346456693,0.28846153846153844,0.2204724409448819
1112,SP:ffd382a81da7298be3a1e5fe9dd539cb4c18658b,"This paper proposes PoNet, an efficient model with linear complexity for modeling long sequences. This model replaces the self-attention layer of the Transformer model with its pooling network.   To aggregate information from different levels, their pooling network consists of three pooling strategies:  a. Global aggregation that aggregates information from all tokens in a sequence. This method is a bit like BERT CLS, and the difference is that they use average representations rather than the representation of the first token. b. Segment Max-pooling that performs max-pooling in each sentence to capture sentence-level information.  c. Local Max-pooling that performs max-pooling in each window to capture local information. Their fusion method is the addition operation.  As to the training objective, they not only use the MLM, but also use the sentence structural objective as in StructBERT.  This paper contributes to a well-defined but highly-influential problem: Efficient modeling for long sequences. The main contribution of this paper is the introduction of a multi-granularity pooling that is more efficient than self-attention and brings higher accuracy than previous efficient methods.  Empirical studies are performed to show the superiority of PoNet over previous SOTA FNet. Their method outperforms FNet in terms of accuracy in their experiment settings but slightly lags regarding time efficiency. Results are shown on the task of (1) Long-range Arena (2) GLUE by fine-tuning pre-trained networks. ","In this paper, the authors aim to resolve the quadratic time and memory complexity of the standard attention mechanism. They introduce a multi-granularity pooling and pooling fusion that captures contextual information from various levels (global, segment, and token).  They conduct experiments on long sequence tasks and large-scale pre-training and fine-tuning. As a result, their model achieves great performance with speed-up. ","This paper proposed PoNet, which is an efficient architecture to replace self-attention in Transformer-based models.  PoNet consists of three components, which are called multi-granularity pooling block.  The first pooling component is the global aggregation module, which is very similar to the additive attention in FastFormer. The different is that in FastFormer, the global query vector is computed by a weighted sum of the query vectors, while in PoNet it is the average of the query vectors. The segment max-pooling and local max-pooling modules are straight-forward, which are max-pooling operations on pre-defined segments and on local sliding windows, respectively.  Experiments were conducted on Long-range Arena (LRA) and large-scale pretraining and fine-tuning. The authors also conducted ablation experiments to analyze the contributions of these three multi-granularity pooling components.","PoNet addresses the quadratic time and memory complexity of Transformer with a new attention mechanism that has only linear complexity. Theoretical (Section 3.2) and experimental results (Table 2) bear this out. Experimental results on standard datasets like Long Range Arena and GLUE are very competitive with SOTA. Ablations were done on the 3 components of the attention mechanism, too.",0.07659574468085106,0.1702127659574468,0.06808510638297872,0.3230769230769231,0.23076923076923078,0.08695652173913043,0.27692307692307694,0.2898550724637681,0.26666666666666666,0.15217391304347827,0.25,0.2,0.12,0.21447721179624665,0.10847457627118645,0.20689655172413793,0.24000000000000002,0.12121212121212122
