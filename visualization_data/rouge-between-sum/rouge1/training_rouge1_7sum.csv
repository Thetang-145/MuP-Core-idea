,paper_id,summary,summary,summary,summary,summary,summary,summary,precision,precision,precision,precision,precision,precision,precision,precision,precision,precision,precision,precision,precision,precision,precision,precision,precision,precision,precision,precision,precision,recall,recall,recall,recall,recall,recall,recall,recall,recall,recall,recall,recall,recall,recall,recall,recall,recall,recall,recall,recall,recall,fmeasure,fmeasure,fmeasure,fmeasure,fmeasure,fmeasure,fmeasure,fmeasure,fmeasure,fmeasure,fmeasure,fmeasure,fmeasure,fmeasure,fmeasure,fmeasure,fmeasure,fmeasure,fmeasure,fmeasure,fmeasure
,,0,1,2,3,4,5,6,0-1,0-2,0-3,0-4,0-5,0-6,1-2,1-3,1-4,1-5,1-6,2-3,2-4,2-5,2-6,3-4,3-5,3-6,4-5,4-6,5-6,0-1,0-2,0-3,0-4,0-5,0-6,1-2,1-3,1-4,1-5,1-6,2-3,2-4,2-5,2-6,3-4,3-5,3-6,4-5,4-6,5-6,0-1,0-2,0-3,0-4,0-5,0-6,1-2,1-3,1-4,1-5,1-6,2-3,2-4,2-5,2-6,3-4,3-5,3-6,4-5,4-6,5-6
0,SP:2cf4a3964537ff5dd1f7b600ab567b4d0b3cc03e,"This work reports the problem of image classification datasets (CIFAR-10 and ImageNet) which contains statistical patterns present in both training and tests that can be leveraged by neural networks to achieve high accuracy, but would not be discerned as salient features by humans. Using Sufficient Input Subsets (SIS), they show that retaining the smallest SIS to keep a confidence of 99% leads to spare sets of about 5% of the original pixels and that these subsets of pixels are not salient features for humans. Most importantly they show that training NNs on these SIS from a previously trained network achieves similar results.","The paper evaluates pathologies of modern neural networks on CIFAR-10 and ImageNet. Specifically, the authors show that these networks exhibit *overinterpretation*, which is a phenomenon where a model can use sparse subsets of an image which are semantically meaningless to humans in order to confidently and accurately classify the image. The authors utilize the Sufficient Input Subsets (SIS) algorithm from the interpretability literature to identify these subsets, but they also propose modifications to this algorithm which allow it to scale to ImageNet. Surprisingly, the authors find that models evaluated on a small subset of SIS pixels from each image (5% on CIFAR-10 and 10% on ImageNet) exhibit only a small drop in accuracy. The authors explore two techniques for mitigating overinterpretation: ensembling and input dropout.","The paper identifies sparse sets of pixels which can be used with deep neural nets to classify images with high confidence. The authors demonstrate results with CIFAR-10, an augmentation corrupted version of it and with ImageNet. They show a number of experiments to obtain insights in why these spurious subsets are classified with high confidence.  For example, they show that the sparse subsets can be used to train a different model to predict well on CIFAR-10 and on Imagenet. Furthermore they show that the SIS sizes are larger on correctly classified images compared to misclassified images, which implies that the sets are indeed used to properly predict on images.","The paper presents an interesting finding that CNNs overinterpret a few non-meaningful pixels in the image to make accurate predictions. The authors use SIS and extend it to Batched Gradient SIS to discover the limited number of pixels such CNNs rely on. The authors conduct extensive experiments on two major image datasets to verify the finding. Besides, ensembling and input dropout are explored to mitigate the overinterpretation problem.","Authors define ""overinterpretation"" as an undesired behavior in deep neural networks where the networks find strong class evidence in semantically unrelated image regions. The behaviour is quantified by identifying the minimal number of pixels sufficient to predict a given class in an input image while masking all the other features/pixels. In this regard, protocols are proposed to evaluate overinterpretation and the models trained on CIFAR and ImageNet are shown to suffer from this problem. ","This paper proposes ""overinterpretation"" which describes the phenomenon that CNNs could achieve high test accuracy while replying on features that lack semantic meaning. To demonstrate overinterpretation on CIFAR-10 and ImageNet, the authors use Batched Gradient SIS to select a small subset of pixels for each image and trained CNNs on the modified images. While humans can not make accurate predictions on those modified images, CNNs can still achieve high test accuracy. Lastly, the authors propose to use ensembling and input dropout to address overinterpretation. ","The work utilizes the SIS (a local feature-importance method) to empirically prove that on existing benchmark datasets, the trained convnets are capable of making decisions based on a very small subset of pixels that are meaningless to the human observer but are nonetheless strong signals. Interestingly, unlike common belief, the same phenomenon is observed in adversarially trained models. The main problem with the work is the discrepancy between claims and results. ",0.4368932038834951,0.39805825242718446,0.21359223300970873,0.2815533980582524,0.3106796116504854,0.22330097087378642,0.4015748031496063,0.29133858267716534,0.29133858267716534,0.3543307086614173,0.23622047244094488,0.24324324324324326,0.22522522522522523,0.3063063063063063,0.21621621621621623,0.36231884057971014,0.4782608695652174,0.30434782608695654,0.32,0.26666666666666666,0.2235294117647059,0.3543307086614173,0.36936936936936937,0.3188405797101449,0.38666666666666666,0.3764705882352941,0.3194444444444444,0.4594594594594595,0.5362318840579711,0.49333333333333335,0.5294117647058824,0.4166666666666667,0.391304347826087,0.3333333333333333,0.4,0.3333333333333333,0.3333333333333333,0.38823529411764707,0.2916666666666667,0.2823529411764706,0.2777777777777778,0.2638888888888889,0.3913043478260869,0.38317757009345793,0.2558139534883721,0.3258426966292135,0.3404255319148936,0.2628571428571429,0.4285714285714286,0.37755102040816324,0.36633663366336633,0.42452830188679247,0.30150753768844224,0.30000000000000004,0.2688172043010753,0.3469387755102041,0.26229508196721313,0.3472222222222222,0.42857142857142855,0.2978723404255319,0.30000000000000004,0.27210884353741494,0.24203821656050956
1,SP:b622788bec805621c2abf11ffa25c0d55e50f4d3,"The paper performs a detailed hyperparameter study of stochastic optimizers specifically designed for large-batch training (LARS and LAMB). With appropriate hyperparameter tuning, simpler optimizers such as Nesterov SGD and Adam match and sometimes outperform those specialized optimizers on ResNet-50 and BERT. The authors attribute the relative success of large-batch methods to minute undocumented aspects in their implementations, such as regularization and scaling on certain operators, which can be back-ported to the other optimizers, and advocate for more relaxed definitions (e.g. number of steps) for benchmarks such as MLPerf Training.","This paper was previously rejected by NeurIPS and ICML. I was one of the NeurIPS reviewers for this paper. Although the authors made some changes, after carefully reading this paper, I found it did not change too much for key techniques and key contributions. So I decided to use my previous NeurIPS review.  ###############################################################################################################  The authors used a huge amount of computing resources to tune hyperparameters of Adam/SGD and claimed that they can match the performance of LARS/LAMB for large-batch training. I think the comparison is not fair.","In data-parallel distributed training, increasing the batch size of the optimizer's updates is the most natural way to reduce wall-clock training time. Prevalent first-order stochastic methods such as SGD, provide a linear speed-up in the batch size, but only up to some critical batch size (c.f., [smooth convex optimization bounds](https://arxiv.org/abs/1106.4574), and [an empirical work suggesting the same](https://arxiv.org/abs/1811.03600)). This critical batch size is typically very different for different learning architectures and optimization algorithms, and it is often difficult to decouple the effect of, * the bias-variance terms in optimization,  * improper or insufficient hyper-parameter tuning, and  * implicit regularization of the optimizer for extremely over-parameterized learning problems.   As a result, many existing works that propose new optimization algorithms, miss important baselines or don't compare against them fairly. This paper highlights such an issue with the recently proposed layerwise normalization techniques [LARS](https://arxiv.org/abs/1811.03600) and [LAMB](https://arxiv.org/abs/1904.00962), which build upon the update rules for SGD w/ Polyak momentum and Adam respectively. These methods were proposed to speed up the large batch (pre-)training of Imagenet and BERT respectively, and have gained a lot of attention in benchmark competitions. This paper underlines that these techniques improve optimization either marginally or not at all when compared to their vanilla first-order counterparts. Moreover, it highlights nuances in their hyper-parameter choices which are very important to consider. Most importantly, it establishes simpler baselines for improving future optimization algorithms for the considered learning tasks.  ","This paper revisits the effectiveness of the optimizers designed for large-batch training such as LAMB and LARS by You et al. (2017, 2019) respectively. While it has been claimed and demonstrated (in perhaps limited settings though) that such optimizers can achieve better performance compared to other generic optimizers such as SGD or ADAM (in the sense that they don’t require a specific batch size), this paper re-evaluates these optimizers while fine-tuning all hyperparameters involved to potentially affect the result and finds that they do not work better as claimed; or, more precisely the standard optimization algorithms including Nesterov and Adam can match or outperform LARS as long as they are properly tuned. The paper provides empirical evidence obtained from Imagenet and BERT experiments to support their finding.","I carefully read the responses from the authors. However, the authors did not address my concern. I'd like to keep my score.  ###########################################################################################################  The authors used a huge amount of computing resources to tune hyperparameters of Adam/SGD and claimed that they can match the performance of LARS/LAMB for large-batch training. I think the comparison is not fair.","In this work, the authors compared the standard optimizers (i.e., Nesterov momentum, Adam) and optimizers with layer-wise normalization (i.e., LARS, LAMB) in training neural networks with large-batch sizes. Although LARS and LAMB were proposed and known as ""large-batch optimizers,"" the authors showed that the standard optimizers with careful hyperparameter tuning could match or exceed the state-of-the-art results by LARS (for training ResNet-50 on ImageNet, batch size = 32K) and LAMB (for pre-training BEAT, batch size = 65K) at large-batch settings within the same step budgets. The authors pointed out that the strong results depend on several optimization/regularization tricks, non-default values of uncommonly-tuned hyperparameters, and a careful learning rate schedule, but such details are not often discussed. With this point, the difficulties of comparing optimizers and the importance of rigorous empirical comparisons and open-sourcing in deep learning optimizer research are highlighted. ","The authors detail the significant effort required to reproduce results from the original LARS and LAMB papers.  Having reproduced results for LARS (for ResNet-50 training on ImageNet) and LAMB (for BERT pretraining), the authors then show that standard/older optimizers (Nesterov momentum and Adam) can match or even exceed LARS and LAMB at large batch sizes.  The authors provide in-depth details on both the difficulties required for them to reproduce the original results (i.e., fixing discrepancies between details in the publications and available source-code from online implementations) and their hyperparameter optimization for Nesterov Momentum and Adam on the datasets evaluated.  Also, insights and lessons are provided, both on the importance of hyperparameter tuning for comparison of DL solvers and on practitioners attempting to demonstrate performance gains for a new method (are not merely due to better tuning or regularization tricks).",0.2872340425531915,0.4574468085106383,0.46808510638297873,0.23404255319148937,0.46808510638297873,0.44680851063829785,0.4,0.3888888888888889,0.5222222222222223,0.36666666666666664,0.34444444444444444,0.2209737827715356,0.0898876404494382,0.26591760299625467,0.20224719101123595,0.21374045801526717,0.3893129770992366,0.3435114503816794,0.45,0.5,0.43790849673202614,0.3,0.16104868913857678,0.33587786259541985,0.36666666666666664,0.2875816993464052,0.2916666666666667,0.1348314606741573,0.26717557251908397,0.7833333333333333,0.21568627450980393,0.2152777777777778,0.45038167938931295,0.4,0.46405228758169936,0.375,0.4666666666666667,0.3333333333333333,0.3125,0.17647058823529413,0.20833333333333334,0.4652777777777778,0.29347826086956524,0.23822714681440446,0.3911111111111111,0.2857142857142857,0.3562753036437247,0.35294117647058826,0.20168067226890757,0.3167420814479638,0.6266666666666667,0.271604938271605,0.26495726495726496,0.2964824120603015,0.14678899082568808,0.3380952380952381,0.2627737226277372,0.2931937172774869,0.35915492957746475,0.32727272727272727,0.25352112676056343,0.29411764705882354,0.4511784511784512
2,SP:ddc796b9185d372f4d0829f436bbca50c3990867,"This paper introduces a Jax package for implicitly differentiating various numerical solvers. Concretely, the authors develop a systemic methodology for producing gradients for a variety of optimization problems. Then, the authors prove that the Jacobian solution to the approximate numerical solution produces close enough gradients. Finally, the authors show the power of their framework on four test tasks.","The paper proposes a modular and efficient framework along with its JAX implementation for the implicit differentiation of optimization problems. The user defines the function F capturing the optimality conditions of the problem to be differentiated; then the framework combines implicit differentiation and autodiff of F to automatically differentiate the optimization problem. The proposed framework is labeled as efficient, since it doesn’t have to unroll the computational graph like in autodiff, and modular since it doesn’t require case-by-case mathematical derivation like in implicit differentiation. The authors show that existing implicit differentiation methods can be instantiated in their framework. They provide and empirically validate new bounds on the Jacobian error when the optimization problem is only solved approximately. The authors implemented four illustrative applications of their framework ( Hyperparameter Optimization Of Multiclass SVM; Dataset Distillation; Task-Driven Dictionary Learning; Sensitivity Analysis Of Molecular Dynamics). Code and implementation in JAX are provided along with the paper. ","This paper presents a module for implicit differentiation in JAX, which allows efficient differentiation through optimization and root-finding problems. The paper presents the general principle of combining the implicit function theorem with autodiff of optimality conditions, and discusses how this can work for a implicit functions such as a range of constrained optimization methods. Finally there are example applications to three machine learning tasks and one molecular dynamics simulation task where this framework improves on currently used methods.","The paper promises extension of Google’s JAX library by automatic and modular differentitation of implicitly defined functions. Given a solver $\theta\mapsto x^*(\theta)$ one asks for its Jacobian $\partial x^*(\theta)$. In cases when $x^*$ does not have an explicit form that can be automatically differentiated, the Jacobian can be computed from the optimality conditions $F$ having the general form $F(x^*(\theta),\theta)=0$. Jacobian $\partial x^*(\theta)$ is then given by a solution of a linear system involving $\partial_1 F$ and $\partial_2 F$. The authors describe a unified implementation in JAX and show the resulting simple interface for end-users. Various use cases are presented on examples and code-snippets figures, including stationary point conditions, KKT conditions, proximal gradient, projective gradient and mirror descent fixed point and more. On the theoretical side, the authors discuss the case when the optimization problem solution is only approximate and derive Jacobian error bounds under certain assumptions on optimality conditions $F$.","This paper provides a unified tool for combining the implicit differentiation technique and the  automatic differentiation method widely used in existing deep learning packages such as PyTorch and TensorFlow. The proposed implementation is easy to use for numeric optimization such as bilevel optimization, meta-learning and hyperparameter optimization because it covers many existing schemes such as fixed point, KKT point, projected method. In the experiments, the authors illustrate how their tool can be useful to simply the implementation. ","A good paper considers a critical problem, but contributions are somewhat weak.  This paper aims at a unified approach for implicit differentiation of optimization problems. The experiments results seem good, but the proposed methods lack novelty.","The authors propose a unified modular framework for implicit differentiation, which removes the need to manually derive the jacobians for each individual case by leveraging automatic differentiation of the optimality conditions. The proposed framework is shown to encompass various existing implicit differentiation schemes (such as stationary point- or KKT conditions) while also creating new ones (e.g. mirror descent fixed point). To handle the practical case in which the optimization problem can only be solved approximately, a theoretically justified bound on the error of the jacobian estimate is provided, showing that the error on the estimated jacobian is at most of the order of the deviation of the approximate solution from the optimal solution. Additionally, a JAX implementation of the proposed framework is described, with the promise of an open-source JAX library for implicit differentiation upon acceptance. Finally, four applications of the framework are demonstrated, highlighting its flexibility and ease of use.",0.43103448275862066,0.3448275862068966,0.4827586206896552,0.25862068965517243,0.1896551724137931,0.4827586206896552,0.2611464968152866,0.3630573248407643,0.2229299363057325,0.09554140127388536,0.4585987261146497,0.3291139240506329,0.31645569620253167,0.20253164556962025,0.4177215189873418,0.17177914110429449,0.0736196319018405,0.38650306748466257,0.15384615384615385,0.4358974358974359,0.4166666666666667,0.1592356687898089,0.25316455696202533,0.17177914110429449,0.19230769230769232,0.3055555555555556,0.1830065359477124,0.5189873417721519,0.3496932515337423,0.44871794871794873,0.4166666666666667,0.47058823529411764,0.15950920245398773,0.32051282051282054,0.4444444444444444,0.21568627450980393,0.358974358974359,0.3333333333333333,0.4117647058823529,0.3333333333333333,0.2222222222222222,0.09803921568627451,0.23255813953488372,0.291970802919708,0.2533936651583711,0.22058823529411764,0.23404255319148937,0.26540284360189575,0.3474576271186441,0.35625,0.2978723404255319,0.15544041450777202,0.46451612903225803,0.21487603305785125,0.3184713375796178,0.2782608695652174,0.28448275862068967,0.2323651452282158,0.1206030150753769,0.39873417721518983,0.21052631578947367,0.29437229437229434,0.15873015873015872
3,SP:f202f3d6780876a0bdd7d7bd4d7047719a145177,"The authors propose a novel skill discovery method that combines standard mutual information maximization with notable changes. The first is that skill now enter a diffusion stage (random actions) before being predicted by a reverse predictor. The second is that environmental resets are exploited in order to construct a skill tree, wherein new skills are executed only after their parent skills. The final change is a pruning/growth strategy similar to VALOR, which is critical for constructing the skill tree.  The authors evaluate their method by comparing it to traditional skill discovery methods in terms of state coverage and performance on downstream tasks. ","This method learns a space of intrinsic goals to maximize coverage of the state space. This work utilizes mutual information between skills and states, where the entropy of states and negative entropy of states given options describe the diffusing and directing objectives respectively. This is done by learning skills with two separate parts, a long scale directed component and a short scale diffusing component. These skills have state distributions that differ by at least a specified factor. It then separates these skills along a hierarchical tree structure, where temporal abstraction occurs by calling lower level options, which have a limited range, controlling the shape of the tree by pruning skills with low difference, and adding new skills when all the skills satisfy the condition. ","The paper presents a framework for an unsupervised learning of robotic skills that balances directness of exploration and coverage of state space. The theory of the method builds on the notion of mutual information between states and skills, where the conditional entropy between states and skill definitions corresponds to the directed part of the learned policy, and entropy of the states to the diffusing part of the policy. The directed part is optimized to increase discriminability of the skills given the states using a discriminator network. The diffusing part performs a random walk policy with uniform distribution over actions.The paper further proposes to optimize the number of skills by requiring the minimum discriminability of skills and discarding skills that fall below the discriminability threshold. Finally, in order to be able to reach states that are farther away, the paper proposes to combine skills in a tree structure and gradually expand the tree by adding new skill nodes to the tree. The approach is shown to outperform other unsupervised skill discovery methods on such tasks as continuous maze, CartPole and Reacher in OpenAI Gym during both, unsupervised skill discovery phase and training of downstream tasks.","The paper proposes an unsupervised exploration method for reinforcement learning, called UPSIDE, that combines learning of directed skills that enable covering distant states, and a diffusive part that explores locally and helps expand the explored region further. The main contributions are both the topology of the policy (division to tree-structured skills and diffusive parts), a theory for training such policies, and a practical implementation that simplifies some of the technicalities induced by the theory. The experiments illustrate well the assumed benefits and include both toy tasks (a point mass in a maze) as well as more complex tasks such as HalfCheetah and Ant from OpenAI Gym. The experiments also compare to prior methods such as DIYAN and provide ablations of the importance of the different components. ","To increase the state space coverage with unsupervised skill discovery, the authors propose UPSIDE. They tackle reaching distant states (direct) and covering neighbor states (diffuse) at different stages. Also, they discover skills by forming a tree, which enables chaining of smaller skills to form far-reaching skills. Also, instead of fixing the number of skills to learn, they maximize the number of skills with a constraint on the output of the discriminator $q_\phi(z|s_{diff})$ and show that it is a lower bound of the mutual information objective. Empirical results on Maze and MuJoCo environments and further analyses are presented.","The paper proposes a novel algorithm for learning unsupervised skills based on empowerment. Specifically, direct-and-diffuse policies are proposed that first optimize empowerment and then after a fixed number of episode steps optimize action entropy. This yields policies that go to a certain point in a directed manner and then move around that point at random. Further, the UPSIDE algorithm is based on tracking the policies with high discriminability (i.e. low H(z|x)). The algorithm keeps a set of policies that are currently above a threshold and attempts to add new policies incrementally. Finally, each new policy can be assigned a parent policy, wherein the parent policy is executed to produce the starting state for the new policy. This allows the proposed algorithm to stack policies sequentially and reach goals further away. Empirically, the algorithm achieves good performance on pointmass, cheetah, walker, and ant tasks.","The idea of this work is to maximize coverage while ensuring learned skills are distinguishable. The learning framework is based on maximizing mutual information (MI) between latent random variables and states.  This paper proposes a decoupled objective for local coverage and directness, on top of it, a tree structure based exploration method is proposed to incrementally compose learned skills for maximum coverage.",0.3106796116504854,0.4854368932038835,0.2815533980582524,0.32038834951456313,0.3786407766990291,0.22330097087378642,0.4596774193548387,0.2661290322580645,0.3467741935483871,0.2903225806451613,0.20967741935483872,0.3282051282051282,0.26153846153846155,0.30256410256410254,0.17435897435897435,0.28346456692913385,0.3543307086614173,0.1732283464566929,0.3431372549019608,0.23529411764705882,0.1891891891891892,0.25806451612903225,0.2564102564102564,0.2283464566929134,0.3235294117647059,0.2635135135135135,0.3709677419354839,0.2923076923076923,0.25984251968503935,0.4215686274509804,0.24324324324324326,0.41935483870967744,0.5039370078740157,0.5,0.39864864864864863,0.5483870967741935,0.35294117647058826,0.30405405405405406,0.3548387096774194,0.23648648648648649,0.3870967741935484,0.45161290322580644,0.2819383259911894,0.3355704697986577,0.25217391304347825,0.32195121951219513,0.3107569721115538,0.2787878787878788,0.35736677115987464,0.26294820717131473,0.38053097345132747,0.2647058823529412,0.2795698924731183,0.3975155279503106,0.3434343434343435,0.34402332361516036,0.2645914396887159,0.314410480349345,0.3272727272727273,0.2328042328042328,0.28,0.29268292682926833,0.26666666666666666
