,paper_id,summary,summary,summary,summary,summary,summary,precision,precision,precision,precision,precision,precision,precision,precision,precision,precision,precision,precision,precision,precision,precision,recall,recall,recall,recall,recall,recall,recall,recall,recall,recall,recall,recall,recall,recall,recall,fmeasure,fmeasure,fmeasure,fmeasure,fmeasure,fmeasure,fmeasure,fmeasure,fmeasure,fmeasure,fmeasure,fmeasure,fmeasure,fmeasure,fmeasure
,,0,1,2,3,4,5,0-1,0-2,0-3,0-4,0-5,1-2,1-3,1-4,1-5,2-3,2-4,2-5,3-4,3-5,4-5,0-1,0-2,0-3,0-4,0-5,1-2,1-3,1-4,1-5,2-3,2-4,2-5,3-4,3-5,4-5,0-1,0-2,0-3,0-4,0-5,1-2,1-3,1-4,1-5,2-3,2-4,2-5,3-4,3-5,4-5
0,SP:0a1ecec6f5447992f3ad0011e1b7efa39da28442,"This paper analyses the training of neural networks from a topological perspective, presenting a pipeline that can measure (pseudo) distances between the network's weights during training. Such information is then employed to study the generalisation error of a neural network.  In contrast to existing methods for estimating this error, this paper does *not* require a specific hold-out data set, as topological features of the neural network are monitored during training. This frees up additional data for fitting, which can be highly relevant in the sparse data regime.","This paper presents some empirical observations about the relationship of a persistent homology-based measure of learning dynamics and validation set error, during the training of deep neural nets. The paper opens with an introduction on persistent homology, then introduces an approach to study the structure of deep nets using topological data analysis (TDA) tools. Three case studies are then presented, for which a measure of change in topological structure of the network during training is compared with the validation set error. The argument of the paper is that the two measures are correlated, and therefore it may be possible to use the topological measure (which depends on the structure of the network only, and not on data) in place of the validation set error to assess the generalization performance of a deep net.","This paper proposes a topological data analysis approach to investigate the inner workings of a deep network. In particular, the generalization performance of a deep network is measured without a holdout set by analyzing the topological change of the network during training. Multiple experiments conducted over various datasets demonstrate that the method performs reasonably for various learning rates, batch sizes, and dropout probabilities. ","This paper proposes a method to evaluate the generalization of a treined neural network using persistent homology. They measure the distance between Persistent Diagrams corresponding to Neural Networks in the training process, and show that the accumulation correlates with the Validation Curve. They also discuss which method(difinition) is appropriate for the distance between PDs.",Summary: The paper proposes to monitor neural networks during training by considering a momentary state of a network (MLP or CNN) as an abstract simplicial complex as defined by considering the network as a weighted directed graph. Then persistent homology features are computed resulting in persistence diagrams which are vectorized to compute distances between subsequent network states during training. The paper proposes to monitor this measure they call homological convergence to identify training stages where generalization error is low -- without the need to estimate performance by means of a validation set.,"This paper analyses the training of neural networks from a topological perspective, presenting a pipeline that can measure (pseudo) distances between the network's weights during training. Such information is then employed to study the generalisation error of a neural network.  In contrast to existing methods for estimating this error, this paper does *not* require a specific hold-out data set, as topological features of the neural network are monitored during training. This frees up additional data for fitting, which can be highly relevant in the sparse data regime. ",0.10227272727272728,0.07954545454545454,0.06818181818181818,0.06818181818181818,1.0,0.12030075187969924,0.06015037593984962,0.06015037593984962,0.06766917293233082,0.0967741935483871,0.08064516129032258,0.11290322580645161,0.07407407407407407,0.1111111111111111,0.06666666666666667,0.06766917293233082,0.11290322580645161,0.1111111111111111,0.06666666666666667,1.0,0.25806451612903225,0.14814814814814814,0.08888888888888889,0.10227272727272728,0.1111111111111111,0.05555555555555555,0.07954545454545454,0.044444444444444446,0.06818181818181818,0.06818181818181818,0.08144796380090497,0.09333333333333332,0.08450704225352113,0.06741573033707866,1.0,0.1641025641025641,0.0855614973262032,0.07174887892376682,0.08144796380090497,0.10344827586206896,0.06578947368421052,0.09333333333333332,0.05555555555555555,0.08450704225352113,0.06741573033707866
1,SP:190e69f0d6538fa7e87d77df6cdf93c7a1d83f90,"This paper address the problem of neural network single-shot structured-based model pruning.   Deep convolutional neural networks grow to achieve higher performance, which also means slower inference and higher computational cost. Model pruning can help with that. Model pruning can be unstructured, which means to remove individual weights, or structured, which means to remove entire substructures, e.g., nodes or channels. This paper focuses on structured pruning.  A key challenge for global saliency-based structured model pruning is to find a good objective that can be efficiently calculated to make the approach scalable to various modern convolutional neural networks. Existing saliency-based pruning methods such as OBD, C-OBD evaluate the effect of removing a single weight or structure on the loss of the neural network in isolation. This work is to devise a 2nd order pruning method that considers all global correlations for structured sensitivity pruning.  The basic idea in this work is to search for the pruning mask M to minimize the joint effect on the network loss approximately. The mathematical approximation focuses on the second-order approximation to the loss.   The key contributions of this work are the proposed 2nd-order structured pruning (SOSP). There are two variants of that. The first one is based on fast hessian-vector products, and it has the same complexity as that with first-order methods. A second one is based on the Gaussian-newton approximation. The first one with fast hessian-vector products do better in terms of scale.  Experiments on VGG, ResNets, PlainNet, DenseNet are shown promising results across various image classification datasets, including Cifar10/100 and ImageNet. ","This paper presents two novel approximation techniques for approximating the importance of a filter (the estimated loss difference between pruned and pre-trained networks) using second order approximations. The main difficulties of second order approximation lie in the computational burden of computing the full Hessian matrix. To circumvent this issue, this paper presents two novel ways to do so. The first approach, dubbed SOSP-I, linearize the neural network and only consider the coupling of the loss function. The second approach, dubbed SOSP-H, approximate the loss difference using a Hessian-vector product. Both approaches have much better efficiency compared to using the Hessian and empirically perform a little better than existing approaches. Overall, the approximations seem novel to me and I find the appendix helpful in understanding the derivations. However, their empirical advantages seem rather limited. Additionally, I think some of the related papers are missed and they need to be discussed and compared against.","This work proposes a structured pruning method that removes feature maps (channels) of convolutional neural networks without losing accuracy while increasing computational efficiency (MACs). The proposed method (SOSP-I/H) is based on a sensitivity based saliency and involves what they call global correlations through the second order partial derivatives with respect to different structures. SOSP shares a lot in common with previous works including OBD and C-OBD for the use of second order information except that they approximately incorporate all elements in the Hessian matrix. Then the paper evaluates the effectiveness of the proposed method for image classification task using VGG, ResNet, and Desnet on CIFAR and ImageNet datasets and achieve accuracies on par with previous works on both global and local (layerwise) pruning. The paper further presents that SOSP works on pruning at initialization and can improve further by performing expand-pruning based on skewed distribution of retained parameters over layers. ",This paper proposed a structured pruning method which include correlations among structures by second order information. The author also improved the computational efficiency by fast Hessian-vector products. The method is demonstrated to achieve comparable/better results than previous methods. And it can also be applied to pruning-at-initialization and reveal architecture bottlenecks.,"This paper presents saliency-based second-order structured pruning methods, namely SOSP-I and SOSP-H. The proposed methods are designed to capture the correlations among all structures and layers, known as second-order structure (Hessian). In particular, SOSP-I employs Hessian approximation while SOSP-H employs exact Hessian. Overall, the idea of this paper is very clear, and I kinda like the discussion part of it. The expand-pruning is also interesting. ","This paper proposes a method to prune deep neural networks. The aim is to reduce the computational cost and inference time while maximally maintaining the classification performance. The motivation of this work is to consider second-order structured pruning (SOSP), which considers the correlation information among the structures and layers when conducting network pruning. The key part of this work is the development of a method called SOSP-H that can have better scalability while considering the second-order correlation information for pruning. Experimental study is conducted to compare the proposed SOSP-H with its variant and other existing related methods, demonstrating its effectiveness.",0.04460966542750929,0.07063197026022305,0.03345724907063197,0.03717472118959108,0.07063197026022305,0.05806451612903226,0.025806451612903226,0.05806451612903226,0.04516129032258064,0.058823529411764705,0.0392156862745098,0.058823529411764705,0.07547169811320754,0.07547169811320754,0.16666666666666666,0.07741935483870968,0.12418300653594772,0.16981132075471697,0.1388888888888889,0.18446601941747573,0.058823529411764705,0.07547169811320754,0.125,0.06796116504854369,0.16981132075471697,0.08333333333333333,0.08737864077669903,0.05555555555555555,0.038834951456310676,0.11650485436893204,0.056603773584905655,0.09004739336492891,0.05590062111801242,0.05865102639296188,0.1021505376344086,0.05844155844155845,0.038461538461538464,0.07929515418502203,0.05426356589147287,0.08737864077669903,0.05333333333333333,0.0703125,0.06399999999999999,0.05128205128205127,0.13714285714285712
2,SP:38b5e8808025e698744d9544e91d334ee6566fa2,"The paper study the similarities between the augmentations/augmentation schemes with the corruptions and reason that the perceptual similarity between them is a strong predictor for the robustness improvements. The authors propose a new metric, Minimal Sample Distance (MSD), to evaluate the perceptual similarity between a corruption and an augmentation statistically. In addition, authors construct a new corruption benchmark ImageNet-C bar that is perceptually dissimilar to ImageNet-C. Results on this benchmark show that recent robust models with different augmentations schemes have reduced performance than ImageNet-C, suggesting that these augmentation schemes share perceptual similarity to ImageNet-C.","This paper proposes ImageNet-\bar{C} which uses a smaller number of carefully chosen corruptions, compared to ImageNet-C. The authors try to argue that previous work is overfitting to ImageNet-C. They claim ""overfitting indeed occurs."" Additionally, they propose ""Minimum Sample Distance,"" showing that they can predict generalization performance using feature embedding distances.","The paper introduces the Minimal Sample Distance (MSD): a measure of the minimal distance, in a trained network representation space, between samples modified with an augmentation and the average of all samples modified by a corruption. It uses this metric to claim that there exists a high correlation between the corruption error and the MSD of a given augmentation. This way, it claims that focusing on benchmarks like ImageNet-C may lead to overfitting to the corruptions present in that benchmark. ","The paper studies the importance of similarity between augmentations and corruptions for improving performance on those corruptions. To measure the distance between the augmentation and corruption distributions, the paper proposes a new metric, Minimal Sample Distance (MSD), which is the perceptual similarity between an average corruption and the closest augmentation from a finite set of samples sampled from the augmented data distribution. It is shown that MSD overcomes the drawbacks of distributional distance measures like Maximum Mean Discrepancy (MMD). A new benchmark, called ImageNet-C-bar, made up of corruptions that are perceptually dissimilar to ImageNet-C, is introduced. Using standard evaluation, it is empirically shown that several recent augmentation schemes show degraded performance on the new dataset, suggesting that they generate augmentations only perceptually similar to ImageNet-C and thus are prone to overfitting. ","In this paper, the authors propose an exciting perspective to rethink the interaction between augmentation and corruption. The perceptually similar augmented training images could be helpful for better performance on common natural corrupted test-time images. The creation of CIFAR/ImageNet-$\overline{C}$ and the related discussion (section 6) indicates the future direction of data augmentation for corruption robustness with enough care in evaluation protocol.","The paper introduces a metric, the Minimal Sample Distance (MDS), that quantifies the distance between the distribution of samples augmented with a certain augmentation and samples affected by a certain corruption. The authors show a correlation between the MDS and the corruption error, and use this metric to show that existing corruption benchmarks, e.g. ImageNet-C, are not broad enough to properly evaluate the robustness of image classifiers. Thus, they expand ImageNet-C with further corruptions that are not perceptually similar to those already available.",0.061224489795918366,0.09183673469387756,0.24489795918367346,0.02040816326530612,0.10204081632653061,0.05660377358490566,0.1320754716981132,0.03773584905660377,0.07547169811320754,0.1125,0.025,0.2,0.04477611940298507,0.11194029850746269,0.078125,0.11320754716981132,0.1125,0.1791044776119403,0.03125,0.11764705882352941,0.0375,0.05223880597014925,0.03125,0.047058823529411764,0.06716417910447761,0.03125,0.18823529411764706,0.09375,0.17647058823529413,0.058823529411764705,0.07947019867549669,0.10112359550561797,0.2068965517241379,0.02469135802469136,0.10928961748633881,0.04511278195488722,0.0748663101604278,0.03418803418803419,0.05797101449275362,0.08411214953271028,0.02777777777777778,0.19393939393939397,0.06060606060606061,0.13698630136986303,0.06711409395973154
3,SP:4f12fdfd15f31f1a0ff9b57a8de53319750c3eec,"Online imitation learning algorithm based on neural density estimation is proposed. The objective of this paper is to maximize $-D_{KL}$ in Eq. (2), and the authors consider maximizing the lower bound of Eq. (2) in Corollary 1 as a surrogate learning objective. The aforementioned neural density model is used to estimate the expert state-action visitation distribution (or occupancy measure) which works as a component of a reward function (the log of the neural density model) during the RL phase, together with the entropy regularization ($r_\pi$) and a critic-based reward ($r_f$). While Adversarial Imitation Learning requires the alternate optimization for policy and reward estimators, the proposed idea does not require the reward learning phase (similar to RED, Disagreement regularized IL and SQIL), which may stabilize the learning procedure. While the proposed idea is straightforward to be understood, the empirical results show that NDI outperforms its baselines especially when the number of expert demonstrations is small. The ablation studies on learned density and varying MI reward weight are also interesting to follow. ","The paper proposes an non-adversarial imitation learning algorithm based on density estimation of expert demonstrations. The proposed method derives a lower bound on the reverse KL divergence ($KL(\pi | \pi_{E})$), which is translated into a composite reward function consisting of 3 addictive terms. Empirical results show improved performance in Mujoco benchmarks. ","This work proposes a novel density matching method for learning from demonstration, which achieves state-of-the-art demonstration efficiency. Prior density matching methods utilize the adversarial methods suffers from the instability of optimization. To overcome this issue, this work proposes to separate the imitation process into expert density estimation phase and density matching phase, where a model-free formulation is derived and provably served as the lower bound of reverse KL divergence between $\pi_\theta$ and expert policy $\pi_E$.","This paper introduces an approach for imitation learning based on density estimation. The approach uses the previously introduced idea of minimizing some divergence between policy and expert occupancy measures, state-action distributions induced by these policies. The authors propose to first estimate expert occupancy measure either using an autoregressive density model or an energy based model. Then authors use the Donsker-Varadhan KL representation to compute a log ratio between $p(s_{t+1}|s_t)$ and $p(s_t)$ where $s_{t}$ is a state at time $t$. Then, the expert occupancy and the KL representation are used as RL reward for imitation learning. ","The authors propose a new imitation learning algorithm that attempts to (indirectly) minimize the KL between the imitation learner distribution and the expert distribution. The main novelty consists in the fact that the objective includes the entropy over the state-action space (as opposed to the average entropy of the action distributions given the state). In order to deal with this entropy term, the authors use a learned approximation to the state-action occupancy distribution.","This paper presents an imitation learning approach based on density estimation of the state-action pairs. The underlying idea is to take expert demonstrations and find a distribution over the inherent state occupancy and then follow-up with a reinforcement learning step. As a result, the learner visits the high density state action pairs while also exploring new states during training. The paper makes a number of interesting contributions, among a new cost function for distribution matching IL and a proof showing that it is equivalent to maximizing a non-adversarial model-free RL objective. In contrast to other methods, like AIL, no adversarial optimization is needed; which typically would have resulted in instabilities. Instead the new objective pushes the lower bound in a single direction. The paper also features a list of tradeoffs between different distribution matching IL methods. Experiments on Mujoco tasks show state of the art performance.   ",0.07471264367816093,0.028735632183908046,0.06896551724137931,0.05172413793103448,0.08620689655172414,0.09615384615384616,0.07692307692307693,0.038461538461538464,0.17307692307692307,0.0375,0.025,0.0875,0.08653846153846154,0.0673076923076923,0.0945945945945946,0.25,0.0625,0.11538461538461539,0.12162162162162163,0.10067114093959731,0.0625,0.038461538461538464,0.02702702702702703,0.06040268456375839,0.028846153846153848,0.02702702702702703,0.04697986577181208,0.12162162162162163,0.04697986577181208,0.04697986577181208,0.11504424778761062,0.03937007874015748,0.08633093525179857,0.07258064516129033,0.09287925696594428,0.07575757575757576,0.05128205128205129,0.031746031746031744,0.08955223880597016,0.03260869565217391,0.025974025974025976,0.06113537117903929,0.10112359550561797,0.05533596837944664,0.06278026905829596
4,SP:57cdb30976e9fc5563bbb07a51d90eec8385e594,"In this paper, the authors present a method for justifying an agent’s policy in a sequential decision-making task when the human proposes specific counterfactuals. The authors first represent the state of the system with a set of intuitive classifiers, and then explain why a counterfactual (termed foil), would fail or cost more than the agent’s policy based on this classifier representation. For example, identifying that “move left” leads to a failure state in a game when the current state has the classifier “platform_edge_on_left” = True, as one its descriptors. Similar ideas extend to identifying an overly costly foil policy. The authors develop how such state characterization can be constructed, how feasibility of actions can be learned from sampling, and how a simple Bayesian formalism can be used to quantify uncertainty arising from sampling and other assumptions. They go on to show how the proposed idea works for two game settings, as well as how these explanations are preferred by human subjects over saliency based ones.",The paper presents a search procedure with concept classifiers to generate symbolic explanations (in terms of preconditions and costs) to justify an agent's action w.r.t. a foil agent. The authors conducted user study to show the usefulness of their method.,"(NB: I reviewed a previous version of this paper. I was in favour of its acceptance. In my opinion, this version is far more clear and addresses the most salient concerns of the reviewers, including me. I'll keep this review self-contained and refer to this submission).  The paper proposes a methodology for providing post-hoc explanations in sequential decision settings without assuming a model of the environment. Usual methods for explainability do not translate well into sequential decision settings, as the explanation is related to the executed actions. In particular, what's the natural form for such explanations? A simple but useful query is to ask about modifications to the plan retuned by the AI agent. These are called constrastive explanations. Those modifications could be a) better, b) lead use some illegal action execution, c) be more expensive.  The method creates a symbolic approximation around the current situation. The approximation allows to show the user a single concept that is missing and makes an action fail, or to increase the cost. The validation with users reports that they are finding them usefull. A more indirect test, less sensitive to the bias of self-report, is whether they can solve a task faster. Those results are also positive.  ","This paper presents a novel approach to generate contrastive explanations in a dialogue setting between a human and a planning agent. The setting assumes that the agent generates and offers an optimal plan to the user, and the user in turn challenges the presented plan offering an alternative (i.e. a contrast/foil). The goal of the agent is the denounce the alternative plan by explaining the infeasibility or suboptimality of the plan to the user in concepts they understand.","The authors propose a method of explainable AI for inscrutable blackbox models. The explanations build on a set of user-defined primitives, independently trained on the blackbox representation (e.g., visual frames of an Atari game), and use an increasingly popular method of providing contrastive explanations. Two forms of foil-based responses are provided: (1) indication of action failure from the planning perspective (preconditions unsatisfied); and (2) an explanation of relative sub-optimality that highlights key aspects of action costs that the user may be unaware of.","This paper proposes a symbolic model implemented alongside a planning AI, which can respond to human  queries about why one plan is taken by the AI, and not another.  The system takes as input a user concept vocabulary, where each concept is implemented by binary classifier indicating whether the proposition is present. This concept vocabulary needs to be initially solicited from the user -- either through user surveys or designed by researchers.  The output of the classifiers is noisy, which is interpreted as confidence over the proposed explanation. The system then uses a symbolic PPDL model with the concept vocabulary to answer queries. The system can issue two types of answers about why a proposed plan is inadmissible: (1) a pre-condition for a given action is missing or (2) the proposed plan is more costly than the chosen plan.  To identify if a concept is a precondition for action A, the system samples a number of states from the model where A is executable. If concept is always present, then it is a pre-condition. Samples are generated by a random walk from one of the states in the model's given plan. In the evaluated problems games the sampling algorithm needs ~500 samples to accurately identify the missing pre-condition. To identify the cost associated with executing a proposed action, a heuristic is used, such that tries to sample the minimal cost when executing the action given the preconditions. (This is not clearly explained )   A small user study was used to evaluate whether the subjects find the proposed explanations helpful. ",0.023668639053254437,0.047337278106508875,0.023668639053254437,0.023668639053254437,0.04142011834319527,0.09523809523809523,0.11904761904761904,0.023809523809523808,0.023809523809523808,0.038461538461538464,0.004807692307692308,0.052884615384615384,0.0379746835443038,0.0759493670886076,0.023255813953488372,0.09523809523809523,0.038461538461538464,0.05063291139240506,0.046511627906976744,0.026923076923076925,0.019230769230769232,0.06329113924050633,0.011627906976744186,0.0038461538461538464,0.10126582278481013,0.011627906976744186,0.04230769230769231,0.03488372093023256,0.023076923076923078,0.007692307692307693,0.037914691943127965,0.04244031830238727,0.032258064516129024,0.03137254901960784,0.03263403263403264,0.032,0.08264462809917356,0.015625,0.006622516556291391,0.05574912891986062,0.006802721088435374,0.04700854700854701,0.03636363636363636,0.035398230088495575,0.011560693641618497
5,SP:69cc1499e1ffdff113346180dd31c60fb1059872,"The paper presents a diagonal quasi-Newton method, by approximating the Hessian with a diagonal matrix. The paper proves a regret bound for the method in the convex case, and shows that in the non-convex setting the expected norm of the gradients goes to 0. The paper then provides experimental results on two image datasets, a small translation task and a small language modeling task.  ","The paper developed a new quasi-Newton algorithm for stochastic non-convex optimization. In contrast to existing works, it uses a (rectified/capped) diagonal matrix to approximate the Hessian, and incorporated techniques to reduce the stochastic variance. It shows first-order convergence guarantee of the algorithm and provided empirical evaluation on 2 CV and 2 NLP datasets.","This paper presents a first-order quasi-Newton method, named APOLLO, for solving stochastic nonconvex finite sum problems with a large number of data points. Each iteration of the method consists of computing a sparse and positive definite (diagonal) approximation of the objective function's Hessian, followed a quasi-Newton step. A regret bound is established for the convex setting and a complexity bound is established for the nonconvex setting, based on prior results on Adam-type optimizers. Finally, a comprehensive set of numerical experiments on three common tasks in vision and language is presented to show the superiority of the proposed method. ","This work proposes a quasi-Newton method APOLLO for nonconvex stochastic optimization. Based on a parameter-wise weak secant condition, a diagonal approximation of the Hessian is constructed, and rectified absolute value is adopted on the approximation. A step-size bias correction technique is used to mitigate stochastic gradient variance. Theoretical convergence analysis is provided under a convex online setting and a nonconvex stochastic setting. Experiments on CV and NLP tasks demonstrate the effectiveness and stability of the proposed method.","The paper proposes a Quasi-Newton inspired optimization algorithm for Stochastic Optimization named APOLLO. It adjusts a previously known update formula to better suit Deep Learning by using 1) a layer-wise diagonal approximation to the Hessian, 2) an exponential average of gradients to address the noise. Overall the algorithm shows promising results on the assigned experiments.","This paper presents the optimization method Apollo, a quasi-Newton method that relies on a parameter-wise version of the weak secant condition to allow for a diagonal approximation of the Hessian. Additionally, the issue of a potentially non-PSD approximation is addressed by replacing the approximation with a rectified absolute value. While the combination of techniques is interesting, my main hesitation comes from the limited discussion concerning other quasi-Newton methods for the same problem setting.",0.09230769230769231,0.2153846153846154,0.09230769230769231,0.06153846153846154,0.12307692307692308,0.05357142857142857,0.07142857142857142,0.10714285714285714,0.07142857142857142,0.13725490196078433,0.049019607843137254,0.11764705882352941,0.08860759493670886,0.22784810126582278,0.07142857142857142,0.10714285714285714,0.13725490196078433,0.0759493670886076,0.07142857142857142,0.10526315789473684,0.029411764705882353,0.05063291139240506,0.10714285714285714,0.05263157894736842,0.17721518987341772,0.08928571428571429,0.15789473684210525,0.125,0.23684210526315788,0.05263157894736842,0.09917355371900825,0.16766467065868262,0.08333333333333333,0.06611570247933884,0.11347517730496454,0.03797468354430379,0.05925925925925926,0.10714285714285714,0.06060606060606061,0.1546961325966851,0.06329113924050632,0.13483146067415733,0.1037037037037037,0.23225806451612904,0.06060606060606061
6,SP:797f40f8ab27a0ce589b18ec143e398b6b44a460,"This paper addresses a challenging problem in 3D representation learning of point clouds, which aims to generalize the representations well to arbitrary orientations. The proposed CSGNN uses a convolutional framework to learn over concentric spherical feature maps, which is incorporated with both intra-sphere and inter-sphere information. The idea is interesting and reasonably well presented. In general, the proposed CSGNN is practical on spherical representation learning. However, there still remains some issues that should be addressed.","This paper presents a novel multi-sphere icosahedral discretization for representation of 3D data. Given meshes or point cloud, the authors map them to multiple layers spheres and apply graph conv on the spheres to learn rotation-invariant features. In the final stage, all the layers are merged via a radial pool operator. The authors claim that such a structure could better preserve information of 3D objects and showcase on mesh and point cloud classification tasks.","In this paper, a multi-resolution convolutional architecture is proposed to learn from concentric feature maps. Different from single sphere representation, both graph convolutions and radial convolutions are employed to extract the intra-sphere and inter-sphere information. Benefit form the radial discretization, the proposed CSGNN achieves state-of-the-art results in testing on arbitrarily rotated data under ModelNet40 dataset. However, there are several drawbacks in the  draft, such as ambiguous figure and  insufficient ablation study.","This paper addresses an important problem in 3D representation learning, which is how to design a robust convolution neural network for arbitrarily oriented inputs. They propose to use multi-sphere icosahedral discretization to representation 3D data at first. And then, several alternative convolution operations are used to further process features. The novel convolution operation is consisting of intra-sphere and inter-sphere convolution. They also design a mapping function to convert point cloud data to mesh data. The experimental results indicate the model can work well.","This paper proposes a graph convolution based multiscale spherical deep neural network. In particular, radial convolutions over concentric spheres of different radii are used in conjunction with the typical spherical convolution. A simple 5-layer architecture is presented to fuse the rich features extracted by both convolutions. Experiments on point cloud classification under arbitrary SO(3) transformations as well as predicting electronic state density of graphene allotropes, demonstrate the validity of the devised method.","The paper presents a concentric sphere representation and icosahedron-based spherical CNNs for 3D point clouds. The contributions are two folds. First, the concentric sphere representation enables learning features volumetrically. Second, two types of convolutions: intra-sphere and inter-sphere are combined towards rotation equivariant and scalable computations. ",0.02631578947368421,0.14473684210526316,0.14473684210526316,0.02631578947368421,0.06578947368421052,0.04,0.08,0.04,0.02666666666666667,0.06578947368421052,0.05263157894736842,0.06578947368421052,0.047058823529411764,0.047058823529411764,0.0,0.02666666666666667,0.14473684210526316,0.12941176470588237,0.0273972602739726,0.10638297872340426,0.039473684210526314,0.07058823529411765,0.0410958904109589,0.0425531914893617,0.058823529411764705,0.0547945205479452,0.10638297872340426,0.0547945205479452,0.0851063829787234,0.0,0.026490066225165563,0.14473684210526316,0.1366459627329193,0.026845637583892617,0.08130081300813008,0.039735099337748346,0.07500000000000001,0.040540540540540536,0.03278688524590164,0.06211180124223602,0.053691275167785234,0.08130081300813008,0.050632911392405056,0.060606060606060594,0.0
7,SP:80c90830c0183d1bd699be9ec09c10cf58581311,"This paper studies learning cross-lingual word embeddings (CLWE) with alignment: mapping two pre-trained monolingual word embeddings to a shared space with a linear/orthogonal mapping. Previous work shows that normalization methods and the spectral properties of monolingual word embeddings have a big impact on alignment methods. Building on these results, the paper proposes a normalization method that regularizes the spectral properties of monolingual word embeddings. The method is then combined with mean centering and length normalization in an iterative process. Empirically, the proposed normalization method improves alignment and leads to better CLWE (measured by scores on bilingual lexicon induction, cross-lingual document classification, and natural language inference).","The paper proposes a new normalization method for monolingual word embeddings before aligning them between different languages. The normalization is based on spectral properties of the monolingual embeddings and aims to level the contributions of those leading singular values while maintaining those smaller ones at the same time. Further inspired by previous works that centers and units the length of monolingual embeddings, the authors propose an iterative normalization that combines the spectral normalization with centering and unit length normalization. The proposed method achieves better performance in both bilingual lexical induction (BLI) tasks as well as downstream tasks like cross-lingual document classification (CLDC) and cross-lingual natural language inference (XNLI). ","This paper describes three new methods for normalizing word embeddings before cross-lingual alignment. The first method, SpecNorm, caps the singular values of the word embeddings to be twice the average (original) singular values. The second method does mean centering, SpecNorm, and L2 normalization. The third repeats this for a fixed number of iterations (5). Experiments on a wide variety of tasks generally show improvements. ","This paper presents a new approach to normalizing (static) word embeddings prior to learning the mapping between them to induce an improved shared cross-lingual word embedding (CLWE) space. The main idea is to improve shared cross-lingual spaces not via learning more sophisticated mapping functions or by designing a new CLWE learning paradigm, but rather to affect the properties of independently trained monolingual embeddings to increase their alignability. In other words, normalization can be seen as a pre-processing step that is orthogonal to the actual mapping/projection functions, and improving normalization can have a positive impact on the final shared space. The core contribution of the paper is therefore the spectral normalization method, built on top of the recent advances and insights from prior work, which yields gains over other normalization techniques from prior work in the intrinsic task of Bilingual Lexicon Induction (BLI) and several transfer tasks (which rely on the CLWEs).","The paper proposes an Iterative Spectral Normalization method to improve pairwise Cross-Lingual Word Embedding alignment. The work supposes that improvement to spectral properties in monolingual embedding spaces are sufficient to yield improvements to Bi-lingual Lexicon Induction, Cross-Lingual Document Classification and Cross-Lingual NLI. The main contributions are (a) the introduction of a simple, portable algorithm to improve pre-processing of embedding spaces for cross-lingual alignment and (b) empirical improvements in multiple domains of cross-lingual embedding oriented tasks.  ","The authors propose a normalisation method for cross-lingual text representations.  The goal is to normalise the monolingual embeddings based on spectral normalisation.  The main contributions are: novel method to normalise word embeddings, the proposed method includes different normalization approaches, and the proposed method improves performance on intrinsic and extrinsic evaluation tasks. The study shows that produced text representations keep their meaning and improve performance on downstream tasks. ",0.19444444444444445,0.05555555555555555,0.14814814814814814,0.12962962962962962,0.06481481481481481,0.05504587155963303,0.11009174311926606,0.10091743119266056,0.11926605504587157,0.078125,0.046875,0.03125,0.07142857142857142,0.032467532467532464,0.07407407407407407,0.1926605504587156,0.09375,0.1038961038961039,0.1728395061728395,0.1044776119402985,0.09375,0.07792207792207792,0.13580246913580246,0.19402985074626866,0.032467532467532464,0.037037037037037035,0.029850746268656716,0.13580246913580246,0.07462686567164178,0.08955223880597014,0.19354838709677416,0.06976744186046512,0.12213740458015267,0.1481481481481481,0.08,0.06936416184971099,0.09125475285171103,0.11578947368421053,0.14772727272727273,0.045871559633027525,0.041379310344827586,0.030534351145038167,0.09361702127659574,0.045248868778280535,0.08108108108108109
8,SP:8bcc40cf0cbc16b7c6d403becfe2b1325b2d00d4,"This paper proposes a CNLCU strategy that incorporates the uncertainty of losses by adopting interval estimation.  In learning with noisy labels, the sample selection approach regards small-loss data as correctly labeled during training.  This paper gives large-loss data a try to explore underrepresented data that are correctly labeled but seem to be mislabeled at first glance. Experiments demonstrate that the proposed method is superior to baselines and robust to a broad range of label noise types. This paper opens up new possibilities in the topics of using sample selection to handle noisy labels.","This paper discusses the potential weaknesses in previous sample selection criteria in learning with noisy labels. And then propose a new selection criterion by incorporating the uncertainty of losses, together with theoretical justification. Experiments on both synthetic noisy balanced/imbalanced datasets and real-world noisy datasets validate the effectiveness of the proposed approach.","This paper proposes a novel algorithm to improve the sample selection in the case of noisy labels training by incorporating the uncertainty of losses. To select samples, the authors propose to use the lower bounds of the confidence intervals derived from concentration inequalities instead of using point estimation of losses. The authors introduce the Soft truncation estimator and the Hard truncation estimator and propose lower bounds for both estimators. The authors introduce an algorithm, the CNLCU Algorithm based on using either the lower bounds calculated for the Soft truncation estimator or the Hard truncation estimator.   The authors validate empirically their results on four benchmark datasets (MNIST, F-MNIST, CIFAR-10, CIFAR-100) and use a diverse set of possible noise functions. The method proposed almost always outperforms all other comparable approaches. ",This paper introduces the uncertainty of losses into the sample selection approach for learning with noisy labels. The authors discuss how to effectively exploit underlying clean examples that are less selected during training. Experimental results verify the effectiveness of the proposed method. ,"The authors discuss potential weakness of previous sample selection criteria and propose new selection criteria based on the uncertainty of losses (not losses themselves).  Some theoretical analysis on the criteria is provided.  Then, they experimentally validate the proposed algorithms (CNLCU-S and CNLCU-H), showing that the proposed algorithms outperform some baselines in learning with noisy labels.","The paper proposes a novel sample selection method for learning with noisy labels (LNL). Based on the typical ""small-loss"" assumption, the motivation is to consider uncertainty about large-loss samples in order to distinguish the two confounding cases: truly mislabeled samples, or clean yet underrepresented samples that are less frequently selected or learned by the model so far. The proposed method explores robust mean estimation to summarize the per-sample loss trajectory, through soft truncation and hard truncation. Concentration inequalities are used to obtain the final selection criteria to perform conservative search. Results are overall promising, in particular showing very good results dealing with label noise + extreme class imbalance (Figure 2).",0.10638297872340426,0.11702127659574468,0.1595744680851064,0.10638297872340426,0.1276595744680851,0.17307692307692307,0.23076923076923078,0.25,0.11538461538461539,0.06923076923076923,0.06153846153846154,0.06923076923076923,0.24390243902439024,0.21951219512195122,0.14285714285714285,0.19230769230769232,0.08461538461538462,0.36585365853658536,0.17857142857142858,0.10810810810810811,0.06923076923076923,0.2926829268292683,0.23214285714285715,0.05405405405405406,0.21951219512195122,0.14285714285714285,0.08108108108108109,0.17857142857142858,0.08108108108108109,0.07207207207207207,0.136986301369863,0.09821428571428573,0.2222222222222222,0.13333333333333333,0.11707317073170731,0.0989010989010989,0.25806451612903225,0.24074074074074076,0.0736196319018405,0.10526315789473685,0.08602150537634409,0.07468879668049792,0.20618556701030927,0.11842105263157895,0.09580838323353294
9,SP:aff00ec3fd79be87ff5d6acdcdb395ec03f80cac,"This paper proposes a graph network(GN) called COPINGNet (“COnstraint Projection INitial Guess Network”), which learns to compute a good initialization for the traditional PBD method. To simulate a physical system, PBD first computes updated locations of vertices then corrects the estimates of the initial position by constraint projection.  The projection step is computationally expensive, and that is where the proposed COPINGNet is applied to generate a good initial guess for the built-in linear system solver (e.g., CG).","This paper presents a learning-based technique to speed up the physics simulation of rod structures using PBD. The key motivation is to notice that solving the nonlinear system using CG is the key bottleneck in the physics simulation. To speed up the CG solver, this paper proposes to train a neural network that predicts a good initial guess for CG in the hope that it will reduce the number of iterations before convergence. This paper presents experiments with tens to over a thousand nodes and shows that the speedup technique reduces the number of iterations used in CG by a factor of at most 2.5 (Fig. 6).","This paper aims to accelerate the iterative physical simulators using graph neural networks, by combining the traditional iterative rod solver and the graph networks. Instead of an end-to-end way, this paper uses the trained GNN to provide the initial guess of the iterative solver to reduce the number of iterations. In the experiments section, this paper shows the improved run time performance compared with the traditional solver and a KKN method. ","This paper proposes to train a graph network to estimate the initial guess of a conjugate gradient solver in a rod dynamics simulation pipeline. By using a better initial position given by the graph network, the CG solver can converge faster than optimizing from zero vectors. The authors show how to construct such a network and demonstrate the accuracy and efficiency of their hybrid method in various simulation settings.","The authors consider the problem of speeding up a specific form of dynamic simulation - position based dynamics, used as a solver to simulate the dynamic evolution of rods. I believe this is a timely topic, both due to the growing importance of simulations in various communities within AI, and because flexible objects are becoming more relevant in some of these applications.  The core concern of this paper is to find ways to speed up PBD solvers. To the extent that the iterative process of constraint projection is computationally expensive, there is scope for finding surrogates that could perform something like this computation quicker. The author's proposal is to use a Graph Neural Network to act as a surrogate for the analytical calculation. In contract to some other works that aim at replacing the entire simulator, the authors restrict the scope here to only using the neural network to emulating the specific projection step, which then serves as an initialisation, speeding up subsequent computation. This is a good idea and the empirical data supports its use. The empirical experiments in this paper are simulations of free evolution of a rod and a spring, clamped on one side, after an initial perturbation. It is shown that the proposed method can be more computationally efficient than an alternative which only uses the full conjugate gradient optimization step from a zero initialization.","The paper proposes an algorithm to accelerate simulations of deformable rods based on position-based dynamics. Despite what the title and abstract suggests, the scope of the paper is limited to a single physical system, and there is no evidence that this approach will work on generic physical systems. The title and abstract should be tuned down and made more specific.",0.0759493670886076,0.06329113924050633,0.08860759493670886,0.12658227848101267,0.0379746835443038,0.08333333333333333,0.07407407407407407,0.07407407407407407,0.009259259259259259,0.06944444444444445,0.1111111111111111,0.027777777777777776,0.11764705882352941,0.014705882352941176,0.02631578947368421,0.05555555555555555,0.06944444444444445,0.10294117647058823,0.043859649122807015,0.05,0.125,0.11764705882352941,0.03508771929824561,0.016666666666666666,0.07352941176470588,0.03508771929824561,0.03333333333333333,0.03508771929824561,0.016666666666666666,0.1,0.06417112299465241,0.06622516556291391,0.09523809523809525,0.06514657980456025,0.04316546762589928,0.1,0.0909090909090909,0.047619047619047616,0.011904761904761904,0.07142857142857144,0.05333333333333334,0.0303030303030303,0.05405405405405405,0.015625,0.041666666666666664
10,SP:cf5222ed98e5d552771fb915beeee4d504b947fe,"The paper proposes to replace the actor/policy network with an iterative version to encode the action distribution parameters, which is inspired by prior work on iterative amortized optimization. This scheme generates the action distribution parameters for each state at the end of an inner loop which takes the objective gradient wrt to the parameters also as input (as opposed to a regular actor network which does one forward pass). This seems similar to prior work on learning to learn/meta learning but the presentation can be improved (more detailed comments below).  Experimental evidence is given for benefits over SAC in mujoco environments.","This paper introduces a novel approach to policy optimization which leverages the connection between amortized variational inference and policy-based RL with function approximation. Specifically, rather than rely on a ""direct"" parameterization of the state-conditioned distribution over actions, in which an input state is mapped directly to distribution parameters (e.g., mean and variance), the approach adds an additional optimization procedure over the distribution parameters themselves, rather than those of the policy network. Experimental results are then used to demonstrate the benefits of this approach for performance, accuracy, and flexibility in the optimization process. ","The authors, via a standard reinterpretation of reinforcement learning as an inference problem, discusses the resulting “amortization gap” that results from using policy networks to optimize the variational lower bound. In order to close this gap, the authors propose to train an iterative procedure parameterized by $\phi$, which uses estimated gradients of the return to iteratively refine $\pi(\cdot | s_t)$ for the current state $s_t$. They then provide an extensive experimental evaluation demonstrating their proposed method and several desirable properties, including adaptability to gradients estimated by different methods.","This paper draws an interesting connection to variational inference, categorizing current policy optimization methods with KL regularization as direct amortized optimizers. The paper shows how direct amortized policy optimization can be suboptimal and proposes a new class of method called iterative amortized policy optimization. This ‘iterative’ amortized policy optimization performs iterative optimization before the regular optimization step, offering several advantages: better at reaching the optima, better at sampling multiple modes, and support more rapidly changing policy. The paper shows nice visualization to support these claims and also run benchmark experiments to show its improvement.","This work analyzes policy optimization from an inference perspective. It first shows that policy optimization with entropy or KL regularization in DRL with a policy network is a form of amortized optimization where instead of directly optimizing the action distribution parameters, we optimize a network which outputs the action distribution parameters. This results in an amortization gap which negatively impacts performance and exploration. Next the authors propose an alternative iterative amortization method for training the policy network. This new approach is shown to have various benefits which the authors demonstrated through a series of experiments on the MuJoCo environments.","**Contributions**: The authors propose to use iterative amortization for policy optimization to help reduce suboptimality in policy optimization. They find that like in variational inference for generative modeling, iterative amortization is able cover multiple modes of distributions and lower the amortization gap, and show slightly improved performance on mujoco gym benchmarks. Additionally, they find that the iteratively amortized policy is able to better exploit the learned Q function and lead to more overestimation bias, and address this by tuning a parameter to make the Q-function backups more pessimistic to compensate.",0.029411764705882353,0.00980392156862745,0.0392156862745098,0.09803921568627451,0.0,0.010638297872340425,0.031914893617021274,0.05319148936170213,0.02127659574468085,0.0,0.056179775280898875,0.0449438202247191,0.03225806451612903,0.053763440860215055,0.061224489795918366,0.031914893617021274,0.011235955056179775,0.043010752688172046,0.10204081632653061,0.0,0.011235955056179775,0.03225806451612903,0.05102040816326531,0.022222222222222223,0.0,0.05102040816326531,0.044444444444444446,0.030612244897959183,0.05555555555555555,0.06666666666666667,0.03061224489795918,0.010471204188481674,0.041025641025641026,0.1,0.0,0.01092896174863388,0.03208556149732621,0.052083333333333336,0.02173913043478261,0.0,0.053475935828877004,0.04469273743016759,0.03141361256544502,0.05464480874316939,0.06382978723404255
11,SP:e1ef24ecb1d18bab9ab0d706efcdc61f8bd4b4b4,"This paper proposes a model architecture for question answering which takes advantage of shallow proposition structure and coreference links using (relational) graph convolutional networks. Subject-predicate-object triples (dubbed ""fact units"") are extracted from the text using dependency parsing, extra coreference links are added using a coreference system, and the resulting graph is initialized with representations from a contextualizing encoder, passed through a GCN, and recombined with the contextual representations via multi-headed attention. The resulting representations serve as input to the final classification layer for answer prediction, as well as a ""logical facts regularization"" step which encourages the final representation of the object in each fact unit to align (via cosine) with the sum of the subject and predicate representations.  In experiments on several datasets designed to test logical reasoning, the proposed architecture scores higher than several baselines.","The authors propose a framework, called Focal Reasoner, to perform logical reasoning to answer questions. The proposed approach first extracts the facts from raw text. The authors describe a fact unit as a triple of (argument, predicate, argument). These collections of facts can be viewed as a graph with predicates as undirected edges between arguments. After forming a subgraph, reasoning is performed by a graph convolution module to predict the correct answer.  The authors show that their approach outperforms existing approaches on two benchmarked datasets. The authors also perform a detailed analysis that sheds more light on the internal workings of the model.","The paper claims that a lot of previous work in QA-based MRC focused only on entity-aware common sense knowledge.  To overcome this, the paper proposes to build a finer-grained local fact, (entity, predicate, entity) triplet, information based on dependency parsing, and then connect the triplet nodes with global information such as coreference, entity information.  Architecture-wise, a passage, question, and options are passed to RoBERTa (or DeBERTa) and then this information goes through the graph-attention network (GAN), based on the graph structure described above. In the experiment section, the authors show that the proposed FOCAL REASONER outperforms the previous SOTA models and run an ablation study on how each component contributes (in Table 5,6).  The main contribution of this paper, as I understand, is bringing more fine-grained local facts and connecting them with global information such as coreference and entity linking.","The paper claims that a lot of previous work in QA-based MRC focused too much on entity-aware common sense knowledge and other clues in the natural language (in passage, questions & candidate options). To overcome this, the authors extract ""fact units"" in the form of ""subject-verb-object"" from syntactic processing (dependency parsing).  With these fact units as a node, they form a supergraph and uses GNNs to process this. There are also many more components: global node, logical fact regularization, enlarged edge types compared to previous work and interactions. In the experiment section, they show that FOCAL REASONER outperforms the previous SOTA models and run an ablation study on how each component contributes (in Table 3). ","This paper presents a graph-based approach to solve logical reasoning questions. This method first extracts fact units from given questions, passages and answer options and builds a graph based on them, where a dependency parser is used to extract <subject, verb, object> triples. Then, the pre-trained RoBERTa large is used to generate representations of the concatenated question-passage-option sequences. Next, a graph is built based on the dependency parsing results of the sequence, and a graph attention network is used to perform neural message passing to exchange information between graph nodes. Last, all information are aggregated and used to do the prediction. Evaluations are conducted on 3 reasoning tasks, ReClor, LogiQA and MuTual, with ablation study performed for the effects of global atoms, fact unit variants/numbers and interaction layers.","This paper presents a system for graph-based reasoning about scenarios. Facts are represented as subgraphs which are connected into a supergraph via a single ""global"" node and coreference nodes. A passage, question, and options are first passed through RoBERTa, then a neural network (graph attention network) passes information through the graph representation before an answer is predicted. Results show that this model works better than a related graph-based technique (the DAGN model), with particularly strong gains on the LogiQA test set.",0.036231884057971016,0.028985507246376812,0.021739130434782608,0.06521739130434782,0.021739130434782608,0.058823529411764705,0.0392156862745098,0.0784313725490196,0.0196078431372549,0.3082191780821918,0.0547945205479452,0.0821917808219178,0.042735042735042736,0.02564102564102564,0.06060606060606061,0.049019607843137254,0.0273972602739726,0.02564102564102564,0.06818181818181818,0.036585365853658534,0.0410958904109589,0.03418803418803419,0.06060606060606061,0.024390243902439025,0.38461538461538464,0.06060606060606061,0.14634146341463414,0.03787878787878788,0.036585365853658534,0.0975609756097561,0.041666666666666664,0.02816901408450704,0.023529411764705882,0.06666666666666665,0.02727272727272727,0.04838709677419355,0.0365296803652968,0.06837606837606838,0.021739130434782608,0.34220532319391633,0.05755395683453238,0.10526315789473684,0.040160642570281124,0.03015075376884422,0.07476635514018691
12,SP:f2325f97eb57b82ce95c86638af9d733d325b45d,"Summary: Reinforcement learning has been shown to overfit to training environments. Some recent works (laskin et al, kostrikov et al) have shown data-augmentation to be an effective technique in curbing the overfitting issue. However, different games have different biases and hence require different data augmentation. This paper proposes to use UCB algorithm to automatically select a good data augmentation (from a list of data augmentations) for a game. However, naively applying data augmentation to RL algorithms like PPO isn’t theoretically sound. To address this problem, the paper introduces novel policy and value regularization. Overall, the paper contains extensive experiments and ablation studies.","The paper proposes a framework for automatic data augmentation in deep RL. It proposes three algorithms based on 1) UCB 2) RL2 3) meta-learning, combining PPO and regularization of both the actor and critic to make sure that the policy and value function are invariant to the transformation corresponding to the augmentation. Experiments on ProcGen and the DMC with distractors show that the UCB-based algorithm (UCB-DrAC) outperforms vanilla deep RL and previous data augmentation baselines and is competitive with the best augmentation type chosen post hoc.","This paper presents a method for improving generalization in reinforcement learning using data augmentation. It identifies a key theoretical limitation of previous work in the naive application of data augmentation to actor critic methods and proposes a solution that avoids biased estimation of the policy gradient objective while still allowing for regularization of the policy and value function. The resulting loss is paired with three distinct approaches for identifying an optimal data augmentation strategy. The best of these approaches, a bandit algorithm, obtains SOTA performance (relative to published works) on the Procgen environment.  ","This paper tackles the problem of generalization in deep RL via data augmentation. It provides a framework for automatic data augmentation based on UCB, RL^2, or MAML. When UCB is combined with regularization of the policy and value function so that their outputs are invariant to transformations (such as rotation, crop, etc.), it shows improvements over similar algorithms on the ProcGen benchmark. ","This paper presents a method that utilizes data augmentation for image-based reinforcement learning. The data augmentation is used to regularize the policy and function approximation in the proposed method. In addition, a method for automatically identifying effective ways of data augmentation is proposed. The experimental results show that the proposed method outperforms the baseline methods.","This paper proposes an automatic data augmentation approach for RL tasks. Specifically, it takes UCB for data selection and introduces two regularization terms for actor-critic algorithms' policy and value function. Then this paper evaluated the approach based on the Procgen benchmark and demonstrated that it outperforms existing methods. It is also shown that the learned policies and representations are robust to irrelevant factors.",0.05825242718446602,0.08737864077669903,0.04854368932038835,0.06796116504854369,0.04854368932038835,0.10227272727272728,0.18181818181818182,0.06818181818181818,0.09090909090909091,0.14130434782608695,0.14130434782608695,0.08695652173913043,0.08064516129032258,0.16129032258064516,0.07272727272727272,0.06818181818181818,0.09782608695652174,0.08064516129032258,0.12727272727272726,0.07936507936507936,0.09782608695652174,0.25806451612903225,0.10909090909090909,0.12698412698412698,0.20967741935483872,0.23636363636363636,0.12698412698412698,0.09090909090909091,0.15873015873015872,0.06349206349206349,0.06282722513089005,0.09230769230769231,0.0606060606060606,0.08860759493670886,0.06024096385542168,0.1,0.21333333333333335,0.08391608391608392,0.10596026490066225,0.16883116883116883,0.1768707482993197,0.10322580645161289,0.08547008547008547,0.16,0.06779661016949151
13,SP:f9ece9e53f7d9bac9c921f1e85c270b826993a5a,"This paper proposes a mechanism based on approximate conditional independence (ACI) to explain why solving pretext tasks created from known information can learn representations that provably reduce downstream sample complexity, as a sufficient condition. In specific, they measure the downstream performance using the approximation error and estimation error, and establish their initial results under a strict condition -- conditional independence (CI) and linear function space in Section 3, then extend it to ACI and arbitrary function space in Section 4, resulting in the main contribution Theorem 4.2 that clearly quantifies the generalization error. The theorem is also verified in Section 5 using simulations on NLP tasks. ","This paper attempts to understand why self-supervised learning works in the following sense: will the sample complexity for a downstream task be decreased (compared to the standard supervised learning without pretraining) if it is pre-trained according to some related auxiliary task? The relation between tasks is formulated as the approximate conditional independence of the dependent variables. The main theoretical results show that the sample complexity (compared to supervised learning) can be decreased under Assumption 3.5, 4.1, and 4.2. ","This paper aims to theoretically explain and investigate the benefit of reconstruction-based self-supervised learning. This paper studies a specific problem with two input features and one downstream label and shows how the specific connection between them can guarantee successful self-supervised learning.  In particular, this paper quantifies how the approximation independence between two input features can be extracted to solve the downstream tasks. Results show that self-supervised learning can successfully learn the ground-truth with much smaller labeled sample complexity. ","This paper suggests a theoretical model to capture why reconstruction-based self-supervised learning leads to good representations, and improved downstream performance. The suggested model is that the pretext task ""factors through"" the supervised labels, i.e. that X_1 -- Y -- X_2 is Markov, where X_1-->X_2 is the pretext task, and X_1 --> Y is the supervised task. In such a setting, it is shown that the optimal pretext function has certain good properties: it linearly separates the supervised task, and is ""low rank"", so can be used for efficient downstream learning.","This paper studies whether reconstruction-based pretext task can provide representations helpful for downstream tasks, which is a question of good interests in the community.  The authors give an affirmative answer that when (approximate) conditional independence holds, reconstruction as a pretext task provides representations such that 1) an additional linear layer is sufficient to ensure small approximation error on classification, and that 2) helps to reduce the sample complexity of the classification task from complexity of the function class (without pretraining) to linear in data dimension.  These theoretical results are empirically verified on synthetic, vision (inpainting), and NLP (sentiment classification) tasks.","- The paper theoretically analyzes self-supervision under some simplifying assumptions. They begin with a conditional independent setting, where X1 is conditionally independent of X2 given Y. - The high-level idea is to first pre-train a model that predicts X2 from X1. This can be done on unlabeled data. Then they can fit a model on top of representations learned by this model, to predict Y. 1. Ground truth self-supervision: They show that if we learn E[X2 | X1], the ground truth self-supervision function, then under mild invertibility conditions, E[Y | X] is a linear function of E[X2 | X1]. Further, E[X2 | X1]'s effective dimension is the same as Y, so this linear function can be fit using very few examples. In contrast, the function from X1 or X2 to Y can be very complicated / non-linear and require many examples to fit. 2. Linear self-supervision: They then examine what happens if we fit a linear function (on feature space) to predict X2 from X1. This can learn a useful lower dimensional embedding, and then we train a linear classifier to predict Y on top of this. They show this is more sample efficient than just training a linear classifier from X1 to Y. 3. Relaxing conditional independence: They quantitatively examine how this degrades when we don't exactly have conditional independence. Their bounds will still be good if things are ""almost"" conditionally independent. ",0.06666666666666667,0.0380952380952381,0.01904761904761905,0.06666666666666667,0.0380952380952381,0.12195121951219512,0.07317073170731707,0.10975609756097561,0.036585365853658534,0.07317073170731707,0.06097560975609756,0.04878048780487805,0.042105263157894736,0.042105263157894736,0.03,0.08536585365853659,0.04878048780487805,0.021052631578947368,0.07,0.01680672268907563,0.12195121951219512,0.06315789473684211,0.09,0.012605042016806723,0.06315789473684211,0.05,0.01680672268907563,0.04,0.01680672268907563,0.012605042016806723,0.07486631016042782,0.04278074866310161,0.02,0.06829268292682927,0.023323615160349857,0.12195121951219512,0.06779661016949153,0.0989010989010989,0.01875,0.06779661016949153,0.054945054945054944,0.025,0.041025641025641026,0.02402402402402402,0.01775147928994083
14,SP:fef1b68e8fefbd9ab2610b3c995a74852ce53a6a,"This paper formulates a framework for reinforcement learning and behavior cloning from weak supervisions (i.e., noisy rewards or imperfect expert demonstration). Specifically, it proposes PeerPL to perform efficient policy learning from the available weak supervisions, which covers PeerRL (for RL with noisy rewards), PeerBC (for imitation learning from imperfect demonstration) and PeerCT (for hybrid setting). The PeerPL idea is based on a new weak supervision objective that is in the form of difference between norm learning loss and a loss incurred by randomly sampling the supervision signals. Experimental results demonstrate that PeerPL significantly outperforms SOTA solutions when the complexity or the noise of the learning environment grows. The proposed idea is useful in practice as it increases the robustness of the learning process to imperfect supervision signals.","This paper tackles the problem of policy learning under weak/noisy supervision. The authors present PeerPL, a unified framework that can train agents using behavior cloning under noisy/suboptimal demonstrations, or using reinforcement learning under noisy rewards. PeerPL uses the idea of “Correlated Agreement” by subtracting the original objective with a second term. The second term evaluates on randomly paired state-action tuples and supervisions, punishing the “blind” agreement between the learning agent and the weak supervision. The authors instantiate this idea on both behavior cloning and RL (PG/DQN), and also evaluate it on the problem of policy co-training. The authors demonstrate that PeerPL outperforms the weak supervision baselines on IL and RL setting, and sometimes it even outperforms agents trained with clean supervision. ","In this paper, the authors propose a general framework for learning sequential decision-making policies with noisy feedback. Two categories of noisy feedback are considered, noisy reward in reinforcement learning and noisy demonstrations in behavioral cloning. The concept of “correlated agreement” (CA) is proposed as a regularization mechanism for a policy to learn from the noisy feedback without overfitting to it. An additional policy co-training setting is also considered. Empirical evaluations control games and Atari show that CA is effective in enabling policy learning from noisy feedback.",This paper proposes a method for RL and imitation learning in the setting where the reward function or expert actions are corrupted with noise. The main idea is to regularize the learned policy to have *high* loss on unaligned state-action-reward examples. Experiments show that the proposed method outperforms baselines that either learn directly using the noisy rewards/actions or learn an auxiliary function that predicts the mean reward/action. ,"This paper proposed a framework to improve the efficacy of sensorimotor control learning. The main idea consists of augmenting the training loss with a specifically designed term that avoids the policy to ""overfit"" to noise present in the reward (for RL) or in the demonstrations (for behavioral cloning).  This extra term aims to minimize the probability that when randomly associating rewards and state-action pairs, the policy performs poorly. This idea, borrowed by the theory of correlated agreement, should improve the policy performance when the supervision (in terms of demonstration or reward) is corrupted by noise. Experiments show that the proposed framework achieves comparable or better results than prior works.","The authors propose a general and unified framework to study policy learning without strong or high-quality supervision signals. This unified framework comprises, but not limited to, the main approaches for policy learning, including those using rewards signals in reinforcement learning (RL) and demonstrations through behavioral cloning (BC). The authors also propose PeerPL, a solution for efficient policy learning using only weak supervision signals, which evaluates an agent’s learning policy with noisy signals, treated as policy of an imperfect “peer agent”. The weak supervision signal comes in the form of correlation with the imperfect peer agent’s report, while the solution encourages generalization through penalizing identical reports.",0.07086614173228346,0.06299212598425197,0.05511811023622047,0.05511811023622047,0.07874015748031496,0.064,0.024,0.048,0.072,0.04597701149425287,0.06896551724137931,0.10344827586206896,0.17142857142857143,0.04285714285714286,0.03669724770642202,0.072,0.09195402298850575,0.1,0.06422018348623854,0.09345794392523364,0.09195402298850575,0.04285714285714286,0.05504587155963303,0.08411214953271028,0.05714285714285714,0.05504587155963303,0.08411214953271028,0.11009174311926606,0.028037383177570093,0.037383177570093455,0.07142857142857142,0.07476635514018691,0.07106598984771574,0.059322033898305086,0.08547008547008547,0.07547169811320754,0.030769230769230767,0.05128205128205129,0.07758620689655173,0.05095541401273886,0.06122448979591837,0.09278350515463918,0.1340782122905028,0.03389830508474576,0.03703703703703703
