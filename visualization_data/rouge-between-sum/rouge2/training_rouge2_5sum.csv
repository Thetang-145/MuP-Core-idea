,paper_id,summary,summary,summary,summary,summary,precision,precision,precision,precision,precision,precision,precision,precision,precision,precision,recall,recall,recall,recall,recall,recall,recall,recall,recall,recall,fmeasure,fmeasure,fmeasure,fmeasure,fmeasure,fmeasure,fmeasure,fmeasure,fmeasure,fmeasure
,,0,1,2,3,4,0-1,0-2,0-3,0-4,1-2,1-3,1-4,2-3,2-4,3-4,0-1,0-2,0-3,0-4,1-2,1-3,1-4,2-3,2-4,3-4,0-1,0-2,0-3,0-4,1-2,1-3,1-4,2-3,2-4,3-4
0,SP:04abdf6d039513f23e00e6686832cd4b950f1d75,"This work proposes a specific parametrisation for the Gaussian prior and approximate posterior distribution in variational Bayesian neural networks in terms of inducing weights. The general idea is an instance of the sparse variational inference scheme for GPs proposed by Titsias back in 2009; for a given model with a prior p(W) perform variational inference on an extended model with a hierarchical prior p(U) p(W | U), that has the same marginal p(W) = \int p(U)p(W | U)dU as the original model. The authors then consider “U” to be auxiliary weights that are jointly Gaussian with the actual weights “W” and then use the decomposition p(W|U)p(U), q(W|U)q(U) for the prior and approximate posterior (which can easily be computed via the conditional Gaussian rules). Furthermore, they “tie” (almost) all of the parameters between q(W|U) and p(W|U) (similarly to Titsias, 2009). The main benefit from these two things is that since the mean and covariance of the Gaussian distribution over W conditioned on U can be efficiently represented as functions of U, whenever dim(U) << dim(W) we get reductions in memory for storing the distributions over the parameters in the network. The authors furthermore, discuss how to efficiently parametrize the joint distribution over W, U, discuss different choices for q(U) (that can lead to either traditional VI or something like deep ensembles). In addition, they also discuss how more efficient sampling from q(W|U) can be realised via an extension of the Matheron’s rule to the case of matrix random variables. Finally, they evaluate their method against traditional mean field variational Bayesian neural networks and deep ensembles on several tasks that include regression, classification, calibration and OOD performance. ","This paper proposes a layer-wise variational inference scheme for Bayesian neural networks that uses ""inducing weights"" for each layer before up-sampling to the full (layer-wise) weight distribution. The scheme is equivalent to a hierarchical prior formulation with an analogous hierarchical variational posterior. Experiments are performed on the standard BDL benchmarks of synthetic one dimensional regression, ResNets on CIFAR-100, and out of distribution detection.","The paper proposes a parameter-efficient method for uncertanity quantification in deep neural network using per layer inducing weights. The paper develops a network weight sampling scheme based on inducing weights. The proposed method employs Matheron's rule for efficient sampling. The method requires fewer parameters than even deterministic networks, while keeping good calibration and performance, at the cost of longer compute times, when few MC weight samples are used. ",The paper proposes to represent uncertainty in neural networks by augmenting neural network layers with inducing weights and then assuming that the resulting vector of weights is drawn from a matrix normal distribution which is structured so that it has fewer parameters than the original network. The parameters of the distribution are learned via variational inference and the authors present a procedure to sample efficiently from this distribution for optimizing the model parameters and for inference at test time. Experiments on classification and out of distribution detection are presented to validate the approach.,"This paper proposes a method for reducing the storage cost of BNNs. The idea is to apply the inducing point method and fast Matheron's sampling rule, commonly used in GPs, to standard variational BNNs. Experiments show that the resulting BNNs stay competitive to standard BNNs while enjoying up to 75% reduction in parameter size.",0.033783783783783786,0.02364864864864865,0.030405405405405407,0.010135135135135136,0.045454545454545456,0.12121212121212122,0.045454545454545456,0.08695652173913043,0.07246376811594203,0.03260869565217391,0.15151515151515152,0.10144927536231885,0.09782608695652174,0.05555555555555555,0.043478260869565216,0.08695652173913043,0.05555555555555555,0.06521739130434782,0.09259259259259259,0.05555555555555555,0.05524861878453039,0.038356164383561646,0.04639175257731959,0.01714285714285714,0.04444444444444444,0.10126582278481013,0.049999999999999996,0.07453416149068322,0.08130081300813007,0.041095890410958895
1,SP:173177f78449ef09647670389b0ffba1e35db0ba,"This paper studies the question of whether episodes (using a split of support & query sets) are necessary for non-parameteric approaches for few-shot learning (examples include Prototypical Networks & Matching Networks). The authors propose a Neighborhood Component Analysis-based method for few-shot learning, where a mini-batch consists of examples from a subset of base classes, with no support or query split. The NCA loss then involves distances computed across all examples in the mini-batch rather than only using distances computed between support and query examples (as is done in Prototypical Networks & Matching Networks). The authors show that their proposed method is able to achieve better performance for 1-shot & 5-shot classification in miniImageNet, cifar-fs, and tieredImageNet benchmarks. They also speculate that the proposed method performs better because of its use of more distance computations in the loss compared to Prototypical Networks & Matching Networks and confirm this by conducting an experiments where they randomly drop distances in their NCA loss and showing this has a negative impact on performance.",This paper conducts a case study for the non-parametric few-shot classification methods (e.g. Prototypical Networks).  It proposes to utilize the classic Neighbourhood Component Analysis (NCA) sampling instead of the original matching or prototypical style episode sampling.  The authors conducted ablation experiments to investigate the properties of this new sampling and compare it with the basic Prototypical Networks (PNs) method.  The final accuracy is comparable with the recent methods on three benchmark datasets. ,"This paper studies the role of the popular episodic training paradigm, in the context of two metric learning-based episodic models: Prototypical Networks (PN) and Matching Networks (MN). They show that these popular methods underperform compared to the closely-related NCA model which is non-episodic, i.e. does not separate the examples sampled in each training batch into disjoint support and query sets. They argue that the superior performance of NCA is because, due to not performing that separation, the total number of pairwise comparisons that are used in the loss computation is larger than those used for PN/MN, making the gradients more informative. Indeed, in episodic models, each query example is only compared to support examples, but not to other query examples, resulting in significantly fewer comparisons. Experimentally, they show that for a fixed batch size (where the ‘batch size’ in the case of the episodes is given by the combined size of the support and query sets), NCA outperforms PN and MN, despite its simplicity in terms of its smaller number of hyperparameters. They also show that randomly discarding comparisons from NCA leads to similar performance to the analogous PN/MN models and perform a set of ablations to “bridge” the gap between PN and NCA, further strengthening their finding that support/query separation hurts performance. ","This paper presents a new perspective on the episodic learning of nonparametric few-shot learning methods. The main claim is that current popular nonparametric methods, such as Prototypical Networks (PNs) and Matching Networks (MNs), are not data-efficient because less gradient signal is propagated during training due to the artificial division of the data points to support and query sets. The authors instead propose to use the standard learning protocol with batches and an equivalent loss function based on the Neighbourhood Component Analysis (NCA). This loss function exploits all connections between data points in the batch. The authors then propose three techniques to perform a few-shot classification during evaluation based on k-NN, nearest centroid, and soft assignment. Through extensive experiments, the authors justify their perspective and show comparable results to other recent FSL methods (not necessarily nonparametric ones). ","The paper's starting point is the question whether the episodic training is beneficial, or not, for FSL / Prototypical Networks. The work can be seen as a follow-up of the recent works showing that simple baselines can outperform rather sophisticated few-shot learning models. Towards answering this question, this paper points out that Prototypical Networks (PN) are related to Neighborhood Component Analysis (NCA), and NCA can be considered as an episodic training-free alternative of PN.",0.040697674418604654,0.10465116279069768,0.08139534883720931,0.05232558139534884,0.04054054054054054,0.13513513513513514,0.10810810810810811,0.0684931506849315,0.0273972602739726,0.05755395683453238,0.0945945945945946,0.0821917808219178,0.10071942446043165,0.11842105263157894,0.0136986301369863,0.07194244604316546,0.10526315789473684,0.1079136690647482,0.07894736842105263,0.10526315789473684,0.05691056910569107,0.09207161125319692,0.09003215434083602,0.07258064516129033,0.020477815699658702,0.09389671361502347,0.10666666666666667,0.08379888268156424,0.04067796610169491,0.07441860465116279
2,SP:45d0d17b384044473db2e2e164c56558044d2542,"The paper is concerned with closing the gap between the amount of training in deep networks and in developing brains, as the current deep learning models use an unrealistically large number of synaptic updates. The authors address that with three strategies: less training, weight clustering and training in a subset of layers. All methods are tested individually and in combination with each other on primate ventral stream data.  ","## Updated the score  This paper proposes to address an important research question for connecting biological (BNN) and artificial neural networks (ANN). Although after training, ANN replicates various salient features of BNN, the way they are often trained is biologically implausible and thus, it is hard to argue that ANNs are suitable for modeling BNNs convincingly. In particular, this work focuses on the already existing CORnet who has shown a high Brain-score. The idea of the authors is to show that they can largely reduce the number of updates when using their methods while still retaining a high Brain-score, thus proposing a potential training mechanism for BNNs.",The paper is about ANN being best-known models of developed primate visual systems. However this fact does not yet mean that the way those systems are trained is also similar. This distinction and a step towards answering this question is the main motivation of this work. The authors demonstrate a set of ideas that while drastically reducing the number of updates maintain high Brain Predictability according to the BrainScore. The significance of this result in my opinion largely depends on how well we can map those observations and methods to biological meaning and knowledge on how primate brains are trained (see the discussion point below).,"This paper presents an empirical study that elucidates potential mechanisms through which models of adult-like visual streams can ""develop"" from less specific/coarser model instantiations. In particular, the authors consider existing ventral stream models whose internal representations and behavior are most brain-like (amongst several other models) and probe how these fair in impoverished regimes of available labeled data and model plasticity (number of ""trainable"" synapses). They introduce a novel weight initialization mechanism, Weight Compression (WC), that allows their models to retain good performance even at the beginning of training, before any synaptic update. They also explore a particular methodology for fine-tuning, Critical Training (CT), that selectively updates parameters that seem to yield the most benefit. Finally, they explore these methods/algorithms' transfer performance from one ventral stream model (CORnet-S) to two additional models (ResNet-50 and MobileNet).","The paper addresses the question of how many weight updates are needed to train a deep network before it takes on biologically realistic representations. The paper uses CORnet-S (a network that has been proposed to resemble primate ventral stream), and BrainScore (a benchmark of how closely related deep network responses are to visual responses in primate ventral stream). Three ways of reducing the numbers of weight updates are explored, each of which is found to vastly reduce updates while moderately reducing BrainScore. First, the network is simply trained for fewer epochs. Second, weights are initialized with clusters of weights found after training. Third, only a subset of layers is updated. A combination of methods leads to 80% of full brain predictivity with 0.5% of the standard number of weight updates. ",0.029850746268656716,0.05970149253731343,0.05970149253731343,0.1044776119402985,0.07476635514018691,0.037383177570093455,0.028037383177570093,0.02857142857142857,0.02857142857142857,0.02857142857142857,0.018691588785046728,0.0380952380952381,0.02857142857142857,0.05343511450381679,0.0761904761904762,0.02857142857142857,0.022900763358778626,0.02142857142857143,0.022900763358778626,0.030534351145038167,0.022988505747126436,0.046511627906976744,0.03864734299516908,0.0707070707070707,0.07547169811320754,0.03238866396761133,0.025210084033613443,0.024489795918367346,0.025423728813559324,0.02952029520295203
3,SP:71bc23f11137956757268354ed02ac3799373323,The paper suggests a new approach to the construction of ensembles of deep neural networks (DNN). Unlike previous methods which usually deal with multiple DNNs of same structure authors propose to form an ensemble of networks with different architecture. The main claim is that using diverse architectures increases diversity and hence the quality of predictions. To find the best architectures they use methodology inspired by neural architecture search (NAS) in particular random search and regularized evolution. The method for neural ensemble search (NES) is algorithmically simple although computationally hard. On several experiments the authors show NES outperforms standard deep ensembles formed from networks with same (even optimal) structure both in terms of test NLL and in terms of uncertainty estimation under domain shift.,"This paper proposes neural network ensembles with varying architectures, arguing these will be produce better uncertainty estimates, and considers various straightforward techniques to aggregate (ie. ensemble) the predictions from a pool of independently trained heterogeneous neural network architectures. The aggregation algorithm in this work is simply Ensemble Selection from Caruana et al., and random search / evolution methods are considered for automatically constructing the neural architectures in the base learner pool, rather than requiring these are predefined. ","This paper proposes a novel approach for searching ensembles of neural networks to improve the quality of uncertainty estimation and  performance improvement under the dataset shift. The main idea of the paper is to construct a pool of architectures, from which a new ensemble member is selected using a forward greedy algorithm. The paper is empirical, and all claims are strongly supported by thorough experiments. ",The paper explores whether one can use Architecture Search to enhance ensemble diversity. They start with the observation that embeddings generated by different architectures (for multiple different initialization per architecture) are well separated from each other. They then try out a couple of architecture search methods to find ensembles with diverse architectures that minimize the loss.,"This paper introduces ‘Neural Ensemble Search’, a method for (automatically) constructing ensembles of NNs which (in contrast with most iterations of deep ensembles) generally do not have the same architecture. The authors show that their method consistently outperforms baseline ensemble methods in accuracy and uncertainty calibration. They hypothesize that the success of this method is mainly due to the varied architectures leading to increased diversity of the predictive distributions of ensemble members, and they provide evidence supporting this claim.",0.00819672131147541,0.07377049180327869,0.03278688524590164,0.08196721311475409,0.05333333333333334,0.013333333333333334,0.04,0.015625,0.046875,0.0,0.013333333333333334,0.140625,0.07272727272727272,0.1282051282051282,0.0625,0.01818181818181818,0.038461538461538464,0.01818181818181818,0.038461538461538464,0.0,0.010152284263959392,0.0967741935483871,0.04519774011299435,0.1,0.05755395683453237,0.015384615384615384,0.0392156862745098,0.01680672268907563,0.04225352112676056,0.0
4,SP:94f20c8d3fe45e4943690240a38f325fb377bf3a,The paper brings very little novelty or insight. It is unclear that the introduced architecture complexity worth marginal improvements (given high variance and only 5 random seeds) on 2 out of 11 tasks (5 from the main paper and 6 from appendix). This might be a good workshop paper but it clearly does not meet the high acceptance threshold of ICLR.,"This work presents a simple technique (Flare) to incorporate explicit temporal information to enable effective RL policy learning in challenging continuous control environments using pixel-based state representations. The approach is inspired from recent advances in the video recognition approaches which employ optical flow information and late fusion to incorporate temporal information. Typically, RL algorithms employ a frame-stacking heuristic to incorporate temporal information (early fusion). Though computing optical flow is slow and can been prohibitive for real-time applications, the authors present a simple alternative to using optical flow, i.e. difference of latent state vectors as a proxy for explicitly encoding motion information with latent vectors representing the observed state (late fusion). Experimental results on challenging continuous control tasks in the DMControl Suite show that Flare can achieve up to 1.9x higher scores than a baseline algorithm (RAD) which uses the frame-stacking heuristic to incorporate temporal information.",The paper proposes a simple modiﬁcation to existing deep RL architectures when the input is a sequence of frames. It makes use of a specific encoding of the frames and a specific way of combining CNN and fusion of the information from the different frames. This architecture does not require any additional auxiliary tasks or supervision signals as compared to traditional RL losses. The results of the paper show that the developed approaches are significantly better than baselines.,"This paper proposes adding temporal information in the form of latent flow to deep RL agents. The idea is simple: temporal differences between latent embeddings of consecutive frames are computed; the differences are then concatenated to the embeddings for downstream processing by an RL controller. This idea is inspired by idea of late fusion in video processing and stands in contrast to the current approach of frame-stacking, which is more akin to early fusion. The authors have validated their approach on a series of DMC tasks as well as Atari games.","In this paper, the authors modify two state-of-the-art reinforcement learning algorithms (RAD-SAC and Rainbow-DQN) by changing the way information from multiple timesteps is aggregated. Instead of stacking frames at a pixel level, the authors propose to encode each frame individually and then pass the encodings and their pairwise differences to the actor/critic networks. It is shown that when learning policies from pixels this way of fusing information substantially outperforms the baseline variants in 3/5 environments from the DeepMind control suite and 5/8 environments from Atari2600. Furthermore, experiments with low-level observations on 5 DeepMind Control Suite tasks support the particular choice of information fusion over using recurrent networks or stacking states directly without including state differences. ",0.0,0.06666666666666667,0.0,0.03333333333333333,0.013333333333333334,0.05333333333333334,0.006666666666666667,0.02564102564102564,0.05128205128205128,0.03296703296703297,0.0,0.05128205128205128,0.0,0.016260162601626018,0.02564102564102564,0.08791208791208792,0.008130081300813009,0.02197802197802198,0.032520325203252036,0.024390243902439025,0.0,0.057971014492753624,0.0,0.02185792349726776,0.017543859649122806,0.06639004149377595,0.007326007326007327,0.02366863905325444,0.03980099502487563,0.028037383177570093
5,SP:98450ef54b363e5e68b19b7bc1327490d69825fa,"The paper presents a simple addition to the Balanced Accuracy approach - which the authors refer to as ‘importance’. However, there is nothing in the formulation of this concept which requires that this is an importance and could in fact be any form of weighting. The paper evaluates the new metric - but only agains the Balanced Accuracy metric (which seems quite restrictive).","This paper presents a weighted balanced accuracy to evaulate the performance of multi-class classification. Basically, the performance for a multi-class problem can be evaluated by decomposing the original multi-class problem into a number of binary ones based on one-against-rest manner, and then evaulating the performance scores for each of the binary ones using any well-known metric for binary classification, and then, aggregating the performance scores. The main aim of this paper is to present a weighting scheme when aggregating the scores.","The authors advocate for class-stratified weighted macro-averaging as an appropriate scalar-valued evaluation for classification under class-imbalance. In the absence of domain expert specified weights, they also propose a weighting function that emphasizes rare classes (referred to as rarity), and a multi-importance criteria based on a normalized product of weightings. Furthermore, they point out that these weightings can be used for training via existing methods in commonly used tools (e.g., class-based instance weighting, class-based loss scaling). Finally, they show performance under different weighting for three log message classification tasks, a sentiment classification task, and a URL classification task (e.g., malware, NSFW, phishing),  — demonstrating that different weighings lead to different orderings of evaluation results and that these weights can be effectively used in training.","In this paper, the authors have proposed a novel evaluation framework for imbalanced data classification. Specifically, the proposed evaluation metric is designed to improve the grading of multi-class classifiers in domains where class importance is not evenly distributed. Generally speaking, the problem authors paid attention to is really existed and is important in imbalanced classification problem. Moreover, the writing in this paper is very well and is very easy to follow. ","The paper presented a simple yet general-purpose class-sensitive evaluation framework for imbalanced data classification. Their framework is designed to improve the grading of multi-class classifiers in domains where class importance is not evenly distributed. They provided a modular and extensible formulation that can be easily customized to different important criteria and metrics. Experiments with three real-world use cases show the value of a metric based on our framework, Weighted Balanced Accuracy (WBA), over existing metrics – in not only evaluating the classifiers’ test results more sensitively to important criteria but also training them so.",0.06666666666666667,0.05,0.016666666666666666,0.05,0.046511627906976744,0.05813953488372093,0.06976744186046512,0.007692307692307693,0.023076923076923078,0.323943661971831,0.046511627906976744,0.023076923076923078,0.014084507042253521,0.03125,0.03076923076923077,0.07042253521126761,0.0625,0.014084507042253521,0.03125,0.23958333333333334,0.0547945205479452,0.031578947368421054,0.015267175572519085,0.038461538461538464,0.037037037037037035,0.06369426751592357,0.06593406593406592,0.009950248756218905,0.02654867256637168,0.2754491017964072
6,SP:b4f4d7fa65a97351a51241ef5aca4f8315c35c5b,"The paper proposes a new loss to improve test-time BN adaptation for domain adaptation. The proposed loss consists of two components: the diversity maximization loss and the confidence maximization loss. Specifically, they use a running estimate for the diversity loss based on KL divergence. They propose the hard and soft likelihood ratio for the confidence loss which has large gradients for high confidence predictions.   ","In the spirit of full disclosure: I have recently reviewed this paper, and several parts of my previous review are still applicable, thus I am copying in these parts when appropriate.  This paper presents a method for test time adaptation based on several techniques. These include a self-supervised adaptation objective based on log likelihood ratios, an additional regularizing objective to encourage diverse predictions, and an input transformation module that is also trained with the aforementioned objectives. Together, these techniques lead to better performance on ImageNet-C and ImageNet-R compared to Tent, a recent and similar test time adaptation method based on entropy minimization.","This paper presents a method for test time adaptation based on several techniques. These include a self-supervised adaptation objective based on log likelihood ratios, an additional regularizing objective to encourage diverse predictions, and an input transformation module that is also trained with the aforementioned objectives. Together, these techniques lead to better performance on ImageNet-C and ImageNet-R compared to Tent, a recent and similar test time adaptation method based on entropy minimization.","Fully test-time adaptation is a promising approach to handling data shift and making the DNNs more robust. This paper presents a new method for fully test-time adaptation. Specifically, they propose a non-saturating surrogate of entropy and combine it with the diversity regularizer. They empirically demonstrate the efficacy of the proposed method, comparing it against a recently proposed test-time adaptation method, Tent, using various backbone networks. ","Test-time adaptation by entropy minimization can help models adapt to dataset shifts like corruptions without altering training. This work extends tent, an entropy minimization method, by proposing alternative non-saturating losses, adding a diversity regularizer, and adapting the input data along with the model parameters. The input is adapted by applying a convolutional image transformation model between the input and classification model. These extensions do not need more optimization iterations or supervision than the baselines: the method adapts online and efficiently without auxiliary supervision. Experiments on the corruption benchmark ImageNet-C and the newer benchmark ImageNet-R report reduced generalization error. The improvements are there but marginal, and they are consistent across multiple baseline architectures (ResNet, DenseNet, MobileNet, etc.). However, the clean accuracy reduced, so the proposed method does not strictly dominate prior work.",0.03125,0.03125,0.09375,0.046875,0.7019230769230769,0.09615384615384616,0.0673076923076923,0.136986301369863,0.0958904109589041,0.10294117647058823,0.019230769230769232,0.0273972602739726,0.08823529411764706,0.022388059701492536,1.0,0.14705882352941177,0.05223880597014925,0.14705882352941177,0.05223880597014925,0.05223880597014925,0.02380952380952381,0.029197080291970802,0.0909090909090909,0.030303030303030304,0.8248587570621468,0.11627906976744186,0.058823529411764705,0.14184397163120566,0.0676328502415459,0.0693069306930693
7,SP:c1bfd8893b1f9e54afbe95410eae68f08eed1f9d,"This paper proposes a new algorithm called TOM2C to solve the multi-agent reinforcement learning problem. To achieve goals in the MARL problem, communication between agents is important. However, it is often challenging due to scalability and communication costs. To solve this problem, the authors adopt the Theory of Mind to multi-agents. The agent infers the mental states and intentions of others upon partial observation. TOM2C has two kinds of agents: a planner that decides sub-goal and reaches a consensus, and a low-level executor that takes actions. The authors also provide a communication reduction method based on CTDE.","* The paper proposes a Target-oriented Multi-agent Communication and Cooperation mechanism (ToM2C) using the Theory of Mind.   * It uses an observation encoder, ToM net, message sender, and decision-maker. The ToM net is designed for estimating the observation and inferring the goals (intentions). The message sender uses a graph neural network using inferred state and intention to generate local graphs. Once the agent receives the messages, it decides its goals using the inferred goals of others and received messages using the actor-critic.   * The empirical results of the model on the multi-sensor multi-target covering scenario is presented and compared with 4 different baselines from state-of-the-art MARL methods - HiT-MAC, I2C, A2C  and Heuristic Search algorithm.  ","This paper proposes a multi-agent communication and cooperation framework, ToM2C, where agents need to decide who and when to communicate with, and how to allocate subtasks (specifically, subtasks are defined by physical locations that agents need to navigate to / cover). The key idea is to train the agents to infer others' observations and goals, which may in turn helps achieve more effective and efficient communication/cooperation. The method adopts ToMnet-like architecture for a ToM module, and uses GNNs to fuse information and inference from multiple channels. Compared to the previous GNN architectures for multi-agent policies, it proposes two new designs, inferred-goal filter and communication reduction, which are enabled by the goal inference. The evaluation was done in a 2D continuous environment simulating a target coverage task. Compared to recent approaches, HiT-MAC and I2C, the proposed method reaches a higher reward with slightly fewer messages in this single task.","The paper presents a new method for communication and cooperation in multi-agent settings. The method relies on modelling other agents' intentions and internal states using Theory of Mind based neural nets. The predictions from the ToM model are used to decide how to communicate and coordinate with other agents. The authors test the method on two common multi-agent cooperation tasks to achieve SOTA communication efficiency and reward performance. The authors also show the utility of modelling mental states and using communication through ablation studies. Finally, the model shows flexibility in generalization, with consistent performance across different settings.","This paper addresses the problem of reasoning about the mental states of other agents in multiagent reinforcement learning using Theory of Mind, focused on cooperative MAS domains such as multi-sensor target tracking.  A deep learning architecture is proposed that combines four networks to create a centralized solution: (1) one network that uses attention to encode observations for communication messages, (2) another than estimates the observations and goals of the agents in the environment (generalizing prior work to more than two cooperating agents), (3) a third that learns a graph determining who should communicate with whom, and when communication should happen, and (4) an actor-critic network for determining the policies of agents.  Experimental results varying the number of sensors and targets in the environment demonstrate the performance of the proposed solution against baselines.",0.09,0.07,0.11,0.08,0.06666666666666667,0.06666666666666667,0.06666666666666667,0.05263157894736842,0.046052631578947366,0.061224489795918366,0.075,0.046052631578947366,0.11224489795918367,0.06015037593984962,0.05263157894736842,0.08163265306122448,0.06015037593984962,0.08163265306122448,0.05263157894736842,0.045112781954887216,0.08181818181818183,0.05555555555555555,0.11111111111111112,0.0686695278969957,0.058823529411764705,0.07339449541284404,0.0632411067193676,0.064,0.04912280701754385,0.051948051948051945
8,SP:f88a0263fe87db598ed9d3b537430324ee29ddf2,This paper describes a novel methodology of volume coding for encoding and decoding sentences. The algorithm is based on arithmetic coding. The qualitative result reveals that the generations from AriEL to be more valid comparing to other models.     ,"The paper proposes to use an autoregressive language model's hidden state to index multidimensional arithmetic coding representations of vocabularies and sentences. This way the latent space's geometry is directly connected to the words chosen at the cost of sensible inductive bias between similar sentences. Evaluations of unconditional samples on toy data and small-scale real data are made. While interpolations are presented, the proposed model and the autoencoders it compares to are only ever evaluated for fluency and grammaticality, but not for semantic relatedness.","This paper introduces an entropic coding method that is able to compress a sentence into a latent space and perform text generation through a uniform sampling from the implicit latent space. Experiments are conducted on a toy dataset and a narrow domain text dataset to show the effectiveness of the proposed method w.r.t. AE, VAE, and Transformer models. ","This paper proposes AriEL, a sentence encoding method onto the compact space [0, 1]^d. It leverages essences of arithmetic coding and kd-tree to encode/decode sentences with a fixed region of the space. With the property of arithmetic coding, in theory, it can map sentences with any lengths into individual values, and any points on [0, 1]^d can map back into corresponding sentence. Although the method relies on neural network based LMs to assign sentences into corresponding regions, the generality of mapping between any sentences/points is kept while changing the LM's behavior. The idea is interesting.","This paper proposes a sentence embedding called AriEL. Specifically, based on arithmetic coding and k-d trees, AriEL maps sequences of discrete data into volumes in the latent space, and can then retrieve sequences by random sampling. AriEL is compared to other standard techniques such as Transformer and Variational Autoencoders. Results show that it can generate more diverse and valid sentences.",0.02702702702702703,0.02702702702702703,0.05405405405405406,0.13513513513513514,0.023529411764705882,0.023529411764705882,0.047058823529411764,0.05084745762711865,0.06779661016949153,0.06,0.011764705882352941,0.01694915254237288,0.02,0.08333333333333333,0.03389830508474576,0.02,0.06666666666666667,0.03,0.06666666666666667,0.1,0.01639344262295082,0.020833333333333336,0.029197080291970802,0.10309278350515463,0.027777777777777776,0.02162162162162162,0.05517241379310345,0.03773584905660377,0.06722689075630252,0.075
9,SP:fbcb2bbd4ca1133e8ae2178e02d3a7393ec4e05d,"This paper presents an interesting idea of using neural-network-based RL to solve a type of vehicle routing problems, where the vehicles are tasked with visiting spatial locations to deliver items, and are subject to load capacity and delivery time constraints. In order to solve this problem, the authors propose an encoder-decoder architecture to decide for each robot, where to move next. The encoder is inspired from the Covariance Compositional Networks and the decoder utilizes an attention module, and the network is trained via REINFORCE. The results show that the proposed method outperforms the baseline in unseen test cases, in terms of task completion rate and the specified cost function.","This paper proposed a neural architecture for learning to solve multi-robot task allocation (MRTA) problems. The MRTA problem is modeled as a MDP, and then the so-called covariant attention-based neural architecture (CAM) is proposed. The main paper contribution is the CAM architecture.  Case studies are presented in which the proposed CAM is compared with other learning based and non-learning based methods. In the reported experiments CAM obtained smaller errors (average cost) and smaller processing time. ","This paper proposes a new method, including neural network architecture, for solving time-constrained multi-robot task allocation (MRTA) problems. The proposed approach models the target problem as a Markov Decision Process (MDP) over graphs and use Reinforcement Learning (RL) methods to solve the problem. The proposed learning architecture is called Covariant Attention-based Mechanism (CAM).  The architecture is shown to have better performance than an existing state-of-the-art encoder-decoder method regarding task completion, cost function, and scalability. Though the performance is still lower than non-learning-based baseline methods, i.e., BiG-MRTA, the computational cost is significantly smaller than the baselines. ",The paper proposes a graph learning approach for solving the multi-robot task allocation (MRTA) problem. It frames the problem as a Markov Decision Process (MDP) and trains a policy with a graph neural network architecture using REINFORCE. Results show that the proposed approach scales better compared to a non-learning baseline and is more accurate than a multi-headed attention (MHA) approach.,"This paper considers the multi-robot task allocation problems. To address the limitations of existing studies, such as real-world constraints, larger-sized problems and generalizations, this paper proposed a learning architecture, Covariant Attention-based Mechanism. They further conduct adequate evaluations and the results have shown great improvements over the state-of-the-art methods.  ",0.02702702702702703,0.07207207207207207,0.04504504504504504,0.02702702702702703,0.20512820512820512,0.11538461538461539,0.10256410256410256,0.18095238095238095,0.10476190476190476,0.06451612903225806,0.038461538461538464,0.0761904761904762,0.08064516129032258,0.05555555555555555,0.1523809523809524,0.14516129032258066,0.14814814814814814,0.3064516129032258,0.2037037037037037,0.07407407407407407,0.031746031746031744,0.07407407407407408,0.057803468208092484,0.03636363636363636,0.17486338797814208,0.1285714285714286,0.12121212121212123,0.22754491017964074,0.13836477987421386,0.06896551724137931
