,paper_id,summary,summary,summary,summary,precision,precision,precision,precision,precision,precision,recall,recall,recall,recall,recall,recall,fmeasure,fmeasure,fmeasure,fmeasure,fmeasure,fmeasure
,,0,1,2,3,0-1,0-2,0-3,1-2,1-3,2-3,0-1,0-2,0-3,1-2,1-3,2-3,0-1,0-2,0-3,1-2,1-3,2-3
0,SP:0085e0bb1a265a3925540fbc4873aae60b8d01ce,"- The authors propose a method of learning a consolidated image feature representation from a collection of related task-specific teachers that transfer well on novel recognition tasks.  - To achieve it, the authors utilize multi-teacher multi-task model distillation framework that jointly distills one or several task-specific teachers with a generalist one (trained with Imagenet dataset).   - Each teacher is set to operate on a different set of classes, and a multi-head student is trained to emulate all teachers.  - Experimental results show that the proposed method doesn’t need to revisit original training data of each teacher, but gains better performance. ","This paper studies the problem of multi-model consolidation, which aims to merge (consolidate) the knowledge from multiple models into one model so that we can better transfer this merged model for downstream tasks. Different from the traditional knowledge distillation or multi-teacher distillation, the proposed method aims to transferability of merged models for the downstream tasks, whilst achieving good performance on source tasks. The key technique (technical contribution) is to add a generalized model during the knowledge distillation, which can be regarded as s ""special"" teacher model.    The authors conduct extensive experiments on multiple datasets and most of the experiments are under a few-shot linear probe transfer learning setting.","The paper proposes a task-agnostic Knowledge Distillation method that includes a generalist model as an additional teacher, to limit student forgetting and representation collapse. The authors call this method representation consolidation and claim it can benefit both related and unrelated downstream tasks. The authors experiment with ImageNet and iFood image classification tasks, the accuracy of the proposed method is generally higher than using Knowledge Distillation only.","The paper introduces a representation consolidation method to properly aggregate the pre-trained knowledge from multiple teachers for transfer learning. It claims that a generalist teacher is necessary for preserving the transferability of distilled representations, and task-specific teachers contribute to better performance in the same-domain downstream tasks. The introduced method performs on par with multi-task training while neglecting the need for teacher datasets.",0.1568627450980392,0.14705882352941177,0.18627450980392157,0.12612612612612611,0.17117117117117117,0.22388059701492538,0.14414414414414414,0.22388059701492538,0.2878787878787879,0.208955223880597,0.2878787878787879,0.22727272727272727,0.15023474178403756,0.1775147928994083,0.22619047619047622,0.15730337078651685,0.21468926553672318,0.22556390977443608
1,SP:01f652a6b323db3585376a3a8e975a73ec4fed0b,"This paper focuses on the AutoML problem for tabular data and proposes a meta-learning based novel solution. They consider the optimal transport to define distances between two datasets, utilizing the Wasserstein-Gromov distance between the distribution of the top performing hyperparameters for the respective datasets. Given this distance, they propose learning a linear transformation of existing dataset meta-features such that the Euclidean distance between a pair of datasets in this transformed space is proportional to their Wasserstein-Gromov distance. This method is termed Metabu.  The empirical evaluation compares Metabu to existing meta-learning schemes on (i) their ability to capture the desired Wasserstein-Gromov distance, (ii) their ability to find better hyperparameters via sampling without an underlying optimizer, and (iii) their ability to find better seed hyperparameters for hyperparameter optimizers. The results on the OpenML CC-18 suite with 3 machine learning models indicate that Metabu significantly improves upon existing meta-learning schemes. The paper also demonstrates how the learned linear transformation of existing dataset meta-features allow us understand the importance of different existing dataset meta-features and how these vary between machine learning models.  ","In this paper, the authors address the AutoML problem, which aims to automatically select the best ML algorithm and its hyperparameter configuration for a dataset, and propose an approach to this problem that learns meta-features of the dataset. The proposed method, MetaBu, learns new meta-features by optimal transport according to the space of distributions of hyperparameter configurations. Meta-features in MetaBu is known only once and induce a topology in a set of data sets. Experiments on the OpenML CC-18 benchmark have shown that MetaBu meta-features can improve the performance of the state-of-the-art AutoML systems AutoSklearn and Probabilistic Matrix Factorization. Furthermore, the examination of MetaBu meta-features provides hints on when an ML algorithm will work. Finally, a topology based on MetaBu meta-features can estimate the intrinsic dimension of the OpenML benchmark for a given ML algorithm or pipeline. ","The paper presents an approach, called MetaBu, for learning a meta-feature embedding from an existing meta-feature space into a latent space, which is aims at being rank preserving regarding different hyper-parameter configurations. The special kind of embedding and its property of aiming at being performance preserving in the context of AutoML is the main contribution of the paper, in my opinion. The quality of the learned meta-features is assessed through different experiments such as capturing to what degree the embedding is indeed performance preserving and how well AutoML tools perform when initialized with the corresponding meta-features. Moreover, the authors provide a sensitivity analysis of relevant hyper-parameters of MetaBu and demonstrate how to gain insights from the learned embeddings.","This paper tackles the AutoML problem. It proposes to learn a linear combination of manually designed meta-features, which aligns meta-features with the space of hyper-parameter configurations via an Optimal Transport procedure. Experiments on OpenML benchmark demonstrate the power of the proposed method on boosting AutoML systems.",0.18617021276595744,0.13297872340425532,0.11170212765957446,0.16326530612244897,0.16326530612244897,0.10483870967741936,0.23809523809523808,0.20161290322580644,0.42857142857142855,0.1935483870967742,0.4897959183673469,0.2653061224489796,0.208955223880597,0.16025641025641024,0.1772151898734177,0.17712177121771214,0.24489795918367346,0.15028901734104047
2,SP:04f90c10f4ceca0dace727ad875265ce405fff9f,"The authors employ RNN models under different conditions to model MEC cells with both well-understood (e.g., place cells) and poorly-understood (heterogeneous cells) tuning properties. By training the network to minimize the place cell-mediated path integration, the network performed incredibly well in explaining MEC-cell dynamics. By removing neurons in the model, the network's path-integration performance was found to be at least as dependent on the model's heterogeneous neurons as on the stereotypical ones. The authors then finish by introducing a reward-based foraging model, again finding responses that match experimental results very well.","The authors address two important questions regarding the non-grid cells in MEC. First, is their response consistent, and not just noise? Second, can they be used as criteria for testing models of MEC functionality? The second question allows to probe the functional significance of these cells, and the effect of incorporating them on various model properties. The authors’ main tool is a linear mapping from a population of cells into a single target cell. This is done using rate maps of cells, and cross validation on the spatial bins. Several existing models are compared, with one model singled out as corresponding best to experimental data. The functional relevance of grid and non-grid cells is evaluated using ablation studies on the model, where path integration ability is used as a measure. Generalization of the models is assessed using either different arenas or a reward-biased setting.  ","By adopting 'similarity transform' methods, this study compares the predictability of different computational models of MEC neurons. The best model not only can explain the firing patterns of grid cells but also that of heterogeneous cells. These findings further allow them to examine the role of heterogeneous cells which has not been clearly identified with experimental approaches. This study suggests that the heterogeneous cells might play critical roles in path integration even more than what grid cells contribute. This manuscript is well-written, and the results provide a theoretical foundation for future studies using a more experimental approach. I only have minor clarification questions and suggestions to embrace a broader readership such as neuroscientists. ",The authors used linear regression to first demonstrate the robustness of non-grid-like responsive cells in experimental recordings of MEC cells. They then used the same regression to assess the similarity of responses derived from the goal driven training of RNNs to MEC responses. They found that responses derived from RNNS contained these non-grid-like responses and explained neural activity better than a low-rank decomposition of place fields.  These non-grid-like responses were crucial for path integration and likely explained the difference in similarity matching between goal-driven training and low rank decomposition.,0.19,0.15,0.13,0.1564625850340136,0.16326530612244897,0.14035087719298245,0.1292517006802721,0.13157894736842105,0.13402061855670103,0.20175438596491227,0.24742268041237114,0.16494845360824742,0.15384615384615385,0.14018691588785046,0.1319796954314721,0.17624521072796934,0.19672131147540986,0.15165876777251186
3,SP:05c61145f3fc9486728aca19c4543065fe04e99c,"In this paper, the authors use a causal view to investigate the OOD effect on the explanation evaluation of GNNs. They find the confounder between the extracted subgraphs and the model prediction, which makes the evaluation less reliable. To solve this problem, the authors proposed a deconfounding evaluation method based on the front-door adjustment from causal discovery. To generate a reliable surrogate subgraph, they proposed a generative model, which contains three losses for training. The experimental results show the effectiveness of the proposed method (DSE).","The paper sheds an exciting light on the problem of producing a meaningful evaluation of GNN explanation methods (at least a subset of them). The idea is to introduce a deconfounder D to capture the effect of OOD explanations. The authors make an interesting example for a well-known synthetic dataset where the weight of the explanation in the ground truth is lower than a clear non-valid explanation when evaluated using the model to explain. The introduction of the deconfounder D creates a spurious path between the graph variable and the explainer variable. To mitigate this effect, then, they introduce a front-door adjustment to the causal graph. The front door adjustment requires a graph generator and authors use a novel Conditional-VGAE to generate graphs that will also cover the OOD case. The paper finally presents some experiments showing the evaluation method in action.","This paper has done an excellent work of finding the out-of-distribution between the subgraph and graph as the confounder. Further, this paper proposes a conditional variational graph auto-encoder in assessing the causal effects of subgraph on the prediction. They also introduce a surrogate variable to denote this out-of-distribution effect. Through adversarial training, the effects of the proposed model is correctly verified.  ","This paper presents a novel explainer-agnostic method to adjust the biases of feature importance scores of feature attribution for GNNs. The paper first describes the feature importance scores of the GNN feature attribution framework have biases due to the out-of-distribution (OOD) problem. The subgraph important scores are calculated by inputting a subgraph instead of data graphs, but subgraph patterns can fall into regions outside the distribution of training data graphs. To address this problem, the paper proposed a method to generate surrogate graphs within the data graph distribution by CVGAE to make a front-door adjustment for deconfounding these biases by distribution shift. Experiments using several state-of-the-art GNN explainers shows demonstrated the effectiveness of the proposed framework. ",0.313953488372093,0.23255813953488372,0.3372093023255814,0.1232876712328767,0.18493150684931506,0.3181818181818182,0.18493150684931506,0.30303030303030304,0.23577235772357724,0.2727272727272727,0.21951219512195122,0.17073170731707318,0.23275862068965517,0.2631578947368421,0.27751196172248804,0.16981132075471697,0.20074349442379183,0.22222222222222224
4,SP:07fa7cd4344d77a6e0f180d4f251cc8356b5202f,"This paper studies reinforcement learning with a 1-hidden layer neural network and low-rank polynomial function approximation schemes.   Under mostly standard assumptions about realizability, completness etc, they provide algorithms mainly focusing on sample complexity in cases when there is access to a perfect generative model of the transitions and in the fully online setting, where planning is not possible in episodic reinforcement learning.  They improve upon the sample complexity with respect to existing baselines for the respective function approximation schemes.",This paper focuses on analyzing sampling efficiency of nonlinear reinforcement learning with neural network approximations of the $Q$ function. It reduces the sampling complexity from $O(d^{\text{poly}(1/\epsilon)})$ to $O(\text{poly}(d) \exp(k))$ where $k$ is the width of the hidden layer of a two-layer fully connected neural network. The idea is described accurately and the paper is overall well written. ,"The paper considers RL with neural function approximation, extending significantly previous works that only focuses on linear feature approximation. In particular, the authors propose (1) an efficient algorithm using two-layer NNs in a generative model assuming completeness, and (2) an efficient algorithm with two-layer NNs but only assuming realizability.  The results significantly extends prior research on linear methods, or methods with bounded eluder dimension.","This paper studies sample efficient RL with two-layer neural network function approximation with a generative model. In particular, they prove that they can learn an optimal policy with poly samples with only realizability in deterministic MDP, and that the sample complexity for learning a near-optimal policy in stochastic MDPs is in an order of $\epsilon^{-2}$ in two cases: policy completeness assumption and Bellman completeness assumption. The algorithms they use are based on simple (non-optimistic) value regression. ",0.18518518518518517,0.16049382716049382,0.2345679012345679,0.16417910447761194,0.19402985074626866,0.24242424242424243,0.22388059701492538,0.19696969696969696,0.2375,0.16666666666666666,0.1625,0.2,0.2027027027027027,0.1768707482993197,0.2360248447204969,0.16541353383458648,0.17687074829931973,0.21917808219178084
5,SP:0823bd0dbb8045648e81a4c93e9782069cf2c605,"This paper provides several results on the rank of the Hessian of neural networks. The main results are:  (A) upper bounds on the rank of the Hessian of a linear neural network (i.e., identity activations). These are proved by separating the Hessian into the sum of the “outer-product Hessian” and the “functional Hessian”, and bounding the rank of these separately. (B) a conjectured formula for the exact Hessian rank that matches experiments, and is also quite close to the upper bounds. (C) experimental evidence that these results essentially hold for neural networks with nonlinear activations as well, even after training. (D) for 1-hidden layer nonlinear networks, under a mild assumption a very weak (but still nontrivial) bound on the rank is obtained. ","This paper analyzes the rank of the Hessian matrix of fully connected neural networks. Theoretically, for a linear fully-connected neural network with square loss, the authors provided an upper bound for the rank of Hessian with respect to the population loss. They also did many experiments for small neural networks on MNIST and CIFAR-10 to show that their upper bound is almost tight, and works for different types of non-linear activations, losses, and initializations.",This paper derives tight upper bounds on the Hessian rank for deep linear networks. They show that these rank formulae (derived for the linear case) faithfully capture the numerical rank in the non-linear regime. They also demonstrate that these rank bounds hold during the course of training.,"This paper studies the rank of Hessian for deep linear networks. Theoretically, with squared loss, and assuming the weight matrices are full-rank, this paper provides rank upper bound for outer-product, functional and full Hessian, and empirically verifies that the bounds are almost tight. This paper further shows empirically that the numerical rank with nonlinear networks can still be captured by the linear rank bound, and that the Hessian rank decreases during training. Finally, Hessian degeneracy results for 1-hidden-layer networks and extension to the case with bias are also provided.",0.208,0.144,0.224,0.18181818181818182,0.2857142857142857,0.4166666666666667,0.33766233766233766,0.375,0.3010752688172043,0.2916666666666667,0.23655913978494625,0.21505376344086022,0.25742574257425743,0.2080924855491329,0.25688073394495414,0.224,0.25882352941176473,0.28368794326241137
6,SP:090dc0471d54e237f423034b1e1c46a510202807,"The authors propose a new architecture for visual recognition. Like standard convolutional architectures, the representation is structured in a multi-scale hierarchy, with deeper layers being coarser and more abstract. At every level of the hierarchy, a coarse, global representation is appended and simultaneously processed with the more fine-grained, local one. Local features are processed with depth-wise convolutions, and global ones with multi-head self-attention. In addition to this separate processing of local and global features, they are co-processed by cross-attending between local and global features.   In addition to this recognition backbone, the authors adapt Feature Pyramid Networks in a similar spirit. Whereas FPN's use dense convolutions to fuse an intermediate representation with a coarser one, the authors instead use the proposed ""Dual-Stream"" block, combining local depth-wise convolutions, global self-attention, and local/global cross-attention.   The authors evaluate the recognition backbone for classification on ImageNet and detection and instance segmentation on COCO, reporting large gains over standard convolutional architectures such as ResNet, and reasonable gains over more recent transformer-based ones.","This paper proposes a hybrid convolution+transformer model for 2d images. To summarize the model, it is broken up into 4 stages that operate at successively higher resolutions much like a typical Resnet.  At each stage, there are two parallel paths, one that apples a depthwise convolution at high resolution (capturing local effects), and one that applies a standard a multi-headed attention at coarse (capturing global effects) resolution.  These paths are fused by a final attention module which allows for local to attend to global and vice versa.  Another contribution is the DS-FPN (which simply slots one of the above “dual stream” modules into the lateral paths of a typical FPN). Experiments show strong performance on both image classification for ImageNet and detection/segmentation for COCO (relative to competing Transformer-only models like Deit). ","This paper proposes Dual-stream Network (DS-Net) for visual recognition. The two streams correspond to high-resolution convolutions for local features and low-resolution vision transformers for global patterns. The paper proposes an intra-scale propagation module to process the two streams and an inter-scale alignment module to fuse them into one. For object detection, it designs a Dual-stream FPN (DS-FPN) that adds dual-stream blocks to every feature pyramid. The proposed DS-Net achieves good results for image classification, object detection, and instance segmentation.","This paper presented a Dual-stream Network for image classification. It combines the representation of local and global pattern features by using self-attention and convolution together. It also proposed an Intrascale Propagation module to process two different resolutions in each block.  In addition, it also introduced such block to FPN to demonstrate the benefit.",0.16666666666666666,0.12777777777777777,0.09444444444444444,0.16911764705882354,0.11029411764705882,0.2247191011235955,0.22058823529411764,0.25842696629213485,0.3090909090909091,0.25842696629213485,0.2727272727272727,0.36363636363636365,0.18987341772151897,0.17100371747211895,0.14468085106382977,0.20444444444444446,0.1570680628272251,0.2777777777777778
7,SP:09f080f47db81b513af26add851822c5c32bb94e,"The paper proposes an autoencoder architecture that decouples point cloud’s representation into the global latent vector and spherical canonical embedding (in $\mathbb{S}^2$ space) for each of its points. It is trained in the unsupervised way on the category-specific point clouds. This canonical map turns out to be robust to missing parts due to a specially crafted regulariser. What’s new related to the prior art is that the method is somewhat robust to initial orientation of the shape. The formulation is simple and elegant, however, the method is tested only on synthetic ShapeNet data, so it is unclear whether it would work well in a more practical setting. There are also other methodological concerns and clarity problems.","The paper suggests an end-to-end learnable method to find dense 3D point correspondences of point clouds. The authors first project the input point clouds onto a canonical spherical UV space, where corresponding points of different shape instances map to similar coordinates. Then, an instance-based feature vector is concatenated with the UV coordinates and fed to a decoder to retrieve the original point cloud. Since the UV coordinates hold correspondence information, the output point cloud is, in fact, an ordered one.","The paper proposes a canonical point autoencoder for the purpose of unsupervised learning dense correspondences between 3D shapes of the same category. The canonical space is defined on a 3D sphere, but with extra flexibility that the template needs not to fully cover the full sphere. This gives the flexibility to generalize to non genus 0 shapes. The proposed method seems to give noticeable improvement over existing ones.",This paper presents a novel solution for learning dense correspondence between un-aligned 3D shapes in an self-supervised manner. The key contributions this paper is to design a novel canonical space for representing all 3D shapes and learning an auto-encoder to learn a mapping to and from an input point cloud to this canonical space. The effectiveness of this approach is shown for the 3D semantic keypoint transfer and part segmentation transfer tasks across different categories.,0.1652892561983471,0.1487603305785124,0.11570247933884298,0.1927710843373494,0.18072289156626506,0.23529411764705882,0.24096385542168675,0.2647058823529412,0.1794871794871795,0.23529411764705882,0.19230769230769232,0.20512820512820512,0.19607843137254902,0.19047619047619047,0.1407035175879397,0.2119205298013245,0.18633540372670807,0.2191780821917808
8,SP:0a92939e6a1c88bfeb4fd1dea9ee7be4fd60d967,"This paper studies the problem of training GNNs on large-scale graphs. Specifically, it proposes to use sparse moving average for sampling-based GNN training strategies. The convergence rate of the proposed approach has been proved under several assumptions. The authors conduct experiments on several large-scale datasets to show the effectiveness and efficiency of the proposal.","This paper studies neighbor sampling techniques for training GNNs. Previous work has observed that sampling-based GNN training can be formulated as a Stochastic Compositional Optimization (SCO) problem. The authors argue that naive implementation of existing SCO algorithms incurs huge memory cost for training GNNs. This paper propose a Sparse Stochastic Compositional (SpSC) gradient method, which only stores the data for nodes sampled in the past few iterations. Convergence analysis on SpSC is provided and empirical results show that SpSC has better performance than naive SCO. ","This paper proposed an improved variant of the Stochastic Compositional Optimization (SCO) framework to train GNNs, replacing all nodes' moving averages with a sparse representation. The proposed algorithm only requires a fixed-size buffer, regardless of the graph size, solving the memory issue of SCO algorithms and making it practically applicable to large graphs. The paper showed that the proposed algorithm preserves the convergence rate of the original SCO algorithm and experimentally validated that the algorithm could outperform the traditional Adam SGD for GNN training with a small memory overhead.","This paper formulated the graph neural network as a stochastic compositional optimization problem and then developed a new optimization algorithm to train GNN. However, there are some errors. It is not ready for publication.",0.2631578947368421,0.22807017543859648,0.10526315789473684,0.18604651162790697,0.12790697674418605,0.1111111111111111,0.1744186046511628,0.14444444444444443,0.17647058823529413,0.17777777777777778,0.3235294117647059,0.29411764705882354,0.20979020979020976,0.17687074829931973,0.13186813186813184,0.18181818181818182,0.18333333333333335,0.16129032258064516
9,SP:0b23c5683b72dac05a7436cf3b49bd76263801d9,"This paper proposes a new attention mechanism in vision transformers, which is called quadtree attention. This quadtree attention mechanism builds token pyramids and computes the attention in a coarse-to-fine manner. At each level, only top K regions with the highest attention scores from query and key are selected for further attention computation on finer tokens, while other regions simply use (current) coarse attention as a part of output. In the empirical study, the proposed quadtree attention achieves good performance on a wide range of vision tasks (e.g. feature matching, stereo matching, classification and object detection) with less computation.","This paper aims to address the quadratic computational complexity of vanilla Transformers. The key idea is to build token pyramids and computes attention in a coarse-to-fine manner, which reduces the computational complexity to linear. The resulting attention paradigm forms a quadtree structure, and two variants, QuadTree-A and QuadTree-B, are further proposed to improve message aggregation. Experiments are conducted on four different computer vision tasks involving either self-attention or cross attention. Results show that the proposed approach matches the state of the art with much fewer FLOPs and model parameters.","The paper proposed an attention approach to handle the global or long-range attention by leveraging the idea of quadtree structure, meanwhile reducing the quadratic complexity of original attention operation to linear. Experiments are performed on several tasks, e.g. feature matching, image classification, and objection detection. Superior results are achieved on these tasks. ","The paper proposes an efficient attention algorithm based on quadtree for vision transformers. It establishes a feature pyramid for tokens and aggregates sparse key-value features in each pyramid level. The method achieves competitive performance on the stereo, image classification and object detection.",0.25742574257425743,0.16831683168316833,0.1782178217821782,0.14893617021276595,0.0851063829787234,0.2037037037037037,0.2765957446808511,0.3148148148148148,0.4186046511627907,0.25925925925925924,0.18604651162790697,0.2558139534883721,0.26666666666666666,0.21935483870967745,0.25,0.18918918918918917,0.1167883211678832,0.2268041237113402
10,SP:0c522ffa2c90eb88296ad0c7999200a72b8755e2,"This paper considers one common semi-supervised learning algorithm, pseudo labeling, and studies this problem from theoretical point of view. Specifically, it derives an information theoretic upper bound on generalization error in each iterative update of pseudo labeling. They separate the bound into two main parts: one depends on the mutual information between the data samples and model parameters, and the other depends on the KL distance between the underlying data distribution and pseudo labeled samples from previous iteration. Their main conclusion is that as the number of labeled and unlabeled samples grows, the first term vanishes, but the second term does not necessarily vanish.   In the rest of the paper, the authors rely on the simple example of binary Gaussian Mixture Model to give a more understandable and sensible calculation of the their upper bound. Namely, they calculate the KL distance and mutual information terms in the main theorem and study the behavior of generalization error for this model. The conclusion they made is that the iterative pseudo labeling can decrease the generalization error for only the first few iterations and after than has no effect on reducing the generalization error. ","This paper provides a generalization error bound for iterative semi-supervised learning (SSL) algorithms using information-theoretic principles (see Theorem 1). To provide more intuitions, the authors first work with a simple model, i.e., the binary Gaussian mixture model (bGMM). It is shown in bGMM that when the class conditional variances are not too large, the upper bound on the generalization error decreases monotonically with the number of iterations, but quickly saturates. The theoretical results on the simple model are corroborated by experiments on MNIST and CIFAR datasets, where similar phenomena are observed, i.e., the generalization error improves after several pseudo-labelling iterations, but saturates afterwards.",The paper considers the problem of semi-supervised learning where pseudo-labeling is used to iteratively assign labels for unlabelled data batches to enlarge the labelled dataset for subsequent re-training of the classification model. The paper first adapts the recent results of Bu et al (2020) and Wu et al (2020) to this set-up and provides a general information-theoretic upper bound for the generalization error of such a learning algorithm. The paper then specializes its set-up to a binary classification problem with Gaussian class conditionals and presents its corresponding generalization bound. Additional experiments are performed on the more practical datasets with deep neural network classifiers. ,The paper considers a popular approach to semi-supervised learning based on iterative pseudo-labeling the unlabelled data and refining the model parameters thereafter. The paper is supported theoretically for the case of the Gaussian mixture model establishing a generalization error bound based on the KL divergence between the pseudo-labeled and true data distributions. The paper is also supported empirically for binary classification examples coming from CIFAR10 and MNIST datasets ,0.14583333333333334,0.140625,0.16666666666666666,0.18518518518518517,0.16666666666666666,0.22935779816513763,0.25925925925925924,0.24770642201834864,0.4507042253521127,0.1834862385321101,0.2535211267605634,0.352112676056338,0.18666666666666668,0.17940199335548174,0.24334600760456274,0.18433179723502305,0.2011173184357542,0.27777777777777785
11,SP:0c55b1f5e544e1e9510a12981107ae6c9f1eeb2e,"This paper proposes a k-medoid solution for active learning in the context of domain adaptation. The paper builds on top of Mansour et al. (2009) and looks at the discrepancy between source and target distributions. Looking at the whole hypothesis space is very conservative since this would include hypotheses that the learner would never consider as a labeling function. In order to deal with this problem, this paper only considers localized discrepancy (Zhang et al. 2020) where we only consider the hypotheses that are epsilon away from labeling function for source domain, i.e. we are only considering labeling functions that are epsilon away from the source labeling function. Generalization bounds are derived using Rademacher average and localized discrepancy for general loss functions. From these bounds, the paper shows that one can minimize the target risk by solving a k-medoid problem.  ","This paper studies active learning for domain adaption for a set of Lipschitz functions. The paper proposes to use a localized discrepancy to restrict the relevant candidate hypotheses. Under Lipschitzness, the localized discrepancy is further relaxed into a distance measure over the X domain; the authors also design an accelerated K-medoid algorithm to minimize such distance. In special cases (under certain simplifying assumptions), theoretical guarantees presented in this paper show advantages over previous ones. The proposed algorithm also shows empirical advantages over existing ones.","This paper proposes an active learning method to efficiently select data in the target domain that is suitable for learning hypotheses together with data in the source domain in domain adaptation problems.   The authors first propose to evaluate the dissimilarity between the source and target domains by localized discrepancy, where the hypothesis set is restricted to includes only hypothesithes with sufficiently small errors with the label function of the source domain on the training data (the union set of the input of the source domain and the input of the target domain to be labeled). Then an upper bound of the expected risk for the target domain based on this discrepancy. The main theoretical contribution is that the authors have shown that the discrepancy term in this upper bound is further bounded from above by the sum of the distances between the inputs of the training data and the inputs of the target domain, and this is the background for the design of the active learning algorithm.   In domain adaptation, the goal is to solve the problem of minimizing the discrepancy between the two domains. From the statements of this theorem, it can be replaced by the problem of selecting K data in the target domain so as to minimize the sum of the distances between the inputs of the training data and the inputs of the target domain. The latter problem is equivalent to solving the K-medoids problem for clustering, and the authors' technical contribution is that they have proposed an accelerated algorithm for this problem.   Finally, the authors conduct an evaluation experiment of the proposed method under various scenarios of domain shift using three types of benchmark data. ","The paper studies the problem of active learning for domain adaptation. The paper has two main contributions. First, the problem of the selection of a query target sample set is studied theoretically. A generalization bound is presented, which is based on the recently proposed concept of localized discrepancy as opposed to rather classical discrepancy measures that can be too conservative. The second contribution of the paper is an algorithm for the batch selection of K target queries, which is motivated by the theoretical findings. ",0.13286713286713286,0.2727272727272727,0.15384615384615385,0.3058823529411765,0.21176470588235294,0.11743772241992882,0.2235294117647059,0.1387900355871886,0.2619047619047619,0.09252669039145907,0.21428571428571427,0.39285714285714285,0.16666666666666666,0.18396226415094338,0.19383259911894274,0.14207650273224046,0.21301775147928995,0.18082191780821916
12,SP:0d77c22df0830cb675b11ad883d014e3a1933c8e,"This paper analyzes the impact of the choice of ReLU'[0] on the fidelity of neural networks. Even though one would expect that the value of ReLU'[0] should not have any noticeable impact, the authors notice the divergence in learned parametric values that depend on this choice. They also notice that this effect is more pronounced at lower precision (due to rounding effects that are dominant at fp16 and fp32 vs. fp64). They also study this effect for neural network sizes, topologies (including networks with and without batch normalization) and datasets.",The ReLU function is not differentiable at 0 which has a mathematical impact on ReLU neural networks. Nevertheless they are optimized using gradient based methods. The folk wisdom is that singular points only occur with zero probability and therefore the issue is irrelevant. This paper challenges this folk wisdom and identifies that the choice of the derivative at 0 does have a tangible impact on the optimization and classification performance.,"This paper studies a long-neglected problem: the gradient of the non-smooth activation function in training a neural network. Specifically, the authors study the numerical influence of the ReLU derivative at zero on gradient descent. First, a theory is presented to illustrates the trivialness of ReLU'(0), then, the authors give counter-theory experiments to show that ReLU'(0) can affect training results.  In the last two sections, authors demonstrate that low-precision training, SGD optimizer, no batch norm training can magnify the impact of ReLU'(0).","The paper demonstrates empirically that on a variety of architectures and datasets, the derivative of ReLU at 0 is evaluated a significant number of times during training in 16 and 32 bit precision. These events are frequent enough such that defining ReLU'(0) to be anything other than 0 (default value in common DNN frameworks) hurts generalization, and sometimes training as well.  The authors perform a variety of investigations around this effect, such as the influence of training set size, network width, depth, batch size, BatchNorm, Dropout. ",0.15217391304347827,0.1956521739130435,0.17391304347826086,0.17142857142857143,0.17142857142857143,0.20454545454545456,0.2,0.20454545454545456,0.1839080459770115,0.13636363636363635,0.13793103448275862,0.20689655172413793,0.17283950617283952,0.2,0.1787709497206704,0.15189873417721517,0.15286624203821655,0.2057142857142857
13,SP:0d7cbb544bc39203c9c18b4fee56fc94cbe78375,"This paper investigates using the lottery tickets hypothesis (LTH) strategy for pruning neural network weights for speech recognition. The method first explains the general LTH framework and extensions with transfer learning scenarios. The paper shows the effectiveness of the proposed method in the standard ASR tasks, pre-training with other models, and transfer learning, especially in noisy speech recognition tasks.  Some comments: - In the introduction ""End-to-end automatic speech recognition (ASR) (Wang et al., 2019)"": (Wang et al., 2019) is not a representative paper of end-to-end ASR - In the introduction, ""For example, the recognition of speech recorded by distant microphones is challenged by acoustic interference such as noise, reverberation and interference speakers (Kinoshita et al., 2020)"": Again not sure (Kinoshita et al., 2020) is a correct citation. I think the following papers are more appropriate   - Haeb-Umbach, Reinhold, et al. ""Speech processing for digital home assistants: Combining signal processing with deep-learning techniques."" IEEE Signal processing magazine 36.6 (2019): 111-124.   - Kinoshita, Keisuke, et al. ""The REVERB challenge: A common evaluation framework for dereverberation and recognition of reverberant speech."" 2013 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics. IEEE, 2013. - Currently, data augmentation techniques (artificially contaminate the clean speech with various noisy data) are more standard than transfer learning for noisy speech recognition. Please discuss it. - Section 2 ""On the model level, compared to CV models, speech models are mostly based on RNN backbones"": I'm confused about this description. As you mentioned in the previous paragraphs, many speech models now use transformer, conformer, and CNN. - Section 2 ""with 10ms shift is 1,000"": this is not exactly true as we usually use downsampling at the beginning of the encoder layer. - Section 3, RQ3: Frankly, I don't think this is an essential question for me. This sounds very trivial that the use of pre-trained models would improve the performance. - Section 4, What is the main message of Figure 1? I don't find the particular effectiveness of the proposed method from this figure/example. - Section 4, Table 3: Why are the other methods, especially KD-based methods, so worse? - Section 5 ""A common way to address this issue is through speaker adaptation (Zhao, 1994;"": Again, I don't think (Zhao, 1994) is a representative paper for speaker adaptation. MAP/MLLR adaptation techniques exist in this era, and they were standard techniques in the GMM era. - Section 5: I recommend the authors use the real noisy speech data like the CHiME data   ","In this paper, authors propose to use lottery ticket hypothesis (LTH) for ASR model pruning. The whole idea generally inherits from the original LTH paper. The model is first trained from scratch. Then you collect all the model weights and sort them. A proportion of the smallest weights are set and fixed to 0. Given this sparsified subnetwork, one further repeats the whole process and prune more parameters. After several rounds, if the pruned subnetwork still gives at least the same performance of the original full network , it is called a ""winning ticket"". The ""winning ticket"" subnetwork should be lightweight, transferrable, and noise-robust. This work applies LTH to 3 models (CNN-RNN CTC, RNN-T, Conformer) on 3 datasets (TED-LIUM, Common Voice, LibriSpeech). The authors empirically show  1. The winning ticket exists for each method. The subnetwork could be only 4.4% size of the original full model, but still matches the performance. 2. Compared to other pruning methods, LTH performs best. 3. LTH with pretrained model performs better than LTH with random initialization.  Furthermore, the paper shows 1. One can enforce block sparsity for pruning to facilitate chip design. 2. The winning ticket can transfer to a new dataset and work well. 3. The pruned network is noise-robust. ","This paper conducts comprehensive analyses of lottery tickets hypothesis on automatic speech recognition. The authors verified the existence of highly sparse “winning tickets” in ASR task, and analyze its robustness to noise, transferable to other datasets, and supports with structured sparsity. They conduct experiments on different model structures (CTC, RNN-T, Transfomer) and different datasets (LibriSpeech, TED-LIUM, Common Voice) with extensive analyses. ","The paper explores the lottery ticket hypothesis, i.e., the existence of highly sparse subnetworks that can be trained in isolation without sacrificing the performance of the full models within the context of ASR. The authors show that it is possible to find subnetworks which contain just a fraction of the number of parameters of the full models without sacrificing performance on ASR. Three different architectures are considered and results reported on 3 datasets. Additional studies investigating the transferability and robustness of the sparse subnetworks are also conducted.",0.10238095238095238,0.05476190476190476,0.06428571428571428,0.09433962264150944,0.11320754716981132,0.23809523809523808,0.2028301886792453,0.36507936507936506,0.3068181818181818,0.31746031746031744,0.2727272727272727,0.17045454545454544,0.13607594936708864,0.09523809523809525,0.10629921259842519,0.14545454545454548,0.16,0.19867549668874168
14,SP:0db83e057c21ac10fe91624876498d8456797492,This paper proposed a method for driving policy learning based on human-AI copilot. The algorithm learns from human interventions and also tries to minimize the total efforts of human intervention. Comprehensive experiments and comparisons with multiple baselines show that the proposed algorithm can achieve high sample efficiency and reduce unsafe events. The contributions are in the design of the copilot learning method.,"In this work, the authors propose a new algorithm for data-efficient human-in-the-loop learning, Human-AI Copilot Optimization. The main idea is to have experts intervene during training in cases in which unsafe situations arise. The HACO learned policy utilizes a multi-task objective: doing well relative to a learned value function (based on human interventions), keeping an exploratory policy, and keeping human interventions at a minimum. Experimentally, HACO seems to be able to drastically reduce the amount of number of environment training timesteps required to reach basic competencies to the agent in the test environment, while maintaining good task and safety performance.","The paper proposes HACO, a human-in-the-loop reinforcement learning method that safely trains an agent to imitate expert behavior while minimizing the number of expert interventions required. The key idea is to have a human watch over the agent (e.g., in a simulated driving environment), and take control whenever the agent enters unsafe states. HACO uses offline RL to train the agent to imitate the actions taken by the human during these interventions. To discourage the agent from intentionally visiting unsafe states in order to trigger human interventions, HACO also assigns a negative reward to the state transitions preceding a human intervention. Experiments with human participants in a simulated driving task show that HACO trains the agent to achieve higher success rates (in a test environment, without a human in the loop) than baseline methods based on imitation learning and offline RL, while requiring less training data and incurring a lower cumulative training cost.","This paper presents HACO, a human-in-the-loop learning algorithm that aims to learn imitative driving policy while minimizing the number of human interventions. HACO builds on CQL and operates under the no-reward assumption. HACO learns a proxy action-value function by penalizing the policy’s action and maximizing during human interventions. It additionally adds an entropy term to encourage exploration. The policy trains by maximizing the proxy action-value, and penalizing an accumulative intervention cost, computed using the cosine difference between the human and the policy actions. HACO is evaluated in a closed-loop driving simulator. HACO outperforms the selected imitation and offline RL baseline and is on par with RL methods which have access to environment rewards. It is also orders of magnitude more sample efficient than standard RL methods.",0.2698412698412698,0.36507936507936506,0.30158730158730157,0.2830188679245283,0.22641509433962265,0.24203821656050956,0.16037735849056603,0.1464968152866242,0.1417910447761194,0.1910828025477707,0.1791044776119403,0.2835820895522388,0.20118343195266272,0.20909090909090908,0.1928934010152284,0.22813688212927757,0.19999999999999998,0.2611683848797251
15,SP:0e0adc42f6025034d341dc9c17b3f6251afebc2f,"Let $X$ be an instance space, $Y$ a set of labels, $D$ some underlying (hidden) distribution over $X \times Y$. This work studies a new method for converting the output of a probabilistic predictor (e.g. a deep net) to a good prediction set: that is a mapping from $C: X \to 2^Y$, such that for most samples $(x,y) \sim D$, $C(x)$ contains $y$. Formally, the authors study this problem in the PAC setting with covariate shift. The learner is given access to the score function, a labeled sample from the source distribution $P$ over $X \times Y$, and unlabeled samples from a target distribution $Q$ over $X \times Y$ whose marginal over $X$ may be shifted from the source. The goal is to output a prediction set which is as small as possible while still retaining PAC guarantees with respect to the shifted target distribution. This is a well-motivated model in practice. Probabilistic outputs of modern neural nets can be difficult to interpret, and small prediction sets may be useful in settings where one wishes to avoid or check more carefully a few marked outputs (e.g. a problematic medical diagnosis).  Assuming known, bounded importance weights, the authors provide an algorithm satisfying the PAC guarantee by maximizing a cutoff value for the score function that performs well over an empirical sample. They ensure their guarantee holds over the target rather than source distribution by rejection sampling to simulate the target distribution. The authors also extend this to settings where the importance weights are unknown but can be estimated from unlabeled samples. Finally, the authors provide experimental evidence over a couple common settings of covariate shift. They show that their algorithm outperforms baseline methods in the literature in the sense that it maintains PAC guarantees while outputting a smaller prediction set  in expectation. ","This paper proposes a new method to construct approximately correct (PAC) prediction sets for uncertainty quantification in the presence of covariate shift. It is a natural and interesting extension of the previous works [Park et al. 2020a, 2021] on PAC prediction sets. The building blocks this paper took from these previous works are the optimization problem in Equation (1) and the Clopper-Pearson confidence intervals for the Binomial distribution in Section 2.2. The extension is based on a rejection sampling Clopper-Pearson bound given in Section 3.2.  The authors propose an algorithm with and without access to the true important weights. The algorithm is evaluated in the settings of “rate shift” and “support shift” on the DomainNet and ImageNet datasets. The experiments show that the algorithm gives the smallest prediction sets among approaches that always satisfy the PAC constraint. ","The paper presents algorithms for PAC prediction sets under the assumption of covariate shift. Using estimated/known importance weights that encode the shift, the method optimizes for the smallest subset of labels such that with high probability the error of this prediction set is low. A rejection sampling based strategy is then shown to satisfy the PAC constraints. The methodology is extended to the case where the importance weights are uncertain (due to estimation error). It is shown that the robust variant can be solved using the extreme case weights and the rejection sampling based algorithm. Simulations show the efficacy of the method.","This paper is concerned with learning prediction sets (in a PAC sense) under the covariate shift assumption, given a model $f(x,y)$. The form of the sets is restricted to $C_\tau(x) := \{y : f(x,y) \ge \tau\},$ and the problem is set up as learning a $\tau$, using a  labelled source dataset $S_m \sim P^{\otimes n}$ and an unlabelled target dataset $T_n \sim Q^{\otimes n}$ such that with high probability over $S_m, T_n,$ $Q(Y \in C_\tau(X)) \ge 1-\varepsilon,$ where $\varepsilon$ is a given target coverage level. It is desired that $\tau$ is as large as possible to minimise the size of the prediction sets learned.  The main scheme is presented modularly. The paper first describes how finding the maximum $\tau$ whilst ensuring that the number of points in $S_m$ it captures is large enough (as specified by using a Binomial tail inverse) gives valid sets when $Q = P$. Next, it is argued that when $Q$ is absolutely continuous with respect to $P$, and the derivative $w(x) = \frac{\mathrm{d}Q}{\mathrm{d}P}(x)$ is upper bounded and known, then one can importance sample the set $S_m$ to generate a sample from $Q$, which can then plug into the previous procedure. This step fundamentally uses the covariate shift assumption, and, to my understanding, is folklore. The following, then, constitute the main technical contributions.  Next, the assumption of exact knowledge of $w$ is relaxed, and it is argued that if instead for each $x_i \in S_m,$ bounds $\underline{w}_i \le w(x_i) \le \overline{w}_i$ are available, then one can produce a worst-case estimate of the coverage of $C_\tau$ for any $\tau$ (by taking points that were missed to have high weights, and points covered to have low weights), and if this pessimistic coverage also satisfies the constraint. So, for each $\tau$, the worst $w_\tau$ can be produced, which can then feed into the importance sampling procedure above.  Then, it is pointed out that such a confidence bound on the $w_i$s can be learned using a probabilistic classifier $s(\cdot|x)$ to separate data from $S_m$ and $T_n$, which leads to the main proposed algorithm. While it is roughly justified in the appendix that an accurate estimate can be obtained under smoothness assumptions with an appropriately fine gridding of the space (and a consequently huge sample complexity), the concrete proposal is to replace this step with a heuristic method for fitting bounds on the weights, and then plugging these into the above strategy. Note that this is not implemented as a direct optimisation over $\tau$ - instead a set $\mathcal{T}$ of possible values is pre-selected, and the procedure is executed for each $\tau$ (the algorithm recommends an increasing order on the same).  Finally, the paper presents experiments in the DomainNet dataset, and in the ImageNet dataset, where the shift in the latter corresponds to adversarial perturbations. The principal baselines are the weighted split conformal inference (WSCI) method, which in my opinion is an appropriate choice, and the ""PS-C"" method, which simply uses a prediction set that has $1-\varepsilon/b$ coverage on the source data, where $b$ is an estimated upper bound on $w$. The proposed method is ablatively presented, and it is seen that on just the DomainNet data, the method PS-R (which simply takes $(1-s)/s$ as an estimate of $w$) is both reliable and performs well, while the PS-M method, which further integrates samples in bins tends to be slightly optimistic on this dataset. Conversely, in the adversarially perturbed dataset, PS-R produces trivial prediction sets (since presumably this shift goes entirely outside the source domain's support), while PS-M performs well. The proposed method, PS-W, which further uses upper and lower bounds on $w$s after integrating them over bins, tends to be pessimistic (error-rates of $0.06-0.07$ are observed when only $0.1$ is demanded), but is not too much worse than either of these methods, and is effective in both types of shifts.",0.13355048859934854,0.10423452768729642,0.22149837133550487,0.20567375886524822,0.375886524822695,0.4563106796116505,0.2907801418439716,0.3106796116504854,0.09770114942528736,0.2815533980582524,0.07614942528735633,0.06752873563218391,0.1830357142857143,0.15609756097560976,0.13559322033898305,0.23770491803278687,0.1266427718040621,0.11764705882352942
16,SP:0e13f831c211626195c118487f2fff36a6e293f6," The paper proposes to apply a version of Gromov-Wasserstein divergence to graphs. In particular, they relax the weight constraint of the second graph  and try to find a minimizer of the Gromov-Wasserstein distance. They named it Semi-relaxed Gromov Wasserstein distance (srGW) and apply it to solve some problems in graphs, namely, graph partition, graph clustering and graph completion. The experiments were carried out on synthetic data and real dataset such as: Wikipedia hyper link network, Amazon product network etc. The results show comparable and better performance over other methods such as  GW and spectral GW.","This paper proposes a semi-relaxed Gromov-Wasserstein (GW) dissimilarity for learning tasks on graphs. Here the ""semi-relaxed"" refers to removing one of the marginal constraints, with the practical effect that during GW graph matching, one is able to ignore some of the nodes in the source or target and thus obtain matchings that better respect the structure of the problem. What is quite interesting is that this simple relaxation immediately yields benefits in fundamental tasks such as graph partitioning, and the authors validate this observation with extensive experiments on graph datasets. The empirical results are strong, and show state-of-the-art performance compared to several recent baselines.","In this paper the authors propose a modification of the Gromov-Wasserstein problem for matching given matrices $C$ and $\bar{C}$ with its respective histograms $h$ and $\bar{h}$, relaxing the restriction on $\bar{h}$. The problem is then given in an equivalent formulation, which is solved via Conditional Gradient.  The authors propose applications with graphs, such as graph partitioning, clustering, and completion via Graph Dictionary Learning. ","The paper proposed a relaxed version of the Gromov-Wasserstein (GW) distance between two probability distributions on (graph) structured data. The main idea of new divergence is to relax the target distribution via an optimization setting. Based on the proposed divergence, the paper also introduced new formulations for existing problems of learning with graphs such as graph clustering (partitioning), graph dictionary learning, clustering of the graph, and graph completion. The experimental results for those applications on graph data have shown the efficiency of the proposed divergence.",0.24489795918367346,0.15306122448979592,0.25510204081632654,0.17272727272727273,0.23636363636363636,0.31343283582089554,0.21818181818181817,0.22388059701492538,0.29069767441860467,0.2835820895522388,0.3023255813953488,0.2441860465116279,0.23076923076923075,0.18181818181818182,0.2717391304347826,0.21468926553672318,0.26530612244897955,0.2745098039215686
17,SP:0e5812d8ed33d5b6d9d59dbb2312c7b1c9363f3d,"This paper proposes a measure of function complexity that is based on a function itself rather than the function class it belongs to. It uses this complexity measure to establish generalization upper bounds for classifiers. In addition, an empirical method for approximating this measure, and optimizing it, is proposed and evaluated on three small benchmark datasets, exhibiting slightly improved test accuracy over existing baselines. The method is also shown to improve resilience to label noise.",The paper proposes a novel and theoretically inspired regularization techniques for training deep neural networks. The technique involves regulating the network by regressing the output of the model to a simpler model but the simpler model also regresses towards the output of the base model. The experimental results show that the proposed techniques are effective at reducing the generalization gap and improving performance in presence of label noise.,"This paper proposes a Kolmogorov Growth (KG), a novel measure of the complexity of a neural network. They derive new generalization bounds using KG and take inspiration from this bound to propose a new way of regularizing neural networks (network-to-network regularization, N2N). Finally, the paper verifies their regularization scheme on three standard image benchmarks.","This paper addresses the fundamental problem of the generalization ability of deep neural networks and aims to shed light on the theoretical aspects of their generalization ability by introducing a approach based on a new complexity measure called ""Kolmogorov Growth"" (KG). A practical neural network training approach called Network-to-Network (N2N) Regularization is also introduced with the aim of enforcing the low KG condition so that the generalization gap is reduced. Theoretical results are accompanied by relevant experimental evaluations, including a noisy-label case investigation, which helps in validating the proposed KG-based N2N training of neural networks. Overall, the material introduced is seemingly novel, and although tries to address a well-established problem, the results presented are good guidance for research extensions in this direction.",0.13333333333333333,0.17333333333333334,0.2,0.14705882352941177,0.2647058823529412,0.30357142857142855,0.14705882352941177,0.23214285714285715,0.11811023622047244,0.17857142857142858,0.14173228346456693,0.13385826771653545,0.13986013986013987,0.1984732824427481,0.1485148514851485,0.16129032258064516,0.1846153846153846,0.18579234972677597
18,SP:0e8c3a3dba649d496292b41228801feb8507d3b4,"This paper presents a self-supervised learning approach using contrastive loss for representation learning of genomic sequences. The contrastive loss has been used for self-supervised learning in the NLP and computer vision domains and the paper presents its application for genomics. Self-supervised contrastive learning tries to maximize the agreement between augmented views of a sample. Therefore, Self-GenomeNet splits a sequence into two subsequences, and the learned representation of subsequence 1 is compared to subsequence 2 and its revers compliment (positive samples) as well as other sequences (negative samples). The described method has two considerations specific to the genomics tasks - (1) it handles variable sequences (2) it incorporates reverse complement information of the sequences. The method is applied on two prediction tasks and one transfer learning task using sequences from viral, bacterial, and human genomes. Its performance is compared to the supervised model, generative language model, and self-supervised learning models - CPC and Contrastive-sc. The results show improved classification performance over the baseline for both supervised retraining and semi-supervised training settings. ","This submission introduces self-genomenet, a self-supervised training method for learning from DNA sequences. The self-supervision is done by predicting the end of a sequence from its start (both broken into smaller subsequences), through a contrastive loss against other random sequences. The predicted part is also reverse-complemented (RC), making the network learn the expected reverse-complement invariance of the prediction function. The method is extensively tested on several learning tasks, where it shows good performances.","Recently several works have proposed semi-supervised learning methods to leverage unlabeled biological sequences for learning their general-purpose representations. In this work, the authors proposed the Self-GenomeNet, a novel contrastive learning method for nucleotides based on the reverse-complement (RC) context prediction. First, given a sequence, they divide it into two subsequences and transform one into its RC. Then, the model is trained to distinguish their representations from those of other random nucleotide sequences. The authors claimed that the proposed method considerably outperforms previous self-supervised baseline models on three benchmark datasets in both self-supervised and semi-supervised evaluation ","The authors proposed a self-supervised learning method for nucleotide-level genomic data utilizing reverse-complement of genomic sequences. The proposed method achieved a considerable performance improvement. In addition, the authors proposed an architecture called Self-GenomeNet that handles varying-length genome sequences.",0.16,0.19428571428571428,0.08571428571428572,0.23076923076923078,0.1282051282051282,0.1568627450980392,0.358974358974359,0.3333333333333333,0.3488372093023256,0.17647058823529413,0.23255813953488372,0.37209302325581395,0.22134387351778656,0.2454873646209386,0.13761467889908258,0.20000000000000004,0.1652892561983471,0.2206896551724138
19,SP:0eaf058ed224464f6682cbbd80f716c89759f467,"This paper investigates the undesired behavior of soft actor-critic (SAC) algorithm that implementing maximum entropy reinforcement learning (RL).   One of the main contributions of this paper is the empirical investigation of the undesired behavior of SAC. By running SAC with function approximation on no-reward environment, the authors illustrate that the policy obtained by SAC failed to maximize the entropy, while maximizing the entropy is the optimal situation in no-reward environment and the theoretical study of SAC in finite MDP setups guarantees its optimal convergence. The authors pointed out a possible defect in SAC with function approximation: Q value of already visited states tend to be greater than the other states because the policy update increases the entropy of the visited states.   The second contribution is the proposal of a solution to the above mentioned defect. The authors propose Max-Min Entropy RL (MME). To improve the exploration, the soft Q-function has been revised. Instead of adding the entropy term to the Q-function, the revised soft Q-function subtracts the entropy, if I borrow the authors statement, to enhance the chance to visit rare states with low entropy. This is, however, counter intuitive to me as I will describe in the main review part.  The proposed approach has been tested on sparse mujoco tasks and delayed mujoco tasks, where the environments have sparse-reward shapes, as well as on some standard mujoco environments with dense rewards. The algorithm has been compared to different SOTA approaches. Promising performance has been observed.  ","The authors describe a positive feedback loop, that arises in SAC-based implementation of the maximum entropy reinforcement learning and hinders efficient exploration. Q values of frequently visited states are updated more, because these states are stored more into the replay buffer. The policy update increases the entropy of the visited states, i.e. if a high entropy is rewarded in the policy the frequently visited states will be selected even more. In the paper, Max-Min Entropy Rl is proposed that is supposed to break the unwanted feedback loop. It considerably improved the empirical performance in common Maze tasks and Mujoco tasks.","This paper proposes a conjecture that the agent should visit the state with low entropy and then do the exploration of this low entropy state with the maximum-entropy principle. To prove this conjecture, the author does a simple experiment on the 4-room maze and illustrates the empirical entropy of the action at different states. There is an interesting observation that the rarely visited states have low entropy maybe due to the function approximation error or off-policy learning explained by the author. Given this observation, the author proposes an algorithm which penalizes the reward with negative entropy （eq7) . Based on this modified Q function, the agent does the policy improvement as the original soft-actor-critic algorithm. The empirical result shows that the proposed algorithm outperforms the baselines such as SAC and many others.","The paper addresses limitation in max-entropy RL frameworks. These frameworks usually add a regularization term to the reward function to increase policy entropy to encourage exploration, but the effect can be limited in states where the Q-function dominates the expression. The authors propose a new objective that better encourages RL agents to visit rare states with low entropy, and show empirically that their method is able to outperform the baselines used for comparison.",0.14960629921259844,0.14960629921259844,0.1141732283464567,0.24271844660194175,0.1650485436893204,0.11764705882352941,0.36893203883495146,0.27941176470588236,0.38666666666666666,0.18382352941176472,0.22666666666666666,0.21333333333333335,0.21288515406162467,0.19487179487179487,0.17629179331306993,0.20920502092050208,0.1910112359550562,0.15165876777251186
20,SP:0ecbaf1770642b6ac5c9786ba2d18408310fc225,"This paper introduced a simple but effective method to mitigate overfitting, which modifies the existing Flooding scheme. The proposed iFlood can solve the potential problems of the Flooding algorithm and improve the stability. Furthermore, the authors gave some theoretical analyses of the proposed method. The experiments indicated the proposed method is better than the baselines. ",This paper extends the idea of Flooding to a sample-specific level and call it iFlooding. The extension is intuitively important and the authors also offered several analytical discussions to show its importance beyond the intuition. The empirical results are fairly relevant and strong. ,"The paper proposes a flooding loss function that encourages the training loss for each example to be a positive bias instead of zero. The paper has found that such training objective stabilizes the training compared to the regular Flooding, which regularizes the average loss instead. The paper also provides reasoning why such a loss function provides more robustness for training with noisy labels and biased label distributions. The empirical benefit on standard datasets seems marginal, but the benefit on noisy labels is significant.","This paper proposed a stable and effective regularizer to prevent overfitting. Specifically, this paper proposes individual Flood. Different from Flood which constrains training loss on mini-batch level, iFlood gives instance-level constraints on training loss. This paper also theoretically shows that the design of iFlood can be intrinsically connected with removing the noise or bias in training data. ",0.2,0.21818181818181817,0.2,0.22727272727272727,0.1590909090909091,0.13253012048192772,0.25,0.14457831325301204,0.1864406779661017,0.12048192771084337,0.11864406779661017,0.1864406779661017,0.22222222222222224,0.17391304347826086,0.19298245614035087,0.15748031496062992,0.13592233009708737,0.15492957746478872
21,SP:0f3fcffb6dfbf344bd5ef73c3f6d3d84d2f13887,"This paper proposes a causal inference framework for a multivariate point process model. The key idea is to form a series of binary processes Z_t and X_t using a moving window [t-w,t] to indicate whether processes Z and X have at least one event in the window [t-w,t].  An average treatment event (ATE) is defined and propensity scores are proposed for unbiased estimation of ATE. The authors use synthetic experiments and real-world datasets to demonstrate how the proposed method work compared to baseline approaches.","This paper describes a method for the causal analysis of long streams of event data that can be modeled as multivariate point processes (MPPs). The paper defines the average treatment effect (ATE) between pairs of events in MPPs that occur within a window, and derives how to estimate the ATE as the expected difference of average outcome rates.  When covariates differ or assumptions are violated, this estimate may be biased, but adjusting for the propensity score yields an unbiased estimator, which the paper does through inverse probability of treatment weighting (IPTW) (including a stabilized version).  Experiments on data from known, synthetic MPPs (proximal graphical event models and Hawkes processes) show that the proposed IPTW estimation method more accurately recovers the ATE than baseline conditional intensity scores.  Similarly, experiments on a diabetes dataset show that stable IPTW recovers more true causal pairs at 2 out of 3 levels of recall compared to the conditional intensity scores.  These results extend to using the cumulative rather than the conditional intensity rate. ","This paper proposes a model for estimating the average causal effect in point processes where the estimand of interest is the effect of an event occurring within a window on the rate parameter of a point process. The authors use the formulation commonly used for inverse propensity scores of generalized treatments (e.g., from Robins, Imai). The authors describe the properties of the propensity score as a balancing weight by leveraging known results and advocate for the use of stabilization of inverse propensity scores to reduce variance as in Robins(2000). The weights are then used within an outcome model which is a point process model. Empirical results show promising performance compared to a range of alternatives.  ","The paper provides tools to estimate average treatment effect (ATE) between event pairs. The authors first define the quantity for multivariate point processes together with the necessary concepts from Neyman–Rubin causality, then they derive a propensity score in this setting. Finally, they give a method for estimating the ATE and evaluate the method on synthetic and real world datasets. ",0.26373626373626374,0.24175824175824176,0.17582417582417584,0.18452380952380953,0.1130952380952381,0.1282051282051282,0.14285714285714285,0.18803418803418803,0.26666666666666666,0.26495726495726496,0.31666666666666665,0.25,0.18532818532818532,0.2115384615384615,0.2119205298013245,0.21754385964912282,0.16666666666666666,0.1694915254237288
22,SP:0f69e20b9f97439d19e7a93144c8d877cedcd714,The paper addresses the issue of sampling from an unnormalized distribution. The sampling problem is cast as the numerical simulation of the gradient flow associated with the KL divergence between the target unnormalized distribution and the approximating distribution. The challenging part is to estimate the density ratio that appears in the gradient term. The authors propose to use a deep neural network to estimate the density ratio. Numerical results show the usefulness of the proposed method.,The paper proposes a novel way to sample from unnormalized distributions. This is helpful when calculating or estimating the normalizing constant is untractable.  The main idea is to track the gradient flow of the relative entropy in the Wasserstein space of probability distributions. It is known that the flow converges to the target distribution and the paper introduces a variational characterization of the discretized steps. The main benefit of this characterization is that it bypasses the need to know the normalizing constant as well as being amenable to estimation by using a combined particle evolution.  The benefits of the new algorithm are demonstrated through several numerical simulations.,The paper suggests a method for approximating the 2-Wasserstein gradient flow for the relative entropy. The proposed particle-based method uses a neural network function approximation-based approach to estimating the necessary density ratios. Experiments verify reasonable performance compared to MALA and ULA.  ,"This paper considers the problem of sampling from an unnormalized distribution. The unnormalized target distribution can be regarded as a stationary point of the Wasserstein gradient flow of the corresponding relative entropy functional, which can be equivalently identified from a microscopic perspective by defining a time-varying velocity field of the particles. While the exact time-varying velocity field is not exactly available, the authors propose to estimate such a quantity by approximating the corresponding logarithmic density ratio through minimizing the Bregman score. Such an approximation requires only samples from the variable distribution which can obtain by simulating particles following the estimated velocity field.",0.34210526315789475,0.17105263157894737,0.3684210526315789,0.1308411214953271,0.19626168224299065,0.29545454545454547,0.24299065420560748,0.29545454545454547,0.2692307692307692,0.3181818181818182,0.20192307692307693,0.125,0.28415300546448086,0.21666666666666667,0.3111111111111111,0.18543046357615894,0.19905213270142183,0.17567567567567569
23,SP:0fd50d89ffec376d136aa915c9c4e6ae281f5014,"This paper focuses on variants of ExtraGradient (EG) by considering different options for the two step-sizes of the method: one for the extrapolation step and the second for the iterate update.  Building on the analysis of (Diakonikolas et al. 2021),  the paper relaxes the setup therein by allowing a larger range of values for the $\rho$ parameter (see Fig. 2) of weak Minty Variational Inequality (MVI) which parameter controls the degree of nonconvexity.  In particular, it provides the following contributions: *(i)* Primarily it defines Alg.1 where the step size for the iterate update is adaptive and shows the main convergence result (Thm  3.1.) that under some assumptions (Asm 1), Alg. 1 converges on weak MVI problem. *(ii)* It then considers a non-adaptive variant of Alg. 1, dubbed CEG+, which can be seen as a generalization of the EG+ method of (Diakonikolas et al. 2021), and complements the result of the latter by showing that the convergence result is tight on weak MVI; *(iii)* extends CEG+ by using an adaptive scheme for the first step size (for the extrapolation) which uses Lipschitz constant backtracking, and Alg.1 for the latter step size (for the iterate update), dubbed CurvatureEG+, which method is shown to empirically converge on some toy-examples on which CEG+ does not; *(iv)* finally, for the stochastic setup the authors consider one of the step-sizes to be diminishing and the other can be constant, and show that this variant converges on weak MVI problems. ","This paper proposed CEG+ and CurvatureEG+, which extend extragradient method to a proximal variant (regarding the operator $A$), and apply it in nonconvex-nonconcave minimax optimization with Weak MVI condition in both deterministic and stochastic cases, and proved their complexities, which has the same order as those in literature (e.g., Diaonikolas et al., 2021). The authors also studied the lower bound in the simpler case when $A\equiv 0$, showing a difference compared to EG+ in (Diaonikolas et al., 2021).  In the deterministic case, it proposed an adaptive stepsize strategy to allow larger range of the MVI parameter $\rho$, and further a curvature-based strategy to avoid the lower bound requirement of $\rho$. The authors also executes several experiments, showing that CurvatureEG+ can avoid cycling in the experiments.","This paper constructs methods named CurvatureEG+ (and Adaptive EG+ and CEG+), built upon a recently proposed EG+ [Diakonikolas et al., 2021], a variant of EG, that works under the weak MVI condition. Most importantly, the CurvatureEG+ (and Adaptive EG+/CEG+) works for a range (of the weak MVI) larger than that of EG+. The corresponding nonconvex-nonconcave setting includes non-trivial problems illustrated in the paper, where the proposed method converges, while other existing methods reach limit cycles. Unlike EG+, the CurvatureEG+ can handle both constrained and composite cases. A stochastic variant is also studied. Although the experiments consider toyish problems, they seem interesting.","This work extends the extragradient algorithm in Diakonikolas et al. (2021) from unconstrainted and unregularized inclusion problems that satisfy weak Minty inequality (MVI) to their constrained and regularized counterparts. Compared with the original extragradient algorithm, the extended algorithm is proved to converge with a larger range of stepsize choices and MVI-related constant $\rho$ (implying a larger set of applicable problems) in both deterministic and stochastic inclusion problems. The range of $\rho$ is also proved tight by providing a lower bound of $\rho$. The extended algorithm also generalizes the celebrated forward-backward-forward (FBF) algorithm in Tseng (2000). Finally, an improvement of this extended algorithm is proposed using Lipschitz constant backtracking. ",0.12,0.124,0.112,0.17054263565891473,0.18604651162790697,0.22115384615384615,0.23255813953488372,0.2980769230769231,0.25225225225225223,0.21153846153846154,0.21621621621621623,0.2072072072072072,0.158311345646438,0.1751412429378531,0.15512465373961218,0.18884120171673818,0.2,0.21395348837209302
24,SP:100c91da177504d89f1819f4fdce72ebcf848902,This paper proposed a novel idea that generates imperceptible adversarial attacks for ASR by perturbing the phase information. Proof that phase perturbations reduce the magnitude of the spectrogram is provided. White-box-threat-model-based experimental results showed the proposed method can generate effective adversarial examples for an academic Transformer-based ASR system with better efficiency than the state-of-the-art counterpart methods. ,This paper introduces a new way of constructing adversarial examples for automatic speech recognition using the phase of the short-time Fourier transformation. The authors conduct experiments and subjective evaluations to show how their adversarial examples compare to previous state-of-the-art algorithms. The experimental results show that the proposed phase-oriented audio adversarial samples can be produced in reduced time as well as they are less perceivable by humans.,This paper proposes a phase-oriented algorithm PhaseFool to efficiently construct imperceptible audio adversarial attacks with energy dissipation.  The authors leverage the spectrogram consistency of STFT to adversarially transfer phase perturbations to the adjacent frames and dissipate the energy that is crucial for ASR systems. Empirical evaluations show that the attack effectiveness of the proposed attack is high.,This paper proposes a novel adversarial attack method that attacks ASR network. The idea is that the attack is likely to be imperceptible when phase information is perturbed because phase information is known to give releatively small perceptible difference than magnitude information. The perturbation on phase information influences the magnitude information which is known as energy dissipation. Experiment results show that it reaches almost state-of-the-art results in terms of imperceptibility. The advantage of the proposed method is that it can generate the imperceptible perturbation using fewer steps than the previous state-of-the-art method. They also qualitatively show that it is usually the harmonic parts of speech where the energy dissipates. ,0.25,0.265625,0.421875,0.19718309859154928,0.2535211267605634,0.3620689655172414,0.22535211267605634,0.29310344827586204,0.23478260869565218,0.2413793103448276,0.1565217391304348,0.1826086956521739,0.23703703703703702,0.2786885245901639,0.3016759776536313,0.21705426356589147,0.19354838709677424,0.24277456647398843
25,SP:10de45510320b7ddb7ffb18b33e67f7cad609418,"The paper uses the recently developed framework of Taylor expansions for value functions (ref [29] in the paper), expands the results, and shows how they could be applied to Off-Policy Evaluation, and by a non-trivial extension to meta-learning. The main algorthmic contribution of the paper is the application of the ideas to meta-learning, but I feel the main contribution is a conceptual one, further developing the ideas from [29] and drawing theoretical connections between OPE and meta-learning.  The paper includes an experimental section demonstrating marginal improvement over baselines in several domains.","This paper uses Huang and Jiang (2020) to talk about gradient-based meta learning in reinforcement learning using the language of off-policy evaluation. The logical connection behind those concepts stands from the fact that policy gradient methods in reinforcement learning are in fact instances of the likelihood ratio derivative estimator, which itself stems from important sampling. Proposition 3.2 and 3.3 are obtained from Huang and Jiang. Proposition 3.1 tries to extend their results to talk about higher order derivatives. The author's finally propose to use ideas from Taylor policy optimization from Tang et al. (2020) as a variance reduction method within the doubly robust policy gradient framework of Huang and Jiang.   ","This paper considers meta-RL with gradient-based adaptations, which relies heavily on the estimation of the Hessians of value functions. Though there are previous approaches that focus on unbiased/biased estimations of Hessian. This paper gives a unified view for estimating higher-order derivatives of value functions, through the lens of off-policy evaluation. This subsumes several previous works, also sheds some light on the bias-variance tradeoff of the estimate. Empirically, the new proposed second-order estimate is also incorporated in meta-RL algorithms to showcase the empirical efficiency. ","The paper is concerned with meta reinforcement learning. The author(s) introduced a framework for learning high-order derivatives of value functions using off-policy evaluation. Specifically, the author(s) proposed to adopt existing off-policy evaluation methods to derive the value estimator and then take the derivative with respect to the target parameters to construct the estimate. This technique is further adopted to implement meta reinforcement learning. ",0.17708333333333334,0.17708333333333334,0.22916666666666666,0.13793103448275862,0.16379310344827586,0.21978021978021978,0.14655172413793102,0.18681318681318682,0.3235294117647059,0.17582417582417584,0.27941176470588236,0.29411764705882354,0.16037735849056603,0.1818181818181818,0.26829268292682923,0.15458937198067635,0.20652173913043476,0.2515723270440252
26,SP:1137ed24393a24f24e9a36e1586e6924a55d627e,"In this paper, the authors study the theoretical speedup of local SGD and one-shot averaging. A synchronization scheme that communicates more frequently at the beginning is also suggested. Some simple numerical experiments are provided to verify the theoretical results.","## Update after rebuttall  I am satisfied with the authors' response and elevate my score to 6.  ##  This paper presents several new theoretical results on the convergence of Local SGD as well as numerical experiments to check the tightness of the theory. In particular, the authors try and remove logarithmic terms from the bounds of Local SGD by making the communication less frequent as the iteration counter increases. The main results of the paper are:  1. Theorem 1 gives convergence for strongly convex functions under uniform-with-strong-growth noise. I argue in my detailed review that this result looks suboptimal.  2. Theorem 2 gives a result for PL functions with sub-Gaussian noise for one-shot averaging. This result is interesting as it promises a linear speed-up, although under somewhat restrictive assumptions.   3. The experiments show that linear speed-up is lost when the number of communication rounds is chosen smaller than predicted by the theory of Theorem 1.  I tend to vote for rejection of this work, even though I appreciate some of the results obtained here. My main concerns are 1) applicability of the message to neural network training, which is highly relevant to FL; 2) the theoretical improvements appear to be small; 3) some small flaws in the proofs.","This paper investigates the gap between local SGD and One-Shot averaging in communication efficacy and linear speed-up with respect to the number of workers. Through extensive analysis, they show that they can tighten the bound for the number of communication required for local SGD to $\Omega\left(N\right)$, which is the number of workers. They have proved this bound for strongly convex and objectives with PL condition, where the strong convexity can be a special case of it. They use the idea of adaptive local step to have a more frequent communication at the beginning of the training and less frequent one at the end. In their experimental section, they show that their proposal can converge to the same rate as other local SGD approaches and Sync SGD using  $\Omega\left(N\right)$ communication rounds.","This paper analyzes local SGD with progressively decreasing communication frequency under the homogeneous data assumption. Unfortunately, the obtained convergence rates are worse than the best-known convergence rates for FedAC (Yuan and Ma) under the same set of assumptions. The paper also analyses One-shot averaging under a slightly different set of assumptions, showing it retains its linear convergence in settings less restrictive than pre-existing literature. Since the major result of this paper is worse than FedAC's guarantee, it is very incremental.    **References**  Yuan, Honglin, and Tengyu Ma. ""Federated Accelerated Stochastic Gradient Descent."" Advances in Neural Information Processing Systems 33 (2020).",0.5,0.4,0.275,0.15023474178403756,0.11267605633802817,0.13768115942028986,0.09389671361502347,0.11594202898550725,0.10679611650485436,0.2318840579710145,0.23300970873786409,0.18446601941747573,0.15810276679841895,0.1797752808988764,0.15384615384615385,0.18233618233618237,0.15189873417721517,0.15767634854771787
27,SP:11ad277db038a77d5935e7504cc640e74bfc4efe,"The paper investigates the effects of weight initialization for deep spiking neural networks (SNNs) on training speed and final accuracy. The main finding is that popular initialization methods for conventional ANNs and RNNs do not match the specific dynamics of SNNs, in particular they ignore the need to have sufficient firing early on to generate good gradients. The paper derives a theoretical first-order approximation on the response curve, and from it a weight initialization scheme that initializes the weights in a region where neurons fire from the first epoch on, but also avoid explosion or reduction of activities throughout the network. In various experiments on standard and spiking variants of MNIST and CIFAR, the initialization scheme shows good performance and fast convergence for different neuron types (e.g. non-leaking vs. fast leaking), and different encoding schemes. For CIFAR10 the results are significantly better than for other initialization schemes."," This paper focuses on the parameter initialization problem of training SNNs. The authors derive the theoretical response of spiking neurons, and propose an initialization method based on slant asymptote, which can overcome the gradient vanishing problem. The results show that the proposed method can effectively improve training speed and accuracy.","This paper enhances the efficient BPTT training of SNNs by proposing an initialization method to match the response of spiking neurons in initial training. Their method bridges the spiking neuron response to the wisdom of traditional deep learning training, which may have an influence on future research like ANN-to-SNN conversions with LIF neurons or other SNN training methods. The authors conduct experiments on CIFAR10, MNIST, and neuromorphic datasets to show the efficacy of their technqiue. One of the main contribution is their theorteical analysis of iterative systems to model first-order integrate-and-fire neurons and investigate the response curve. ","The authors tackle the problem of improving spiking neural net training with better initialization. They observe that error backpropagation efficiency in SNNs depends on availability of responding neurons and show that traditional surrogate backprop methods are inefficient as they take many iterations to move weights to the regime supporting neuronal responses.  The authors approximate the SNN neuron model (LIF, IF etc) I/O function using a piecewise-linear  iterative expression and show that clipping the output to (0,1) and normalizing the variance of the random weight initialization to insure that neurons produce substantial responses even with initial random weights results improves SNN training substantially. The improvement is to a large extent due to early onset of convergence. The authors demonstrate the superiority of their approach on MNIST, N-MNIST, DVS-MNIST and CIFAT datasets and present extensive experiments using different optimization algorithms and response functions.  ",0.10666666666666667,0.16666666666666666,0.2,0.32,0.3,0.22549019607843138,0.32,0.24509803921568626,0.2054794520547945,0.1568627450980392,0.10273972602739725,0.15753424657534246,0.16,0.1984126984126984,0.2027027027027027,0.21052631578947367,0.1530612244897959,0.18548387096774194
28,SP:1257373629c8584c001b69677ebd73e5f0c20d08,"This paper proposes a method to estimate the epistemic uncertainty (uncertainty due to lack of knowledge/data) at a new model input. The paper takes an indirect approach towards this goal by a) first estimate the generalization error at the new input, b) next estimate the aleatoric error (inherent uncertainty in the data distribution / irreducible error), and c) subtract aleatoric error estimate from the total generalization error estimate to obtain the epistemic uncertainty estimate.  The authors claim that this method captures both the uncertainty due to lack of data, as well as model misspecification in the process - while other/existing methods focus mostly on the variance of the posterior distribution (or its approximation) as a measure of epistemic uncertainty (and thereby implicitly assume the model is well specified.)  Estimating the total generalization error itself is performed with a second neural network model, which uses residuals obtained from the primary model as its labels. Estimating aleatoric error either assumes the presence of an Oracle or   The authors apply their technique on both static (fixed data set) as well as interactive (active learning) settings, though mostly focused on the mean squared error loss function.","This paper proposes a new approach for computing epistemic uncertainty. The proposed approach, DEUP, builds a new model (in addition to the original model) which predicts epistemic uncertainty, defined as generalization error minus aleatory uncertainty. I highlight some features of DEUP below: - DEUP works with a hold-out dataset that is used for training the error predictor.  - In case there does not exist a hold-out set or in interactive settings (like RL or active learning), DEUP is extended to be used in a cross-validation setting and the features used to fit the error predictor is extended to include data density estimates and model variance. DEUP is evaluated on different settings including OOD data, sequential model optimization and RL.","This paper uses out-of-sample prediction error as a measurement of epistemic uncertainty. Using this definition, it develops an estimator: direct epistemic uncertainty prediction.  The idea is to have a main predictor to learn the task, and an error predictor to predict the generalization error. Empirical studies show that their proposed estimator produces better estimation on downstream tasks such as sequential model optimization and reinforcement learning. ","Given some supervised task, this paper redefines the uncertainty of a solution as its generalisation risk. The authors then propose to learn a secondary function to estimate the generalisation risk of the first. This is done by using held-out points as training data for the secondary model. In turn, this held out used to estimate the primary model error comes from a k-fold split.  The inputs to the secondary model are the data points being evaluated, estimates of the predictive variance of the primary model, density estimates from a generative model and whether the point has been observed by the primary model.  The authors posit that the advantage of their method is that it can capture uncertainty due to model selection bias, something omitted by existing work, which focuses on the variance of the learnt estimator. The authors provide a diverse range of experiments: OOD rejection in image classification, active learning for drug discovery, function optimisation and exploration in reinforcement learning. ",0.14583333333333334,0.09895833333333333,0.20833333333333334,0.175,0.225,0.29850746268656714,0.23333333333333334,0.2835820895522388,0.24539877300613497,0.31343283582089554,0.1656441717791411,0.12269938650306748,0.17948717948717952,0.14671814671814673,0.22535211267605634,0.2245989304812834,0.19081272084805653,0.17391304347826084
29,SP:13db440061fed785f05bb41d0767225403ecf7a1,"- The paper is about continuous learning for language models. The authors leverage existing LAMA tasks and collect a new test benchmark with updated information and new information.  - They investigate several existing CL algorithms, and they propose a new metric called FUAR to measure trade-off between forgotten time-invariant knowledge and updated or newly acquired knowledge. - They provided some findings on their continuous LM learning. ","The authors formulate a new continual learning (CL) problem called Continual Knowledge Learning (CKL). Particularly, they distinguished three sub-tasks in CKL, i.e., the retention of time-invariant world knowledge, the update of old knowledge, and the acquisition of new knowledge. They also introduce a new benchmark and metric to quantify the performance of various state-of-the-art models on these sub-tasks. They find that CKL demonstrates unique challenges that are not present in previous CL setups. Critical causes of knowledge forgetting in CKL are also discussed.","This paper presents a new continual learning problem setup: continual knowledge learning (CKL) and constructs an associated benchmark resource. The benchmark is based on slot filling-based knowledge probing tasks (i.e., the LAMA analysis). The authors show the empirical performance of some existing CL methods, ranging from regularization, rehearsal, and parameter expansion. And they show a few findings based on their experimental results, e.g., learning rate can be sensitive to balance the tradeoff between forgetting and learning new knowledge, and CKL methods might have transferrable performance across different LMs. (e.g., T5 and GPT). ","The paper studies continual knowledge learning of language models, which is an interesting and important problem. Particularly, a new benchmark and a metric are introduced to quantify the retention of time-invariant world knowledge, the update of outdated knowledge, and the acquisition of new knowledge. To establish baselines for the CKL benchmark and validate the rationality of the proposed benchmark and metric, the author conducts extensive experiments with a pre-trained encoder-decoder model (T5) based on various training methodologies including regularization, rehearsal, and parameter expansion methods.  The paper is well organized and easy to follow. The proposed continual knowledge learning problem is quite interesting and important. The FUAR metric is also technically sound. The authors also conduct comprehensive experiments to verify the rationality of the proposed benchmark under the various settings.",0.2,0.2,0.3076923076923077,0.24444444444444444,0.3333333333333333,0.22916666666666666,0.14444444444444443,0.13541666666666666,0.15151515151515152,0.22916666666666666,0.22727272727272727,0.16666666666666666,0.16774193548387095,0.16149068322981366,0.20304568527918784,0.23655913978494625,0.27027027027027023,0.19298245614035087
30,SP:14330a1a1c33ec18de096ffb038ba06f04c7dccb,"As the title suggests, the paper is a comparison of recent continual learning methods that prevent catastrophic forgetting and their effectiveness in some text classification tasks using popular pretrained language models such as BERT, RoBERTa, etc. The paper divides continual learning methods into three categories: (1) rehearsal-based, (2) regularization-based, and (3) dynamic architecture. The experimental results show that rehearsal based methods are superior to the other two, and also that BERT is generally better than other candidates. The paper then proposes a new probing techniques to find out what makes rehearsal-based method better and what's happening inside BERT. The paper finds that the last layer has the biggest catastrophic forgetting and lower layer is less impacted.","This paper explores the continual learning performance when combining different PLMs and common continual learning methods with 3 challenging NLP classification tasks.  To benchmark these combinations the methods are evaluated in task-incremental and class-incremental learning settings over various NLP end-tasks, which covers common learning settings in continual learning and NLP. There is also a layer-wise performance analysis to identify which layers keep or forget task relevant information during training.  Overall the paper shows that forms of replay outperform other methods like regularization.","The authors perform a comprehensive study of how pretrained language models work in the continual learning setting. The authors study 5 relevant pretrained language models (masked and unmasked) and somewhere between 3 and 6 continual learning strategies depending on where in the paper they are counted. In addition to a thorough everything-by-everything evaluation, the authors hone in on the details of how the different models and CL approaches are reflected in the transformer layers. The authors find that the different language models studied perform relatively differently, both qualitatively and quantitatively, and these insights may provide useful for directing future improvements."," This paper conducts an empirical study on the catastrophic forgetting of pretrained language models. On two continual learning settings (class incremental and task incremental), the paper evaluates multiple pre-trained models on different data sets, to see how severe the catastrophic forgetting issue is for these pre-trained models. Then the paper also tests the effectiveness of multiple continual learning methods on such pre-trained models and draws some conclusions. ",0.15,0.175,0.15,0.16279069767441862,0.1511627906976744,0.17647058823529413,0.20930232558139536,0.20588235294117646,0.2571428571428571,0.13725490196078433,0.18571428571428572,0.2571428571428571,0.17475728155339806,0.18918918918918917,0.1894736842105263,0.14893617021276598,0.16666666666666666,0.20930232558139533
31,SP:146ef14e569e10172a7dc602acd3fadf2c3bef8b,"The paper first demonstrates the importance of feature invariance (language invariant representations) and class-prior invariance across languages on zero-shot cross-lingual performance. By analyzing the zero-shot performance on different languages on the MARC reviews and WikiANN NER tasks, and comparing against the class conditional distance between the source language (English) and target language feature representations, the authors illustrate the how high feature invariance results in better zero-shot performance. By also synthetically modifying the class prior for the target language, the authors demonstrate how increasing differences in class priors result in decreasing zero-shot cross-lingual transfer performance.  Building on their observations, the authors propose an approach that: (i) Introduces an adversarial loss term to penalize distortion in average class conditional feature representations between the source and target languages. (ii) Adds an importance weighting term to ensure the approach doesn't fail under class prior shifts.  Empirical studies demonstrate that the proposed approach improves significantly over the vanilla zero-shot model on both MARC sentiment analysis and NER tasks, also improving over self-training on sentiment analysis. Comparisons on synthetic datasets (sub-sampled from NER and MARC datasets) that enhance the class prior shift highlight the robustness of the approach under large class prior shifts where previous approaches fail.","This paper looks at the problem of unsupervised cross-lingual transfer (termed UCL) in the paper through the optics of domain adaptation. After empirically analysing and validating that distributional shifts in class priors might cause a huge problem for UCL (which wasn't tackled in previous research), the authors proceed with an introduction of a new method that aims to mitigate that problem. The idea is to get rid of that shift through a approach called importance-weighted domain adaptation (IWDA), which is largely the adaptation of the work from Tachet des Combes et al. (NeurIPS 2020) to the UCL problem.  The results on two tasks in the UCL setup (NER and MARC classification) show slight gains over the standard zero-shot transfer when IWDA is applied, with more prominent gains reported when a stronger domain shift is observed - however, such a setup has been created mostly artificially, to further demonstrate the benefits of modelling the shift in the model.","In this paper, the authors provide substantial analyses on the cross-lingual transfer performance in the multilingual neural language models and reported that the performance is strongly correlated with representation invariance and negatively affected by distributional shift in class priors between data in the src/tgt languages. Based on these findings, the authors propose an unsupervised cross-lingual learning method, called importance-weighted domain adaptation (IWDA), where it performs feature alignment, prior shift estimation, and correction. The authors experimented on two different NLP tasks such as multilingual NER and multilingual sentiment analysis tasks, and experimentally showed the effectiveness. Besides that, they demonstrated that the proposed approach improves performance further, when combined with existing semi-supervised learning approaches.","This paper investigates a method for improving the cross-lingual transfer of pretrained multilingual models. The paper first empirically analyzed the influence of representation invariance and distributional class shift. Then, the paper proposed a method to improve the representation invariance and correcting the class shift. Experiments showed its superiority under large prior shifts.",0.1509433962264151,0.15566037735849056,0.09905660377358491,0.2125,0.13125,0.1623931623931624,0.2,0.28205128205128205,0.39622641509433965,0.2905982905982906,0.39622641509433965,0.3584905660377358,0.17204301075268816,0.20060790273556228,0.15849056603773584,0.24548736462093865,0.1971830985915493,0.22352941176470587
32,SP:1598bad835a657e56af3261501c671897b7e9ffd,"Authors have introduced the concept of anti-backdoor learning to learn clean models using backdoor-poisoned data. They have proposed Anti-Backdoor Learning (ABL) to automatically break backdoor attack during training with poisoned data. They have identified two weaknesses from backdoor attack. First weakness is that models learn from poisoned data at a much faster rate than these learn from clean data. The stronger the attack, the faster the models converge to a backdoor. The second weakness is that the backdoor task is tied to a specific class (the backdoor target class). Based on the two weaknesses, the authors designed a two-step method to learn a clean model with poisoned data.  The first step is to isolate poisoned samples from the training dataset, and the second step is to use the isolated samples to unlearn backdoor. The first step happened in the early training stage and the second stage takes over the training after the first stage. As compared to other state of the arts, the isolation step is very precise, and in many case, the precision is easily to reach 100%. The second step is very effective to unlearn backdoors. Even if only 1% of the poisoned samples are isolated, it can counter up to 70% poisoned samples to learn a clean model. Overall, this is a good paper and the proposed method outperformed the competing methods under the authors’ experimental setting. ","This paper observes that models converge much faster on backdoored data than clean data. Within a few epochs, the training loss for backdoored data will decrease to zero, but not for clean data. Based on such an observation, this paper aims to train clean models on poisoned data without knowing the backdoored portion in the training set. Specifically, it proposes a new training procedure called anti-backdoor learning (ABL), which splits the traditional training procedure into two stages. In the first stage, ABL uses a threshold to prevent loss values for individual samples being too small by flipping the sign of the loss. It then selects a small portion (1%) of the training set with small losses as the backdoored set and the remaining as the clean set. In the second stage, ABL applies gradient ascent on the backdoored set and gradient descent on the clean set. The evaluation is conducted on three standard benchmarks and ABL is compared with three existing backdoor removal baselines. The experimental results show ABL has the best performance on both reducing attack success rate and retaining normal accuracy in most cases. A list of studies on different components of ABL and comparisons with other alternative methods are interesting and informative.","This paper observes that deep neural networks learn backdoored data faster than benign samples. Based on this finding, the paper proposes Anti-Backdoor Learning (ABL), which can learn a benign model on poisoned datasets. Specifically, at the beginning of model training, ABL maximizes the training loss gap between clean samples and backdoored examples and detects backdoored data based on the differences in their loss values. Then, ABL unlearns backdoored model with the detected data.","The proposed work identifies that backdoor attack examples converge fairly quickly compared to clean examples during training, due to the nature of their objective. Using this, the authors identify backdoor examples and use a gradient ascent method to unlearn the backdoor, while gradient descent is used for clean examples. The proposed method is shown to be effective in reducing Attack Success Rate (ASR) while increasing Clean Accuracy (CA) on multiple datasets and attacks. ",0.17094017094017094,0.08974358974358974,0.09401709401709402,0.14563106796116504,0.12135922330097088,0.13513513513513514,0.1941747572815534,0.28378378378378377,0.3013698630136986,0.40540540540540543,0.3424657534246575,0.136986301369863,0.18181818181818185,0.13636363636363635,0.14332247557003258,0.21428571428571427,0.17921146953405018,0.1360544217687075
33,SP:15c243829ed3b2505ed1e122bd499089f8a862da,"The setting in the paper is the classic unsupervised domain adaptation problem, where we are given a labeled sample from a source distribution and an unlabeled sample from a target distribution. The goal is to minimize the risk on the target distribution. Theoretical results led to a breakthrough in practice - the Domain Adversarial Learning architecture (Ganin et al., 2016).   The paper suggests looking at the paper from a game theory perspective. This is natural, as the objective is to minimize the loss on the source distribution while maximizing the distinction between the distributions.  The optimal solutions of the game are characterized by the local NE.  Motivated by the results from game theory, the authors suggest replacing Gradient Descent (due to its limitation in this optimization problem) with other optimizers - ODE (ordinary differential equation) solvers.","The authors exhibit a strong link between game theory and domain-adversarial training. They show the optimal point in the latter is a Nash equilibrium of a three players game. From this perspective, the authors show that standard approaches, like gradient descent, cannot work in this setting as the method is known to be divergent in such a case. Instead, they propose to use Runge-Kutta methods (for example) to discretize the ODE, which gives insights for novel algorithms with better convergence guarantees.","This paper analyzes adversarial domain learning (DAL) from a game-theoretical perspective, where the optimal condition is defined as obtaining the local Nash equilibrium. From this view, the authors show that the standard optimization method in DAL can violate the asymptotic guarantees of the gradient-play dynamics, thus requiring careful tuning and small learning rates. Based on these analyses, this paper proposed to replace the existing optimization method with higher-order ordinary differential equation solvers. Both theoretical and experimental results show that the latter ODE method is more stable and allows for higher learning rates, leading to noticeable improvements in transfer performance and the number of training iterations. ","This manuscript considers the adversarial domain adaptation training problem, specifically the gradient reversal method, from the perspective of game theory. The authors show that gradient-based optimizers without an upper bound on the learning rate violate asymptotic convergence guarantees to local NEs. The authors further show that these constraints can be lifted by higher order ODE solvers. In the experimental part, the authors evaluate their method i.e. Runge-Kutta ODE solvers of order 2 and 4 with different general and also game optimized gradient-based optimizers on a MNIST/USPS digits dataset. Furthermore, they show hyperparameter robustness of their method and finally, the method is tested on more complex image and NLP datasets and compared to current SOTA methods. Overall better results are achieved. ",0.13432835820895522,0.17164179104477612,0.13432835820895522,0.26506024096385544,0.25301204819277107,0.24074074074074073,0.21686746987951808,0.21296296296296297,0.144,0.2037037037037037,0.168,0.208,0.16589861751152077,0.19008264462809918,0.138996138996139,0.23036649214659688,0.20192307692307693,0.22317596566523604
34,SP:15ed638782cc0398df38ec49eed5c5ca9962d3b9,"**(motivation)** Many approaches exist to generate counterfactual images, but what should be done when no training data is available? Current approaches lead to adversarial-looking approaches, which are not satisfying enough as an explanation.  **(approach)** The authors use Deep Inversion and a novel objective that ensures the data stays in distribution and propose a method to evaluate the quality of counterfactuals.  **(experiments)** The authors run experiments on Celebi and ISIC.  ","Summary: The paper proposed a novel approach to achieve deep model inversion for a pre-trained deep classifier using the classifier alone, without having access to the training dataset or generative models. Given a query image, the proposed deep model inversion generates a counterfactual explanation that is realistic looking and produces the desired classification outcome.    Earlier work on image synthesis by inverting deep classifiers either creates unrealistic images or creates perturbation of query image that successfully changes the classification decision but resulting pixel-level manipulations are not interpretable/meaningful (similar to an adversarial attack).   The proposed approach has four essential components: 1.	Counterfactual image is synthesized using an image prior. The authors experimented with an un-trained UNET-based model as a deep image prior (DIP) and Fourier mapping with SIREN activation as implicit neural representation (INR) for mapping noise to the counterfactual image. 2.	To ensure high similarity between counterfactual and query images, the authors used ISO to minimize discrepancies over the entire image and LSO to minimize differences between embeddings extracted from different layers of the deep classifier. 3.	To ensure that the generated counterfactual image is realistic and lies on the data manifold, the proposed model uses a manifold consistency (MC) loss. The MC-loss is quantified using an auxiliary loss predictor function in DEP or the radial basis kernel similarity in DUQ. Both methods provide ways to capture epistemic uncertainty due to out-of-distribution (OOD) samples. 4.	The model uses a functional consistency loss to ensure the counterfactual image produces a desired outcome from the deep classifier. Finally, the model uses progressive optimization to learn the image prior layer-by-layer.  ","The paper proposes a deep inversion approach that uses a pre-trained classifier to generate the counterfactual explanations of a given image. In other words, the method introduces small, discernible perturbations in an image that changes its class while keeping it realistic. The method makes use of certain inductive biases such as strong image priors, manifold consistency objective, and progressive optimization strategy to generate those counterfactual explanations.","In this paper authors propose to generate counterfactuals from a query image and a given trained deep classifier. One is interested to find out a semantically reasonable alternative explanation, where image would be classified from class y to y'. This is all performed without the help of an explicit generator (such as GAN). ",0.3,0.14285714285714285,0.11428571428571428,0.09057971014492754,0.06159420289855073,0.19402985074626866,0.07608695652173914,0.14925373134328357,0.1509433962264151,0.373134328358209,0.32075471698113206,0.24528301886792453,0.12138728323699423,0.145985401459854,0.13008130081300812,0.1457725947521866,0.1033434650455927,0.21666666666666665
35,SP:16618b226e42a07095dcf9204ce4c0e3b2ed8bd8,"This study investigates defense against backdoor attacks for models that have already been trained. It proposes, in particular, a min-max formulation for backdoor defense, in which the inner maximum seeks a powerful trigger that leads to a high loss, while the outer minimum seeks to suppress the ""adversarial loss"", so as to unlearn the injected backdoor behaviors. To solve the minimax, the authors also propose a method, Implicit Backdoor Adversarial Unlearning (I-BAU). In addition, the authors also provide theoretical analysis including the convergence bound and generalization bound. Extensive experiments demonstrate the effectiveness and efficiency of the proposed method.  ","The paper formulates adversarial training against backdoor poison attacks and proposes to use implicit hypergradient to solve the minimax problem instead of breaking it down into separate inner and outer optimization problems. The authors perform theoretical analysis of the algorithm and find convergence bound and generalization bound for the method. Also, the authors evaluate the proposed adversarial training routine, called I-BAU (Implicit Backdoor Adversarial Unlearning) in three attack settings:  (1) One-trigger-one-target attack, (2) One-trigger-all-to-all attack, (3) Multi-trigger-multi-target attack and compare with six defenses. For each attack setting, the authors test 7 different backdoor attacks. The results show that the efficiency of I-BAU is comparable to the best baseline for each of the attacks and is more time-efficient than other defenses. Finally, the authors show that I-BAU leads to a more stable training comparing to using universal adversarial perturbations in adversarial learning. ","In this paper, the authors proposed a minimax formulation for removing backdoors from a given poisoned model based on a small set of clean data. Unlike previous work, which breaks down the minimax into separate inner and outer problems, the proposed algorithm utilizes the implicit hypergradient to account for the interdependence between inner and outer optimization. The authors also theoretically analyzed its convergence and the generalizability of the robustness gained by solving minimax on clean data to unseen test data. Extensive experiments showed improved backdoor defense performances and less computation time on several backdoor attacks over various attack settings.","The paper studies the backdoor attack problem and proposes a minimax formulation for the defense against adversaries. Theoretically, the paper analyzes the convergence bound and the generalization bound for the proposed method. Empirically, the paper compares efficacy, stability, sensitivity, and efficiency with several competitive methods.",0.24,0.2,0.16,0.15483870967741936,0.12258064516129032,0.15151515151515152,0.15483870967741936,0.20202020202020202,0.35555555555555557,0.24242424242424243,0.4222222222222222,0.3333333333333333,0.18823529411764706,0.20100502512562815,0.22068965517241382,0.1889763779527559,0.18999999999999997,0.20833333333333331
36,SP:19107a648d3d23403a8693b065ee842833a0b893,"The authors model the time evolution of discrete states (e.g. genetic mutations) using a continuous-time Markov chains. They show that the resulting learning task is generally underspecified given cross-sectional data. The authors suggest including additional independent items that can help determine time order, and resolve this underspecification.  The authors implement an approximate likelihood maximization method for learning continuous-time Markov chains, which shows good performance on high-dimensional data, is faster than competing methods, and shows promising results on synthetic and real cancer data.",The authors tackle the problem of time evolution of discrete sets through the view of continuous-time markov chains. The authors provide a theoretical justification of why these problems in general underspecified in the setting of cross-sectional data. They go on to demonstrate that the underspecification can be overcome by use of additional items / features that can help determine time order by deriving a bound on the mean and variance of the observed time for these additional items. An approximate formulation for likelihood maximization is then presented without any assumptions of prior knowledge of the additional items or the structure of the additional items. The theoretical framework is then tested using simulation studies and demonstrate that accuracy of state sequence recovery can be significantly improved with use of the additional items. The framework is also evaluated using a real world cancer dataset from TCGA by ordering of pairs of mutation and copy number aberration events. ,"This paper deals with learning a particular class of multi-component continuous time Markov chains (CTMC) when time stamps are unobserved in the data. In this class binary components change their state irreversibly, rates follow a pairwise interaction model and one component changes at a time. The authors show that modeling independent elements provides information about the posterior distribution of observation time and provide an efficient stochastic approximation to the likelihood’s gradient.",This paper describes a continuous-time Markov Chain method for modeling the time evolution of events. By including independent items the authors are able to increase the limit on the number of items modeled. This research is particularly relevant as larger single-cell sequencing data sets are being collected.,0.40229885057471265,0.16091954022988506,0.14942528735632185,0.10256410256410256,0.08333333333333333,0.1643835616438356,0.22435897435897437,0.1917808219178082,0.2653061224489796,0.2191780821917808,0.2653061224489796,0.24489795918367346,0.28806584362139914,0.175,0.19117647058823534,0.13973799126637557,0.12682926829268293,0.1967213114754098
37,SP:199a281592df47d71c57fdcbd24b40a0b0de9d76,"The paper studies learning problems under so-called user-level differential privacy: in this setting, each one of n users holds m data points of the training set. This type of privacy then requires that under a change of *any number of data points belonging to a single user*, the distributions over the outputs of the algorithm are close in the sense of (epsilon, delta)-DP. A trivial way this could be achieved is group-privacy, where the privacy parameters would degrade linearly with the number of samples m.   The paper first gives a private mean estimation algorithm for the 1-dimensional case, which, given an input dataset that is both bounded in [-B,B] and (tau, gamma)-concentrated (that is, with probability gamma all points are within tau of a center point), returns an estimate that is close to the empirical mean plus a Laplace noise term of order tau/epsilon*n. This is then extended to the d-dimensional setting, using a random rotation of the data to ensure that each dimension is bounded appropriately in ell-2 norm rather than the ell-infty norm, and using the 1-dimensional solution on each dimension. Both these algorithms are *not* user-level DP but are used in the rest of the paper as building blocks of user-level DP algorithms.  The first application of these is a user-level DP d-dimensional mean estimation algorithm which, given samples from a distribution P supported on the euclidean ball with radius B, has squared ell-2 error in the order of sqrt{Var(P)/(m*n)}+\sqrt{d}*B/(\sqrt{m}*n*epsilon). The error due to privacy in this result degrades with 1/\sqrt{m} rather than being independent of m which is what the trivial approach would give us.  Using these tools, the authors then present user-level DP algorithms for empirical risk minimization with smooth convex, strongly convex, and non-convex losses as well as for stochastic convex optimization (SCO) with subgaussian gradients, demonstrating this improvement of 1/sqrt{m} in the error bounds.  They finally prove lower bounds that show that both the latter result on SCO (although not in all generality) as well as the d-dimensional user-level DP mean estimation bound are tight. ","This paper provides algorithms for various learning tasks under the constraint of user-level differential privacy. Specifically, they provide solutions for problems like mean estimation, empirical risk minimisation, stochastic convex optimisation, and learning hypothesis classes with finite metric entropy. They also provide lower bounds showing optimality of their algorithms for mean estimation and stochastic convex optimisation for different settings.  For mean estimation, they consider 1D distributions (and extend to higher dimensions, too) that are bounded within an interval, but are concentrated in some smaller sub-interval. Using this, they show that for uniformly concentrated queries, they can adaptively answer $K$ queries with a small privacy cost depending polynomially in the concentration parameter ($\tau$) and $K$.  For empirical risk minimisation and stochastic convex optimisation, they assume that each user's data is drawn i.i.d. from different distribution, such that all the distributions are close in TV distance.  They also show that even when each user contributes infinite samples, there would still be some error.","This paper deals with DP in a setting where all users contribute m datapoints to the given dataset, and the curator wishes to maintain DP on a user-level. The main result of the paper is a SGD algorithm (or an iterative aggregator) in which for each user the m datapoints are averaged then an update step takes place w.r.t the avg. Utility bounds for the approach are given in Thm 3 and 4. In addition the authors give a (close) lower bound in Thm 5.",The paper proposes a new private mean estimation mechanism with estimation error proportional to the concentration radius of queries. The mechanism is applied to stochastic optimization algorithms to solve user-level private empirical risk minimization (ERM) and user-level private stochastic convex optimization (SCO) with improved performance for concentrated queries.  Contributions: 1. A new private mean estimation mechanism with error scaling with query concentration radius. 2. New algorithms for user-level private ERM and user-level private SCO with error scaling with concentration radius of gradients.,0.09498680738786279,0.0870712401055409,0.07651715039577836,0.09090909090909091,0.11515151515151516,0.13636363636363635,0.21818181818181817,0.375,0.3372093023255814,0.17045454545454544,0.22093023255813954,0.13953488372093023,0.1323529411764706,0.14132762312633834,0.12473118279569892,0.11857707509881422,0.15139442231075698,0.1379310344827586
38,SP:1a75aaef7ba0d2de5804514f0de39d9c769f419b,"The paper proposes Bootstrapping Attentive Neural Processes (NeuBANP), which utilize random sum-to-one weight in the encoder following from Neural Bootstrapper (Shin et al., 2021). The authors argue that this utilization resolves the overfitting problem in attentive NPs, and the proposed NeuBANP is more efficient in computation and memory perspective. Various experiments, such as synthetic examples and contextual multi-armed bandit, are conducted to compare against previous works of NPs and GP.","The authors propose a new bootstrapping method in NP family called NeuBANP. The authors acknowledge the limitations of BANP and used the neural bootstrapping method to make development from BANP. While BANP does better functional uncertainty estimation than ANP, it carries a higher computational burden compared to ANP since it requires multiple computations of the encoder network, the adaptation layer, and additional heuristics. By incorporating neural bootstrapping to the bootstrapping procedure, NeuBANP is a more computationally efficient model, estimates functional uncertainty better than BANP, and also alleviates the overfitting problem that ANP and BANP carries. Experimental results show that it achieves state-of-the-art performance on stochastic optimization problems, including multidimensional Bayesian optimization and contextual multi-armed bandit.",The paper proposed a new class of neural process algorithm called neural bootstrapping attention for neural processes (NeuBANP). This method utilizes efficient Neural Bootstrapping (NeuBoots) to improve Bootstrapped Attentive Neural Processes (BANP) in capturing functional uncertainty. Authors show that NeuBANP achieves state-of-the-art performance in benchmark experiments including Bayesian optimization and contextual multi-armed bandits.  ,"The authors introduce NeuBANP, an extension of B(A)NP that replaces the iterative prediction method of B(A)NP with a single prediction step following the idea behind the neural bootstrapper. This new model is computationally more efficient and able to produce more accurate uncertainty estimates as well as correctly model heteroscedastic uncertainties. The authors provide experimental results on nonparametric regression tasks, Bayesian optimisation, contextual multi-armed bandit tasks and image inpainting (in the appendix).",0.2602739726027397,0.2328767123287671,0.2328767123287671,0.226890756302521,0.19327731092436976,0.22807017543859648,0.15966386554621848,0.2982456140350877,0.2236842105263158,0.47368421052631576,0.3026315789473684,0.17105263157894737,0.19791666666666666,0.26153846153846155,0.22818791946308725,0.30681818181818177,0.2358974358974359,0.19548872180451127
39,SP:1c8d06fe0b2a79d5d0c0f317692c2ee869d1cc0c,"The paper proposes a novel f-divergence Thermodynamic Variational Objective (f-TVO) framework for VI, that extends the TVO towards, a more general, family of f-divergences. The authors propose to use a $\chi$-deformed exponential distribution, which casts the f-TVO objective as integral along the $\chi$-path between p(x,z) and q(z|x) (rather than the geometric path in TVOs) under $\chi$-geometry. The authors propose different variants of f-TVO, that vary between the type of the f-divergance used, as well as how this integral is approximated (K-partitioned ) left-Riemann sum (related to ELBO), right-Riemann sum (related to EUBO) or a 'zig-zag' that alternates between the two. Besides theoretical justifications, results from two sets of experiments show that, in general, the proposed f-TVO perform comparable to or slightly better than the f-VI counterparts, but without clear conclusion wrt the choice of the f-divergence.","This paper proposed new variational inference that combines f-divergence variational inference and the thermodynamic variational objective. The authors introduced several new concepts of exponential families to extend TVO. Finally, the authors provided the estimator of the gradient of the objective function based on the reparametrization trick.","This paper proposed an $f$-divergence TVO, which includes some existing works e.g., RVB, CUBO, ELBO into a unified framework. This paper's main idea is to transform $f$-divergence into a generalized $\chi$-exponenetial family and integral TVO along the $\chi$-path. The paper provides some theoretical analysis and the optimization methods of the suggested framework and supports the proposed $f$-TVO with numerical results. ","The paper presents a new bound as the objective for variational inference. The bound combines the recent progress of thermodynamic variational object which gives a tighter bound than the conventional ELBO, and the f-divergence which induces more possible distribution metrics. Experiments show its better performance on some Bayesian inference tasks.",0.11612903225806452,0.13548387096774195,0.0967741935483871,0.2765957446808511,0.23404255319148937,0.14925373134328357,0.3829787234042553,0.31343283582089554,0.29411764705882354,0.19402985074626866,0.21568627450980393,0.19607843137254902,0.1782178217821782,0.1891891891891892,0.14563106796116504,0.2280701754385965,0.22448979591836735,0.1694915254237288
40,SP:1c9c01a77aee3bf00e33bffd6be9ec49d2e5ba29,"This paper combines the differentiable graph structure learning and neural architecture search together. By conducting the two iteratively, the framework can automate GNN design for tasks without given graph. It also provide an analysis that NAS could help balance GNN and MLP by the informativeness of graph against structure.","This paper proposed one graph neural architecture search method to learn graph structures in one differentiable way. It makes an investigation on how NAS select the operations in GNNs, and then evaluate the NAS method on a set of synthetic datasets with different structures. This paper proposed one feature smoothness constraints which can learn the graph architecture in one differentiable way.","This paper analyzes how DARTS selects its desired architectures, and shows that the gradient based NAS suffers from noises hidden in the graph, resulting in searching suboptimal GNN architectures. Besides, this paper improves the gradient based NAS methods by employing graph structure learning as a denoising process in the search procedure, and thus proposes GASSO, a more effective NAS algorithm. The theoretical analysis given by this paper is important and interesting, but the technical improvement lacks experimental analysis and proof, and important comparison experiments are missing.  "," The paper investigates the ability of gradient-based NAS method on selecting desired operators to construct GNN architectures. It shows that DARTs method can discriminate the benefits of different operators based on the graph structure but suffers from the noised in graph. Based on the findings, the authors propose to add an extra graph-structure learning term during the optimization process to denoise the graph structure and shows improved performance compared to existing baselines. ",0.2857142857142857,0.22448979591836735,0.20408163265306123,0.19672131147540983,0.16393442622950818,0.20930232558139536,0.22950819672131148,0.12790697674418605,0.13513513513513514,0.13953488372093023,0.13513513513513514,0.24324324324324326,0.2545454545454546,0.16296296296296295,0.16260162601626016,0.16326530612244897,0.14814814814814814,0.225
41,SP:1df605fc5fc828304f7b836724d8fd6c233ff80c,"The authors propose a DICE-family method for solving constrained offline reinforcement learning problems. To do this, they adapt ideas from OptiDICE and find a reduction from a nested constrained optimization problem to a single unconstrained optimization problem that can be efficiently represented with a neural network. Additionally, they draw on ideas from CoinDICE to estimate a confidence interval over the cost, which makes their method better at obeying constraints. The  authors compare their method to a number of baselines on both random grid worlds and continuous environments, and find that COptiDICE achieves both good performance and better constraint satisfaction than alternative methods. ","The paper considers the offline constrained reinforcement learning problem and formulates the problem as a CMDP. First, the paper presents the algorithm COptiDICE, which directly estimates the stationary distribution corrections of the optimal policy. Then, the paper shows that COptiDICE outperforms the baseline algorithms in terms of constraint satisfaction and return-maximization. ","This paper studied the policy optimization problem in the offline constrained MDP setting. Compare with previous works, in which policy gradient based approaches are widely adopted, this paper solves the problem via policy visitation distribution that rooted from the primal-dual formulation of Bellman operator, which is novel. In order to guarantee the constraints are always satisfied, this paper provides a novel approach based on CoinDICE to efficiently estimate an upper bound of constraint violation. The author also provide sufficient empirical verifications to support their proposed algorithm.","This paper has presented a DICE-based offline constrained RL algorithm for constrained RL. Experimental results on tabular CMDPs and continuous control tasks show that the proposed method can achieve a better trade-off between reward maximization and constraint satisfaction.  1st contribution: They firstly proposed to tackle constrained offline RL by solving a single minimization problem.   2nd contribution: To mitigate constraint violation in practice, they exploit the distribution correction obtained by solving the RL problem for cost upper bound estimation and then constrain the upper bound. ",0.14563106796116504,0.11650485436893204,0.13592233009708737,0.23076923076923078,0.21153846153846154,0.13793103448275862,0.28846153846153844,0.13793103448275862,0.16279069767441862,0.13793103448275862,0.12790697674418605,0.13953488372093023,0.19354838709677416,0.12631578947368421,0.14814814814814814,0.17266187050359713,0.15942028985507245,0.13872832369942198
42,SP:1f835a54c74d396ae2e8620b01bed0ec53646f3a,"This paper introduced Dynaboard, an evaluation framework in which models are uploaded to and evaluated in the cloud in a way that is reproducible and standardized. Doing so makes it easier to track other metrics that are often neglected, such as throughput, memory usage, fairness, and robustness. Finally, the paper introduces Dynascore, a measure that aggregates performance across different metrics, the weights of which can be dynamically chosen by the user to reflect their preferences.","The authors of this paper introduce dynaboard, an evaluation service framework that is integrated with the dynabench platform. The goal of this service is to evaluate and benchmark the progress of NLP models whilst having a focus on reproducibility and accessibility. Dyanboard allows the models to be evaluated directly in the cloud on multiple metrics, including a focus on memory usage. To compare and rank different models, the authors compute dynascore. Further, the evaluation platform provides flexibility to the end-user to customize the dynascore based on weights. ","This paper presents a platform called Dynaboard for evaluating NLP models in a cloud server and hosting a leaderboard of submitted models. The motive of this web platform is to evaluate models on multiple metrics for performance, memory use and throughput, robustness and fairness metrics for models. Since evaluation is performed on the cloud, the platform also aims to alleviate concerns about reproducibility, accessibility and backwards/forwards compatibility. Finally, users can rank models according to an aggregated metric called Dynascore, where the aggregation criteria is determined by them. The method of computing this aggregated score is based on economic theory, representing each metric as a good, where having more of a good is more beneficial.   To compute the Dynascore measure, the authors use the idea of indifference curves, which is a set of points (models) that provide the same utility. From this curve, they compute the average marginal rate of substitution, which is the rate at which model creators are willing to tradeoff a metric for a 1-point increase in performance.   Finally, the paper presents an evaluation of multiple models on 4 NLP tasks on performance, throughput and other metrics included in Dynaboard, along with their computed Dynascore. The authors find that the rankings obtained using Dynascore are more intuitive than the average z-score and that they largely preserve the rankings on the SuperGLUE leaderboard. ","The paper proposes an evaluation platform ""DynaBoard"" to automatic evaluate the models for different NLP tasks according to 5 metrics: performance, throughput, memory, fairness, and robustness. The scores from these metrics can be aggregated according to the custom preferences of each user. The models can then be ranked according to the aggregated score.",0.29333333333333333,0.32,0.25333333333333335,0.32954545454545453,0.2159090909090909,0.1013215859030837,0.25,0.10572687224669604,0.3584905660377358,0.1277533039647577,0.3584905660377358,0.4339622641509434,0.26993865030674846,0.1589403973509934,0.296875,0.18412698412698414,0.2695035460992908,0.16428571428571428
43,SP:1ff7a4f6f2ef647b7a9d224f8250b46b7935359a,"The paper studies a new variant of fair clustering for different fairness (group utilitarian, group egalitarian, group leximen) and clustering objectives(k-center, k-median, k-means). The main approach is to optimize the fairness objective under the constraint that the clustering cost are bounded by some value U. The main result can be thought of as a bi-criteria approximation: If we allow that U is violated by a factor of (2+alpha) (where alpha is the approximation ratio of some clustering algorithm for the given objective) then we can get an additive approximation on the fairness objective.   In addition to this algorithmic upper bound, the authors also give a number of complexity theory lower bounds for different variants of the problem.   The authors also provide a proof-of-concept study of their algorithm.  ","This paper derives the theoretical bound of fair clustering based on porotype based algorithms. Generally speaking, the authors expect to minimize the unfairness under the given upper bound. In this paper, the authors consider two types of fairness and two fairness clustering setting. Based on this, the authors derive the upper bound of the unfairness and design the corresponding algorithm. The major contributions of this paper lie in theoretical analysis.   ","EDIT: I increased my score following the discussions and rebuttal. The main question in the paper is how can we cluster data so that we optimize for some fairness desiderata. In other words, if we have a target for the amount of clustering cost we are aiming for, how can we find the most fair solution, subject to that cost constraints? Put differently, the authors address the problem of minimizing the amount of ""unfairness"" subject to the clustering cost being below a certain threshold. The two desiderata put forth by the authors in terms of fairness have to do with ""group utilitarian"" objective and with ""group egalitarian"" (they also further study a generalization called ""group leximin"" objective. The authors give both positive results and some impossibility results. Finally, they run some experiments to validate their findings.  These three objectives are desiderata that have been studied in the past. Briefly, the group utilitarian objective minimizes the sum of all fairness violations, whereas the group egalitarian objective minimizes the maximum fairness violation. Going a step further, the leximin (LM) objective minimizes the maximum fairness violation first, then minimizes the second worst etc.   One of the hardness results they prove is that for all three objectives, both fairly clustering and fairly assigning points to fixed centers is NP-hard.  On the positive side, they give an approximation algorithm that yields a (2+a)U-approximation where U is an exogenous parameter that is the upper bound on the clustering cost and a is the approximation ratio of a color-blind clustering algorithm. The algorithms presented for the different objectives depend on some assumptions about the sizes of the clusters. The idea behind the algorithms is to first run a color blind clustering algorithm to select centers, followed by an LP to get a fair assignment and a flow computation.  The authors perform some computational experiments on the adult and the census data sets to evaluate how their fairness guarantees look like. ","The paper studies the fair clustering (with balanced clusters requirement) and the goal of authors is to achieve the “fairest” possible clustering achievable with cost at most U. The idea here is that we know that PoF can be unbounded, and it is unrealistic to assume that in practical applications we cannot sacrifice the clustering quality for fairness with no limit. An important component is how to measure the amount of unfairness and then the goal is to minimize it while not violating the upper bound on affordable clustering cost. They have defined three notions called GROUP-UTILITARIAN, GROUP-EGALITARIAN and GROUP-LEXIMIN. Basically, in fair clustering, for each color h, we are given lower and upper bounds $\alpha_h, \beta_h$ and the goal is to guarantee that in each cluster the number of point from color h is at least $\alpha_h \cdot  |C_i|$ and is at most $\beta_h \cdot |C_i|$. Now, the amount of unfairness is defined by a new set of parameters $\Delta_h$ such that instead of satisfying the fair clustering requirements exactly, we have that the number of points from color h is at least $(\alpha_h - \Delta_h) \cdot  |C_i|$ and is at most $(\beta_h + \Delta_h) \cdot |C_i|$. The authors consider $\min \max_h \Delta_h$ and $\min sum_h \Delta_h$ objectives (GROUP-LEXIMIN is a lexicographical minimizing of \Delta_h: the top priority is to minimize the min value, then among those with the same min value, minimize the second min value and so on). ",0.14814814814814814,0.32592592592592595,0.25925925925925924,0.38571428571428573,0.32857142857142857,0.1676829268292683,0.2857142857142857,0.13414634146341464,0.13409961685823754,0.08231707317073171,0.08812260536398467,0.210727969348659,0.1951219512195122,0.1900647948164147,0.17676767676767677,0.135678391959799,0.1389728096676737,0.18675721561969438
44,SP:2047e943d2337d4fc6b0a269f43c7dfbd8ed9141,"The authors address the problem of estimating the average value of a moment function that depends on an unknown regression function, which is commonly used in causal inference.  By the Riesz representation theorem, the authors design a loss function where the Riesz representer turns out to be the minimizer of the proposed loss.  The authors propose both a Neural Network method (RieszNet) and a random forest method (ForestRiesz), i.e. a multi-tasking Neural Network method where the loss function is a combination of the Riesz representer and regression loss, and a random forest method that leans representation of both the regression function and the Riesz function.  The authors conduct experiments of estimating the average treatment effect and average marginal effects, and show that the proposed RieszNet and ForestRiesz beat the state-of-the-art methods.   ","This paper shows novel methods based on the recent findings on the Riesz representation in econometrics and machine learning, such as Chernozhukov et al. (2021). While the proposed applications make sense and are persuasive, the contributions of this paper are a bit limited. Although the authors support the soundness of the proposed algorithms in experiments, the experiments are a bit simple and may not be sufficient for the justification. ","The authors study the nonparametric estimation of an important class of (causal) estimands that includes the average treatment effect (ATE) in experiments and observational studies under unconfoundedness. The main innovation is the ""automatic"" nature of the procedures, one based on deep learning and one on random forests. While previous work has developed hand-tailored constructions for the ATE (a traditional problem in statistics), the authors show that existing constructions can be generalized considerably based on recent advances in semi- and nonparametric statistics based on Riesz representers. The key challenge this work addresses is the fact that the Riesz representer is typically unknown.  The main proposed method based on deep learning, RieszNet, may be seen as a generalization of the DragonNet procedure by Shi et al. (2019) from the ATE to more general estimands. Even in the case of the ATE, DragonNet and RieszNet are not the same and RieszNet outperforms DragonNet in simulations: RieszNet directly targets the inverse propensities, while DragonNet estimates the propensities and then plugs in their inverse.","This paper considers the problem of estimating an expectation over covariates of some functional of an unknown regression function. The authors propose two estimators, one based on neural networks and one based on random forests. In contrast to many (all?) previous estimators which were derived for specific functionals, these estimators are applicable to general functionals. They employ a (new?) debiasing technique. Moreover, they use a novel multi-task architecture based on the observation that it suffices to estimate the regression function as a function of the Riesz representer. The paper demonstrates improved accuracies compared to prior work and good coverage on semi-synthetic tasks derived from two data sets.",0.1323529411764706,0.23529411764705882,0.19852941176470587,0.2753623188405797,0.2028985507246377,0.17058823529411765,0.2608695652173913,0.18823529411764706,0.24770642201834864,0.11176470588235295,0.12844036697247707,0.26605504587155965,0.17560975609756097,0.20915032679738563,0.2204081632653061,0.1589958158995816,0.15730337078651685,0.20788530465949823
45,SP:2065a8cb8b53140569b64fca1f00f7230f1ae2cc,"This work proposes a novel shape representation using point clouds. This is enabled by unique differentiable poisson solver layer that enables to convert the oriented point clouds to full volume representing the shape. I think the main contribution of this work is the spectral technique that solves poisson surface reconstruction problem, which can be efficiently integrated into neural workflows. While this work mostly focuses on surface reconstruction, I believe this representation can become broadly applicable to various problem domains.","This work introduces a differentiable points (+normals) to mesh layer based on a differentiable formulation of Poisson surface reconstruction. First, the proposed method is validated in point cloud optimisation tasks where the objective function is expressed w.r.t surface meshes. Then, the method is proposed as a way to represent 3D shapes with oriented point cloud in a learning setting. Crucially, this results in a shape parameterization, dubbed Shape As Points (SAP), that is more lightweight and faster than neural implicit fields, while still yielding watertight shapes and being able to handle arbitrary topology.  ","The paper formulates a differentiable Poisson Surface Reconstruction layer, which maps an oriented point cloud to an indicator function over the interior of the object. The Poisson solver is done in the spectral domain on the GPU for computational efficiency. It shares many of its advantages with other neural implicit representations, but because it produces an indicator function over the whole space, there is no need to query a network over an entire voxel grid.   The layer is incorporated into two tasks: an optimization-based 3D shape reconstruction that fits a spherical point cloud initialization to a target point cloud, and a learning-based 3D reconstruction task that matches noisy point cloud input to a ground truth mesh. The non-differentiable step of Marching Cubes in these pipelines has its gradient approximated with the inverse surface normal, which is quite natural. The experimental results seem good, and beat out the existing state-of-the-art, and an ablation study is performed as well on the learning-based task to investigate the form of the feature extraction network there.",This paper proposes a differentiable adaptation of the Poisson Surface Reconstruction algorithm for oriented point sets. It discretizes the Poisson equation over a 3D grid and use spectral methods to find the solution for indicator function values over this 3D grid. The final grid is fed to marching cubes algorithm to obtain the final mesh. The differentiability of the approach allows to use the proposed method in various optimization-based applications from which the authors consider optimization-based and learning-based 3D reconstruction tasks. Proposed models are evaluated on datasets with different level of details and noise present in the inputs and compared to both classical and more recent learning-based methods.,0.17721518987341772,0.21518987341772153,0.189873417721519,0.25263157894736843,0.18947368421052632,0.19101123595505617,0.14736842105263157,0.09550561797752809,0.13392857142857142,0.1348314606741573,0.16071428571428573,0.30357142857142855,0.16091954022988508,0.13229571984435798,0.15706806282722516,0.17582417582417584,0.17391304347826086,0.23448275862068965
46,SP:20abe4d70152590c3c44fcb50c5d0293e25874ff,"The paper tackles the problem of adversarial attacks against federated learning. The main proposal is RVFR, an original method that allows to protect against backdoor attacks targeting the specific “vertical federated learning” setting. The proposal is provided with a theoretical analysis of its effectiveness, and is then evaluated on two well-known datasets, showing better results than prior works in adversarial scenarios.  Overall, the presentation of the paper should be improved. The quality of the English text is appropriate. Figures and Tables should be improved. The topic addressed by the manuscript is relevant and in-line with ICLR. The references must be expanded with security-related works. The contribution is potentially significant.  STRENGTHS: + Relevant and trendy subject + Evaluation on two datasets + The theoretical analysis is appreciable  WEAKNESSES - Very poor Introduction, Abstract and Background (Related Work) - Unclear threat model - No Tradeoff","This paper studies robust vertical federated learning framework against backdoor attacks. The authors utilize robust autoencoders with purified training to remove the backdoor features and obtain clean global model. The authors also proved that, with some assumptions, the exact uncorrupted features can be recovered.","This paper studies the robustness of vertical federated learning. They use intuitions from robust feature subspace recovery for low rank matrices to design a defense framework for vertical federated learning. They empirically show the effectiveness of their framework for some settings. They also try to provide theoretical evidence for the robustness of their scheme, but I do not find their theoretical studies illuminating. ","The paper proposes a robust vertical federated learning framework called RVFR against backdoor attacks. In the framework, the agents train the local feature extractors and send the embedded features to the server. Then, the server trains a robust auto-encoder to recover and purify the features. Last, the purified features are used to train the global model. The paper provides theoretical analyses of the feature recovery under different threat models. The experiments show that RVFR is more effective when defending the backdoor attacks compared with the other baselines.",0.1,0.10714285714285714,0.15714285714285714,0.25,0.45454545454545453,0.1746031746031746,0.3181818181818182,0.23809523809523808,0.25,0.1746031746031746,0.22727272727272727,0.125,0.15217391304347827,0.14778325123152708,0.19298245614035087,0.205607476635514,0.30303030303030304,0.1456953642384106
47,SP:217c4205a99f9b37283137826c4be6ab9bfb4e8e,This paper aims at identifying and quantifying transferability in learning algorithms. The paper proposes a new upper bound on the target error. The proposed algorithm is an adversarial optimisation problem that empirically proved to be effective. ,"This paper defines a notion of ""transferability"" between features from different domains, which is different from common distribution discrepancy measures such as total variation and Wasserstein distance. This measure of transferability is measured with labeled samples from both domains, and provides a bound on the target error. The empirically test transferability of the features learned by current domain generalization algorithms, and some don't do that well in this transferability measure. They propose an algorithm based on optimizing the measure of transferability across training domains and test it on some domain generalization datasets, including a satellite dataset.","Authors introduced the notion of transferability between pairs of domains (i.e. joint distributions over input x output spaces) along with a discrepancy measure, which can be used to assess how dissimilar are two data sources relative to a given class of predictors. Such a quantity may be useful in either assessing how well is a trained model likely to perform on new data, or as a training signal so that more preference is given to predictors able to generalize out of the training data distribution. The main strength of this work in my opinion is the fact that the proposed transferability measure accounts for the joint distributions over data and labels. However, while I appreciate the depth of the discussion and results in terms of the properties of the proposal, I believe a key missing discussion is the effect that commonly used sets of assumptions have in the proposed transferability measure. Moreover, the evaluation lacks in evidencing the benefits of the proposal relative to current strategies. Please refer to the following for further details.","This paper aims to provide a formulation of transferability. The main intuition is that any near-optimal source classifier should be also near-optimal on the target domain. So the authors quantify transferability by measuring the relation between near-optimal classifiers in source and target domains and derive genralization bounds based on this. In addition, new algorithm is proposed based on the intuition.",0.3333333333333333,0.2777777777777778,0.3055555555555556,0.23711340206185566,0.18556701030927836,0.09142857142857143,0.12371134020618557,0.05714285714285714,0.1746031746031746,0.13142857142857142,0.2857142857142857,0.25396825396825395,0.18045112781954886,0.09478672985781988,0.22222222222222224,0.16911764705882354,0.225,0.13445378151260504
48,SP:21819b54433fa274657d9fe418f66407eee83eeb,"This paper studies supervised learning models with fairness constraints. They specifically consider equalized loss fairness constraint.  When a traditional (convex) loss minimization problem is cast with additional fairness constraints, the corresponding problem is non-convex. They provide algorithms to efficiently solve this problem up to global optimality. They demonstrate the performance of their algorithms on real-world data. ","The authors study fair prediction subject to Equalized Loss (EL), and they introduce a variety of approaches for exactly and approximately solving the problem of finding the globally optimal predictor that satisfies EL. First, they show how to solve a sequence of convex constrained optimization problems in order to solve the larger non-convex problem. Next, they show how to approximately solve this problem more efficiently by using unconstrained convex optimization. Lastly, they evaluate both of their approaches on two datasets.","The authors consider minimization of convex losses constrained by either bounded loss on each group, or bounded difference of losses over two groups. The second formulation is non-convex, whereas the first formulation is convex.   When the losses are strictly convex on both the demographic groups, so that their optima are distinct (I think this is the condition they need, but they use a more restrictive condition in the paper), they can find the ""EL"" fair predictor by solving a sequence of convex constrained optimizations, by exploiting a monotonicity property. They next give a more computationally efficient approximate algorithm for finding the EL fair predictor. ","    This paper studies the problem of fair supervised learning under the Equalized Loss (EL) fairness notion, which is formulated as a non-convex constrained optimization problem. The authors introduce two algorithms that find the global (sub-)optimal solution by solving a sequence of convex (constrained) optimizations. Empirically, the algorithms perform well.",0.27586206896551724,0.1896551724137931,0.27586206896551724,0.18518518518518517,0.18518518518518517,0.18095238095238095,0.19753086419753085,0.10476190476190476,0.3137254901960784,0.14285714285714285,0.29411764705882354,0.37254901960784315,0.23021582733812948,0.13496932515337423,0.29357798165137616,0.16129032258064516,0.22727272727272727,0.24358974358974356
49,SP:220db9ed147bbe67de5d82778720a1549656e48d,"The paper proposes Latent Score-based Generative Model (LSGM) which introduces a score-based prior in the Variational Autoencoder (VAE) framework. In contrast to previous score-based models that operate in the data space, LSGM uses a score-based model in the latent space. A sample from a base distribution (standard Gaussian) is denoised using the score-based prior in the latent space and is then mapped to the data space using a decoder. The authors further discuss how the various terms in the ELBO can be computed to train the model in an end-to-end fashion without requiring the time-dependent marginal score function. Multiple variance reduction techniques and training tricks have also been proposed for the resulting objective function. Both quantitative and qualitative results demonstrate the ability of the model to generate high fidelity images.",The paper proposes a generative model based on the VAE framework with a sophisticated prior / inference scheme based on denoising score-matching. Authors develop extensive machinery to adapt related ideas from score-based generation in the observed space to latent space. This greatly improves sampling time while preserving and sometimes improving sample quality and/or likelihood of data. I believe the paper is an interesting addition to the existing collection of generative modelling techniques.,The paper at hand proposes end-to-end training of score-based generative models in the latent space of a variational autoencoder. The VAE is pre-trained using a normal prior which is then replaced by the score-based generative model that is then jointly trained with the VAE. The paper also introduces a novel training objective based on the cross entropy between the encoder distribution and the SGM prior. Two techniques for variance reduction of the loss function are discussed. ,"The paper proposes to learn a flexible VAE prior using score-based generative model. Contrary to the existing score-based model that builds on high-dim pixel space, the paper applies the score-based model directly on the latent space which is typically low-dimensional.  The effectiveness of the model has been verified using generative FID score, negative log-likelihood as well as various ablation studies. ",0.18115942028985507,0.2028985507246377,0.17391304347826086,0.21621621621621623,0.24324324324324326,0.24691358024691357,0.33783783783783783,0.345679012345679,0.36363636363636365,0.19753086419753085,0.2727272727272727,0.30303030303030304,0.2358490566037736,0.2557077625570776,0.2352941176470588,0.2064516129032258,0.2571428571428572,0.272108843537415
50,SP:22d01913b78ef447b064c65a646fa301b861d3f7,"This paper develops a practical gradient-based hyperparameter optimization method, HyperDistill, that meets the following criteria   a) scalability in hyperparameter dimension and memory constraints,    b) accuracy (hyper-gradient update terms do not depend on only the last step of gradient updates)   c) applicability to the online setting.  The main difficulty lies in estimating the gradient of the weights with respect to the hyperparameters. The authors do so by approximating it as a single Jacobian-vector product, using a ""distilled"" weight and dataset pair.  Experimentally, HyperDistill achieves better validation losses and higher quality true hypergradient estimates than a variety of recent, relevant baselines, and does in an efficient fashion.","The paper proposes a novel hyperparameter optimization algorithm in meta-learning to overcome previous limitations of only being able to see a longer horizon and not being scalable to high dimensional hyperparameters. In particular, the authors propose to distill the hypergradient second-order term into a one-step Jacobioan-vector product. The authors show that, with their approximated algorithm, it is possible to perform hyperparameter optimization for higher dimensional hyperparameters and longer horizon length even in an online setting. Empirically, the authors show the advantages of the proposed approach in several benchmark datasets.","This paper proposes a new online hyperparameter optimization algorithm. Applying meta-learning for hyperparameter optimization is reasonable and interesting but suffers from the second-order gradient computation. Implicit Function Theorem and Unrolled differentiation can be used to approximate the meta-gradient but also causes various problems. In this paper, the authors propose an interesting method by approximating the second-order approximation with knowledge distillation. ","The authors propose a hyperparameter optimization algorithm in meta-learning, where parameters w/o being involved in the inner loop optimization are treated as hyperparameters. The proposed algorithms approximate the second-order hypergradients via knowledge distillation. They further evaluate the effectiveness on tinyImageNet and CIFAR100. ",0.17592592592592593,0.14814814814814814,0.09259259259259259,0.1935483870967742,0.17204301075268819,0.21875,0.20430107526881722,0.25,0.2222222222222222,0.28125,0.35555555555555557,0.3111111111111111,0.1890547263681592,0.18604651162790697,0.13071895424836602,0.22929936305732485,0.2318840579710145,0.25688073394495414
51,SP:231655b9fad6c76eb0ff1ba305ed421f5c293623,"The community has been looking at compositional generalisation more recently, and while Fodor and Pyshylynn is an oft cited work, there is a lack of a good definition of compositionality that researchers can agree on.   This paper suggests that a noisy channel with an encoder decoder setup is crucial for the emergence of compositionality. The authors propose a simple definition of compositionality in a non-hierarchical case, and discuss how 1) if no assumptions or prior information about the data is incorporated into the training process, the resulting methods will be uncompositional, 2) if this inductive bias is injected through the use of a noisy channel, there are losses (namely J_1, J_2) that can be used to encourage the rise of a compositional encoding.   The basic setup involves communication through a noisy channel. They then discuss how, if the encoding is compositional, there are particular losses (J_1, and J_2) that will be optimal. More importantly, when J_2 is optimal, the encoding is compositional. In their experiments, they use J_1 primarily, and demonstrate compositional emergence (according to some metrics) under some circumstances.  ","The paper presents theoretical and empirical results showing that a specific type of utterance noise in a sender-receiver emergent communications architecture leads to compositional utterances, under certain constraints, and that the resulting loss function has all local minimum as global minimum (they don't state explicitly that all local minimum are global minimum, but there results strongly implies to me that they are). The following constraints need to be met in order to satisfy the theoretical proof: - message length = number of attributes - the input to the sender network should be a bag of attributes, not a non-symbolic, e.g. pixel, input (note that the paper calls attributes 'factors') - the vocab size of the utterance should exactly match the number of attribute values - the utterances are discretized using Gumbel, so we can backprop through them, and not using a discrete sampling, which would require REINFORCE - a specific type of noise is added to the utterances, which randomly flips/corrupts each specific token with an identical probability $\epsilon$ - the noise should be more than 0, and less then (vocab size - 1) / vocab size  The paper presents theoretical results basd on Locatello et al that show that without sufficiently strong inductive biases, that one cannot the utterances to be disentangled.  The paper presents comprehensive empirical results for scenarios which both somewhat align with the theoretical constraints above, and also that deviate strongly from these constraints. For example, all the experiments use non-symbolic inputs, which is therefore not a bag of attributes input. Most of the experiments use a message length that does match the number of attributes, but some experiments are done using longer utterances, and show similar results to the shorter messages.  The paper evaluates the results using 4 existing compositional metrics: topographic similarity, conflict count, context independence, and positional disentanglement. ","This paper adds results to the literature on learning compositional emergent communication protocols. Two results are theoretical: that inductive biases are required in order for compositional communication to emerge, and that adding a noisy channel with a specific loss function leads to compositional language emergence. The third result is a small-scale empirical corroboration of the latter theoretical result in a speaker-listener game.","The authors analyze the effect of a _noisy channel_ on an emergent communication protocol.  In particular, they show that languages that minimize losses involving this noise are compositional [in their sense of the term] and then conduct a very thorough set of experiments while varying noise in a standard emergent communication.  They find that compositionality tends to go up for some low levels of noise, but then goes down after that (and accuracy tends to go down consistently with more noise).  There is also a very thorough set of experiments manipulating myriad factors in the setup and finding that the overall patterns just discussed are robust.  The paper is a very interesting theoretical and experimental contribution to the emergent communication literature.  My only minor complaint would be that there are so many experimental results that it's easy for the reader to lose the forest for the trees; more synthesis of the overall trends would be welcome.",0.18716577540106952,0.08021390374331551,0.13903743315508021,0.0695364238410596,0.11589403973509933,0.265625,0.11589403973509933,0.234375,0.16560509554140126,0.328125,0.2229299363057325,0.10828025477707007,0.1431492842535787,0.1195219123505976,0.15116279069767444,0.11475409836065571,0.15250544662309368,0.15384615384615385
52,SP:244f5d31ec93b7a4bfc4b257ee6cdd5cfdb18a38,This paper proposes a variation of the Variational Autoencoder (VAE) model to learn disentangled spatial and temporal representations from ST raster data. A separation module is designed in the proposed network. The proposed model is validated on three real-world datasets and achieves better performance compared with the baselines listed in the paper.  ,"The paper proposes a VAE model for mobility forecasting. It focuses mainly on the disentanglement of spatial and temporal features by modelizing explicitly two groups of features in the VAE schema, a group of spatial features that are time-independent and a group of temporal features using a sequential prior. In order to make a prediction, the VAE is used to extract features from three groups of sequences describing the trend, the period, and the last few timesteps before the forecasting period.  An ablation study is provided concerning those three groups of sequences. Experiments show competitive results wrt SOTA.","This paper proposes learning a disentangled representation of spatio-temporal mobility data using a VAE-based architecture, which essentially tried to decompose spatial and temporal features and model them independently. Disentanglement in the learnt representation is encouraged through the introduction of total correction as a regularization term in the VAE loss. The authors empirically demonstrate the potential of the proposed approach in 3 different mobility datasets (BikeNYC, TaxiNYC and TaxiBJ). Overall, the results suggest competitive predictive performance with other state-of-the-art approaches.","The authors propose a novel neural architecture to structurally learn disentangled spatio-temporal representations in the context of mobility forecasting. This work argues that predictive models can benefit from enforcing the independence of spatial and temporal dynamics and proposes a VAE-inspired architecture for doing so.  By conducting experiments on three mobility datasets, the authors empirically evaluate the prediction performance of the proposed approach against various baselines. Additionally, further experiments attempt to (i) quantify the effectiveness of different temporal features (short-term/daily/weekly correlation), and (ii) formalize and justify a strategy for feature selection.",0.32075471698113206,0.37735849056603776,0.3018867924528302,0.20202020202020202,0.18181818181818182,0.25,0.1717171717171717,0.23809523809523808,0.16842105263157894,0.23809523809523808,0.18947368421052632,0.22105263157894736,0.2236842105263158,0.29197080291970806,0.2162162162162162,0.21857923497267756,0.18556701030927839,0.23463687150837986
53,SP:2488d3697a4ea732526b3ef11fbbd93e27d42e81,"The authors report on a technique to address learning rate hyperparameter tuning for deep learning referred to as optimizer grafting. Specifically, the paper proposes a meta-algorithm (referred to as M#D) that blends the steps of two optimizers by combining the step magnitude of one (M) with the direction of the other (D). The technique of optimizer grafting allows for the transfer of the overall implicit step size schedule to a new optimizer, resulting in reductions in computational cost of optimizer hyper parameter search. The second primary result is leveraging the technique to identify a non-adaptive per-layer learning rate correction to SGD which allows it to train a BERT model to state-of-the-art performance. Analogous results are presented for vision models for global (non-per-layer) schedules for AdaGrad.  The authors describe grafting meta algorithm  (M#D) as, at each iteration, M#D feeds the same input (w_t, g_t) to both M and D which manage their states independently and produce w_M, w_D. Then the norms of the steps each would have taken is computed, and used to combine M’s magnitude update with D’s direction update. Partitioning is managed to implement global versus per-layer grafting.2 optimizer hyperparameter searches with the same computational budget, but different performances.  The authors present an empirical study on the transfer of implicit step size schedules between optimizers, comparing SGD and Adam to Adam#SGD for task of BERT pre-training. They show that Adam#SGD is able to achieve performance at/near Adam.   The paper also presents results for image classification for ImageNet and CIFAR-10), for AdaGrad, SGD, and SGD#AdaGrad, showing  SGD#AdaGrad outperforms SGD and AdaGrad. However, without error bars, it is hard assess the actual results.  Finally, the paper shows results for grafting distilling a non-adaptive correction to D, eliminating the need to run M in parallel - that is, transferring a global, time-dependent non-adaptive multipliers for the learning rate. The results show for the global variant for ResNET (SGD, AdaGrad), the discovered learning rate is comparable to the one used on SGD and achieves a top1 accuracy of 72.46. For the per-layer variant, learning rate schedule enables a simple per-layer step size correction without adaptive preconditioning. The authors present proof-of-concept results for simplifying the discovered schedule as a way to support the robustness of their transfer approach. ","This paper proposes a method called optimizer grafting. It uses two optimizers in one training session. One is to decide the update direction of parameters, and the other is to decide the update stride of parameters. This paper proposes a new optimizing mode and take a large amount of experiment exploration. ","The authors investigate the entanglements between the optimizer and the learning rate schedule and propose  the technique of optimizer grafting, which allows for the transfer of the overall implicit step size schedule from a tuned optimizer to a new optimizer, preserving empirical performance. This provides a robust  plug-and-play baseline for optimizer comparisons, leading to reductions to the computational cost of  optimizer hyperparameter search. Using grafting, they discover a non-adaptive learning rate correction to  SGD which allows it to train a BERT model to state-of-the-art performance.","The authors propose learning rate grafting as a method to explore the power and dynamics of optimizers. Learning rate grafting partitions the parameters of the networks into groups, and for each group takes the direction of the weight update from one optimizer and the magnitude from another optimizer. The paper then shows that grafting allows for achieving the performance of a tuned optimizer using that tuned optimizer's group-wise magnitudes along with an untuned optimizer's group-wise directions.",0.051597051597051594,0.13022113022113022,0.07616707616707617,0.21568627450980393,0.23529411764705882,0.23076923076923078,0.4117647058823529,0.5824175824175825,0.3875,0.12087912087912088,0.15,0.2625,0.09170305676855894,0.21285140562248994,0.1273100616016427,0.15492957746478872,0.18320610687022904,0.2456140350877193
54,SP:249a72ef4e9cf02221243428174bb749068af6b2,"This paper investigates the phenomenon of reward hacking as a function of agent capabilities. They introduce four diverse RL environments with nine misspecified rewards and demonstrate that more capable agents are better at exploiting the misspecification. They find instances of phase transitions where a small increase in agent capability produces a large change in behavior that sharply decreases the true reward.  To mitigate the reward hacking problem, they propose to set up an anomaly detection task, given a trusted model with moderate performance on the true reward, where the anomaly detector's task is to identify whether policies from a different model are satisfactory for the true reward. They provide several baseline anomaly detectors and show how they perform on different tasks. ","This paper provides a systematic study of “reward hacking” in the environments with the misspecified rewards. The authors conduct a set of experiments with 4 environments, several types of reward misspecification in each of them and several agents of different expressivity (model capacity). They notice that often the agents that are more capable end up obtaining high proxy reward, but low real reward. Besides, often the transition to the low real reward happens very quickly and authors call this phenomenon “phase transition”. Finally, they propose a baseline for anomaly detection to identify this phase transition.","This paper studies reward hacking, a common but understudied phenomenon, across a set of environments. Reward hacking emerges in several tasks, meaning that the resulting policy has a high proxy reward but a low true reward. A key finding is that reward hacking increases with agent capabilities so that increasing capability lowers the true reward. This holds across several ways of increasing capabilities (model size, training steps, action space, etc). The authors also find ‘phase transitions’ where a small increase in capability results in qualitatively new reward hacking behavior, a phenomenon that may require novel monitoring strategies. One such strategy is anomaly detection, for which the authors introduce a benchmark and baselines. ","This paper targets the very important problem of reward-hacking that occurs when the objectives optimized by intelligent agents are misaligned with respect to the tru objectives of the algorithm designer. The paper presents an empirical study across a range of different settings including a simple driving simulator, covid modeling, and a single atari game. The experiments show evidence of reward hacking as a function of modeling power of the agent and the size of the state-space. The paper concludes with some ideas and initial directions on how to potentially mitigate reward hacking. ",0.19672131147540983,0.22950819672131148,0.18032786885245902,0.2736842105263158,0.22105263157894736,0.16964285714285715,0.25263157894736843,0.25,0.23404255319148937,0.23214285714285715,0.22340425531914893,0.20212765957446807,0.22119815668202764,0.2393162393162393,0.20370370370370372,0.2512077294685991,0.2222222222222222,0.18446601941747576
55,SP:24cdcb12fca34680d8b34bc61c51b9003368228a,"Summary:  This paper aims to operationalize the symmetry-based disentanglement idea proposed in Higgins et al., 2018, and proposes a novel loss function that can be used to both evaluate and learn disentangled representations (including learning from datasets where only a subset of samples are fully labeled). The paper includes some experimental validation of the proposed ideas on synthetic datasets. It also includes cross-comparisons between both the LSBD-based metric and method to state of the art disentangling metrics and methods, which demonstrate the complementary aspects of the LSBD method (e.g., its ability to capture multi-dimensional disentangled factors, for example rotations in a euclidean plane).  Overall, the paper is clearly written and makes a positive contribution in pushing for disentangling metrics that are motivated by a novel formalism. The impact of this paper is slightly limited due to the strength of supervision required (full supervision of all factors of variation) for implementing both the D_LSBD metric and the loss.","This paper makes two contributions: 1 - It presents a framework for measuring linear symmetry based disentanglement (LSBD). In particular, it addresses the challenge that LSBD is only defined with respect to an optimal decomposition of the latent space $Z = Z_1 \oplus \ldots \oplus Z_k$ that needs to be identified by the metric. 2 - The metric serves as additional supervision in the training of a VAE on toy datasets with at most two symmetry transformations. The results show quantitative improvements in terms of disentanglement.","This work focuses on developing a metric evaluation metric and a semi-supervised VAE-based model for linear symmetry-based disentanglement (LSBD)[1]. When assuming knowledge of linearly disentangled group representation $\rho$, the authors deduce that LSBD is fulfilled if the last requirement is met (equivariance of a map h). The authors propose to measure the equivariance by simply quantify the equivariance of the encoding map w.r.t. to the group and observations. Although a straightforward thought, computing the metric is challenging. Therefore, the authors propose an approximation by calculating the dispersion of the inverse group representation obtained through a PCA. Similar to the evaluation metric, the authors propose LSBD-VAE, a semi-supervised version of $\Delta$-VAE to regularise the latent space based on its dispersion. They show through empirical evaluation that their method outperforms other unsupervised disentanglement approaches w.r.t. to the proposed evaluation metric. ","This paper proposes a new metric $D_{LSBD}$ to measure Linear Symmetry Based Disentanglement. The metric is motivated from an idealized setting before outlining a practical upper bound that can be realized. The authors also apply their new metric as an additional loss term to a $\Delta$-VAE model and show that it greatly enhances Linear Disentanglement while also agreeing with past notions of Disentanglement. The experiments, while simple and using only $SO(2)$ as a subgroup, are demonstrative of the proposed approach.",0.1165644171779141,0.15337423312883436,0.13496932515337423,0.3058823529411765,0.24705882352941178,0.15436241610738255,0.2235294117647059,0.16778523489932887,0.26506024096385544,0.174496644295302,0.25301204819277107,0.27710843373493976,0.1532258064516129,0.1602564102564103,0.17886178861788618,0.22222222222222224,0.24999999999999994,0.1982758620689655
56,SP:24d637e8c3489bfe50b17bf684097776ad6ee485,"The paper studies the asymptotical behaviors (with respect to the suboptimality gap $\Delta$) of key statistics in the standard upper confidence bound (UCB) algorithm in standard multi-armed bandits (MAB). The contributions are as follows.  1. The paper proves that the **asymptotical arm-sampling rate** converges to a constant, and gives its **analytical form** (Eq. 2). In particular, the paper discovers a non-trivial ""**moderate gap**"" regime for UCB.  2. The paper proves an asymptotic regret lower bound for UCB **up to $(1 + o(1))$**  in two-armed bandits.  3. The paper proves that the **asymptotical empirical sum** converges to a Brownian motion for UCB in two-armed bandits. 4. The paper proves the asymptotical arm-sampling rate for Bernoulli Thompson sampling in two-armed *deterministic* bandits.  5. The authors extend their results to MAB.","The authors study the asymptotic behavior of arm-sampling distributions under the UCB and the Thompson sampling. They provide an asymptotic characterization of the distributions, and show the arm sampling rates asymptotically deterministic regardless of the hardness of instances. With this characterization, focused on canonical UCB algorithm, they provides the first algorithm-specific worst case bound and the first diffusion-limit performance. ","The paper studies the arm sampling behavior of UCB and Thompson sampling algorithms. For the two-arm case, the asymptotic behavior of arm sampling is characterized for different regimes (small, large, and medium) of suboptimality gap. Using this characterization, the minimax regret of UCB is shown to O(n\logn), where n is the time horizon. They highlight the incomplete learning phenomenon in Thomson sampling where sample-split could be arbitrarily imbalanced along a sample path even when both the arms have the same mean.","Classical bandit algorithms such as UCB and Thompson Sampling are well-understood in terms of their performance—for example, an upper bound on their regret—but, until recently, little attention has been given to their actual behavior—for example, the rates at which specific arms are sampled. This work seeks to address these questions by analyzing the arm sampling distribution achieved by both UCB and Thompson Sampling. They provide results characterizing these distributions asymptotically and, in addition, characterize the distribution of the rewards obtained by running UCB.",0.11851851851851852,0.16296296296296298,0.11851851851851852,0.3064516129032258,0.25806451612903225,0.16470588235294117,0.25806451612903225,0.25882352941176473,0.1839080459770115,0.2235294117647059,0.1839080459770115,0.16091954022988506,0.16243654822335024,0.20000000000000004,0.14414414414414414,0.2585034013605442,0.21476510067114093,0.16279069767441862
57,SP:24ea12428bd675459f0509aa7cee821fa236382e,"The authors proposed a multi-task learning framework with a shared feature encoder (body) and separate input embedding (head) and application-wise decoder (tail) components for each task in a distributed/federated learning setting. A number of multi-head self-attention modules from the transformer model are adopted here as the body part. The Federated Split Task-Agnostic Learning (FESTA) paradigm is proposed to compute/update the model parameters of head/tail parts on the client-side and the body parts on the server-side. A list of chest x-ray datasets is employed for the experiments, including datasets for COVID-19 classification (normal, pneumonia, and COVID-19), pneumothorax segmentation, and pneumonia detection. Superior results of the proposed method are reported in comparison to other learning paradigms. The manuscript is overall well-prepared, while several things could be improved and addressed(listed below). ","This paper presents a federated split task-agnostic (FESTA) framework that can take an effective use of the vision transformer's (ViT) to perform several CXR tasks simultaneously. The authors discover that the design of ViT is ideal for split learning, in which a network is designed into a client-server mechanism. They integrate ViT into their framework by combining the benefits of federated and split learning through its decomposable design. According to the authors, a model trained with the FESTA framework can leverage robust representations from multiple related tasks to improve individual task performance.","This paper combines: split learning, federated learning, multi-task learning, vision transformers, and COVID-19 chest x-ray (CXR) classification. Specifically, the authors propose the Federated Split Task-Agnostic (FESTA) framework, motivated by privacy concerns of training models with patient level healthcare data (CXRs). This framework is for training large deep learning models based on private datasets distributed over clients (i.e. hospitals). Here, each client has a local copy of a ""head"" and ""tail"" of a vision transformer model, while the server has a shared body of a vision transformer. The training process is as follows: 1.) clients perform forward passes using their local data through their head networks and send the embedded representations to the server 2.) the server performs a forward pass with the embedded representations and sends the resulting representations back to their respective clients 3.) the clients finish the forward pass through their tail networks, compute the loss, and send gradients back through the server 4.) server continues backprop, client finishes backprop, everyone updates their weights (the server's network weights will be updated considering data from _all_ clients) 5.) occasionally, all the weights from the clients head and tail networks will be averaged together (FedAvg)  Additionally, the authors consider the case where some clients are training on different tasks (e.g. image classification, segmentation, detection) using the same modality (CXRs). Here, step 5 is applied over clients with the same task.","This paper presents a federated learning (FL) framework for COVID-19 xray image analysis, which leverages the natural scalibility of visual transformer to establish a multi-task FL process. The proposed method jointly learns from different clients of various x-ray-based tasks including classification, detection and segmentation in a split learning manner, in which each client preserves their only transformer head and tail while jointly learning a shared transformer body maintained in the global server. Experimental results show that the established multi-task federated learning framework with transformer improves individual performances for each COVID-19 diagnosis task and are comparable with centralized training model.",0.1258741258741259,0.2097902097902098,0.13986013986013987,0.28421052631578947,0.25263157894736843,0.11392405063291139,0.18947368421052632,0.12658227848101267,0.19047619047619047,0.11392405063291139,0.22857142857142856,0.2571428571428571,0.15126050420168072,0.15789473684210525,0.16129032258064516,0.16265060240963855,0.24000000000000002,0.15789473684210525
58,SP:25414fe1c6203f9b623c5317a4ffaba478085c4c,"This paper proposes a framework to analyze both optimization and generalization properties under the Uniform-LGI condition (Def 1). From my understanding, the main results consist of two parts:  * Optimization: define the Uniform-LGI as an extension of the PL condition, prove the corresponding convergence result with a sublinear rate, and bound the optimization path length. * Generalization: use the Rademacher complexity to estimate the generalization error. The Rademacher complexity scales with the diameter of the parameter set, thus can be bounded by the optimization path length, which connects with the optimization results.  Then the paper apply this framework to three application models: first establish the Uniform-LGI, then calculate the optimization path length and estimate the generalization error.","The authors study the connection between optimization and generalization for gradient flow (GF) on loss functions that satisfy a global version of the Lojasiewicz gradient inequality. Under this assumption, they prove convergence of GF to a global minimum and they find an upper bound on the optimization length — measured as the integral of the $\ell_2-$norm of the gradient from time $t=0$ to the final time. This upper bound depends on the specific choice of the loss function and on the number of samples. With an additional assumption on the hypothesis class (encompassing, e.g., linear shallow networks, two-layer networks) the first result is used to derive an upper bound on the generalization gap. The bound depends on the choice of the loss function, its initial value, and the length of the optimisation path. This leads to the main result that shorter optimization paths induce smaller generalization gap. The authors apply this result to three models (underdetermined $\ell_p$ linear regression, kernel regression, and overparametrized two-layer networks with ReLU activation) with a given target function. They compute non-asymptotic expressions for the generalization bounds at fixed ratio between sample size and ambient dimension, and show that in these cases the bounds are non-vacuous when the dimension increases. ","The paper provides a novel generalization bound for the gradient flow equation related to the length of the optimization path. The bound is valid for loss function that locally satisfies Łojasiewicz gradient inequality, which is applicable to different machine learning models such as underdetermined $\ell_p$ linear regression, kernel regression, and overparameterized two-layer ReLU neural networks. Explicit derivations are provided for these three models to show that the length-based generalization bound is non-vacuous. ","The paper studies generalization and optimization of kernel-based and one-hidden layer neural network models. The convergence guarantee is shown for a Uniform-LGI loss function in Theorem 1. Then, in Theorem 2 the paper proves a generalization guarantee assuming a particular parametric model in Equation (3). Finally, the paper applies these results to p-norm regression, kernel regression, and one hidden layer neural network learning problems.  ",0.288135593220339,0.15254237288135594,0.11864406779661017,0.1650943396226415,0.09905660377358491,0.2236842105263158,0.16037735849056603,0.23684210526315788,0.20588235294117646,0.4605263157894737,0.3088235294117647,0.25,0.20606060606060606,0.18556701030927836,0.15053763440860216,0.24305555555555552,0.15,0.2361111111111111
59,SP:262a5aaa4e675b2aac6bd14d3aa007bf411ce550,The paper considers the problem of learning a causal model in the POMDP setting. It assumes the learning agent has the ability to collect online experiences through direct interactions with the environment and can access a large collection of offline experiences obtained through the observation of another agent. It further assumes that the observed agent can act based on privileged information hidden from the learning agent. The paper formulates model-based reinforcement learning in this setting as a causal inference problem. The paper then proposes to use offline data as a regularizer during learning. The paper presents empirical results on a number of toy problems.,"This paper studies the problems of evaluating interventional distributions (i.e., system dynamics) of a partially observed Markov decision process (POMDP) from samples collected from a combination of randomized experiments and observations of a privileged expert who could access the latent state. The POMDP is presumed to have a finite horizon, e.g., the physician could only perform a finite number of treatments for the same patients. The authors propose an unbiased estimator for evaluating system dynamics from the experimental data. As for the observational distribution where the unobserved confounding exists, the authors derive bounds over unknown system dynamics, estimable from observations.","This paper considers the model-based reinforcement learning (RL) problem by combining the offline and online data. The online data, i.e., the interventional data, is generated from the standard partially-observable Markov decision process (POMDP), while the offline data, i.e., the observational data, is generated from the privileged POMDP where the offline learner had the access to the state information (i.e., the unobserved confounder) to make an action. The authors proposed an augmented learning procedure to safely combine these two separate different sources and learn a more efficient policy. Their method is shown both theoretically and empirically better than not using offline data. My main concerns lie in their framework and assumptions, novelty compared with existing literature, and comparison studies.","The authors study the POMDP problem from the causal perspective, and the propose to combine offline and online data to infer the transition model via deconfounding. On the theoretical side, they show that the proposed method is correct and efficient in terms of generalization guarantees. On the experimental side, they evaluate the proposed method on three synthetic toy problems. ",0.17142857142857143,0.18095238095238095,0.1619047619047619,0.20588235294117646,0.11764705882352941,0.13008130081300814,0.17647058823529413,0.15447154471544716,0.288135593220339,0.17073170731707318,0.2033898305084746,0.2711864406779661,0.1739130434782609,0.16666666666666669,0.20731707317073175,0.18666666666666668,0.14906832298136646,0.17582417582417584
60,SP:288ce587a277299765bdd4cea75a8c23e12de2b0,"This paper proposes a new variant of GCN model, namely BankGCN, based on a novel graph convolutional operator. The main idea of BankGCN is to the sharing scheme among different filters to adaptively capture information from different frequencies. Through detailed discussion, BankGCN is claimed to be equivalent to the learnable message passing mechanism across K-hop neighborhood. Experiments on graph classification tasks over a collection of graph benchmark datasets demonstrate that BankGCN could outperform many state-of-the-art spectral-based GNNs.","The manuscript proposes a novel convolutional operator dubbed BankGCN, that given a node input signal, first projects it in different subspaces using a group of projection functions (one for each subspace), and then applies to each projection an adaptive filter.  As projection functions, the authors use Linear projections. The model was tested on the Graph classification task.","This paper proposes BankGCN, which simplifies spectral graph neural networks by utilizing an adaptive filter bank to extend the capabilities of GCNs beyond low-pass features. Compared to existing spectral graph convolutional networks, which have numerous free parameters, the proposed method reduces parameters by sharing learnable filters. The empirical study validates that BankGCN shows good performance on the graph classification task.","The authors introduce BankGCN, a graph convolutional network that learns (chebyshev) polynomial filters over the graph. BankGCN includes an initial subspace projection, and a cosine similarity regularization to encourage diverse filters. BankGCN is compared to a number of other graph networks on standard whole graph classification tasks.  The authors claim that most architectures of MPGCNs are limited in that they (1) focus on low frequency information and (2) lack a proper sharing scheme between filters, and that BankGCN addresses these two issues. ",0.14634146341463414,0.18292682926829268,0.21951219512195122,0.21052631578947367,0.17543859649122806,0.21311475409836064,0.21052631578947367,0.2459016393442623,0.21951219512195122,0.19672131147540983,0.12195121951219512,0.15853658536585366,0.1726618705035971,0.2097902097902098,0.21951219512195122,0.20338983050847456,0.14388489208633093,0.1818181818181818
61,SP:28ac9848fb69d1c59fd751fbeee9a4ac799db897,"The paper is tackling the topic of fine-grained representation. In contrast to comparable methods, ARP focus on sub-sampled via strided operation to represent fine-grained information.  ARP is an intuitive strategy to extract critical regions and is more concise compared with multi-stage cascade architecture. This fact facilitates the efficiency.","- **Motivation**. The paper argues that    - (a) there are some small but important regions of an interesting object, which are usually ignored by the previous fine-grained methods;   - (b) excessive reduction operations on image resolution fade the discriminative features.     - **Method**. According to the above observations, the paper proposes a pooling algorithm called Adaptive Region Pooling (ARP). This module has two procedures:   - (a) learn to crop image regions;   - (b) downsampling various size regions using bilinear operation.   - **Experiments**. The proposed method is verified in two tasks: fine-grained classification and re-identification. ",This paper presents an adaptive region pooling (ARP) method for fine-grained representation learning. ARP crops the features with the estimated scale. The features are further downsampled to a consistent size through bilinear downsampling. Experiments conducted on two tasks validate the effectiveness of the proposed method.,This paper proposes an adaptive region pooling (ARP) module that adaptively estimates the scale factors and crops the discriminative regions based on the estimated scale factors. It aims to focus on the most discriminative region and simultaneously contain more fine-grained information. The authors apply the ARP module to fine-grained image recognition and Vehicle Re-Id tasks.,0.23076923076923078,0.1346153846153846,0.21153846153846154,0.16853932584269662,0.1797752808988764,0.34782608695652173,0.1348314606741573,0.15217391304347827,0.1896551724137931,0.32608695652173914,0.27586206896551724,0.27586206896551724,0.1702127659574468,0.14285714285714288,0.2,0.22222222222222224,0.21768707482993196,0.3076923076923077
62,SP:28fe2b3deb6a8f24f26d48240da38d280673b8f2,"This paper proposes an algorithm for choosing a small set of labels to improve labeling function model performance both directly and for downstream tasks. Additionally, the authors provide a general method to convert standard labeling functions to ""soft"" labeling functions which are differentiable with respect to some parameters (e.g. a threshold). If the labeling functions are differentiable, this paper provides a method to update the labeling function parameters. Finally, experimental results show that the method introduced outperforms other active labeling approaches for weak supervision.","The paper gives a method to iteratively and interactively improve the label model in weak supervision. The approach consists of two steps, first is standard weak supervision way of weighted combination of labelling functions to generate labels. Novelty and improvement mainly comes from the second step, where true label for most uncertain data point is queried using which the parameters of the labelling functions are improved, which in turn lead to a more accurate weak supervision model. A key requirement and assumption in this paper's setup is that labelling functions are given by some learnable parameters ( i.e. they can be differentiated w.r.t. their parameters), which allows parameters updates using the true labels acquired. Empirical results on various real world datasets in medical domain show that in some cases this approach can yield a more accurate model in comparison to pure active learning approach and some recent baselines which combine weak supervision with active learning. These results also show that the paper's approach can get accuracy comparable to fully supervised model as well.  ","In this work, the authors propose the WARM method to help conduct iterative and interactive weakly-supervised learning. Active learning is used here to refine the labeling functions by focusing on data points that are once labeled. The authors further incorporated gradient propagation to alternatively update the LF parameters and the DP model. Experimental results show that the WARM method can improve the quality of training data.","This paper proposes WARM, an active learning approach to weakly/programmatically supervised learning.  In the WARM approach, which bases off of the data programming/Snorkel paradigm for weak supervision, users write labeling functions (LFs) to programmatically label training data; these labeling functions are then modeled by the Snorkel framework for weak supervision and used to train downstream models.  In the WARM setup, these LFs are assumed to be, or cast as, differentiable.  The paper then proposes an active learning approach to sampling labeled data points to tune the parameters of these LFs, and validates this approach on several medical datasets.",0.27058823529411763,0.23529411764705882,0.23529411764705882,0.1016949152542373,0.15254237288135594,0.2537313432835821,0.12994350282485875,0.29850746268656714,0.2,0.26865671641791045,0.27,0.17,0.17557251908396948,0.26315789473684204,0.2162162162162162,0.14754098360655737,0.19494584837545126,0.2035928143712575
63,SP:296102e60b842923c94f579f524fa1147328ee4b,"The authors consider the problem of few-shot attribute learning. Contrary to most past approaches on learning with attributes, including classical zero-shot learning, the authors deal with the case where the attributes seen at test-time are previously unseen. As a result, few samples must be used to quickly adapt the learning algorithm to new attributes. The approach works in 3 stages. The first uses a classical representation learning algorithm to learn a visual representation, SimCLR. Following this, the network is given supervised training to fine-tune the representation. In the last step, the authors adapt a previously devised few-shot learning strategy that consists in training a linear classifier. ","This paper investigates a new and interesting topic, few-shot attribute learning (FSAL), where the model is trained with base attributes with abundant samples and tested with novel attributes with only a few samples. Experiments were performed on three benchmark datasets with attribute annotation. The paper found that self-supervised learning helps to train a backbone that generalizes well to novel attributes, compared to training the backbone with only base attributes. The author also compares several few-shot learning models, e.g., MatchingNEt, MAML, ProtoNet, in this FSAL setting and found that the logistic regression layer works the best.","The article proposes to address attribute detection with a small data constraint in the training phase (few-shot approach). The experimental protocol is divided in a series of episodes, each defined by a support set that specifies the attributes to be predicted and a query set used to evaluate the prediction. The generic algorithmic structure to solve this problem is divided in two steps: a common pre-training phase that combines self-supervised representation learning and supervised fine-tuning, followed by a supervised learning phase to solve each episode task. A training dataset containing image samples with attributes is used to learn the pre-trained representation space. The pre-training phase is compared to other approaches on three benchmarks implementing few-shot attribute prediction episodes. An indicator computed from a logistic regression between attribute values is proposed as a way to predict transferability between the learning dataset and the few-shot problem. ","This paper proposes the problem of few-shot attribute learning (FSAL) that follows the standard few-shot/meta-learning paradigm but focuses on attributes instead of object classes. The authors argue in Section 2 that the multi-label nature of attributes (smiling & wearing eyeglasses can be present simultaneously) makes this problem different; the context of each positive example in each episode can drastically change what the model is asked to learn.   This paper also proposes three benchmark datasets to study this problem, based on Celeb-A, Zappos50K, and ImageNet-with-Attributes. The paper evaluates multiple existing approaches on these benchmarks as well as improves upon them further. ",0.2072072072072072,0.23423423423423423,0.1981981981981982,0.24242424242424243,0.1919191919191919,0.16447368421052633,0.23232323232323232,0.17105263157894737,0.205607476635514,0.15789473684210525,0.17757009345794392,0.2336448598130841,0.21904761904761905,0.1977186311787072,0.20183486238532108,0.19123505976095617,0.1844660194174757,0.19305019305019303
64,SP:29a42fdae15b9da955513f71e3100ebd0146a28a,This paper presents a neural-network simulator that learns to solve constraints inspired by classic physics-based simulation. The proposed simulator uses a graph neural network to encode a constraint solver and shows results in a number of simulation environments.  The main contribution of the paper is its idea of using graph networks to encode constraint-based simulation. ,"This paper presents to simulate physics via a constraint-based approach instead of direct prediction. In particular, the authors first employ GNN taking as input the history positions and dynamics, to capture the interaction between different particles within the system, whose output is considered as the constraint satisfaction scalar. Then, the gradient with respect to the constraint function is applied as the update of the dynamics over a certain number of solver iterations. The experiments are conducted on a variety of challenging physical domains, including simulated ropes, bouncing balls, colliding irregular shapes and splashing fluids.",This manuscript proposes to learn the numerical solutions to Lagrangian physical simulation with a constraint-based inference method on graph neural networks. This method involves an iterative update during inference and thus enables test-time dynamical correction. The contributions are: (1) this manuscript builds a scalar predictor to indicate how well the constraint is agreed. (2) this manuscript proposes using the graph neural networks as the backbone to deal with a variable length of the physical domain. They also examine the effectiveness with a bunch of experiments including the state prediction experiment on four different environments and multiple ablation studies towards different hyper-parameters.,"This paper aims to add explicit/human-defined constraints to learning-based simulation frameworks, where a learned constraint function implicitly regularizes the dynamics, and future predictions are generated via a constraint solver. The authors built the framework on top of graph neural networks (GNNs) to capture the compositionality of the underlying system and enforce the constraint using an implicit constraint function optimized via gradient descent.  The authors tested the proposed method in four physical simulation environments, including rope, bouncing balls, bouncing rigids, and BoxBath. Experiment results show that the proposed C-GNS has a competitive or better performance compared to prior learned simulators. In addition, they have also demonstrated that C-GNS can generalize to unseen, hand-designed constraints by applying more solver iterations than experienced during training to improve the accuracy on larger systems.",0.2413793103448276,0.2413793103448276,0.3103448275862069,0.22105263157894736,0.2631578947368421,0.21153846153846154,0.14736842105263157,0.1346153846153846,0.13333333333333333,0.20192307692307693,0.18518518518518517,0.16296296296296298,0.1830065359477124,0.1728395061728395,0.1865284974093264,0.21105527638190957,0.21739130434782608,0.18410041841004182
65,SP:29e2e1daa6a32ef71ad225bd2fc27e33dece86c5,"[Sorrenson et al 2020] proposed to do nonlinear ICA via volume preserving mixing functions, when a variable u is observed that makes the latent variables conditionally independent. Via maximum likelihood optimization, the ICA model uses a volume-preserving encoder and then maximises a latent distribution, which is a factorized Gaussian conditional on the observed variable u.  The submitted paper does a theoretical identifiability analysis on that procedure. It assumes that the data is generated by an independent distribution, given observed u, then mixed by a volume-preserving mixing function. Furthermore, it assumes that both the mixer and the encoders are twice differentiable, there exist two observed u variables, u^1 and u^2 such that that the two resulting distributions of encoded latent variables have the same mean and that the set of ratios of the two standard deviations for each variable is distinct. If these assumptions hold, the paper proves that the original sources are identifiable up to a permutation and dimension-wise diffeomorphism.  The authors do three experiments: 2D mixture of Gaussians, triangle images and MNIST. For the first two, the authors quantitatively estimate the source identification and for all datasets, qualitative results on identification are given.","In this work, the authors propose a framework for nonlinear ICA, in which the mixing function is a volume-preserving transformation, and the conditions for the sources can be relaxed compared to some prior works. The authors prove the identifiability of the proposed framework and implement the framework by volume-preserving Flow-based models. Numerical experiments on both synthetic and real data are performed to corroborate the theoretical results.","The paper proposes a framework for nonlinear ICA with that restriction that the mixing function is a volume preserving transformation. The authors prove that given this restriction as well as a few other conditions the sources are identifiable. The model set up is very similar to the one used in Sorrenson et al.'s work ""Disentanglement by nonlinear ica with general incompressible-flow networks (gin)"": A real NVP model is made volume preserving by a slight modification of the flow map and this flow is used to learn the mixing function. The sources are assumed conditional independent given an auxiliary variable $u$, which are the labels of classes in their experiments. The mixing function is learned by maximizing the Gaussian likelihood of the reconstruction sources, which is also conditionally independent given $u$.  The authors demonstrate empirically that some of the conditions of their identifiability proof might not be necessary because the sources can still be reconstructed empirically although the condition is not satisfied. Hence, identifiability might even hold true for an even larger class of problems. They also compare their method to iVAE and show that their framework can identify the true sources of generated data much better than this baseline. Furthermore, they apply the framework to MNIST and can identify a few important source variables explaining most of the variance of the dataset.","The authors propose to use volume preserving transformation to solve the disentanglement problem in latent variable models such as ICA. The authors have explained that in literature that ICA can be expressed as a factorial member of the exponential family, however, this arrives at a solution which is not identifiable. The authors have explained that there are two ways of achieving disentanglement, this could be done by either (1) restricting the sources or (2) restricting the function f.   Interestingly that authors have proposed to use a volume-preserving transformation typically found in normalizing flows to be able obtain identifiability. There is a volume preserving transformation from the sources to the mixed signal which represents the generative process. Then there is a volume preserving transformation from the mixed signal to the latent space (which is not the source signal). Then a factorial multivariaate Gaussian is used to identify the conditional independence of each source signal.  In the classical ICA problem setup, the dataset does not contain any labels. While in this setting the input dataset for ICA appears to contain labels (u?).",0.10552763819095477,0.19597989949748743,0.1658291457286432,0.42028985507246375,0.3188405797101449,0.17410714285714285,0.30434782608695654,0.17410714285714285,0.18232044198895028,0.12946428571428573,0.12154696132596685,0.2154696132596685,0.15671641791044777,0.18439716312056734,0.17368421052631577,0.19795221843003413,0.17599999999999996,0.1925925925925926
66,SP:29f44f2f7d0e9748eed6732ed19ca3335acb04e3,"This paper introduces a (possibly) cheaper approximation of Sliced Wasserstein (SW) distance. The proposed technique is based on calculating 1D Wasserstein distance between Gaussian projection directions. This is motivated by the fact that Wasserstein distance between Gaussian distributions admits a closed-form solution, which leads to an easy compute of the proposed approximate SW and discard a large number of uniformly sampling random projections for the Monte Carlo approximation of the original SW distance. ","This paper develops a novel deterministic approximation for the Sliced-Wasserstein Distance (SWD), based on the fact that a random linear projection of a high-dimensional vector has an approximately Gaussian distribution. In many cases, the approximation involves no tuning parameters and can be computed much more quickly than the usual Monte Carlo estimate of SWD, especially for very high-dimensional data. If the $d$ dimensions of the data are not too strongly correlated, then the approximation converges no slower than $d^{-1/8}$ as $d \to \infty$. Experiments with synthetic data validate this theoretical result, as well as the computational benefits of the proposed approximation. Finally, the paper demonstrates an application to image generation.","The paper presents a method to estimate the Sliced-Wasserstein distance. The method is based on the Gaussian approximation of the projected data, thus the SW distance could be approximated through the SW distance between Gaussians which has a closed-form. Hence, the method is better than the Monte Carlo SW distance in term of computational speed. The authors also demonstrate their method on toy data set and real data sets, i.e. MNIST and Celeb A.","The paper proposes a new approach to approximate the sliced Wasserstein distance. In particular, the paper extends the central limit theorems for random projections to derive a deterministic approximation of sliced Wasserstein (based on the closed form of Wasserstein distance between two Gaussians) that is efficient (low error) in high dimension under centering assumption (weak dependence assumption). On the experimental side, the paper illustrates the derived theory on the synthetic data and compares the proposed approach to the conventional Monte Carlo approximation of the sliced Wasserstein distance.",0.2972972972972973,0.2972972972972973,0.2972972972972973,0.17391304347826086,0.19130434782608696,0.2857142857142857,0.19130434782608696,0.2857142857142857,0.25287356321839083,0.2597402597402597,0.25287356321839083,0.25287356321839083,0.23280423280423282,0.2913907284768212,0.2732919254658386,0.20833333333333334,0.21782178217821785,0.2682926829268293
67,SP:2af5c866ed17f156b406153d3261baaa42cf95fb,"## Summary and contributions. Authors propose 3D ""spherical neurons"" leading to rotationally equivariant layers. They do so by building on the spherical and geometric neurons introduced in Melnyk et al. (2021), which leverages the conformal space (for $\mathbb{R}^n$) to perform operations. The authors then solve for the steerability constraint for this neuron and empirically show that the proposed approach overperforms Melnyk et al. (2021) on rotated 3D data. ","The paper proposes a method for constructing steerable spherical neurons, building on the recent Geometric Neurons developed in Melnyk et al 2021. The main technical result in the paper is a steerability constraint for a geometric neuron, as given by eq. (13) in the paper. This constraint is used in an implementation whereby a steerable model is constructed and is then use for two tasks. The first task involves the use of steerable spherical neutrons for the classification of 3D Tetris objects seen under rotations, and the second is a similar experiment applied to 3D skeleton data. The results demonstrate a very large performance boost when using the steerable versions. The second experiment builds on this idea to construct a version that adapts a possibly imperfect initial rotation estimate, using the representation. This second experiment is in the spirit of demonstrating equivariance under 3D rotations with perturbations.","This paper proposes to make the geometric neurons of Melnyk et al.'21 to be steerable so that objects undergoing arbitrary rotations can be classified with higher accuracy. This is done in multiple stages. First, the neurons of Melnyk et al. are trained to convergence with a hyperspherical output layer. Then, the frozen weights are transformed such that the input of a *steerable neuron* can be written as a linear combinations of the rotated versions of itself. The experiments act as a sanity check while demonstrating the validity of the algorithm on a simple human pose dataset.","The paper aims to derive a steerability constraint for spherical neurons (3D point classifiers with spherical decision boundary). The steerability constraint enables test-time optimization of a pre-trained classifier to make predictions equivariant to 3D rotation perturbations applied to the input. When input rotation perturbations are unknown, the authors propose a method to recover the unknown rotations and therefore make rotation-invariant predictions. The experiments on a few small scale datasets verifies some of the claims.",0.3333333333333333,0.2028985507246377,0.15942028985507245,0.17006802721088435,0.1360544217687075,0.18556701030927836,0.1564625850340136,0.14432989690721648,0.14285714285714285,0.25773195876288657,0.2597402597402597,0.23376623376623376,0.21296296296296297,0.1686746987951807,0.1506849315068493,0.20491803278688525,0.17857142857142858,0.20689655172413793
68,SP:2b3916ba24094c286117126e11032820f8c7c50a,"This paper proposes a pipeline to induce geometrical deformations due to expressions on a person’s 3D face. The existing works either uses single image based 3D reconstruction that manifest the expression present in image or learn a neural representation of the deformation (latent code) which can be used to transfer the deformation due to expression from one face to other. This paper uses direct expression parameters and action units instead of a latent code and predict the deformation on 3D face for an expression instead of manifesting it from image (unlike reconstruction methods). To this end, the authors use an existing robust single image based 3D face deformation method (Chen et al.) to initialise the texture map and geometry as a displacement map of a 3D face (using Basel Face Model) from an image.  Because there is no ground truth of the new expression, they train it using adversarial loss. They produce the deformation using the Action Units(AU) as input and tries to produce those AU from the output displacement map. For rendering, the method uses the Neural Texture which is trained with the texture map produced by the Chen et al. The rendering has two components. One is the coarse level rendering which utilises the shape and expression parameters of the Basel Face model and the detail rendering consisting of the information of the displacement map due to the AU obtained through this method. Both are combined to get the final appearance.  They compared the result with DECA (Siggraph 21) which is a FLAME 3D model based method and shows that the identity, shape and expression rendering is better in the proposed method. ","The paper proposes to improve the quality of the 3D reconstructed faces by taking into account the facial expression. Using a set of generative losses some of the details are recovered or hallucinated. The visual examples provided confirm the improvements, however better numerical evaluations and comparisons are required.","1. This paper proposes a new method FaceDet3D that can generate geometric facial details from a single image given any desired target expression. The method contains two components: first pass the input image to the Detail Hallucination Network to infer the geometric facial details of the subject as there expression changes; then a rendering network is used to generate the detailed rendering result using the 3D face geometry information.  2. The two component models are trained separately using a large scale in-the-wild images and a small video dataset. During the training of the rendering network, the novel Augmented Wrinkle Loss and Detailed Shading Loss were used.  3. The authors further evaluated the method and compared it to DECA to show that the proposed method could generate plausible facial details for any desired expressions and could render the result photo-realistically. 4. Ablation studies were also done to prove the effectiveness of the components in the proposed method. ","They propose FaceDet3D to generate facial expressions with details. To do this, they propose a Detail Hallucination Network to generate the target detail map from the source detail map along with Expression Adversarial Loss [1] and Superresolution Losses. Then they propose a Rendering Network. In addition to Photometric Loss and Expression Adversarial Loss [1],  they add Augmented Wrinkle Loss and Detailed Shading Loss. The result is an image with target expression containing details (mainly wrinkles), comparison with DECA (a previous work) shows they improve FID and FaceID by large margin.  [1] Conditional Image Synthesis With Auxiliary Classifier GANs ",0.06159420289855073,0.15217391304347827,0.09057971014492754,0.3125,0.16666666666666666,0.1761006289308176,0.3541666666666667,0.2641509433962264,0.25510204081632654,0.09433962264150944,0.08163265306122448,0.2857142857142857,0.10493827160493828,0.19310344827586207,0.13368983957219252,0.14492753623188406,0.10958904109589039,0.2178988326848249
69,SP:2bcf42173d9d82fb3e517405deba4aa3d6f9d8d6,"This paper proposes RoMA, a robustness evaluation framework based on local sampling and probability computation.  The main contributions are: 1. Proposal of the ($\epsilon$,$\delta$) local robustness score for assessing the probability of random local samples that have different predictions than a given data input with a $\delta$-confined top-1 confidence.  2. Use of Box-Cox transformation for input data to improve statistical estimation.  3. The method can be implemented in a model-agnostic fashion. ","The paper presents a statistical method - Robustness Measurement and Assessment (RoMA) to measure the expected robustness of a neural network model. The robustness is defined as the probability that a random input perturbation causes an incorrect prediction. The presented approach is a blackbox approach. Different output labels are observed to exhibit different robustness values.   The basic premise of the paper is that the adversarial perturbations are not naturally normal, but a transformation (Box-Cox) can be applied to make them a normal distribution before applying statistical estimation techniques (Anderson-Darling test + z score). ","Present a method to measure the expected robustness of a neural network model, by determining  the probability that a random input perturbation might cause misclassification, providing formal guarantees regarding the expected frequency of errors that a trained model will encounter after deployment. The method can be applied black-box. Applied the approach to compare the robustness of different models, and measure how a model’s robustness is affected by the magnitude of input perturbation. ","Summary:  The paper introduces a statistical method to measure the robustness of deep neural networks. The novel  part of the method is that it's designed to measure the probability of random points near an input being  adversarial, instead of probability of adversarial examples existing in the vicinity of an input. To measure it,  the authors proposes to use Box-cox transformation to transform distribution of confidence scores to normal,  then calculate the probability based on it.  ",0.2631578947368421,0.19736842105263158,0.2236842105263158,0.3010752688172043,0.25806451612903225,0.25675675675675674,0.21505376344086022,0.20270270270270271,0.22077922077922077,0.3783783783783784,0.3116883116883117,0.24675324675324675,0.2366863905325444,0.2,0.2222222222222222,0.33532934131736525,0.2823529411764706,0.2516556291390728
70,SP:2eb193c76355aac08003c9b377895202fd3bd297,"Existing NAS benchmark datasets (e.g., NAS-Bench-101) do not provide full training information of architectures, which make it infeasible to evaluate the multi-fidelity algorithms (e.g., learning curve extrapolation). To address these issues, this paper create surrogate benchmark datasets, namely NAS-Bench-111, NAS-Bench-311 and NAS-Bench-NLP11, with singular value decomposition and noise modeling methods. Extensive experimental results with the full training information on these datasets show the superiority of the proposed method.","This paper proposes another benchmark dataset for NAS, NAS-Bench-x11. Compared to the original benchmarks NAS-Bench-101, NAS-Bench-301 and NAS-Bench-NLP, NAS-Bench-x11 contains the full training information for each candidate architecture especially the learning curves so that it allows multi-fidelity NAS algorithms  NAS-Bench-x11 is a surrogate benchmark, which trains a surrogate model to predict the learning curve of the vast amount of candidate architectures. Prediction of the learning curve can be accomplished by singular value decomposition and noise modeling.  ","The authors describe a mechanism to predict learning curves which they use to generate surrogate benchmarks from existing NAS-Bench versions without learning curves. The authors explore different ways to create learning curve embeddings, different ways to predict these embeddings using the architecture representation as an input and different ways to model the training noise. Furthermore, the authors discuss a short study on using learning curve extrapolation in context of speeding-up NAS.","Existing NAS benchmarks often only include the architecture performance at the final or some selected checkpoint epochs. However, per-epoch information is essential for developing and evaluating multi-fidelity NAS algorithms. This work proposed to fit a surrogate model to predict the learning curve given an architecture encoding. Utilizing the proposed predictor, the author extends three existing benchmarks NASBench-101, NASBench-301, and NASBench-NLP to include per-epoch accuracies. Further, to demonstrate the necessity of developing multi-fidelity algorithms for NAS, the author also describes a procedure to extend the single-fidelity algorithms to multi-fidelity ones by using learning curve extrapolation to filter out candidate proposals with partial training. Despite possible room for improvements on the performance of the learning curve predictor used, the proposed method would enable researchers to develop and rapidly test multi-fidelity NAS algorithms. I vote for acceptance.",0.24050632911392406,0.16455696202531644,0.26582278481012656,0.14606741573033707,0.2808988764044944,0.2602739726027397,0.21348314606741572,0.1780821917808219,0.14583333333333334,0.1780821917808219,0.1736111111111111,0.13194444444444445,0.22619047619047616,0.17105263157894735,0.18834080717488788,0.16049382716049382,0.2145922746781116,0.17511520737327188
71,SP:2f6e266b03939c96434834579999707d3268c5d6,"The paper introduces an INR-based design that utilizes an MLP network to encode spatio-temporal dynamics of video. The authors also propose an efficient discriminator to detect unnatural motions. The design is an extension of INR-GAN (Skorokhodov et al., 2021) architecture. The paper claims that the method obtains SOTA performance on the UCF101 dataset. ","The paper proposes a video generation approach based on implicit neural representations (INRs). They construct a ""dynamics-aware"" GAN; where by the generator model used derives from the coordinate-based models used to capture continuous representations of images in prior  INR work. The discriminator architecture derives from the 2D discriminator traditionally used in image GAN research; they simply expand the input channels from the traditional 3 channels (i.e. rgb) used; to 7 input channels. This discriminator processes a stack of images from one video simultaneously, two frames from different time points in the video and their difference image.","The paper present a new method to video generation. They built on top of the INR-GAN framework and extend it to temporal domain. They, therefore, get continuous time and space image generation--something all other video methods lack (technically one should be able to interpolate motion codes of existing CNN-based works, but spatially none of them is continuous to my knowledge). They model a video as a function $f(x,y,t)$. They condition this functions on motion codes and on content codes. The upper layers get information from the content, the lower layers get information from both content and motion. They further introduce a discriminator that is conditioned on the time difference between frames. It allows them to train on longer videos.  ","This paper leverages the implicit neural representations paradigm to build generative adversarial networks for video generation. Implicit neural representations encode continuous signals into parametrized neural networks, and the authors claim that this mitigates the issue related to the inefficient modeling of videos as 3d tensors of RGB values. Their GAN includes an INR-based video generator, and a motion discriminator that is more efficient at identifying unnatural motions. Their approach yields FVD improvements in various existing datasets.",0.21428571428571427,0.23214285714285715,0.17857142857142858,0.18181818181818182,0.1414141414141414,0.12,0.12121212121212122,0.104,0.12987012987012986,0.144,0.18181818181818182,0.19480519480519481,0.15483870967741933,0.143646408839779,0.15037593984962408,0.16071428571428573,0.1590909090909091,0.1485148514851485
72,SP:2fb4af247b5022710b681037faca2420207a507a,"The paper addresses multi-goal reinforcement learning. It presents empirical results with a variation of an existing algorithm, Hindsight Experience Replay (HER). Specifically, the paper uses HER alongside AlphaZero. ","This paper presents AlphaZeroHER, an extension of the AlphaZero learning-informed Monte Carlo Tree Search algorithm in which the ideas of Hindsight Experience Replay (HER) are applied to augment training and therefore better support goal-directed planning tasks. Consistent with the general HER procedure, the approach generates new data under the assumption that the goal was one of a set of sampled ""subgoals"", requiring that that the reward and target policy be recomputed. The target policy is chosen to be the empirical policy achieved during the episode, for though it is not optimal for the new goal, it did indeed reach the subgoal. The authors demonstrate performance on a handful of goal-directed planning tasks—including BitFlip, navigation, and a quantum compiling task—on which they show improved performance over AlphaZero without the addition of HER."," The authors discuss the impact of Monte Carlo Tree Search (MCTS) algorithms.  MCTS allows us to roll out trajectories using state value estimates to determine local actions, however computational cost can be high. This means that the approach is prohibitive for large problem spaces.  RL seeks to learn a control policy that generalizes well over the state space. This can be challenging and MCTS algorithms can help with this.  In goal reaching problems the agent takes as input the current state of the env as well as the goals state.  The only trajectories that confer positive reward are those where the goal is reached. RL can struggle here as the rewards are sparse due to the success criteria.  More informative reward functions can be used (e.g. distance to goal) but this isn't always possible.  Hindsight Experience Replay (HER) is applied to offline RL algorithms to improve sample efficiency where a replay buffer is maintained over retained trajectories and a set of sub-goals are defined over visited states with returns applied accordingly.  The authors propose to apply HER to AlphaZero to solve goal directed tasks and thereby avoid computationally intense tasks. They evaluate the approach against a set of simulated environments and compare to AlphaZero, including a novel Quantum compiling environment.  Much of the challenge in solving games like Go and Chess comes down to the size of the state space.  AlphaZeroHER works by sampling new sub-goals from already visited states and thus avoiding both the high computational cost involved in tree re-weighting and also the problem of sparse rewards.   This is done by retracing episodes and sample M subgoals from a trajectory of length T and training the policy and value networks on these hindsight inferred goals.  Since AlphaZero is on on-policy the policies used during play are retained when computing updates from the sub-goal rollouts which alleviates the computational burden.  The authors show that AlphaZeroHER outperforms AlphaZero over a number of simulated environments where the latter often fails to achieve learning much while the former achieves strong results.   ","For the AlphaZero approach, the rewards of goal-directed planning environments are too sparse to learn efficiently. To solve this problem, the authors proposed the AlphaZeroHER method by combining AlphaZero with Hindsight Experience Replay(HER) to enable AlphaZero agents to learn in goal-directed environments. The experiments showed that AlphaZeroHER is better than AlphaZero on BitFlip, 2D Navigation Task, 2D Maze and Quantum Compiler environments. ",0.3793103448275862,0.3448275862068966,0.3103448275862069,0.29411764705882354,0.13970588235294118,0.07514450867052024,0.08088235294117647,0.028901734104046242,0.13846153846153847,0.11560693641618497,0.2923076923076923,0.4,0.13333333333333336,0.05333333333333334,0.19148936170212766,0.16597510373443983,0.1890547263681592,0.12652068126520682
73,SP:2fdca838ac3453e44cff395f1b760d839a5813bf,"This is a mainly theoretical paper that studies the capacity of equivariant representations, which is, the ability to linearly classify objects under different transformations of a symmetry group of interest. The main result in the paper is showing that the number of dichotomies (different binary classifications) is related to the dimension of the fixed subspace of the representations, rather than the dimension of the representation. The authors discuss several examples and also propose several usages of this theory to deep learning of symmetric objects: analysis of pooling operations and induced representations ","This paper addresses an underexamined area, the question of expressivity in equivariant neural networks.  In general, there is a large gap between our theoretical understanding of expressivity and generalization for non-equivariant networks and that for equivariant ones; this paper helps to close the gap.  In particular, given a set of equivariant representations coming from an equivariant architecture, the authors determine how expressive these features are for an invariant binary classification problem.  To do so, they define a reasonable generalization of perceptron capacity for invariant classifiers and compute it.  The answer turns out to be relatively simple and is mainly determined by the multiplicity of the trivial representation in the group representation type of the equivariant representation, which is not difficult to compute.  The authors also consider the case of useful but not quite equivariant operations such as pooling. Lastly, they provide a convincing empirical verification of their results.","The paper provides an analysis of network capacity of group equivariant NNs. The analysis is based on determining the fraction of linearly separable dichotomies. It is found that this fraction is determined by the the number of trivial irreps that appear in the decomposition of the representations into irreps. I.e., the fraction is determined by the number of sub-spaces that are left-invariant by the group action. The paper is intuitive, well written, and theoretical results are confirmed by experiments.","This paper studies the expressivity of a representation constrained by group equivariance. The authors first use a classical notion of the perceptron capacity to offer a quantification of the expressivity. Then, an algorithm is designed to efficiently classify the $\pi$-manifolds. Besides, the authors further apply the theoretical results to some practical examples and give bounds on the capacity when pooling operators exist.",0.2967032967032967,0.23076923076923078,0.1978021978021978,0.15436241610738255,0.14093959731543623,0.18292682926829268,0.18120805369127516,0.25609756097560976,0.2857142857142857,0.2804878048780488,0.3333333333333333,0.23809523809523808,0.22499999999999998,0.24277456647398843,0.23376623376623376,0.19913419913419914,0.19811320754716982,0.20689655172413793
74,SP:318ace9202e42d1d278eb79fe1853138e1d00a06,This paper studies human bounded rationality using multi-agent reinforcement learning. It particularly focuses on the rational inattention model and proposes the RIRL framework which allows multi-timestep dynamics and information channels with heterogeneous processing costs. The proposed RIRL framework is evaluated in two specific Principal-Agent problems. ,"The paper proposes a RIRL, which incoporate rational inattention suboptimality into the optimal RL model. The authors simulate RIRL in a single agent and a multi-agent setting. They compare RIRL behavior with the rational model in those settings and discuss the emergent behavior of RIRL. ","Standard Bayesian-type decision making models have that given some signal X, our decision makers choose an action A to maximize utility given the signal X. This gives a conditional distribution A|X. Rational inattention models say that the there is limted processing capacity in the agent and model this by requiring that A | X has a capacity constraint (i.e. which under some extra assumptions essentially becomes maximum amount of mutual information).   The authors put this into RL by, basically, writing a policy P(a|s) into an observation component (O(observation | state)) and an action conditional on the realized observation with an information penalty (so the higher capacity the channels, the more penalty).   The authors apply this RIRL model to some canonical game theory games.","The paper proposes Rational Inattention Reinforcement Learning (RIRL), a multi-agent reinforcement learning framework that incorporates the behavioral model of rational inattention. Building on the classical formalization of rational inattention, the objective penalizes for the mutual information between the action selected and the observable state of the world. They also extend their framework to support multiple channels of information with heterogeneous cognitive costs.   They empirically evaluate RIRL in two principal-agent problems, building on classical models from the economics literature. The first simulation consists of a single agent, while the second simulation explores interactions between multiple agents following the RIRL-actor architecture (although only the principal faces inattention). ",0.20833333333333334,0.20833333333333334,0.4583333333333333,0.30434782608695654,0.34782608695652173,0.14173228346456693,0.21739130434782608,0.07874015748031496,0.2037037037037037,0.11023622047244094,0.14814814814814814,0.16666666666666666,0.2127659574468085,0.11428571428571428,0.28205128205128205,0.16184971098265893,0.20779220779220778,0.15319148936170213
75,SP:318b3c294a475960c13a4914b035fd3a2ea84661,"This paper considers imitation learning problem which aims at obtaining a policy that imitate expert behavior. Authors considers using reinforcement learning with a stationary reward constructed by expert datasets. Through theoretical analysis, the corresponding imitation policy is proved to achieve high expected per-step intrinsic reward and extrinsic reward. The difference between the expert policy and the imitation policy is bounded with a high probability. Some empirical experiments are performed on continuous control tasks. The results show this method is comparable with other algorithms while the algorithm is simpler. ","In this paper, the authors show that, for deterministic experts, imitation learning can be done by reduction to reinforcement learning with a stationary reward. theoretical analysis both certifies the recovery of expert reward and bounds the total variation distance between the expert and the imitation learner, showing a link to adversarial imitation learning. Experiments are given to confirm that the reduction works well in practice for continuous control tasks. ","The paper contributes a theoretical analysis of imitation learning (IL) under deterministic experts. The paper shows that IL in this setting can be reduced to RL with a stationary reward, and the stationary reward minimizes the total variation distance between the expert and the learner. Several empirical experiments were presented to validate the theoretical analysis.","The authors propose an algorithm for imitation learning which rewards the agent for observing state-action pairs that are part of the demonstration-set. The main contribution of the paper is the theoretical analysis which shows that the algorithm, given sufficient expert data, matches the expert’s occupancy distribution and expected reward. The proof comes in three parts: first, the authors relate expert rollouts to the limiting distribution of the expert’s policy. Second, the authors put a probabilistic bound on the expected agreement with expert-data based on the mixing time of the agent. Finally, the authors use this bound on expected agreement to bound the difference in expected extrinsic reward. The authors also provide an empirical evaluation in standard control benchmarks.",0.3146067415730337,0.21348314606741572,0.21348314606741572,0.36231884057971014,0.2753623188405797,0.2909090909090909,0.4057971014492754,0.34545454545454546,0.15447154471544716,0.45454545454545453,0.15447154471544716,0.13008130081300814,0.3544303797468355,0.2638888888888889,0.1792452830188679,0.40322580645161293,0.19791666666666666,0.1797752808988764
76,SP:33008c957718d546ecb2d7b8800ef5b03700ace4,"This paper proposes a transform-and-control policy to optimize the robotic agents' designs. Contributions include: - A novel perspective on agent design: rather than formulating agent design as a bi-level optimization, this paper embeds both design generation and control into a single decision-making process such that both design and control are optimized by the same RL algorithm. - In this formulation, the training experience from different designs is shared to improve sample efficiency. - Joint-specialized MLP on top of the GNN policy that further finetunes the control of individual joints.","The paper poses the problem of morphology design for robots as RL training of one joint GNN policy. Their policy first generates the robot's morphology and then evaluates the design with a common behavior policy that conditions on it. The authors also introduce a technique called JSMPL to allow asymmetric morphologies. Experiments in the Mujoco simulator demonstrate a large improvement over evolutionary methods in terms of sample efficiency and final performance (although the latter is less clear, as neither method has clearly converged). Ablation studies show that the GNN architecture is essential, but are less clear about the impact of JSMPL. ","The paper introduced a reinforcement learning algorithm that simultaneously optimizes the design as well as the controller of a simulated robot to perform locomotion tasks. The core idea is to train a conditioned policy that performs the task in three stages: 1) morphology design of the robot, 2) design parameter adjustment, and 3) controlling of the robot to perform the task. By integrating the design process into the policy learning framework, they are able to design novel and effective agents to complete a variety of tasks. To support the proposed algorithm, graph neural networks is heavily used to support different morphologies. They further propose a joint-specific architecture to improve flexibility of the network, which improves the performance of the algorithm.","The paper proposes an algorithm for simultaneous agent design and policy optimization. The choice of the body structure is treated as another action available to the agent. Therefore, the policy is parameterized by graph neural networks (GNNs), and it outputs i) the skeleton structure, ii) node attributes such as bone length, size, motor strength, and iii) motor control commands. Thanks to the parameterization via GNNs, the policy can be trained with PPO. Experiments show that the proposed method outperforms prior approaches, which mainly employ evolutionary methods for optimization, whereas the proposed method leverages more sample efficient policy gradient algorithms.",0.17582417582417584,0.21978021978021978,0.17582417582417584,0.20588235294117646,0.16666666666666666,0.17355371900826447,0.1568627450980392,0.1652892561983471,0.16161616161616163,0.17355371900826447,0.1717171717171717,0.21212121212121213,0.16580310880829016,0.18867924528301888,0.16842105263157894,0.18834080717488788,0.1691542288557214,0.19090909090909092
77,SP:336c1b8a7f293a78dfab18e7b454b0ec39822293,"This paper revisits the problem of neural network's systematic generalization ability from the perspective of meaningful learning, or more specifically, semantic linking. Based on this view, they propose two data augmentation methods from either the inductive learning perspective or deductive learning perspective. They train different model variants with such augmented data. The empirical results on SCAN, GEO, and ADV show that models can behave systematically. They further group some data augmentation methods on the machine translation task and semantic parsing task into the inductive or the deductive category, and show these augmentation methods can bring benefit in real data. ","The paper introduces an interesting idea of improving the systematic generalization ability via meaningful learning. Through providing augmented data for inductive learning and deductive learning, the sequence-to-sequence model can be more generalizable to compositions of new concepts. It tests on real data to provide evidence of the efficacy. ","This paper considers the problem of learning novel words from a few examples. The authors name their approach as ""meaningful learning,"" which, at a high level, means that we should relate the new word with existing words. The concrete technique they proposed is to use domain-specific rules to generate new data that contains novel words based on the existing examples.","This paperintroduce semantic linking for systematic generalization through the analysis of inductive and deductive learning from a meaningful learning perspective. They show that both prior knowledge and semantic linking play a key role in systematic generalization, which is in line with the so-called 'meaningful learning theory'. Interesting results are attained from SCAN to real data.",0.17,0.16,0.17,0.22,0.22,0.21311475409836064,0.34,0.26229508196721313,0.30357142857142855,0.18032786885245902,0.19642857142857142,0.23214285714285715,0.2266666666666667,0.1987577639751553,0.21794871794871798,0.1981981981981982,0.20754716981132074,0.22222222222222224
78,SP:340c5353a63884b49cfdc46ddb6153b28b2e894f,"This work proposes a theoretical framework to draw connections between several phenomena in machine learning observed empirically but never adequately understood- the relationship between excess model capacity, perturbation robustness, and backdoor poisoning. They show how under specific settings, adversarial training can detect backdoored data or recover a classifier robust to the backdoors at the very least. Similarly, they show how learning a backdoor-robust classifier is equivalent to pruning corrupted train data. They further demonstrate the applicability of their theorems on the MNIST dataset.","The authors propose a theoretical study on the robustness of binary classifiers against backdoors. To this end, they provide a theoretical framework and define a measure to assess classifiers' memorization capacity. This formulation allows the authors to demonstrate the existence of backdoors in some settings and explain why adversarial training reduces the vulnerability to backdoors. ","The paper gives a variety of backdoor-related theoretical results. In particular it,  1. gives a theoretical framework for discussing backdoor poisoning attacks, 2. shows that the attacker does not need to know the true data distribution to launch an attack against an ERM learner, 3. introduces _memorization capacity_, which measures the amount of off-distribution data a learner can memorize, and shows that nonzero memorization capacity is both necessary and sufficient for the existence of a backdoor attack 4. (the attack constructed in 4. can succeed with limited poisoned data regardless of how much clean data there is), 5. under assumption of finite VC-dimension, shows that adversarial training can be used to detect backdoor attacks, and 6. under the same assumption, shows that the problem of learning a backdoor-resistant model is equivalent to the problem of filtering poisoned data from the dataset. ","The paper offers a theoretical framework to analyze and understand backdoor attacks. They define a notion of memorization capacity to characterize vulnerability of the learning problem to backdoor attacks. Under this framework they give examples of learning problems where explicit backdoors can be constructed easily and problems which are intrinsically robust to backdoors. Further, they also show that under certain assumptions, backdoor filtering and robust generalization are nearly equivalent. Which suggests, it is sufficient to design backdoor filtering algorithms. ",0.15476190476190477,0.30952380952380953,0.21428571428571427,0.32727272727272727,0.2909090909090909,0.15172413793103448,0.23636363636363636,0.1793103448275862,0.22784810126582278,0.12413793103448276,0.20253164556962025,0.27848101265822783,0.18705035971223022,0.22707423580786024,0.22085889570552145,0.17999999999999997,0.23880597014925375,0.19642857142857142
79,SP:34e1b51ff5d524490332aed51b9c411209c89a20,"The manuscript describes GeneBERT, a self-supervised and multi-modal pre-training approach for genomic data. GeneBERT combines 1D genome sequence data with a 2D representation of regulatory elements in different cell-types in three different pre-training tasks. A large-scale single-cell ATAC dataset is used to identify pseudo-bulk ATAC profiles for 17 cell-types spanning > 1M genomic locations. The sequence of these genomic locations provides the 1D representation of the data which are used in self-supervised training with masked genome modeling and next genome segment prediction loss functions, both inspired by the BERT framework. A 2D representation is derived using accessibility per regulatory element and cell type, and is self-supervised using a infoNCE loss. The pre-training framework is then compared in a variety of biological tasks including promoter prediction, TFBS prediction, splice site prediction. The authors then present ablation studies highlighting the importance of the different components of the loss function.","This work proposes an approach, called GeneBERT, for pre-training genome data in a multi-modal and self-supervised manner. They take the 1d sequence of genomic data and a 2d matrix of (transcription factors × regions) as the input and optimize three pre-training tasks to improve the robustness and generalizability the model. Specifically, they introduce two main objectives for sequence pre-training, Masked Genome Modeling (MGM) and Next Genome-Segment Prediction (NGSP). GeneBERT consists of three main components: sequence pre-training, region pre-training, and sequence-region matching. The main contribution of the paper is that they use transcription factor information in genomic regions which makes the model more generalizable to other cell types than the previous DNABERT that only uses the sequence information. ","The authors describe a pipeline to perform self-supervised learning on genome sequences, guided by accessible chromatin peaks. Their learning procedure is inspired by the NLP method BERT and uses several tasks from that work and its successors. After pre-training transformer layers, they fine-tune on several regulatory sequence classification tasks and demonstrate high performance.","In this paper, the authors developed a transformer-based model, GeneBERT to align DNA sequences with regulatory elements. In particular, GeneBERT first applies transformers to learn representations of sequencing data and regulatory regions (e.g., open chromatin), and then aligns the representations of two modalities for identifying region-aligned sequences. The authors applied GeneBERT to recent scATAC-seq data in fetal and use aligned sequences to predict promoters, CTCF binding sites, a disease type and RNA splicing sites.",0.2088607594936709,0.0949367088607595,0.11392405063291139,0.088,0.12,0.16071428571428573,0.264,0.26785714285714285,0.23076923076923078,0.19642857142857142,0.19230769230769232,0.11538461538461539,0.2332155477031802,0.14018691588785048,0.15254237288135594,0.12154696132596683,0.1477832512315271,0.13432835820895525
80,SP:352c177d89b9460acee0c78364e6d9c153c6a93c,"This paper proposes a generation from a language model not only from an initial state, but also using a goal state. Instead of Brownian motion, the authors employ a draw from Brownian bridge by designating initial and end states, called Time Control. Experimental results show the proposed generation from Brownian bridge is more natural and coherent for text-infilling task, and also preserves text  structures both by automatic evaluation and human evaluation.","This paper introduces a method to enhance the global coherence of text generated from Language models. The proposed method (Time Control). Under the assumption in the latent space of sentence embeddings, the incoherent text can be seen as ""Brownian motion"" in the latent space. In order to enforce a goal to the generated text authors by fixing a start and end to this Brownian motion the process of text generation can be modeled as a Brownian bridge.   From this assumption, the authors drive a method that consists of three steps (1) training an encoder to map sentences to a latent plan defined as Brownian bridge (2) training a decoder to reconstruct sentences from the given context + the true encoded vector of the target sentence from planning latent space using the trained encoder (3) at inference time: given a start and endpoint, a target trajectory of vectors $z_0, ..., z_t, ..., z_T$ is sampled and use the decoder to generate a sentence based on this bridge.   Authors run several experiments to (1) evaluate the hypothesis that the encoder can capture local text dynamics using sentence order prediction task (2) evaluate the decoder to generate local incoherent text using the text-infilling task. (3) capture global text statistics by measuring the statistics (length of Wikipedia sections for city articles) of the generated text and compare them to the ground truth. (4) Evaluate the overall coherence of the long-generated text. Overall the results look convincing except for some caveats (see the areas of enhancement)  ","The authors propose to use a Brownian bridge process to model global coherence of a long piece of text. They show how to train such a model in an encoder-decoder style setup, using a contrastive loss to model the Brownian bridge dynamics. The authors then verify aspects of their model with a series of experiments to show that their model with an underlying generative process outperforms competing approaches on a variety of local and global coherence and generation tasks.","This paper proposes to model the evolution of sentences in a document via a stochastic process; specifically a Brownian Bridge process. The paper start off by assuming that the generated sequences by autoregressive models like GPT-2 follow Brownian motion in that they tend to get incoherent and ""meander"" in the semantic space. This paper aims to reduce this random behavior by pinning the endpoints of the trajectory and model the generation by Brownian bridge process instead. The key intuition behind this process is that given two endpoints z0 and zT, the evolution of z along time t is a Gaussian with mean that is some linear combination of z0 and zT. This paper models text by training an encoder for sentences x that produces the embedding z by training over triplets (x0, xt, xT) where 0<t<T that encourages zt to follow Brownian bridge dynamics and uses contrastive loss with a negatively sampled x't for training.  The approach is tested for local coherence, long range order sensitivity, and generation of long sequences and is compared against ablative and external baselines. The proposed approach does lead to learning of embeddings that are obtainable via linear combination and this leads to improved performance on sensitivity to sentence order in documents and document generation. ",0.3611111111111111,0.18055555555555555,0.2916666666666667,0.09881422924901186,0.17786561264822134,0.3125,0.10276679841897234,0.1625,0.09813084112149532,0.3125,0.2102803738317757,0.11682242990654206,0.16,0.17105263157894737,0.14685314685314685,0.15015015015015015,0.19271948608137046,0.17006802721088435
81,SP:35c14ef59d9ed68f3f6e3a8cac95fdf3216a9d8f,"This paper goes back to a well known algorithm, Relative Entropy Policy Search (REPS) and provides convergence guarantees for REPS by exploiting its relationship of RL via duality. Recently, there has been a surge of work exploiting the dual formulation of RL objectives, and this paper exploits that relation to performance convergence analysis of the REPS algorithm. Besides, very recently, a number of works has provided convergence of policy search and policy gradient algorithms - and the contributions of this work can be seen in light of recent works. The major contributoon of this work is to provide convergence guarantees for the dual objective of REPS, relying solely on on-policy samples and using accelerated gradient ascent on the dual formulation (whereas prior works exploiting duality often need to rely on off-policy samples, estimating distribution ratios).  ","The paper studies the convergence of the relative entropy policy search (REPS) method for solving LP-formulated MDP problems. The authors first assume the exact computation of gradients and prove the sublinear convergence of REPS regarding the regularized primal objective function and the policy. Second, the authors propose to estimate gradients using samples and prove the convergence or sample-complexity of sample-based REPS.  The main contribution is a convergence theory of REPS in both exact and stochastic settings. ",In this paper the authors the sample complexity of the relative entropy policy search (REPS). This algorithm aims at finding the optimal policy in an MDP using linear programming formulation. First the authors consider the planning setting where they assume access to the exact model of the environment. For this result they show that near stationary policies result in a near optimal policy. Next they provide the convergence when the updates are being performed by biased stochastic gradients. ,"The paper studies the performance of the classic Relative Entropy Policy Search (REPS) algorithm of Peters et al. (2010) in the context of tabular reinforcement learning, focusing on the effect of optimization errors on the quality of the policy extracted from the solution. REPS is based on formulating the policy optimization problem as a regularized linear optimization problem which is equivalent to an unconstrained convex optimization problem, and calculating policy updates via approximately minimizing the dual function. The current paper sets out to understand the optimization issues surrounding solving these problems, and provides the following contributions:  - Showing that the policy suboptimality can be controlled in terms of the gradient norm associated with the approximate solution and some problem-dependent constants. - Showing that the dual REPS objective is smooth, which implies that bounds on the additive optimization errors can be translated to bounds on the gradient norm at the approximate solution. - Providing concrete performance bounds for an accelerated gradient descent method when the gradients of the objective can be evaluated exactly. This setting is rather unrealistic, but the results are insightful in that they shed light on the best attainable performance to be expected from this approach.  - Providing concrete performance bounds for a stochastic gradient descent method that uses a generative model to obtain estimates of the gradients. The challenge here is that one cannot straightforwardly obtain unbiased gradient estimates, and one has to take several sample transitions to make sure that the bias in the updates is small enough. The authors address this challenge thoroughly and obtain guarantees that are polynomial in the desired accuracy level.",0.17647058823529413,0.1323529411764706,0.25,0.27848101265822783,0.4177215189873418,0.358974358974359,0.3037974683544304,0.23076923076923078,0.12781954887218044,0.28205128205128205,0.12406015037593984,0.10526315789473684,0.2232558139534884,0.16822429906542055,0.1691542288557214,0.2802547770700637,0.19130434782608693,0.1627906976744186
82,SP:36d11071cbf989e1f02232d39f52a42e781a5b2b,"The paper studies how does contrastive learning works from a theoretical perspective focusing on the spectral properties of the graph of the data induced by augmentations. They propose a new loss that is close up to a constant to the matrix factorization of the normalized adjacency matrix of the augmentations graph, under some assumptions they show that minimizing this loss provides downstream classification guarantees. Then, using standard Rademacher analysis they move from population guarantees to finite sample guarantees. Finally, they show empirically that their loss gives results close to state of the art. ","This paper theoretically analyzes contrastive learning relying on a concept of population augmentation graph. In the graph, two augmented samples are connected if they can be augmented from the same original instance. Based on this setup, - This paper shows that spectral decomposition can be reformulated as a contrastive learning objective. Furthermore, this paper shows that minimizing the objective can guarantee a high linear probe accuracy under some mild assumptions. - This paper shows the guarantee can be extended to the finite-sample regime and learning the linear probe with labeled data. - This paper demonstrates the efficacy of the proposed spectral contrastive objective empirically in the standard setup: the proposed objective achieves similar performance as other state-of-the-art SSL methods. ","This paper derives generalization bounds for learning using a contrastive objective under realistic assumptions on the positive pair generation process. The approach taken is to consider the so-called augmentation graph - the set of nodes comprises all augmented views of population distribution, with edges connecting nodes that correspond to different views of the same input datapoint. The core assumption made is that this graph cannot be split into a large number of disconnected subgraphs. This set-up aligns well with the intuition that in order to generalize, the contrastive notion of “similarity” must extent beyond the purely single-instance-level, and must somehow connect distinct inputs points.  My view is that this work is the most significant advance in the statistical learning theory of contrastive learning since the paper “A theoretical analysis of contrastive unsupervised representation learning” by Arora et al. This paper under review addresses the primary weakness in the work by Arora et al., which used an unrealistic assumption on positive pair generation. I will strongly advocate for acceptance of this paper.  ","The paper studies the theoretical foundations of contrastive self-supervised learning. To do so, the authors study contrastive learning through the lens of the augmentation graph - the graph of augmented examples, with edges connecting ""similar"" examples. The authors suggest a novel contrastive loss function, s.t. minimizing this loss is equivalent to spectral clustering on the augmentation graph. Using this, the authors prove that if the augmentation graph has few edges that cross between sub-graphs of different labels, then using a linear readout on the learned representation, a small loss can be achieved w.r.t the supervised objective. The authors also analyze the sample complexity of learning the representation and learning the linear probe from sampled data. Finally, the authors show empirically that the suggested contrastive loss performs similarly to some baseline contrastive loss functions.",0.24731182795698925,0.21505376344086022,0.3118279569892473,0.225,0.25833333333333336,0.1781609195402299,0.19166666666666668,0.11494252873563218,0.2116788321167883,0.15517241379310345,0.22627737226277372,0.22627737226277372,0.21596244131455403,0.149812734082397,0.25217391304347825,0.1836734693877551,0.24124513618677046,0.19935691318327975
83,SP:3721f1b12d87e95f5aa4c977a1714d5c54cb70f7,"This submission empirically studies the efficacy of adversarial training for mitigating the effect of label noise in training data. Their findings are as follows: 1) ""Smoothing effect"" of adversarial training: 	a) on a 2-dimensional synthetic binary classification dataset where two points are incorrectly labeled, they show that vanilla training yields a classifier that memorizes the bad labels by forming ""clusters"" around the incorrectly labeled points, whereas adversarial training does not yield such clusters 	b) for CIFAR injected with 20/40% random label noise, they ran vanilla and adversarial training on the noisy data and found that if you look at the distribution over labels within the neighborhood of a random incorrectly labeled point, on average the entropy of that distribution is higher for the classifier obtained by adversarial training than by vanilla training 2) For vanilla training on CIFAR (also MNIST) injected with label noise, the gap between accuracy on the correctly labeled training data and the incorrectly labeled data closed over the course of training, whereas this gap does not close or seems to close much more slowly for adversarial training. 3) Over these same noisy datasets, adversarial training seems to mitigate the impact of noisy training data on (clean) test accuracy, unlike vanilla training for which generalization degrades as label noise increases 4) They consider a quantity they call the ""geometry value"" of a data point (x,y), which corresponds to the number of PGD steps needed to find a differently labeled point in the neighborhood of x. This quantity was originally introduced by [Zhang et al. 2021b], and in the present paper they find that: 	a) compared to loss(x,y), it appears to be a more effective way to effective way to distinguish correctly labeled data from incorrectly labeled training data, as well rare data from typical data 5) They propose a ""robust annotator"" for labeling unlabeled data that has possibly been subject to adversarial perturbations. The algorithm repeatedly alternates between 1) identifying training data points with high loss and low geometry value and re-labeling them according to the current classifier and 2) running a step of adversarial training. It appears to do slightly better than a PGD-based annotator baseline when trained over CIFAR injected with label noise. They also note that the geometry value can provide some kind of ""confidence score"" to go along with the label annotations.",This paper studies the connection between noisy labels (NL) and adversarial training (AT). The contribution of this paper is two-fold. The first one is to adopt the number of PGD attack steps as a criterion for sample selection to correct noisy labels. The second one is that adversarial training can serve as a way to correct noisy labels. These two contributions indicate that adversarial training can be applied to more general model robustness problems.  ,"This paper studies the adversarial training in the context of label noises. Specifically, it is discovered that adversarial training can prevent the model from overfitting to the label noises, leading to a more smooth landscape. In addition, the authors point out that the number of PGD step sizes can be considered as a useful metric to distinguish instances with correct or incorrect labels.","This paper focuses on understanding adversarial training in the presence of label noise by conducting empirical studies. Based on their observations, the authors propose to use _PGD step number_ of adversarial training as a new measure for sample selection to correct noisy labels. Moreover, they present two use-cases, namely 1. a _robust annotator_ algorithm to label unlabeled instances, and 2. PGD step number as a _confidence score_ for the labeling of unlabeled instances. Empirical observations are primarily made on CIFAR-10 images.",0.07323232323232323,0.06818181818181818,0.07575757575757576,0.28,0.28,0.2698412698412698,0.38666666666666666,0.42857142857142855,0.3614457831325301,0.3333333333333333,0.25301204819277107,0.20481927710843373,0.12314225053078555,0.1176470588235294,0.12526096033402925,0.30434782608695654,0.26582278481012656,0.2328767123287671
84,SP:374bfeb067fcea966c97e1721d65cd9d03d26ed3,"This paper proposes various improvements to deep sequential latent variable models (or deep state space models). Namely, 1) the constrained optimization method from the Taming VAEs paper [Rezende & Viola, 2018], 2) a method for improving empirical priors over the first time step’s latent variable (referred to as VHP), and 3) a new model that combines the locally linear mappings of Karl et al., 2017 and the KVAE variable separation from Fraccaro et al., 2017. These improvements are demonstrated on pendulum and reacher datasets (both with images and joint angles), showing improvements in system identification, prediction accuracy, and the ability to use the state representations from MBRL. Overall, the paper provides several helpful tips for constructing and training deep state space models.","This paper proposes a framework for learning neural network-based state-space models from raw data. In comparison to the standard RNN-based approach [1], there are several modifications: (i) a modified ELBO formulated in the context of a constrained optimization (CO) framework [2]. CO prioritizes reconstruction term at the beginning of the training; (ii) hierarchical prior [3] instead of a simple Gaussian; (iii) a modified Kalman-VAE framework [4] instead of RNN-based transition model. Experimental evidence on pendulum data and the reacher environment indicates that the resulting framework outperforms baselines in predictive modeling and ELBO values.   [1] A Recurrent Latent Variable Model for Sequential Data  [2] Taming VAEs  [3] Learning Hierarchical Priors in VAEs  [4]  A Disentangled Recognition and Nonlinear Dynamics Model for Unsupervised Learning","This paper has two main contributions.  1. The authors adapt REWO for helping with disentanglement of learned representations and improving predictive accuracy 2. The authors replace the RNN encoding of the nonlinearity in [6] by the locally linear model of [11-12] while keeping closed form inference  The authors then illustrate how combining these together leads to better accuracy in learned latent dynamics for a set of models, in particular they show that their approach can recover the unobserved velocity as a latent variable in motion models.",The authors make three contributions. 1) They apply the constrained optimization framework for VAE type models to the deep state space model case. 2) They add in the hierarchical prior approach to model the initial distribution for the first state. 3) They propose a model with linear transition matrices given by a neural network weighting learned basis matrices. Auxiliary observation variables are also introduced to allow for a linear observation model along with an extra encoder/decoder. This means filtering and smoothing distributions can be computed analytically.,0.1885245901639344,0.13114754098360656,0.13934426229508196,0.14173228346456693,0.14173228346456693,0.14942528735632185,0.18110236220472442,0.1839080459770115,0.19540229885057472,0.20689655172413793,0.20689655172413793,0.14942528735632185,0.18473895582329317,0.15311004784688995,0.16267942583732053,0.16822429906542055,0.16822429906542055,0.14942528735632185
85,SP:3833662cf92249d83e65a1200f9e2890b5b23e95,"Authors introduced Model-augmented Prioritized Experience Replay (MaPER) as a variation to the canonical prioritized sweeping. The main difference is that they extended the critic network on its last layer to also predict the reward and transition model. Then they changed the prioritization of sample for replay to also include the error in the reward and transition model, in addition to the TD-error. Experimental results in various MuJoC domains showed the performance boost introduced by MaPER on top of existing state-of-the-art techniques. Furthermore Authors carried an ablation study, investigation the impact of 1) incorporating the MaPER while all methods used the same network 2) incorporating various error values for prioritization, and 3) increasing the network size.",The paper proposes a new method for prioritizing experiences used in the prioritized experience replay (PER) method. The proposed approach is simple: the critic network also learns the reward function and transition function. The two errors (absolute value) are added to the absolute TD error to calculate the priorities in the PER method. The authors provide some intuitions to their method. Experiments on both Mujoco and Atari environments are presented to show the method’s effectiveness. ," *Summary Of The Paper Model-augmented Prioritized Experience Replay In this paper, the authors consider using the dynamics prediction error in the priority calculation. The priority is then used to decide the probability for each sample during training. The method is applicable to both model-based and model-free setting ","This paper studies the problem of experience replay. It proposes Model-augmented Prioritized Experience Replay (MaPER), a novel experience replay method, based on the intuition that the model is easier to estimate than Q-value. It also proposes a modification to critic network, Model-augmented Critic Network (MaCN), by predicting the reward and dynamics model additionally. Experiments on MaPER show that MaPER can be applied to both discrete action space (DQN) and continuous action space (SAC), both model-free RL algorithms (SAC) and model-based RL algorithms (MBPO), as well as sparse reward tasks, and improve the baseline algorithms a lot. ",0.20833333333333334,0.13333333333333333,0.175,0.21052631578947367,0.23684210526315788,0.32,0.32894736842105265,0.32,0.2079207920792079,0.32,0.1782178217821782,0.15841584158415842,0.25510204081632654,0.18823529411764703,0.19004524886877827,0.25396825396825395,0.20338983050847456,0.21192052980132453
86,SP:39062dbbe9a30a7b47fa51179c15db34a3380a0b,"In this paper, the authors present an automated normalizing flow(NF) architecture search method. The method employs a mixture distribution formulation that can construct an optimal flow model with n layers of transformations from the transformation set. Besides, the authors introduce a block-wise optimization method to deal with exponentially growing optimization complexity. In the experiment, the authors proved the effectiveness of the optimization method which via approximate upper bound. And AutoNF has a better performance-cost trade-off than hand-tuned SOTA flow models. ","This paper proposes a DARTS-like method for searching automated normalization flow models. Instead of directly using the output ensembles, which leads to infeasible flow models, this work proposed distribution mixture to guarantee that the supernet is always a valid flow model. The upper bound of the loss function is optimized jointly with resource constraints. Experiments on small-to-medium scale datasets valid the effectiveness of the proposed method.","In this paper, the authors proposed to adapt a differentiable architecture search formulation (Liu et al., 2019) based on learned weighting of an ensemble of modules to automated search for Normalizing Flow architectures. The authors made several adaptations to the original approach for the normalizing flow problem, due to the invertibility constraints that prevent direct linear summation of different transform operations. Furthermore, the authors proposed to optimize the full network using an approximated upper bound of the KL divergence, instead of directly optimization. The authors proposed two methods to decompose the optimization problem: grow method, which is more straightforward and greedy, and block method, that alternatively adjusts each block. The authors experimentally compared their proposed method with manually specified architectures across various datasets, including POWER, GAS, HERMASS, MINIBOONE and BSDS300. The results seem mixed, as the searched model outperforms the manual model in some contexts but not others.",This work provides the novel approach for searching flow architectures. Compared to the standard approaches used to find the best deep architecture in standard network networks this problem is more challenging due to the need of invertible transform and requirement for determinant of the Jacobian to be easy to calculate. To solve the problem the authors propose to apply the weighting for candidate transformations. In order to enforce invertible properties of the model the authors suggest to use the mixing distribution approach instead of mixing the base transformations. They formulate the problem of learning best weights and show that optimal solution for soft weights is not optimal for binarised versions. Therefore they propose to optimize the upper bound of the proposed loss function instead. Further they show how to deal with the problem with larger number of layers. Some experiments are also performed to show the quality of the approach with respect to the baseline that is expert-based selection. ,0.2,0.27058823529411763,0.2235294117647059,0.30434782608695654,0.3333333333333333,0.22297297297297297,0.2463768115942029,0.1554054054054054,0.11875,0.14189189189189189,0.14375,0.20625,0.22077922077922077,0.19742489270386263,0.15510204081632653,0.19354838709677422,0.20087336244541484,0.21428571428571427
87,SP:3925fc528de17b8b2e93808f5440ea0503895b75,"This paper proposes AdVQA, a new (adversarial) test set for the Visual Question Answering.  Annotators and a SOTA model are put in the loop to make sure the model can be fooled. Experiments show that other VQA models tend to perform poorly on AdVQA, and a comprehensive analysis discusses the questions and answers that are in this dataset (compared with VQA and TextVQA).","Update after rebuttal: Thanks to the authors for their rebuttal in response to my questions/concerns. After reading the rebuttal and the other reviews, I am updating my score to 6, to better reflect the contributions made by the work. _______________________________________________________________________________________________________________ The paper introduces an adversarial VQA dataset collected with human-and-model in the loop by directly asking humans to write questions to attack the winning model of VQA challenge. Extensive evaluation of existing models on this adversarial VQA dataset suggest that current VQA systems are far from really solving the VQA problem. Many well-performed models on VQA v2 seem to fail on AdVQA. AdVQA potentially can be used to benchmark the advances of VQA models in the future, considering VQA v2 performance is close to human parity.","This paper works on the topic of visual question answering and aims to test how well state-of-the-art models perform on adversarial examples. Specifically, the authors collect a new adversarial benchmark on VQA, named ""Adversarial VQA"" (AdVQA), including a large evaluation dataset of 28,522 examples in total. The data samples in AdVQA are collected by putting both human annotators and state-of-the-art models in the collection loop: Annotators are requested to create questions that can fool the state-of-the-art models, and these questions are further verified by other annotators. The authors benchmark state-of-the-art models on these collected adversarial questions and their performance drops significantly.  The main contribution of this paper is the collected AdVQA dataset which includes adversarial examples created by human. The experimental results suggest future research directions on VQA.","This paper collects a new validation dataset via crowdsourcing to benchmark the progress of state-of-the-art VQA models.  The dataset (containing 22K questions and 10x answers) is adversarially collected such that annotators have to come up with questions that can successfully fool a state-of-the-art VQA model, and such questions are undoubtedly more challenging to various existing models. The authors perform thorough data analysis as well as extensive experiments with a variety existing models, to validate the usefulness of the new dataset. This new dataset can potentially be a valuable benchmark to track the progress of VQA research, if there is no obvious bias in the dataset (potential dataset bias is not thoroughly discussed in this paper).  ========================================================================================  Thanks for the response from the authors. I've updated my rating after reading the rebuttal as well as the comments from other reviewers.  ",0.30158730158730157,0.3492063492063492,0.2857142857142857,0.21705426356589147,0.1937984496124031,0.24822695035460993,0.14728682170542637,0.15602836879432624,0.12413793103448276,0.19858156028368795,0.1724137931034483,0.2413793103448276,0.19791666666666669,0.21568627450980393,0.17307692307692304,0.2074074074074074,0.18248175182481755,0.24475524475524477
88,SP:3945d1fb07900d63b7706ca0bce5e451ebfe476b,"This paper studies how to understand the quality of knowledge representations encoded in point cloud DNNs. Based upon the Shapley value, the paper presents six metrics to evaluate the model vulnerability toward different types of input variations. In addition, it presents an analysis regarding the smoothness and multi-order interactions of these attributions, pointing out the restrictions of current point DNNs. The paper makes interesting observations and draws useful conclusions, supported by a range of experimental evaluations.","At a higher level, this paper is proposing several ways to measure the sensitivity and smoothness of representations learnt by 3D point cloud networks. This paper presents several ways to measure the sensitivity of point cloud networks to rotation, scale, translation and local structure. The Paper also proposes different metrics to evaluate the smoothness of encoded local 3d structures and the complexity of representations learnt by the network. The analysis is done using several point-based neural networks trained on the ModelNet40 classification task and shapenet part segmentation task.","This paper measures the sensitivity of various point cloud classification networks (like PointNet, PointNet++, DGCNN) to rotation, translation, scale, and local 3D structure. It also proposes ways to measure the 3D smoothness of a network. Based on these analyses, the work provides various insights. One of the main insights being that the DNNs don’t handle rotation very well. Another insight is that training with adversarial perturbations helps.","This paper introduces metrics that reflect the representation quality, and properties, acquired by a deep-net that is applied in 3D Point-Cloud (PC) processing tasks. Concretely, the authors introduce six novel metrics that in aggregate capture different types of regional sensitivities e.g., the sensitivity of the DNN in rotations on specific regions/rotation-angles of the input PC. They further explore the extent that modern architectures in standard benchmarks acquire “spatial smoothness” --  do nearby points have ‘similar’ effects/importance for the end task? And finally, they explore the acquired representation complexity of the DNN by adapting a 'multi-order interaction' metric to PC data.    These metrics are being applied on a well-selected subset of modern PC-DNN architectures such PointNet++ or DGCNN when these are trained either for PC-object classification, or part-based object-segmentation. The analysis of the metrics in these tasks/architectures highlights a number of interesting points that oftentimes are very simple/intuitive: e.g., PointNet fails to encode local 3D structures (e.g., compared to PointNet++) — or includes results that are more subtle, but still highly intuitive & rich: e.g., adversarially training a PC-DNN with rotation/translation attacks; will force it to learn more global PC structures compared to standard training; which is intuitive as the local structures, per the new metrics, are those that on average are more susceptible to rotations.",0.2857142857142857,0.19480519480519481,0.33766233766233766,0.2808988764044944,0.2696629213483146,0.29411764705882354,0.24719101123595505,0.22058823529411764,0.11255411255411256,0.36764705882352944,0.1038961038961039,0.08658008658008658,0.2650602409638554,0.20689655172413793,0.16883116883116883,0.3184713375796179,0.15,0.13377926421404682
89,SP:39845a353e75e2f854c3dc649db3817d89ad9875,"This paper proposes a new architecture for continuous normalizing flows that explicitly models images at multiple resolutions. The authors achieve this by learning an unconditional distribution of coarse images and then learning conditional distributions at progressively finer resolutions, each with a continuous normalizing flow. In contrast to the typical “squeeze” layers used in normalizing flows, where the noise is only passed at the first layer, the proposed model injects noise at several resolutions (which has already proved successful with other generative models). The authors introduce a new (invertible) transformation to go from a coarse image to a finer image which preserves the range of the image while having unit determinant (implying the transformation leaves the log likelihood unchanged). The whole model is then trained via maximum likelihood.  The authors evaluate their method on standard image datasets and perform several ablations to test the contributions of their model.  The main contributions of the paper in my eyes are then: - The introduction of a new architecture and layer for learning multi resolution normalizing flows - Demonstration that the proposed model can be trained with limited computational resources even on fairly large scale datasets (such as Imagenet128) - Ablation studies evaluating the improvements from each aspect of the proposed model ","The paper describes the novel approach to generate high-resolution images in progressive manner. The approach is based on set of conditional flows that take the image generated from previous stage as conditioning factor and generate the image with the higher resolution. The quality of the method is compared to the reference approaches, mainly focusing on WaveletFlow as the most similar method.","In this work, the authors propose a multi-resolution strategy for continuous normalizing flows. The proposed approach consists of a general wavelet based decomposition/downsampling where the transformation obeys some useful mathematical properties such as volume and range preservation. Empirical results indicate likelihood estimation performance better or on par with benchmarked methods. The proposed model also trains significantly faster with fewer parameters.",The paper proposes a multi-resolution variant of continuous normalizing flows for images. They show empirical results for image sizes upto 64 x 64 where they seem to be better than regular continuous normalizing flows. The key proposed benefit seems to be in number of parameters and training times.,0.09268292682926829,0.09268292682926829,0.07804878048780488,0.16129032258064516,0.12903225806451613,0.20967741935483872,0.3064516129032258,0.3064516129032258,0.32653061224489793,0.16129032258064516,0.16326530612244897,0.2653061224489796,0.14232209737827714,0.14232209737827714,0.12598425196850394,0.16129032258064516,0.14414414414414417,0.23423423423423428
90,SP:3a19340d6af65e3f949dda839a6d233369891c46,"This paper considers a simplified model of the polynomial neural network, and shows that the eigenvalues of the induced tangent kernel have a slower decay rate, compared with the classical two-layer network. This also means the polynomial neural networks can learn the high-frequency components faster. The empirical evidence supports this tendency.","The paper studies ReLU-activated two-layer neural networks. The main contribution is that unlike traditional deep neural networks (DNNs), this type of polynomial neural network (PNN) also learns high-frequency information quickly. This claim partially verifies why PNNs are suitable for tasks like face recognition, where high-frequency components are crucial for performance. ","The paper is dedicated to the study of spectral bias in polynomial neural networks. For feed-forward neural networks with ReLU activation function it is well-known that learning high-frequency terms is made slower (Rahaman et al.).   Authors claim that polynomial networks are able to learn both low frequency and high-frequency terms almost equally fast. Theory, based on spectral analysis of an integral operator associated with the NTK kernel, is provided.  Theoretical approach was taken from arXiv: 1905.12173, but for NTK kernel of the network with an additional multiplicative interaction layer.  Then, experiments that support claims are provided.  It is claimed that multiplicative interactions in the architecture of a NN may be the reason for an ability to learn high frequencies.  ",The paper gives an analysis of polynomial networks in the NTK regime. They show a theoretical speed-up in learning higher frequencies when using polynomial networks. They also have some experiments that verify these properties. ,0.16981132075471697,0.24528301886792453,0.1320754716981132,0.25925925925925924,0.1111111111111111,0.08064516129032258,0.16666666666666666,0.10483870967741936,0.2,0.11290322580645161,0.17142857142857143,0.2857142857142857,0.16822429906542055,0.14689265536723164,0.1590909090909091,0.15730337078651685,0.1348314606741573,0.12578616352201258
91,SP:3abcd0700eaf11d964c280d996a1dd4f34280b1c,"The paper proposes a leave-one-out kNN supervised method for pre-training on large-scale datasets.  The paper claims such a method will benefit downstream tasks by NOT enforcing each class, whether visually similar or not, to cluster together. Instead, each sample just needs to be close to the nearest K neighbors.  In this way, the model will not be overfitted to pre-train datasets. To implement this method, the paper uses a loss of weighted soft kNN loss with cosine distance and softmax normalization. To solve the large search space and feature update problem of kNN, the paper uses ideas from MoCo, utilizing a memory queue as a feature bank to store features of previous N samples and First-in-first-out to update the memory queue. To solve the convergence issue, the paper uses ideas from SimCLR and utilizes MLP instead of the linear layer to project features into a more compact space.   The proposed is tested on a standard fine-tuning setting, using ImageNet as the up-stream pre-training dataset and multiple fine-grained datasets as down-stream fine-tuning datasets. The proposed method is compared with both the supervised and self-supervised methods,  ","The paper proposes a new supervised method for visual pretraining by leveraging human labels. To avoid the intra-class instances to collapse into a single vector, the method implicitly encourages sub-clusters to occur within a category by a leave-one-out k-nearest neighbor classification loss. Compared with contrastive learning and several supervised baselines, the approach shows improved performance on several downstream tasks on car/flower/pets classifications, etc.","The paper presents an interesting approach that expands the recent success of self-supervised pre-training with *instance discrimination* to supervised pre-training. The key insight is to have each class being represented not just by a single weight vector, but in a non-parametric fashion via KNN lookup from a MoCo memory bank. The paper avoids to compare the results for direct, supervised learning, but rather focuses on transfer learning to downstream tasks -- to me this has some empirical novelty. The transfer learning experiments are done extensively, including many settings for quantitative comparisons and qualitative visualizations. ","This paper proposes LOOK, a new supervised pre-training method which can maintain intra-class semantic differences for better transferability to downstream tasks. Specifically, LOOK proposes to use a KNN classifier instead of the simple linear classifier for the pre-training task. Experiments show that LOOK outperforms previous supervised and unsupervised pre-training methods.",0.10050251256281408,0.12060301507537688,0.11055276381909548,0.21428571428571427,0.17142857142857143,0.13402061855670103,0.2857142857142857,0.24742268041237114,0.4074074074074074,0.15463917525773196,0.2222222222222222,0.24074074074074073,0.14869888475836432,0.16216216216216217,0.1739130434782609,0.17964071856287425,0.1935483870967742,0.17218543046357615
92,SP:3aee15083ee1c0a75fedd67a50f9d729bf5ee411,"The authors prove that two-layer neural networks can be perturbed adversarially at a fixed data point and at random initialization (with high probability), i.e.\ for a neural network $f$ with randomly initialized parameters and a data point $x$, there exists a small perturbation $\delta = O(1)$ such that $|f(x)| = \Omega(1)$, but $f(x+\delta)\cdot f(x) < 0$. The result holds for a wide range of widths and activation functions. They present numerical evidence for their theoretical claims.","The authors consider the existence of adversarial examples in neural networks trained by gradient descent.  They demonstrate that for two layer networks, one gradient update following random initialization can find adversarial examples.  Their results extend previous work by Daniely and Schacham to more general settings (for two layer networks) and rely upon a different suite of techniques. ","The paper studies the phenomenon of adversarial examples for random two-layer networks with ReLU or smooth activations. It is shown that one step of gradient descent yields adversarial examples (i.e., a switching of the sign of the network output) with high probability, under a much milder condition on the number of neurons compared to previous work, i.e. sub-exponential in the dimension instead of sub-linear for Daniely and Schacham [2020]. The proof is based on characterizing the landscape of the network function at different scales, showing that at relatively small ""mesoscopic"" scale of perturbations (o(sqrt(d)),  but still O(1)), the gradient is close to constant while being Omega(1), making it easy to find an adversarial example with a single gradient step. Some experiments are provided to illustrate the theory, and to illustrate how the phenomenon extends to multiple layers.","The paper studies adversarial examples in random depth-2 ReLU networks.  They consider depth-2 networks, where the first layer is drawn from a normal distribution such that the norms of the weights vectors are roughly 1, and the second layer is drawn from the uniform distribution on {-1/\sqrt{k},1/\sqrt{k}} where k is the width of the network. They assume that k is at most sub-exponential in the input dimension, and is bounded below by some polylogarithmic expression. For an input x on the sphere of radius \sqrt{d} the output of the network is w.h.p. O(1) (the input x is fixed, and the probability is over the choice of the network). The authors show that there is w.h.p. \delta with ||\delta||=O(1) such that sign(f(x)) \neq sign(f(x+\delta)). Moreover, such \delta can be found with a single gradient step. Thus, although the inputs are of size \sqrt{d}, there is w.h.p. a perturbation of size O(1) that changes the sign of the output. They prove the result for both smooth and ReLU activations. Finally, they validate the results empirically, and also demonstrate that ש similar phenomenon occurs in deeper networks.  Daniely and Schacham [2020] showed that such a property holds in ReLU networks where the sizes d_1,…,d_t  of the layers (where d_1 is the input dimension) are such that for every j we have d_{j+1} = o(d_j) and d_t = \omega(1). For the special case of depth-2 networks it required width k=o(d). Thus, the improvement here w.r.t. k is exponential.  The proofs roughly follow the following steps. First, they show an O(1) upper bound on f(x) and an \Omega(1) lower bound on ||\nabla f(x)||. Then, they bound the variation of the gradient near x. Finally, they use standard arguments to bound f(x-\eta \nabla f(x)) for an appropriate \eta. The proofs of these steps for the case of the ReLU activation is different from the case of smooth activations, and the proofs for the ReLU activation with small k is different from the case of large k. ",0.14634146341463414,0.23170731707317074,0.3780487804878049,0.3508771929824561,0.3333333333333333,0.3082191780821918,0.21052631578947367,0.13013698630136986,0.08288770053475936,0.136986301369863,0.05080213903743316,0.12032085561497326,0.1726618705035971,0.16666666666666666,0.13596491228070176,0.19704433497536947,0.08816705336426915,0.1730769230769231
93,SP:3dc67f04c04466b0fe5aebb01c7578cd24caee0c,"This paper presents Actively Learning over Pseudo-Labels for Surrogate Losses (ALPS), an active learning algorithm for binary classification with surrogate losses in the streaming setting. The main idea is to generate pseudo-labels for points whose labels are not requested and train on the union of data points with ground-truth labels and those with pseudo-labels. Since the pseudo-labels may not be accurate, the authors propose requestor functions that have a final say in whether a pseudo-label is used, and in effect, enable control over the noise generated by erroneous pseudo-labeling. Under an assumption involving the requestor function class, the authors prove bounds on the generalization error and label complexity of ALPS that are roughly on the same order of prior work (e.g., IWAL). ","This paper studies the problem of online active learning for classification. In order to learn a classification algorithm, the paper proposes to employ surrogate loss function. The error of the proposed method is analyzed and some empirical results are presented.","The authors propose an active learning algorithm in the streaming setting for binary classification tasks. The algorithm leverages weak labels to minimize the number of label requests and trains a model to optimize a surrogate loss on a resulting set of labeled and weak-labeled points. The paper contains a number of theoretical results and supporting empirical results. Overall, its a very strong paper.","This paper focuses on stream-based (online) active learning for training classifiers in the binary classification setting. In particular, the authors propose a strategy, the so-called ALPS*, that trains classifiers by optimizing a surrogate loss over both the labeled instances and sequentially predicted data points by the learner. Theoretical guarantees on label complexity and error of the returned model are provided, and are shown to be competitive with the existing bounds. A set of experiments are conducted on datasets using multi-layer perceptrons as classification models, where the performance of ALPS is illustrated to have good performance with small labeling cost.   *Actively Learning over Pseudo-labels for Surrogate losses",0.11538461538461539,0.14615384615384616,0.19230769230769232,0.375,0.45,0.296875,0.375,0.296875,0.22727272727272727,0.234375,0.16363636363636364,0.17272727272727273,0.1764705882352941,0.1958762886597938,0.20833333333333331,0.28846153846153844,0.24,0.21839080459770113
94,SP:3e5ac4add9ab8a986fdf027b6e6a7d59698b3031,"The paper presents an approach for hierarchical cost sensitive classification in which abstentions are allowed. It shows a bijection between original cost sensitive problem and the set of layer wise abstaining losses. It is based on using the existing distributionally robust cost sensitive classification and extending to it to the hierarchical setup. The proposed methodology is demonstrated on birds and cell classification datasets, which are claimed to be large-scale. It is compared to a relatively old DARTS method from 2012.","The authors propose a new framework for cost-sensitive hierarchical classification. First, they decompose it into level by level learning to abstain (with different abstain costs per class) sub-problems. To solve these subproblems, authors apply deep distributionally robust learning (DRL) approach that directly minimizes the abstaining loss (based on Fathony et al. 2018). These two elements create a method named the Layer-wise Abstaining Loss Minimization method (LAM). The proposed method is compared with DARTS on two datasets and achieves attractive performance. The authors also demonstrate that this decomposition makes it easier to achieve the desired performance profile by adjusting abstaining losses of the layers.","The paper studys cost-sensitive hierarchical classification problems. The novelty is very limited, the experiments are vey tirial. I recommend to reject.","The submission study the problem of cost-sensitive hierarchical classification (CSHC) with given label taxonomy via learning to abstentions each layer within the hierarchy. Indeed,  1. CSHC subject has had many researches while using abstention or reject decision has also had many works in flat classification, briding both seems relatively few for which the authors present LAM to achieve a so-called new method.  2. Using DRL framework to solve the learning to abstain problems in each layer makes optimization almost decouplable layer by layer, the rationale behind its strategy should attribute to the proved bijective correspondence between the hierarchical cost-sensitive loss and the set of abstaining losses. 3. LAM achieves better performance on limited benchmarks.",0.2222222222222222,0.12345679012345678,0.20987654320987653,0.09433962264150944,0.16037735849056603,0.36363636363636365,0.16981132075471697,0.45454545454545453,0.1452991452991453,0.45454545454545453,0.1452991452991453,0.06837606837606838,0.19251336898395724,0.1941747572815534,0.17171717171717174,0.15625,0.1524663677130045,0.11510791366906477
95,SP:3ea7edab6ae65758b99615be07b7778188a6ff9f,"This paper proposes a model to infer structured 3D object representations from a 2D scene in an unsupervised fashion so as to represent the visual scene in an object-centric way. Specifically, the inference part adopts a similar mechanism as Slot Attention to derive object slot latent code, and then maps slot latent code to 3D object representations with MLP. The rendering part takes the idea from 3D neural rendering where a shared NeRF function is used to represent all objects excluding the background. Rendering is performed by querying NeRF with 3D location, 2D view direction as well as object latent to get object color value and occupancy value. Object rendering is composed into scene rendering according to location derived at the inference stage and weighted by occupancy value.  To sum up, the paper interprets a 2D visual scene with 3D object-centric representation. With the existing 2D object-centric scene segmentation method and 3D neural rendering approach, it achieves comparable segmentation performance and derives manipulable object representation.","The paper aims to decompose a scene into objects and infer the representations of 3D occupancy, color, and pose for each object from a single image of the scene without supervision. To this end, the paper proposes an autoencoding solution by combining the Slot Attention encoder with the GIRAFFE decoder. Each object is represented as a Neural Radiance Field (NeRF) additionally parameterized by the latent variables inferred from the encoder. The decoder then compositionally renders the objects. The experiments show that the proposed model (1) achieves competitive 2D segmentation performance on CLEVR6, (2) supports object-wise scene manipulation, and (3) outperforms non-object-centric methods on CATER snitch localization when combined with a powerful transformer.","This paper proposes a model which is able to segment 3D scenes into objects by a combination of slot-attention (For inference) and a mixture of object NeRF functions which mix together (in 3D) to compose a scene. The method receives a single input image (with the camera coordinates though these are fixed) and extracts a set of slots - one slot for each object. These slots are decoded using a NeRF renderer: one part (the shape) generates the density, one (the appearance) generates the colours and one (the pose) transforms the points of the object to the appropriate pose in scene space. Results are demonstrated on CLEVR data as well as CATER (which is visually very similar) and some downstream tasks.","This paper proposes a novel unsupervised scene decomposition model that infers object shapes, appearances and 3D poses. The benefits over existing models are the structured, 3D object representations which allows to manipulate objects in the scenes such as moving and replacing objects. This paper also shows that the inferred object representations can be used in a visual reasoning task.",0.16666666666666666,0.15476190476190477,0.11904761904761904,0.22608695652173913,0.13043478260869565,0.12396694214876033,0.24347826086956523,0.21487603305785125,0.3389830508474576,0.21487603305785125,0.2542372881355932,0.2542372881355932,0.1978798586572438,0.17993079584775085,0.17621145374449337,0.22033898305084748,0.17241379310344826,0.16666666666666666
96,SP:3ef8660de61a1a73858934fcf8edfec104133ae7,"The paper has two main contributions:  1) It proposes a unifying framework called ""Adaptive Measurements"" that covers many differentially private synthetic data generation algorithms and show how these algorithms can be derived by modifying the loss function and distributional family.  2) Based on the above framework, two specific synthetic data generation algorithms called Private Entropy Projection (PEP) and Generative networks with Exponential Mechanism (GEM) have been proposed. Since PEP attempts to explicitly model the data distribution, it does not scale well to higher dimensions. GEM uses the well-known generative neural networks to overcome the above problem and can also effectively use public data for good initialization.","The authors unify several algorithms for DP synthetic data generation for query release. Using this framework they create two algorithms, private entropy projection (PEP) and generative networks with the exponential mechanism (GEM). Then they empirically evaluate their algorithms in publicly available datasets and show that their algorithms (mostly) outperform existing ones. They also show how GEM can use public data to improve its accuracy. ","# Summary of contributions  The paper studies the problem of synthesizing differentially private (DP) data for query release. Here we are given a sensitive dataset $P$ together with a set $\mathcal{Q}$ of queries of interest. The goal is to generate a DP synthetic dataset $D$ so that answering the queries using $D$ gives similar answers to those of $P$; more formally, we want to minimize the maximum error across the queries, i.e. $\max_{q \in \mathcal{Q}} |q(P) - q(D)|$. Several works have proposed algorithms for this problem, including MWEM [Hardt et al., NeurIPS'12] and FEM [Vietri et al., ICML'20].  The first contribution of the paper is in observing that essentially the previously proposed algorithms all fit into a single framework which they call ""Adaptive Measurements"". Specifically, the algorithm proceeds in iterations and, in each iteration, it uses the exponential mechanism to select one or more queries, asks for noisy values of those queries and it then updates the current synthetic dataset $D$ by minimizing some objective (which involves the answers to the queries so far and the current guess dataset).  With the above in mind, the authors then decide a new algorithm which fits into this framework called Private Entropy Projection (PEP) algorithm. At each step, PEP tries to find the distribution $D$ with maximum entropy among those whose max error w.r.t. the noisy measurements is at most $\gamma$ for some parameter $\gamma$. The authors note that PEP also gives exponentially weighted distributions, similar to MWEM, but the weights are more elaborated. Empirically, the authors show that PEP performs better than MWEM when $\epsilon$ is sufficiently large (and performs similarly for small $\epsilon$) on reduced versions of ADULT and ACS datasets for 3-way marginal queries.  The reason a reduced version of those datasets are used is because the original datasets have high-dimension, which is infeasible for MWEM or PEP since they have to keep the entire distribution explicitly. To overcome this, the authors propose a different algorithm called GEM which instead assume that the distribution is generated to some network $G_\theta$ with parameter $\theta$ (assuming that the inputs to the network is i.i.d. Gaussian). This can help reduce the computational cost dramatically since now only the network parameters $\theta$ is needed to be maintained. Here, GEM still works in the ""Adaptive Measurements"" framework and the authors use gradient descent to minimize the $\ell_1$ loss w.r.t. the noisy measurements so far. Note that this might not be easily differentiable for generic queries; fortunately, for k-way marginal queries, the authors point out that they are just product of the network's outputs and as a result the gradient can be easily found.  Finally, the authors consider the setting where there is some public data which one hopes to use to improve the quality of the synthetic dataset. Previous state-of-the-art called PMW$^{Pub}$ [Liu et al., 2021] simply uses limits the distribution to only those on the _support_ of the public data and runs MWEM. This has several limitations, such as when the support of the public data is not sufficient to express the sensitive data, or when some features are missing in the public dataset. The authors show that a modified version of GEM (called GEM$^{Pub}$) is more amenable for these cases: just use the public data to find an initial generator network, and then proceed with the GEM algorithm as usual. They demonstrate that this approach indeed works well in practice: although the performances of GEM$^{Pub}$ and PMW$^{Pub}$ are similar for large & similarly-distributed public data, GEM$^{Pub}$ significantly outperforms PMW$^{Pub}$ when public data is incomplete or its support not closely resembling the sensitive data.","This paper tackles the problem of synthetic data generation for query release. First, it presents a general algorithmic framework (Adaptive measurements) for the task, and interprets a long line of prior work under this lense. At a high level, these works solve the min max problem in Equation 1.   Given this formulation, the paper proposes two algorithms, PEP and GEM. PEP selects a maximum entropy distribution to minimize the error for a given query set by solving the dual of the constrained optimization problem. They offer an iterative algorithm for this.  To avoid the exponential runtime of approaches like PEP (because they maintain a distribution over entire data domains), the paper also proposes GEM, which uses a generator network to represent this high dimensional space.  GEM can easily incorporate public data (with various missingness) by using it for pretraining.   Empirically, GEM outperforms prior work in accuracy to privacy tradeoffs on ACS and the adult datasets, and is more robust to domain shift in the public data than PMW or PEP. This gain is most pronounced when the data is high dimensional. ",0.21495327102803738,0.34579439252336447,0.24299065420560748,0.453125,0.328125,0.09569377990430622,0.359375,0.05901116427432217,0.143646408839779,0.046251993620414676,0.11602209944751381,0.3314917127071823,0.26900584795321636,0.10081743869209808,0.18055555555555555,0.08393632416787267,0.17142857142857143,0.1485148514851485
97,SP:3f33489b98ba6145fd4e334669493f15a63455f4,"In this work, the authors consider linear quantile regression, paying particular attention to the ""coverage"" properties of the resulting predictor. The traditional goal of quantile regression is to learn a quantile of the conditional distribution of output $Y$, conditioned on the inputs $X$. Denoting the predictor returned by any quantile regression algorithm by $\hat{f}(\cdot)$, one can use this to construct a predictive interval for a freshly-drawn $Y$. The authors consider the one-sided interval $C(X) = (-\infty, \hat{f}(X)]$, and the event $\\{ Y \in C(X) \\}$. The authors call the probability of this event (over the draw of a pair $(X,Y)$) the coverage of $\hat{f}$. Their main results elucidate conditions under which the traditional pinball-loss based ERM quantile regression solution is biased below the desired quantile level, that is, the predictive interval tends to be ""too small,"" due to $\hat{f}$ under-estimating the desired quantile.  In particular, they show that in the limit where the sample size and dimensionality are proportional, the coverage (the aforementioned probability, which depends on $\hat{f}$ and thus the training data) converges below the desired quantile, with this error depending directly on the ratio of dimension to sample size $d/n$. Furthermore, they show (under fixed $n$ and $d$) how for a certain family of data distributions and a linear quantile model, given any estimator of the weights, the downward bias term scales with the weight estimation error. Their empirical tests also effectively highlight the phenomena described formally.","This paper presents a theoretical study on the coverage of quantile regression algorithms. More specifically, the authors prove theoretically that linear quantile regression exhibits an inherent under-coverage bias, with an order d/n difference in the marginal coverage regardless of the noise distribution (but Gaussian for the covariates). They also show that the primary source of this under-coverage bias is the estimation error in the linear coefficients, while the estimation error in the bias term can have either an under- or over-coverage effect. Numerical experiments further support their findings.","Quantifying predictive uncertainty is a crucial task for high-stakes prediction problems. The paper focus on quantile regression, a common approach for quantifying predictive uncertainty in regression problems. Authors consider realizable linear setting when the true model belongs to the considered functional class. With a focus on the under-parameterized setting, they theoretically justify the under-coverage of the learnt quantile regressors. Established theoretical results are backed up with supporting empirical evidence.","This paper gives theoretical insights into the under-coverage phenomenon often observed in quantile regression, i.e., for a continuous distribution, the probability of observing  a true label below a learned  $\alpha$-quantile is often below $\alpha$, which is consider a failure in estimating the uncertainty. Two important results are shown. The first one provides a characterization of the under-coverage in the setting of a Gaussian linear model in the under-parameterized high-dimensional proportional limit (i.e. $d,n \rightarrow \infty$ , $d/n\rightarrow\kappa$ with small $\kappa$) independently of the noise distribution. The experimental results show that the formulas match pretty well the observed coverage in realistic situations of dimension around 100, on synthetic and real data. The second important result shows that the source of the under-coverage stems essentially from the error in estimating the high-dimensional linear coefficient, under some assumptions on the underlying source.",0.11553784860557768,0.08764940239043825,0.14741035856573706,0.15217391304347827,0.33695652173913043,0.2222222222222222,0.31521739130434784,0.3055555555555556,0.24503311258278146,0.19444444444444445,0.2052980132450331,0.10596026490066225,0.16909620991253643,0.13622291021671826,0.18407960199004977,0.17073170731707318,0.2551440329218107,0.14349775784753363
98,SP:3f74dc3dc2cb444b3097aae1288dad5355e9a4d4,"This paper proposes and analyzes a framework for Federated optimization called FedLin. It combines client-specific local steps with (possibly) biased compression at the server and/or clients and guarantees linear convergence to the exact minimum. Algorithmic contributions are supported with lower bounds for the case of infrequent communication and carefully designed synthetic experiments on least squares and logistic regression problems, which highlight theoretical claims.","This paper proposes an optimization method for federated learning, FedLin, that is specifically designed to ensure fast convergence even in the presence of heterogeneous data. Notably, the authors show that FedLin converges to the global optimum (or just a critical point, in the case of non-convex functions) without decaying the client learning rate. This is in contrast to methods such as FedAvg, FedProx, and FedNova, which require decaying the client loss in order to circumvent ""objective mismatch"", where the methods are actually optimizing an altered surrogate loss.  This work also derives lower bounds matching the convergence of FedLin on quadratics, that explain the price and cost of performing multiple local update steps. They also analyze convergence using gradient sparsification at both the client and server. Finally, the paper shows the empirical benefits of FedLin over other methods on synthetic least squares and logistic regression problems.","This paper proposes a new algorithm called *FedLin* which,   T.1) queries a full gradient oracle,   T.2) de-biases the local gradients using a correction term (with the last synchronized iterate in memory) to deal with objective heterogeneity,  T.3) error corrects to deal with compression at the client and the server, and  T.4) uses a client-specific learning rate to deal with the different number of local steps on each client.   Thus, FedLin is able to accommodate both objective and system heterogeneity along with sparse gradients and local steps. In doing so it is an early work that can deal with these multiple aspects of federated learning at once, albeit at the cost of an expensive gradient oracle.   The paper provides the following theoretical guarantees for FedLin,  R.1) matching upper and lower bounds in the strongly convex-smooth setting without compression,  R.2) upper bounds without compression in the convex-smooth setting and the non-convex setting,  R.3) an iterate sub-optimality recursion for the strongly convex-smooth setting while using a stochastic gradient oracle,  R.4) upper bounds for client and server compression in the strongly convex-smooth setting.  See the main review for comments on these results. ","This paper considers a standard distributed optimization setup in which clients periodically coordinate with a server to train a global model. The focus is on the deterministic setting, in which each worker computes full local gradients. It is first highlighted that standard algorithms for this problem do not converge to the true global minimizer without trading off speed (for example through diminishing step-sizes).  Thus, the FedLin algorithm, which converges linearly to the true minimizer, is introduced to get the best of both worlds. Several theorems covering various settings (strongly convex, convex, non-convex, stochastic) are given, and show that FedLin matches the communication complexity of parallel GD, regardless of the number of local steps used. Then, a matching lower bound is given for FedLin to show that the strongly convex convergence result is tight.    Then, several gradient sparsification schemes are investigated (at the server and at the workers), which show that server-side sparsification is far less impactful than client-side sparsification. The impact of gradient sparsification is evaluated experimentally in Section 7. ",0.36923076923076925,0.3384615384615385,0.2153846153846154,0.2191780821917808,0.2191780821917808,0.1568627450980392,0.1643835616438356,0.10784313725490197,0.08045977011494253,0.1568627450980392,0.1839080459770115,0.1839080459770115,0.2274881516587678,0.16356877323420074,0.11715481171548117,0.18285714285714286,0.2,0.16931216931216933
99,SP:3fbc5ebb4c598e849b3ecbb2886289e20bf1ea14,"This paper presents a contrastive learning approach for unsupervised text retrieval. As the approach is unsupervised, similar to the Inverse Cloze Task (ICT) approach, the authors propose to do independent cropping of a paragraph to generate a pair of ""pseudo-question"" and ""pseudo-document"", which is used to train the model. The model is trained using contrastive learning following the MoCo algorithm. Experiments are performed on the recent BEIR benchmark for zero-shot and few-shot retrieval evaluation. The proposed approach often performs better than the BM25 algorithm on zero-shot retrieval on the BEIR benchmark. There are also some ablations presented to analyze different aspects of model training such as the effect of number of negative examples, training data, data augmentations etc.  ","This paper proposes to use cropping strategy to train a neural model for IR. It is compared to BM25 and several other neural models, including BERT and the ones using inverse cloze task. The goal of cropping is to train a model that fits better the IR tasks. It is hypothesized that this task is more similar to IR than other pre-training strategies. The method is tested on a set of benchmarks of zero-shot retrieval and MSMARCO with training data. It is shown than the proposed method fine-tuned on MSMARCO can outperform the compared methods, including BM25. When no fine-tuning is used, the method is almost at par with BM25. ","This paper proposed a contrastive learning-based method for zero-shot dense IR. Based on the contrastive learning framework, 3 positive pair construction methods and 2 negative pair construction methods are used and compared. Evaluations are performed on the BEIR benchmark, with the best average result achieved compared to a list of IR (both sparse and dense) baselines in the zero-shot setting.","This paper studies pre-training of Siamese Transformer encoders for the zero-shot dense retrieval problem. The author consider contrastive learning loss function to optimize the encoders and investigate variants of ICT for data augmentations. While not much novelty in the techincal sides, the author indeed present solid empirical study on zero-shot retrieval benchmark as well as extension to the few-shot evaluation.",0.22764227642276422,0.16260162601626016,0.14634146341463414,0.13157894736842105,0.11403508771929824,0.23809523809523808,0.24561403508771928,0.31746031746031744,0.28125,0.23809523809523808,0.203125,0.234375,0.2362869198312236,0.21505376344086022,0.19251336898395718,0.1694915254237288,0.14606741573033707,0.23622047244094488
100,SP:401ef5fe2022e926b0321258efac1f369f186ace,"In the manuscript, the authors propose SQuant, which is a data-free quantization method that can apply post-training quantization (PTQ) without any backpropagation.  Specifically, SQuant is taking advantage of approximated Hessian information. Based on the assumptions and deductions in the paper, SQuant tries to optimize constrained absolute sum of error (CASE) instead of MSE.  The authors show many experimental results to validate the effectiveness of SQuant.","This paper proposes a data-free quantization method based on the second-order Taylor expansion of  loss, where the Hessian matrix is approximated with different levels: element-wise, kernel-wise and channel-wise. The authors progressively determine the quantized weights from element-wise to kernel-wise and then to channel-wise. The derivation and solution of the quantization are novel. Empirical results show that the proposed method outperforms recent data-free methods.  ","This paper proposes a new data-free quantization method that does not require back-propagation nor fine-tuning. The key idea is adopting Hessian-based optimization that can be decomposed into three parts (SQuant-E, K, and C) corresponding to the three dimensions of the weight tensor (using a few approximations, such as cross-layer independence to simplify the optimization). Then, instead of MSE, the authors introduce CASE (constrained ASE) of weight perturbation. The experimental results show that the proposed DFQ method outperforms even GDFQ that is basically a kind of QAT. The proposed technique is especially useful for a low-bit quantization.","In this paper, the authors propose a data-free quantization algorithm of deep neural networks called SQuant. The main idea of SQuant is to decompose the Hessian-based optimization objective into three components: element-wise, kernel-wise and channel-wise components. In order to jointly optimize these three objective functions, a constrained absolute sum of error (CASE) is studied and a progressive algorithm is used. Experiment results show that SQuant algorithm is able to keep higher accuracy with the same number of bits, compared to several baseline algorithms.",0.22388059701492538,0.31343283582089554,0.3880597014925373,0.3472222222222222,0.3194444444444444,0.24271844660194175,0.20833333333333334,0.20388349514563106,0.29545454545454547,0.24271844660194175,0.26136363636363635,0.2840909090909091,0.21582733812949642,0.24705882352941175,0.33548387096774196,0.28571428571428575,0.2875,0.26178010471204194
101,SP:408d9e1299ee05b89855df9742b608626692b40d,"# Summary  This paper proposes a method for using intermediate representations from a pre-trained model for better transfer to other tasks. A linear classifier is trained on features from multiple layers. A feature selection strategy is chosen, where first a linear classifier is trained on all features with group-lasso regularization, and then another classifier is trained only on features whose regularization score exceeds some threshold. This approach is evaluated in a low-resource transfer scenario of image classification, and compared both to training on all features without the two-step feature selection but with regularization, and to the standard fine-tuning approach. The proposed method works slightly better than fine-tuning with a ResNet model, and slightly worse with a ViT model, although the comparison is not completely fair, as discussed. There are different performances on different categories of images.   ","This paper explores the utility of intermediate layers for linear probing in transfer learning. Specifically, authors proposed Head-to-Toe probing (HEAD2TOE), that selects features from all layers of the source model to train a classification head for the target-domain. Experiments on the VTAB benchmark shows that Head2Toe matches performance obtained with fine-tuning on average, but critically, for out-of-distribution transfer, Head2Toe outperforms fine-tuning.","This paper propose Head2Toe, a method that exploits intermediate representations of DNNs to improve performance, and OOD generalization of transfer learning. The key idea of the Head2Toe is to augment traditional linear probing with intermediate representations. It uses group lasso as regularization to learn weights of different features, and then select features based on the learned weights and validation performance.Experiments on VTAB benchmark show that Head2Toe outperforms linear probes and is competitive compared with fine-tuning. An important finding is that Head2Toe improves OOD generalization.","The main focus of this paper is the study the use of intermediate layers in a deep pretrained model on downstream tasks. Authors argue that the the fact that the good performance while fine tuning on downstream tasks even if data scarce is due to the prior existence of useful representations deep in the model which are brought up during training. Authors propose a new approach. Head2toe, which consist in utilizing intermediate representations, while freezing the weights of the network for training for downstream tasks Experimentation is carried out starting with a ResNet-50 and ViT-B/16 models on a variety of datasets collected on the VTAB collection, showing that the approach where Head2toe  outperforms linear finetunning (training a classification head on top of the model, while freezing the rest of the weights) and matches the performance of finetunning the whole model.",0.1276595744680851,0.15602836879432624,0.1702127659574468,0.27941176470588236,0.3235294117647059,0.2558139534883721,0.2647058823529412,0.2558139534883721,0.16783216783216784,0.22093023255813954,0.15384615384615385,0.15384615384615385,0.17224880382775118,0.19383259911894274,0.16901408450704225,0.24675324675324672,0.2085308056872038,0.19213973799126638
102,SP:40fd96105e77063de4a07d4b36fe19385434c533,"The paper presents 3 main theoretical results: - An RNN with 40 unbounded-precision neurons is Turing-complete. - An RNN with 54 bounded-precision neurons and two stacks is Turing-complete. - An RNN with a finite number of bounded-precision neurons and no stacks can simulate a Turing machine with a bounded tape, where the maximum tape length is related to the number of RNN neurons.","This paper analyses the Turing completeness of recurrent neural networks, and proposes a memory-augmented RNN architecture reminiscent of existing stack RNNs.   Specifically:  First, a construction for simulating any Turing machine with an unbounded-precision RNN is presented, in which effectively the left and right sides of the Turing machine's tape (with respect to its head) are encoded as two 'stacks' in the RNN. This construction is analysed and requires less simulation time than that of Siegelmann and Sonntag [1] (3T instead of 4T+O(used tape length), where T is computation time of the simulated Turing machine). Additionally the authors analyse the exact precision of the neurons necessary (as function of the simulated Turing machine's computation length) for the simulation (as opposed to previous works, which simply note a requirement for infinite precision).  Second, noting that it is unavoidable that the precision must grow when simulating longer Turing machine computations (to maintain the contents of the tape), the authors propose augmenting the RNN with a dynamic memory module, such that the neurons themselves may have constant, bounded, precision. They theoretically analyse their proposed architecture and show how a Turing machine may be simulated in it.   ","This work proposes a dynamic-growing memory module for RNNs, which serves to simulate Turing machines of bounded precision.  First, the authors prove how to encode a Turing machine to an unbounded precision RNN. The encoding uses fractal encoding for symbols, like previous work, and it replaces each step of the Turing machine as 3 steps in the RNN. Leaning in the work of [14], they prove that there is a 40-neuron unbounded precision RNN to simulate any Turing machine (i.e., showing Turing-completeness). Next, they consider the previous unbounded-precision definitions with the memory module to obtain a bounded-precision RNN.The memory module is a stack of neurons, that requires two neurons in the RNN to control it: push and pop. Then it is proven that the proposed bounded-precision RNN utilizes 2 of these stack to simulate a Turing machine. Further, each stack is divided in groups of $p$ neurons, such that the most active neurons are in the RNN (near to each side of the memory head in the Turing machine). Leaning again in [14], they can show that there is a 54-neuron with $p$-precision RNN with 2 growing memory module, that can simulate any Turing machine. This proves Turing-completeness of the precision-bounded RNN. Finally, they remove the growing memory module and simulate the memory within the neurons of an RNN, showing that an infinite number of bounded-precision RNN can simulate any Turing machine. the work finishes with an interesting discussion of the results and limitations.    ","This theoretical work shows the existence of un-bounded precision RNNs for simulating turing machines, with growing/shrinking memory module. Although the paper seems quite interesting and well-written, I am afraid I don't have the right background to make an accurate judgment on the novelty and potential impact of the work. I share few thoughts below, similarly due to lack-of-knowledge and limited reviewing time I was not able to check the proofs. ",0.35384615384615387,0.4153846153846154,0.15384615384615385,0.2828282828282828,0.09595959595959595,0.08171206225680934,0.11616161616161616,0.10505836575875487,0.13157894736842105,0.2178988326848249,0.25,0.27631578947368424,0.17490494296577946,0.16770186335403728,0.14184397163120568,0.2461538461538462,0.1386861313868613,0.12612612612612614
103,SP:41a9806ee6c0c84e046b7de79eb54dfe00de6995,"The paper suggests a method to the GNN toolbox. The proposed method applies multiple, independent evaluations of a graph, each including random dropout applied to the graph nodes. The different evaluations are then aggregated to a single classification/regression result.","The authors propose DropGNN, a GNN exension where multiple forward passes of the model are aggregated. In each of the passes, each node in the graph is dropped with probability $p$ independently. Aggregating a sufficiently large number of dropout combinations effectively breaks symmetries that lead to traditional GNNs to not be able to distinguish between certain neighborhoods. The authors present theoretical results that DropGNNs can distinguish neighborhoods that traditional GNNs (e.g., GIN) cannot. The authors evaluate their method on both synthetic and real-world datasets.","This paper proposes DropGNN, a novel technique for improving the expressive power of graph neural networks. DropGNN is simple. It removes nodes randomly (even in test time unlike dropout) and aggregates the results of several runs. This paper provides some examples and theoretical results that show when DropGNN improves its expressive power compared to vanilla GNN.","The paper proposes Dropout Graph Neural Networks (DropGNNs), an approach that utlizes dropout in a way that allows GNNs to be more expressive. In DropGNN, multiple runs  are conducted where, in each run, the nodes of the graph are dropped out with a certain small probability and passed through a  GNN to produce a distinct embedding. The embeddings of the multiple runs are aggregated (with a sum) after a non-linear  transformation of the node features. DropGNNs achieve competitive results on synthetic and real-world datasets.  ",0.25,0.2,0.325,0.12790697674418605,0.2558139534883721,0.21428571428571427,0.11627906976744186,0.14285714285714285,0.1511627906976744,0.19642857142857142,0.2558139534883721,0.13953488372093023,0.15873015873015872,0.16666666666666666,0.20634920634920634,0.15492957746478875,0.2558139534883721,0.16901408450704225
104,SP:427100edad574722a6525ca917e84f817ff60d7e,"## After rebuttal I am impressed by the comprehensive experiments and baselines and increase my score. Authors provide very complete baselines and also compare in other datasets, which greatly reduces the concern that the proposed rule overfits to the ODDS benchmark.   ## Summary This paper learns a contrastive representation that helps differentiate between normal and abnormal data in the tabular data. Contrary to other contrastive learning methods which learns to differentiate between different examples, this paper instead differentiates between in-window vector and out-window vector for each example under a sliding k-sized window (k is a hyperparameter). Since the features are unordered, they shuffle the feature orders to get multiple score and average them. They show that it outperforms other baselines in a large suite of tabular data, ODDS benchmark, with default hyperparameter rules (e.g. the k is set based on number of samples N and d). They compare mainly with recent methods DROCC, GOAD and COPOD. ","This paper introduces an anomaly detection algorithm for tabular data based on contrastive learning in the semi-supervised setting (training set assumed to be only normal data). The contrastive learning task is based on masking: the features from a single training example are split into two groups (pairs), one being a subsequence of the features, and the other its complement set; the learning task is then to differentiate pairs of splits that overlap (positive) vs not (negative) (from the same split or not) for each sample. The contrastive loss defined by this task is used to train the model and as the anomaly score. The proposed method is extensively evaluated on a range of tabular datasets where it outperforms recent methods.","- The authors propose a novel anomaly detection framework that can be useful for tabular data. - The authors utilize the contrastive learning to learn the relationship between features in the tabular data. Then, using the loss as the anomaly scores for evaluation. - The authors provide extensive experimental results which are promising.","This paper proposes a contrastive learning method for unsupervised anomaly detection on general multivariate (tabular) data. The idea of the method is to train two encoders, one that embeds a subset of k features and one that embeds the residual d − k features, such that the resulting embeddings are closely aligned via the contrastive loss, where all other subsets of k features are taken as negative examples respectively. This is then repeated over all subsets of features of size k. Thus, the intuition of the approach is to extract common dependencies (e.g., correlations, statistical redundancy, etc.) between the features by achieving a small contrastive loss over the training data, thereby obtaining a high contrastive loss for anomalies (which presumably lack these common dependencies). An empirical evaluation on the Arrhythmia, Thyroid, KDD, and KDDRev datasets (where results from the literature exist) as well as 30 additional datasets from the ODDS library show that the proposed method performs favorably over other recent competing methods (DROCC, GOAD, COPOD) on tabular data.",0.14556962025316456,0.0949367088607595,0.15822784810126583,0.15702479338842976,0.2727272727272727,0.38,0.19008264462809918,0.3,0.14792899408284024,0.38,0.1952662721893491,0.11242603550295859,0.16487455197132614,0.14423076923076922,0.1529051987767584,0.2222222222222222,0.2275862068965517,0.1735159817351598
105,SP:43c8bcc93aa034965fa6d959d44b529ffc110cc7,"This paper focuses on solving the cold-start issue in Cross-Domain Recommendation (CDR) setting. For this purpose, the authors propose a DisAlign framework, which mainly has two modules, i.e., rating prediction module and embedding distribution alignment module. The main novelty relies in the embedding distribution alignment module. In it, the authors propose Stein path alignment and proxy Stein path alignment. The experiments on benchmark datasets, compared with six baselines, show the superior of the proposed model. The experimental analysis also shows the effectiveness of the model ability. ","In this paper, the authors study a relatively new problem called cross-domain cold-start recommendation (CDCSR), where the goal is to transfer knowledge from a warm source domain with user-item ratings and item descriptions to a cold target domain with item descriptions only (i.e., without any user-item interactions). Notice that the users in two domains are aligned. In particular, the authors propose a novel recommendation framework, i.e., DisAlign, which contains two main modules, i.e., a rating prediction module and an embedding distribution alignment module. ","Summary:  The paper addresses the cold-start item recommendation with auxiliary information. To attack the heterogeneity/ domain discrepancy across the source domain and target domain, Stein path is adopted to align the domain distributions. In specific, the distribution of target item auxiliary embedding is to be aligned with both source item auxiliary embedding and source item collaborative embedding. A proxy Stein path is proposed to reduce the cubic time complexity. Good experiments are achieved on real-world datasets by comparing with many SOTA methods. ","This paper proposes a framework for cross-domain item cold-start recommendation. The main idea follows the algorithm of Stein Variational Gradient Descent, by iteratively transporting the auxiliary embeddings of cold items in the target domain to match the embedding distribution of warm items in the source domain. The authors further propose to reduce computational time costs by selecting typical proxies of cold items.",0.2696629213483146,0.2247191011235955,0.1797752808988764,0.17777777777777778,0.18888888888888888,0.21428571428571427,0.26666666666666666,0.23809523809523808,0.25,0.19047619047619047,0.265625,0.28125,0.2681564245810056,0.2312138728323699,0.20915032679738563,0.1839080459770115,0.2207792207792208,0.2432432432432432
106,SP:447df6679b2880def833d4f444bf10e61cdf0e1c,"This paper proposes two techniques for generating adversarial examples: Selective Projected Gradient Descent (SPGD) and Orthogonal Projected Gradient Descent (OPGD). In order to fool both the victim model $f$ and a detector $g$, SPGD selectively optimise either $f$ or $g$ depending on whether the modified input is misclassified as the target class, while OPGD further orthogonalizes the gradients. Evaluation on four previously unbroken, state-of-the-art defence methods demonstrate the effectiveness of the proposed attacks.","This paper considers the problem of finding adversarial examples that simultaneously defeat a detector of adversarial examples. An argument is made that existing attacks often achieve one goal at the expense of another. This argument motivates the proposal of two attack techniques. These are evaluated against four existing detection-based defence methods, with successful results.","The authors propose an attack that could break 4 adversarial detection methods published recently. Traditionally, attacks against detection methods have attempted to maximize the loss for both classification and detection simultaneously. However, using a toy example the authors show that this is suboptimal, as it may not find the worst-case adversaries. The authors propose to minimize the loss (targeted attack setting) iteratively by optimizing either only for the classification pipeline or the detection pipeline at a time.  The attack first considers the classification loss and further tries to fool the detection pipeline until the classification prediction remains incorrect. The authors also propose a variant of the attack by considering gradient steps for the classification pipeline to be orthogonal to the gradients of the detection pipeline and vice versa. Finally, the paper shows that these two proposed attacks completely circumvent four recent adversarial detection methods.",This paper targets on attacking the defensive mechanism of adversarial examples detection. It proposes a new optimization algorithm to simultaneously meet two different requirements. It verifies its effectiveness on several state-of-the-art adversarial example detection methods.,0.15789473684210525,0.23684210526315788,0.14473684210526316,0.2545454545454545,0.2,0.07586206896551724,0.21818181818181817,0.12413793103448276,0.2894736842105263,0.09655172413793103,0.2894736842105263,0.2894736842105263,0.18320610687022898,0.16289592760180996,0.1929824561403509,0.13999999999999999,0.23655913978494625,0.12021857923497269
107,SP:44dc6a69f5d65ca0b271177ac67d1beb12a154a0,"This paper describes a novel task 'silent video dubbing' and proposes a method to solve it. The task involves generating realistic audio to accompany silent video, with the help of text. This is essentially similar to vid2speech [14], but with text guidance. The pipeline consists of several stages. (1) generating phoneme and video embeddings; (2) aligning text and video using the embeddings from (1) and attention network; (3) generating speaker embeddings to condition the decoder from images; (4) using (2) and (3) to generate the result voice. The authors provide example in supplementary materials. The experiments are performed on both single-speaker and multi-speaker SVD.","This paper presents ""Neural Dubber"", a method for dubbing silent videos based on given scripts. It essentially performs lip-synced TTS, with an added component (ISE) that ensures that the generated speech characteristics match the given face image.   Experiments verify that the proposed TTS method does not hurt SOTA TTS performance, while controlling the prosody such that the aligned speech is much more synchronized than other methods compared to. The supplementary video qualitatively demonstrates some compelling results of the proposed method.","In this paper, the authors propose to solve the problem of text to speech generation based on video. Thus, this is video-text guided speech generation task that they term as silent video dubbing. The method is based on a transformer architecture that combines text and visual lip motion representations as encoders and outputs the mel-spectrograms through the decoder of a transformer.   The method is compared with TTS systems and comparisons are done using human evaluation scores and audio-visual synchronisation scores. This is done over LRS2 and Lip2Wav for one out of 5 speakers. Comparisons are not done using standard PESQ, STOI or Word error rates. These are standard metrics used for TTS systems. Comparison is also not provided with single speaker Lip2Wav task or multi-speaker Lip2Wav on LRW dataset as done by Lip2Wav or Vid2Speech, or such methods that generate speech based on lip motion alone (not requiring text). ","The authors describe the problem of automatic dubbing of videos, ie generating speech from a script that matches the active speaking face in a video. To the best of this reviewer's knowledge, this is the first paper addressing this particular problem. The two main contributions to solve this problem is 1. a method to synchronize the generated TTS with the speaking face 2. sampling the video for a face image to infer a timbre for the speech to better match the face.  ",0.1320754716981132,0.2169811320754717,0.16037735849056603,0.18518518518518517,0.1728395061728395,0.11764705882352941,0.1728395061728395,0.1503267973856209,0.20481927710843373,0.09803921568627451,0.1686746987951807,0.21686746987951808,0.14973262032085563,0.1776061776061776,0.17989417989417988,0.1282051282051282,0.17073170731707316,0.15254237288135594
108,SP:45ba2709aca444f50a133d71f33be9d2c1f887e8,"1. This paper introduces the problem of multi-objective online convex optimization. 2. The authors propose to use PSG as the performance metric, and show that it is related to the dynamic regret.  3. The authors develop algorithms that enjoy sublinear multi-objective regret, and also conduct experiments to show the effectiveness of the proposed methods. ","The paper formulates a novel framework for multi-objective online convex optimization. The novel framework, similarly to the single-objective online convex optimization framework, can be viewed as a two players repeated game where at each round the online learner selects a point $x_t$ and the (possibly adversarial) environment selects a vector valued loss function $F_t (\cdot )$. To extend the notion of regret to the multi-objective setting at each round the suboptimality of $x_t$ is measured using the Pareto suboptimality gap (PSG). Two algorithms (OMMD-I and OMMD-II) which upper bound the extension of the dynamic regret in the multi-objective setting are designed. The two algorithms can be viewed as extensions of the min-norm method for offline multi-objective optimization in the online setting. In both algorithms a composite gradient, which is a convex combination of the descent directions for every objective, is calculated. This composite gradient is used to select, together with a Bregman reguralization, the descent direction and consequently the point $x_{t+1}$. The main difference between the two algorithms OMMD-I and OMMD-II is that in the latter the composition of the descent directions in two consecutive rounds will possibly be ""more similar"" (due to the regularization term $|| \lambda - \lambda_{t-1}||_1$). ","In the standard online learning model, the aim of the learning algorithm is to minimize its cumulative regret, defined by the difference in cumulative losses between the algorithm and the best decision taken with the benefit of insight in some reference set. In the generalized model of “multi-objective” online learning, the feedback is no longer a single scalar, but a vector of losses, each defined for a distinct objective. The performance of the learning algorithm is often defined in terms of “Pareto regret”, for which each instantaneous regret is captured by the notion of Pareto Suboptimality Gap (PSG). Although multi-objective online learning has been studied in the (possibly contextual and linear) bandit setting, many questions remain open. Notably, this paper focuses on the full-information setting where a vector-valued loss function is supplied at the end of each round. Based on the Online Mirror Descent (OMD) paradigm, two versions of multi-objective OMD are provided and analyzed for the dynamic version of Pareto regret (the static version being left as an open issue). Comparative experiments for both versions are performed on several benchmarks. ","The paper studies the problem of multi-objective optimization in an online setting, where the objective functions are time-varying. An algorithm is proposed to solve this problem. The algorithm queries the gradient of all objective functions, and returns a convex combination of these gradients as the input to the mirror descent algorithm. The performance of the algorithm is measured compared to a sequence of Pareto optimal points, where a single component of the loss vector cannot be improved without causing harm to the other components. Under some set of assumptions, Theorem 1 provides an upper bound on the dynamic regret of the algorithm. ",0.4107142857142857,0.32142857142857145,0.3392857142857143,0.17209302325581396,0.14418604651162792,0.17204301075268819,0.10697674418604651,0.0967741935483871,0.18269230769230768,0.1989247311827957,0.2980769230769231,0.3076923076923077,0.16974169741697417,0.1487603305785124,0.2375,0.18453865336658357,0.19435736677115986,0.22068965517241382
109,SP:460d4cc1a5c01e34abe37d9eb1b74dd3734b1d55,"The paper proposes an adaptive tree search algorithm for text generation. It uses an autoregressive model as the value network to produce the playout for each node. While decoding, the paper proposes a metric called max rank to address the shortcomings of utilizing the sum of token level log-probability. Results are conducted on several machine translation datasets, and the proposed method outperforms beam search in most cases.","This paper presents a novel algorithm called beam adaptive tree search (BATS) to enable the incorporation of search objectives that cannot be easily factorized. Unlike the regular Monte Carlo tree search algorithms that relies on a large number of playouts to update the value function, BATS relies on ""informed playout"", which is guided by the greedy decoding of the autoregressive models. To further constrain the search space, the paper also proposed a constrained node expansion criterion that gradually increases the lower limit of the node depth $d_{min}$, and allows for expanding $k$ nodes for each depth limit $d_{min}$.  Aside from the search algorithm, the paper also explored a couple of model changes to counteract the calibration issues, including two modified search objectives and using MRT-trained autoregressive models. Experiments show that while beam search performs better for the weaker models, BATS does improve the translation quality of the hypotheses when stronger MT models are used. Analysis on different beam size also show that unlike beam search, BATS can operate at large search budget without performance degradation under stronger models.","This paper proposes an adaptive tree search algorithm for NMT models. One advantage of this algorithm is that it does not make any assumptions about search objectives, and this enables the proposed algorithm to be applied on top of more general search objectives. In addition, it studies the issue of beam search bias and revisits some tricks to alleviate it as well as a new tricks. Combined with these tricks, the proposed algorithm delivers clear BLEU improvements over a strong baseline.  ","**Summary**  This paper proposes an adaptive tree search algorithm BATS, an MCTS variant, for NMT that could optimize any desired objectives/metrics (e.g. BLEU). The algorithm values each internal node by scoring a greedy searched rollout (instead of that of authentic MCTS) with an autoregressive model, which avoids search biases caused by other heuristics (e.g., beam search would be biased towards shorter translations or incomplete partial generations). Plus, a new objective Max Rank is proposed with a better correlation with translation quality. Experiments show that BART works well in comparison with beam search, which is also shown to bound the progress of more robust modeling in NMT.    ",0.3088235294117647,0.25,0.2647058823529412,0.10497237569060773,0.13259668508287292,0.2839506172839506,0.11602209944751381,0.20987654320987653,0.1651376146788991,0.2345679012345679,0.22018348623853212,0.21100917431192662,0.16867469879518074,0.22818791946308725,0.20338983050847456,0.1450381679389313,0.16551724137931034,0.24210526315789474
110,SP:461ed47339e08dafea90a7c015d2f20e534daeb7,"The paper proposes *Bootstrapped Meta-Learning,* a new meta-learning algorithm for hyperparameter optimization. Drawing inspiration from temporal difference learning techniques in reinforcement learning, the meta-learner is asked to predict the result of additional unrolled steps of the optimization process, by minimizing a carefully selected distance to a target generated during training. This allows for longer meta-learning optimization horizons, without the need for differentiation through longer optimization trajectories. The method is tested for hyperparameter optimization for reinforcement learning, including learning the exploration hyperparameter for a behaviour policy, and in multi-task meta-learning.","This paper broadly considers meta-learning, a.k.a. bilevel optimization, across single-task, multi-task, supervised learning, and reinforcement learning settings. The authors aim to resolve two issues with the standard outer-loop gradient-based optimization of the meta-parameters (assuming a differentiable inner-loop): first, since the meta-learning objective is typically computed from learner parameters after applying up to $K$ inner-loop updates, the meta-optimization is myopic in that it does not optimize for further inner-loop improvement after $K$ steps; second, since the functional form of the learner's objective $f$ is used to drive the outer-loop updates, the meta-learning objective inherits the curvature of $f$. The main algorithmic contribution consists of a family of meta-learning objectives called bootstrapped meta-learning, in which meta-parameters are optimized to bring post-inner-loop learner parameters $x^{(K)}$ closer to a bootstrap target (which are also learner parameters) computed from $x^{(K)}$. The authors show that bootstrapped meta-learning generalizes the ""direct"" (my terminology) gradient-based meta-parameter optimization used in many previous works, recovering it when using specific choices for the bootstrap computation function and learner parameter matching function. With certain strong assumptions, the authors theoretically motivate the use of gradient-based bootstrap target functions for bootstrapped meta-learning in terms of optimization progress. The authors make several experimental contributions: they use bootstrapped meta-learning to achieve state-of-the-art model-free performance on Atari-57, demonstrate the viability of bootstrapped meta-learning in few-shot image classification on miniImageNet, and show that the more flexible, general form of bootstrapped meta-learning can enable meta-learning parameters that do not appear in the computation graph for the task objective, e.g. meta-learning the exploration rate of the behavior policy in $\epsilon$-greedy $Q$-learning.","‌‌The paper presents a new meta-learning algorithm to address two shortcomings of standard meta-optimization algorithms: curvature (the meta-learner's objective is typically constrained to the same type of geometry as the learner), and limited evaluation (the meta-objective is evaluated only with-in a K-step horizon, ignoring future learning dynamics). The proposed algorithm addresses these two issues by minimizing the distance to a bootstrapped target under a chosen metric. Empirically, the new algorithm  achieved a new state-of-the art for model-free agents on the Atari ALE benchmark and yielded gains in multi-task meta-learning. Theoretically, some guarantees on performance improvements are provided. ","The paper presents Bootstrapped meta-gradients (BMG), an extension of typical meta-gradients (MG) for the task tuning meta-parameters that control the learning process (i.e. update step) of a learner.  In general terms, MG applies a (meta-)parameterised update rule to a learner for K steps, and then backpropagates through these updates to update the meta-parameters in the direction that improves the performance of the adapted learner.  The authors identify two limitations to this approach, and propose BMG to address them. (i) MG is myopic, in the sense that it does not account for future learning dynamics beyond these K steps, therefore BMG proposes to bootstrap a target from the K-step parameters (in practice by continuing to optimise w.r.t. parameterised update rule for L-1 steps, and then taking a final step w.r.t. a fixed objective to ground the signal). (ii) MG updates are necessarily restricted to be within the geometry of the parameterised learning process.  In contrast, BMG introduces a matching function to measure the distance between the learners K-step parameters and bootstrapped target in an arbitrary (and hopefully more-suitable space).  After framing the problem and BMG, the authors provide a discussion of the necessary conditions for BMG to guarantee performance improvements, though ultimately the presented algorithm is justified empirically using experiments on (i) a toy RL problem, (ii) the Atari RL test suite, (iii) multi-task few-shot adaptation on an image recognition task.  In all settings, BMG provides significant improvement over meta-learning baselines — most impressively achieving a new SOTA on Atari.  Moreover, key features of BMG are highlighted, including the ability to extend the meta-learning horizon without increasing the number of updates steps through which we must backpropagate, and that behavioural parameters outside of the update rule (specifically, epsilon in epsilon-greedy exploration) can also be meta-learned.",0.35789473684210527,0.3368421052631579,0.3368421052631579,0.1353135313531353,0.21122112211221122,0.3761467889908257,0.11221122112211221,0.29357798165137616,0.10223642172523961,0.3761467889908257,0.20447284345047922,0.13099041533546327,0.1708542713567839,0.3137254901960784,0.1568627450980392,0.1990291262135922,0.20779220779220778,0.19431279620853084
111,SP:462112ea1a59ab8101ed9d908c5d838edeb844ca," This paper proposes PP-GNN, a novel graph neural network that learns multiple adaptive polynomial filters acting on different subsets of the eigenvalues. The authors combine GPR-GNN with existing efficient algorithms for generating top and bottom eigen components to reduce the expensive complexity of eigendecomposition. They show that the piece-wise polynomial method can approximate a latent optimal filter better than a single polynomial in theory. ","This paper considers the problem of designing polynomial graph filters for use in graph neural networks. The motivation for doing so stems from the notions of homophily and heterophily in labeling the nodes of a graph: the former requiring the use of low-pass filters, and the latter requiring the use of high-pass filters. Here, the authors consider the design of spline polynomials for spectral-domain filtering, as opposed to the typical ``global'' polynomials used in graph filter design. That is, they first partition the spectrum of the graph matrix into low-pass, band-pass, and high-pass components, learn polynomials over each partition, and then combine these polynomials, enforcing continuity at the boundary of each interval via a penalty function.  Of course, doing this requires a full eigendecomposition of the graph matrix, which has high complexity. To ameliorate this issue, the authors propose an efficient variant of their spline spectral filter. This approach only uses a coarse partition of the spectrum, rather than using the full eigendecomposition.","This paper focuses on the task of node classification (semi-supervised learning) in heterophilic graphs. The authors identify that low-pass filter-based GNNs perform poorly on heterophilic graphs, precisely because they tend to smooth out differences in neighboring node features. Thus, they propose to look into filters capable of learning high-frequency content that can thus learn labels even if neighboring nodes do not share a label. In particular, the authors propose to learn different low-order polynomials for different parts of the spectrum.","In this paper, the authors aim to develop GNN that can better adapt to the given prediction task (both homophily and heterophiliy). Specifically, the authors extend the existing polynomial filter and propose to learn a filter function as a sum of polynomials over different subsets of the eigenvalues. The effectiveness of the proposed GNN architecture is demonstrated on diverse node classification tasks. The ablation studies were carried out to understand the proposed GNN architecture.",0.2537313432835821,0.1791044776119403,0.208955223880597,0.1242603550295858,0.1242603550295858,0.18823529411764706,0.10059171597633136,0.1411764705882353,0.1891891891891892,0.24705882352941178,0.28378378378378377,0.21621621621621623,0.14406779661016947,0.15789473684210525,0.19858156028368792,0.16535433070866143,0.1728395061728395,0.2012578616352201
112,SP:46e8c6a9d7729e5112b3c9f8ff91d9557ea524c1,"The paper proposes a novel coordinate-based network architecture which proposes to process each of the input coordinates independently in the first layer instead of together in a fully connected layer. This input style results in a speed-up in terms of evaluation of the network, and thus faster training and inference in tasks where coordinate-based MLPs are used, without incurring a significant degradation in terms of the quality of the signal fit. These benefits are demonstrated for the tasks of image representation, video representation, and 3D shape representation.","This paper proposes a new architecture for implicit neural representations, called CoordX, which splits each dimension of the input signal into separate branches (e.g. the x and y coordinates of pixel locations in an image) and processes each of these separately before fusing them. The authors achieve this by projecting each of these branches into a hidden feature and then using shared fully connected layers to process these. Each branch is then fused by an outer product which then reconstructs the entire input grid (e.g. for an image of size H x W, 2 branches take in H locations and W locations respectively which are then combined into H x W features by the fusion operation). The fused layers are processed by a few more MLP layers to output the predicted features. In addition, the authors propose a method for effectively subsampling the grid during training as well as different splitting strategies for the branches.  The authors perform experiments on various data modalities, including images, videos, 3D shapes and NerF scenes.  The main contributions of the paper in my eyes are then: - Introducing a new architecture for implicit neural representations that in certain cases can improve training/inference speed without incurring a decrease in reconstruction - Experiments on various data modalities showing the strengths/weaknesses of the method ","Summary: This paper proposes a modification to INR models on multidimensional coordinate grids where a subset of the earlier layers operate on the decomposed coordinate grid. In this setup, the grid (which is assumed to be regularly sampled to permit this decomposition) is broken into its constituent components, (e.g. x and y instead of (x,y)), passed through a single linear layer unique to that component, then through a stack of shared layers, followed by an outer product to return to the joint (x,y) space, and at least one layer that operates on the joint space. This approach lightly reduces parametric efficiency but strongly improves compute efficiency (both in terms of FLOPS, memory usage, and actual observed runtime) for common implicit neural representation tasks, including fitting images, videos, shapes, and volumetric rendering via radiance fields. ",The paper proposes an interesting tweak to the network architecture to accelerate CoortMLP. The idea is to split the input coordinates along the dimensions and then share weights before fusion.  The authors analyze the theoretical upper bound (as far as the MAC ops are concerned) and show about 2X speedup on actual machines.,0.35555555555555557,0.24444444444444444,0.16666666666666666,0.1461187214611872,0.0867579908675799,0.0948905109489051,0.1461187214611872,0.16058394160583941,0.2830188679245283,0.23357664233576642,0.3584905660377358,0.24528301886792453,0.2071197411003236,0.1938325991189427,0.20979020979020976,0.17977528089887637,0.13970588235294115,0.1368421052631579
113,SP:46f5874c8cbdb0832e92adcea85ca8a1b9ddc28a,"This paper studies the ability of Markovian reward functions to represent tasks defined as a set of acceptable policies, partial ordering over policies, partial ordering over trajectories. It first provides a negative result showing that some tasks cannot be specified through a Markovian reward in some environments. Then, it designs a set of linear programs to simultaneously check if a specific task can be encoded into a Markovian reward and to compute such reward. The paper includes an empirical evaluation in a simple domain.","The authors consider the problem of expressing a __task__ via a reward function and discount rate. They show that for various formalizations of what a 'task' is, there exist reward functions which cannot express that task. The authors present an efficient algorithm for deciding the expressibility of a task, and for designing such a reward function. They study certain properties of how many tasks are expressible in a given environment, and demonstrate how their approach can accelerate learning. ","In the reinforcement learning context, the paper proposes three possible definitions for the notion of task, as a set of acceptable policies (SOAP), a partial ordering over policies (PO), or a partial ordering over trajectories (TO). For each definition, the authors show that there exists a task and a controlled Markov process for which there is no reward function that can express that task. Furthermore, the authors notably provide a polynomial algorithm based on linear programming to find such reward function if it exists. Some experiments on random problem instances are also performed to validate the theory.","- An exploration and a concrete answer to the question of what is a task, and a critical examination of the reward hypothesis. The approach that this paper proposes is to define a set of all possible tasks in a Controlled Markov Process (CMP), and then ask the question, does there exist a Markov reward that realizes the given task.  - The paper provides answers the above question in the negative through a counter-example. In summary, the authors provide three modes of expressing the task intent: set of accetable policies (SOAP), partial ordering over policies (PO), partial ordering over trajectories (TO). The key result is that there are tasks defined as per all three methods that are not realizable as a Markov reward optimization problem  - The authors finally propose a linear-programming based polynomial time reward design algorithm that generates a Markov reward for the SOAP task specification and returns Null if such a reward does not exist. ",0.17857142857142858,0.30952380952380953,0.34523809523809523,0.2692307692307692,0.3076923076923077,0.36082474226804123,0.19230769230769232,0.26804123711340205,0.18471337579617833,0.21649484536082475,0.15286624203821655,0.2229299363057325,0.1851851851851852,0.287292817679558,0.24066390041493776,0.24000000000000002,0.2042553191489362,0.2755905511811024
114,SP:47889067620e5ac2e304681769af9d1d930f6d2b,The authors propose a method for computing concept based counterfactual explanations - i.e. using the presence or absence of concepts (human understandable) for explaining the models prediction. In the paper they focus on image classifiers.,"Goal: provide a human readable explanation to why a given classifier made a mistake on a given example.  Approach:   Step1: For the given input domain (here images), first obtain a set of concepts that are human interpretable. These concepts each describe a particular aspect of an image, and a human is expected to be able to look at image and say whether a particular concept applies or does not apply to an image (on scale of [0,1])  Step2: learn a map from the input domain to the concept space using an SVM  Step3: generate counterfactual perturbation of example that ensures that :1) classifier is now correct on example, 2) perturbation is valid (i.e. realistic) and 3) perturbation changes few concepts. The perturbation is the addition of a linear combination of concepts by different amounts.  Key to this is that generating the perturbation requires the true label and query access to the classifier.  Step4: present the user the weight vector of the concept perturbations.  Validation: two ways to evaluate, one controlled and one in the wild.  Controlled evaluation: using dataset of animal images in different settings, confound the class label, e.g. dog, with background, e.g. snow, then ask if approach can recover the confounding factor, here snow. They show that on a median basis, the method finds the right confounding factor in the top 2 spots.  In the wild evaluation: on dermatology and chest x-ray datasets, they studied the mistakes of of a  trained model. The evaluation of their findings is based on discussion with clinicians. ","Explaining errors using semantically meaningful concepts is a critical tool for improving the performance, robustness, and trustworthiness of machine learning models deployed in application areas everywbere.  While there have been several proposed lines of work in this area, none is yet accepted as standard practice.  The authors present a method combining two prior methods in explainability: counterfactual explanation and concept activation vectors, and meld them together as coneptual counterfactual explanations.    They describe the training procedure, and show some examples on both ImageNet and real diagnostic imaging applications.","The paper proposes a Conceptual Counterfactual Explanation for explaining a model's mistakes.  It starts by learning a ""concept"" using a linear separator in the models representation space and a small amount of data annotated with that concept.  Then, it explains a model's mistake by finding a change in those concepts that changes the model's prediction, does not change too many concepts, and does not change the concepts in an unrealistic way.  The method is verified by showing that, for OOD test points where the model makes mistakes, this explanation can identify the concept responsible for those mistakes.  ",0.4,0.2571428571428571,0.34285714285714286,0.06923076923076923,0.09230769230769231,0.13793103448275862,0.05384615384615385,0.10344827586206896,0.12,0.20689655172413793,0.24,0.12,0.09491525423728814,0.14754098360655737,0.17777777777777776,0.1037463976945245,0.13333333333333336,0.12834224598930483
115,SP:49435d70bf8e16d5dbf34577cf8d3a5b21b1f25a,"The authors present a systematic empirical study of the effect of planning and model learning on generalization performance, using the MuZero agent. They use 2 environments, Procgen and Meta-World, to respectively explore procedural generalization to new variants of the environment with the same reward structure and task generalisation to new structures of the reward function in the same environment. Their main contributions are their empirical results, specifically that additional reconstruction or self-supervised losses enable the MueZero agent to achieve state-of-the-art performance in procedural generalization, but are not enough to promote a similar increase in performance in task generalization tasks in Meta-World. Finally, they also explore the data diversity dimension, showing that having more diverse data during training can help procedural generalization even more.","This paper explores the application of the MuZero agent for tasks which require generalization across environments, namely ProcGen and MetaWorld.  On ProcGen, they find that MuZero in its standard form performs on par or better than strong model-free methods. Furthermore, when combined with auxiliary self-supervised learning (SSL) losses, there is a significant jump in performance which achieves a new state of the art. The paper includes interesting control experiments disentangling the effects of different components. For example, it shows that both MuZero’s modified targets for the value functions, as well as the tree search for action selection, each separately contribute to performance. Another interesting finding is that adding auxiliary SSL objectives can help generalization performance on unseen environments, even when they do not improve performance on the training environments, which I found surprising but useful.   Results are also reported on task generalization benchmarks from MetaWorld. Here the results are less strong, and self-supervision does not appear to help. There appears to be some transfer between tasks, but it is limited. ","This paper evaluates how well model-based RL, specifically MuZero, generalizes in comparison to model-free RL. It empirically compares how planning, representation learning, and data diversity affect the generalization of agents. To evaluate the effect of planning, a Q-learning agent is constructed to be as similar as possible to MuZero without the MCTS. In experiments, it is found that planning, self-supervised representation learning (reconstruction, contrastive, self-predictive) and data diversity all improve generalization performance. However, results are not similar in the Meta World benchmarks, where self-supervision did not seem to improve results much. The paper concludes that self-supervision is a promising approach to improving the generalization of MBRL agents in procedural environments, but perhaps it does not improve task generalization.","This paper measures the generalization ability of model-based agents, i.e. MuZero, in comparison to model-free agents. The authors identify three key factors for procedural generalization: planning, self-supervised representation learning, and procedural data diversity. However, they find that these factors do not necessarily provide the same benefit for task generalization. They argue for a move towards self-supervised model-based agents trained in rich, procedural, multi-task environments. ",0.1937984496124031,0.20155038759689922,0.11627906976744186,0.15517241379310345,0.10344827586206896,0.224,0.14367816091954022,0.208,0.2112676056338028,0.216,0.2535211267605634,0.39436619718309857,0.16501650165016502,0.2047244094488189,0.15,0.1806020066889632,0.1469387755102041,0.2857142857142857
116,SP:4aa5f00830fda36b6ca2f53d88c3a8a963058ec0,"The paper builds a memory and computation-efficient version of the KPConv model, which is a point cloud processing model. The paper uses many techniques to achieve this including, depthwise kernels, attention over kernels, and also neural architecture search (NAS). The paper conducts experiments to demonstrate the effectiveness of their techniques.","This paper aims to accelerate the inference of 3D point cloud neural networks. The authors first borrow a few designs from efficient 2D neural networks: depthwise convolution and inverted residual bottleneck. They then propose to reweight the weights of kernel points with attention to boost its representation power. Finally, the authors adopt the predictor-based neural architecture search to explore the best model under a resource constraint. They have evaluated their proposed solution on the small-scale ModelNet40 dataset and the large-scale SemanticKITTI dataset and have achieved reasonably good empirical performance.","This paper addresses point-based methods for classification and semantic segmentation in 3D applications. It aims to improve the computational efficiency of this kind of methods to make it better fit the applications of limited resources such as mobile scenarios. To achieve this, this paper conducts the following works. First, it develops a mobile attention kernel point convolution (MAKPConv) scheme to improve the performance of existing kernel point convolution. Second, it utilizes neural architecture search (NAS) technique to design the MAKPConv-based network. In this part, this paper proposes a wide & deep neural predictor for the NAS process. Experimental study is conducted on benchmark datasets and tasks to demonstrate that the resulted deep networks can achieve higher computational efficiency and improved performance. Ablation study is also conducted to illustrate the key components in this work. ","This paper aims to address the point cloud analysis task by:  1) improving the existing KPConv operation via considering the kernel relationship;  2) designing the networks via a predictor-based NAS approach.  The improved KPConv operation. i.e., MAKPConv, can model the local structure efficiently, and the searched network has fewer parameters while performing better than the baseline on two datasets, including ModelNet40 for classification and SemanticKitti for segmentation.",0.2549019607843137,0.3333333333333333,0.17647058823529413,0.2717391304347826,0.20652173913043478,0.1259259259259259,0.14130434782608695,0.1259259259259259,0.13043478260869565,0.18518518518518517,0.2753623188405797,0.2463768115942029,0.1818181818181818,0.18279569892473116,0.15,0.22026431718061673,0.23602484472049687,0.16666666666666666
117,SP:4b3dad77d79507c512877867dfea6db87a78682d,"The paper proposes a scalable quasi-Bayesian inference for instrumental variable regression. Specifically, the paper builds a quasi-posterior on the recent quadratic kernel IV loss function. For scalability, the paper extends the randomized prior trick based on the recent dual formulation of the quadratic loss. The approach has a theoretical guarantee that incorrect solutions are excluded while valid kept. Moreover, the convergence analysis for the randomized prior trick with the stochastic gradient descent-ascent is present. Experiments validate the approach.","The paper studies a quasi-Bayesian approach to instrumental variable (IV) regression, based on using an empirical estimate of the ‘likelihood’ to obtain a Gibbs posterior. Such an approach has been studied in earlier literature due to the difficulty of incorporating moment conditions in IV via a likelihood. The paper pursues a Gaussian process approach and proves some theory, namely that the resulting posterior puts most of its mass on functions (almost) satisfying the moment constraint. The paper provides a computation approach based on the randomized prior trick and illustrates the method on various simulated datasets.","The paper concerns the uncertain quantification for instrumental variable (IV) regression that it is an approach for estimating causal effect from confounded observational data and, proposes a scalable quasi-Bayesian procedure. The procedure was built upon the kernelized IV models (Muandet et al (2020)). The contribution is forming a quasi-posterior by employing a Gaussian process prior and constructing a kernel conditional expectation estimator using a randomized prior trick. Theoretical properties and simulation studies are provided. ","The authors introduce a scalable quasi-Bayesian method for nonparametric instrumental variable (IV) regression that provides uncertainty quantification for the IV models. They base their approach on kernelized IV models and define a quasi-posterior as the Radon-Nikodym derivative of a generalized method of moments (GMM) objective function with respect to the standard Gaussian process prior. They go on to show that the quasi-posterior exhibits satisfies certain posterior contraction criteria that lead to asymptotically sound results, and prove a conservative posterior contraction rate under a set of typical assumptions. They then show how to achieve scalable inference for the quasi-posterior using a previously proposed “randomized prior trick” for Gaussian process regression, and provide a proof for the convergence of the inference algorithm. Finally, the authors showcase the performance of their method in a series of simulated experiments of varying complexity, by comparing it against other approaches that provide uncertainty quantification (Bayesian, bootstrap).",0.30864197530864196,0.20987654320987653,0.3333333333333333,0.20833333333333334,0.3125,0.35526315789473684,0.2604166666666667,0.2236842105263158,0.17419354838709677,0.2631578947368421,0.1935483870967742,0.17419354838709677,0.2824858757062147,0.21656050955414013,0.22881355932203387,0.23255813953488372,0.2390438247011952,0.23376623376623376
118,SP:4be92f235a78f030c4f09c920dc41eab0ba69aa8,"In this paper the authors inspect the popular Physics Informed Neural Network (PINN) set of models and characterize their behavior (specifically their failure modes) in the context of two popular PDE systems, namely diffusion and convection. In each case, the authors demonstrate that PINNs learn effective representations in simple regimes while failing to learn representations of the PDE systems in more complex regimes. In addition, authors also demonstrate that it is not the inability of the neural network to fit the functions (due to some potential inability to represent non-trivial functions), but rather the way the PINNs incorporate the PDEs into the learning pipeline that is the reason for the failure. Finally, the authors also propose casting the PINN problem as a sequential prediction task (i.e., predicting all collocation points per time period) as opposed to tasking the neural network with simultaneously learning the full spatial and temporal domain.  ","The authors provide a detailed empirical analysis of the numerical difficulties encountered when training so-called ""physics informed neural networks"" (PINNs), a recent class of models which incorporate physical (often differential) equations into the loss function of a neural network. The authors perform a systematic analysis on two relatively simple but common use-cases in scientific literature, convection and diffusion equations, and demonstrate the PINNs fail to adequately learn the underlying dynamics of the system even in simple cases. They show that these failures are due to the effect of the convection/diffusion coefficients and forcing terms on the conditioning of the dynamical system, i.e. larger coefficients or forcings tend to make the problem increasingly ill-conditioned, thereby impacting characteristics of the loss function. The authors then propose two possible workarounds for these issues; improved initialization via a warm start or preconditioning and re-framing the problem as a sequence-to-sequence learning task.","The paper studies the behavior of Physics Informed NNs (PINNs) on two common PDEs of physical relevance. It observes that PINNs fail to learn a good solution under standard training regime when the convection coefficient or viscosity coefficient is high, even when the closed-form solution does not depend on the constants.  The authors then seek to explain this behavior from an optimization standpoint. They plot the loss landscape along the two dominant hessian eigenvectors. They hypothesize that the optimization problem is ill conditioned due to the regularization term in the optimization objective that enforces the pde constraint.  They also run experiments to argue that sequence to sequence learning might work better for these ill conditioned problems.","This work studies two specific (yet essential) PDE learning cases in the PINN framework. Notably, the work proposes two main conclusions to explain the difficult training in PINN networks:  - the loss landscape varies greatly with the PDE coefficient values (and works well with only low diffusion/conduction coefficient),  - the loss landscape (at the end of training) varies greatly with the physics informed penalty coefficient.   The main explanation to such difficulties lies in the condition number associated to PINN penalty. The work proposes two solutions: a warm-up, and a sequence to sequence approach.",0.25165562913907286,0.16556291390728478,0.11920529801324503,0.1935483870967742,0.16129032258064516,0.17094017094017094,0.24516129032258063,0.21367521367521367,0.1935483870967742,0.2564102564102564,0.26881720430107525,0.21505376344086022,0.2483660130718954,0.18656716417910452,0.14754098360655737,0.22058823529411764,0.20161290322580647,0.1904761904761905
119,SP:4c00bcc561832b581f479905b5e3310aeb3bdce2,"The paper solves a problem similar to Neural Body (reconstruction / free-view rendering of clothed humans from videos based on SMPL fits) but does not require training a neural network per scene; instead, it generalises from training sequences and, at test time, runs only a feed-forward model conditioned on keyframes. This is important for many applications since training a neural net per scene is not practical, and also the proposed extension is not trivial: the paper brings novel ideas (using attention for time-wise and camera-wise aggregation). Experimental evaluation is comprehensive and the quality is even superior to per-scene optimised Neural Bodies. Given this, I suggest to accept the paper, although there is some room for improvement.","This work proposes a method that can synthesize free-viewpoint videos for dynamic humans in a sparse multi-camera system.  As is claimed and demonstrated in the paper, by introducing transformer modules to aggregate spatio-temporal information in the sparse multi-view data, the learned neural radiance fields are generalizable to unseen human identities and unseen poses.  The proposed method is tested on public multi-camera datasets, showing promising results.","This paper presents a generalizable NeRF network that can produce novel view synthesis for novel dynamic human scenes from sparse camera views, in a feed-forward manner. It leverages visual features from tracked parametric body model and similar to PixelNeRF, it conditions the NeRF representation on pixel-aligned image features for appearance generalization. A novel combination of a temporal Transformer and a multiview Transformer is proposed to integrate multi-frame and multi-view visual observations. It demonstrated stronger generalization capability than prior work.  ","This paper presents a method to synthesize a novel view image of a person given the sparse multiview images. The paper proposes time-augmented skeletal features---the pixel-aligned features that encode NeRF-like rendering fields by aggregating image features across  time and views, parametrized by a human mesh model. They use transformer networks to learn fusing multiview image features over time. The resulting features are used to generate an implicit function that can predict RGB and opacity. They demonstrate that the proposed method is generalizable to unseen identities and poses, validated on ZJU-MoCap and AIST datasets.  ",0.11666666666666667,0.125,0.125,0.21428571428571427,0.3,0.2289156626506024,0.2,0.18072289156626506,0.15306122448979592,0.18072289156626506,0.21428571428571427,0.19387755102040816,0.1473684210526316,0.14778325123152708,0.13761467889908258,0.19607843137254902,0.25,0.2099447513812155
120,SP:4c2928f6772664d63c02c29f913b476e1c932983,"This paper focuses on the negative sharing problem in multi-task learning, which has not been studied sufficiently in existing work. The authors propose the Safe Multi-Task Learning (SMTL) model and several of its variants to avoid negative sharing and achieve safe multi-task learning. Both theoretical analysis and comprehensive experimental results are provided to demonstrate the effectiveness of the proposed method.","This paper presents a multi-task learning approach that avoids negative sharing in training deep neural networks.  A novel network architecture is proposed, which consists of a public encoder shared by all the tasks, private encoder for each task, and a gate for each task to combine encoded features from public and private encoders. Their experiment results indicate the proposed approach is effective on image recognition related tasks.","The paper propose a simple method for safe multi-task learning where there is no negative transfer or ""negative sharing"" among tasks. It jointly trains shared encoder, task-specific (private) encoders, gate, and decoder. The gate computes importance for each output of shared and task specific encoder and combine the outputs with simple convex combination. ","This paper aims to solve the negative sharing problem, which is defined as that a multi-task learning model has inferior performance than single-task learning on some tasks, for multi-task learning. To address negative sharing, Safe Multi-Task Learning (SMTL) model is proposed. The model combines hard-sharing model and single-task learning together and is expected to achieve performance that not inferior than single-task learning. ",0.2222222222222222,0.20634920634920634,0.3492063492063492,0.25,0.22058823529411764,0.23636363636363636,0.20588235294117646,0.23636363636363636,0.3188405797101449,0.3090909090909091,0.21739130434782608,0.18840579710144928,0.21374045801526717,0.22033898305084743,0.3333333333333333,0.2764227642276423,0.21897810218978103,0.20967741935483872
121,SP:4c925cde6e5b9813946452fdd6b47816e2490f49,"The paper argues that deep Gaussian Processes with large widths and more than 2 layers have undesirable properties. In particular, the paper shows that firstly, deep GP prior in the GP limit is essentially shallow, and secondly, deep GP posterior then loses data adaptivity in the hidden layers. The paper further examines the role of depth and argues that the deep GP prior, if not equal to a GP, has heavy-tailed behaviors.  ================================== Post-rebuttal:  Thanks for the reply and clarification!  While I agree that very large width hurts in the context of deep GP as shown in the paper, I also agree with other reviewers that the message that very large width hurts generally is overly strong. The literature has a mixed bag of results concerning widths -- theoretically or empirically, so a general statement should be validated very carefully.  I'd like to keep my score.","The paper shows that in the limit of infinite width a Deep Gaussian Process (DGP) converges to an ordinary Gaussian Process (GP). Indeed, in the limit of infinite width the covariance function of each hidden GP layer becomes deterministic (as a consequence of the strong law of large numbers). This allows proving that the marginal prior covariance  of the DGP becomes gaussian (Theorem1). The  consequence of this theorem is that as the width increases DGP models lose their ability to adapt the hidden covariance functions to the input data (section 4). This theoretical result is supported by numerical experiments (section 6.1). The paper also addresses the convergence of a DGP to a GP with increasing depth and width (section 5). Since it is known that deep neural networks (DNN) approach a GP as they are made wider and wider,  Theorem 1 is used to  claim that a very large width can be detrimental to the expressivity of a DNN (and not only a DGP).   ","The authors show that in the infinite width limit, deep GPs becomes a single-layer GP, generalizing the result that deep neural networks at initialization become a GP in the infinite width limit. The authors show that increasing depth of the GP makes the distribution of the final layer of deep GPs deviate from being Gaussian at finite width. The authors claim that this simplification implies that large widths are detrimental to neural network performance beyond some point.",     This paper provides a narrative of composite Gaussian process models. In specific the paper provides insights into the interplay between depth and width in the context of capacity and representative power. The paper further draws conclusions on the implications that this has for composite finite basis function models such as neural networks. ,0.22448979591836735,0.17006802721088435,0.12244897959183673,0.1696969696969697,0.09090909090909091,0.15384615384615385,0.2,0.32051282051282054,0.34615384615384615,0.358974358974359,0.28846153846153844,0.23076923076923078,0.21153846153846156,0.2222222222222222,0.18090452261306533,0.23045267489711935,0.13824884792626727,0.1846153846153846
122,SP:4d49bcb069a76f108c0e2de50750827f45eb5676,"This paper aims to battle memorization overfitting problem in meta learning using causality. The authors view the process of meta-learning through the lens of causality which produces several causal graphs i.e. Fig. 1(b) for the general one and Fig. 2 (a-d) for special cases. Based on the causal graphs, the authors use 'front-door' adjustment and propose two algorithms MAML Dropout and MAML Bins. ","This paper studies the problem of memorization overfitting in meta-learning and proposes to construct a causal graph for gradient-based meta-learning. From a perspective of causal graph, this paper demonstrates how existing methods solve the memorization problem. Further, it proposes a novel causal intervention principle to debias the spurious correlation. Specifically, two implementations of the proposed principle are conducted, i.e., by sampling multiple versions of the meta-knowledge via Dropout and grouping the meta-knowledge into multiple bins. Experiments on four benchmark datasets demonstrates the effectiveness and compatibility of proposed algorithm.","This paper studies the memorization overfitting problem in meta-learning from a causal perspective. It provides explanations to current solutions and proposes to update the meta parameters by maximizing the interventional distribution $p(\theta | do(\theta'), S, Q)$. It provides an identification strategy for the interventional distribution by the front door criterion.","The paper addresses the undesirable memorization problem in the context of gradient-based meta-learning, where the meta-learning knowledge obtained as a result of memorizing all meta-training tasks yields poor generalization and is detrimental to task specific adaptation. While previous work has focused on regularization-based and augmentation-based solutions to prevent the memorization of all meta-training tasks, the authors argue these methods and their benefits are still not well understood and propose a causal perspective of meta-learning in a unified causal framework. In this context the authors identify the universal label space of the base model as the confounding factor of memorization causing spurious correlations between initializations learnt in different meta-training steps. Informed by this analysis, the authors demonstrate why existing meta-learning overfitting solutions work, and propose two deconfounder approaches to address the issue of memorization, namely i) sampling multiple versions of the meta-knowledge via dropout (MAML-Dropout), and ii) grouping the meta-knowledge into bins (MAML-Bins).   ",0.27941176470588236,0.23529411764705882,0.36764705882352944,0.2127659574468085,0.39361702127659576,0.3269230769230769,0.20212765957446807,0.3076923076923077,0.15060240963855423,0.38461538461538464,0.22289156626506024,0.10240963855421686,0.2345679012345679,0.26666666666666666,0.2136752136752137,0.273972602739726,0.2846153846153846,0.1559633027522936
123,SP:4e79b326bbda5d1509e88869dde9886764366d41,"This paper addresses the task of finding similar-sound voices with application to voice-dubbing (e.g. finding an actor to record dialog in English, translated from original French dialog, such that the English speaker sounds similar to the original French speaker). The paper proposes a method called ""label refining"". This method is based on p-vectors, a prior representation found to model similarity between characters. The method uses 1) non-expert ""initial labels"" to train a p-vector system 2) use k-means to cluster the resulting p-vectors and 3) use the groups learned by k-means to re-group the labels (the ""refined labels""). To evaluate the method, English-French pairs of voice data from the video games Mass-Effect 3 and Skyrim are used. Performance is measured using clustering measures v-measure and purity-K, as well as accuracy on the test set (which it seems the ground-truth labels are mapping to the correct dubbing speaker). The method achieves a higher accuracy of 0.70 over a state-of-the-art system that achieves 0.69. A second experiment is performed using Skyrim data as a subsidiary corpus used to cluster, with the goal of ""bringing out vocal characteristics"" from the initial labels, which shifts the optimal K towards 2, which the paper claims is because the new representations start to model gender.","The paper presents a ""label-refining"" technique, which helps users pick voices to provide a better user experience in dubbed video games. The idea is that a voice talent's voice in the target language should match the character's voice characteristics in the source language. The method seems to work by attaching labels to data-driven clusters, and refining these using a second corpus, on which the labels' value for discrimination is measured.","The paper proposes a label refinement approach. Starting from an initial set of labels, k-means clustering is used to refine the labels. The approach is described in the context of dubbing/voice casting. The method tries to obtain voice characteristics which can be further used for dubbing/voice-casting. Experiments are shown on video game datasets MassEffect and Skyrim and they primarily investigate how different parameters of k-means (# of clusters) and distance measure affect the label refinement. ","The paper tackles a voice similarity system task for voice casting problem. The authors trained voice embedding network using voice character label and cluster the embedding features and used these clusters to train final voice embedding network. The introduction is well-written, however, the proposed method is known or marginal improvements from the existing technical skill in machine learning community (pseudo labeling with embedding feature clustering).",0.10087719298245613,0.10526315789473684,0.07894736842105263,0.21621621621621623,0.16216216216216217,0.1518987341772152,0.3108108108108108,0.3037974683544304,0.2727272727272727,0.20253164556962025,0.18181818181818182,0.18181818181818182,0.152317880794702,0.15635179153094464,0.12244897959183672,0.20915032679738563,0.17142857142857143,0.16551724137931037
124,SP:4eafae76b923b75534cd28e6e04774ea69e3c2d1,"This paper presents a method to construct a molecular graph, which is inspired by a spanning tree. In specific, a molecule is constructed by a sequence of actions, each of which adds an atom, adds a bond, starts/ends a branch, adds residual edges for a circular structure, and terminates. The generative process is controlled by a transformer-based neural network, which is specialized to tree construction procedure. The empirical studies show that the proposed method can learn the distribution of molecules comparably or better than the existing methods, and can be used to molecular optimization.","The paper proposes a novel spanning tree-based graph generation (STGG) framework. The key idea is to represent the molecular graph as a sequence of decisions according to a **novel spanning tree-based grammar**, and then model these decision sequences using the transformer.  To accommodate such a novel algorithm, several interesting techniques are involved, e.g., representing molecules as bipartite graphs, using tree-based positional encoding for transformer. The authors claim that the proposed method allows generating valid molecular structures and inferring the intermediate graph structure.","This paper proposes a new paradigm of graph generation, a transformer network sequentially generates a sequence of decisions to generate a spanning tree for a bipartite graph. These decisions have 7 forms such as attach atom, attach bond, branch start,  etc. After the spanning tree, they add residual edges. The authors also have a focus on generating valid graphs (which is seen in the results) in which they mask out invalid decisions during the generation process itself.   ","This paper proposes a spanning tree based generative model (STGG) for molecular graphs. STGG sequentially generate a molecule's spanning tree and fill in the residual edges on the way. The spanning tree construction is similar to the standard SMILES representation, but the model operates on a molecular graph rather than a SMILES string. STGG adopts a tree-based transformer with relative positional encoding for tree generation, and a attention-based predictor for residual edge prediction. The method is evaluated on standard ZINC250K, QM9, and MOSES benchmarks and outperform existing baselines.",0.20833333333333334,0.19791666666666666,0.19791666666666666,0.2558139534883721,0.27906976744186046,0.22077922077922077,0.23255813953488372,0.24675324675324675,0.2087912087912088,0.2857142857142857,0.26373626373626374,0.18681318681318682,0.21978021978021978,0.21965317919075142,0.20320855614973263,0.26993865030674846,0.2711864406779661,0.2023809523809524
125,SP:4f5c00469e4425751db5efbc355085a5e8709def,"This paper proposed a method to generate imperceptible attack in black box attack scenario by generating local perturbation blocks in salient regions. It used salient object segmentation to obtain the salient region, then applied a tree search method to find smallest blocks within the salient region that can cause the maximal change in predicted class logits. Experiments on 1000 Imagenet examples are conducted, compared to several existing baselines, showing that the proposed method can improve achieve more imperceptible attacks (where imperceptibility is measured by metric MAD).","The paper studies how to reduce the perceptibility of the perturbations to the original images produced by black-box adversarial attacks (for the $\ell_\infty$-threat model). In particular, it proposes to use a prior based on segmentation techniques to localize the changes on the subject of the image and leave the background unaltered. Moreover, the Saliency Attack is introduced to further reduce the fraction of the original image which is modified to induce misclassification.","This paper proposes a black-box attack where, by relying on segmentation priors, the perturbation is applied only in the salient region. This allows one to obtain reduce perceptibility with a limited number of queries and a small reduction in success rate. More specifically, once the salient region has been identified, a refining procedure is carried out to find small areas where the perturbation should be added. Experiments are performed on ImageNet and results are compared with those of some SOTA methods and their variants that work on saliency regions.","There has been a lot of interest in improving the query efficiency of black-box attacks in the recent past. However, these techniques produce examples that a human in the loop can quickly identify. The authors propose using segmentation priors to improve the black-box attacks so that the perturbations are restricted to the salient regions of the image. In addition to this, they also present a technique that improves the imperceptibility without forgoing the query efficiency.",0.16279069767441862,0.29069767441860467,0.19767441860465115,0.18666666666666668,0.22666666666666666,0.14444444444444443,0.18666666666666668,0.2777777777777778,0.22077922077922077,0.15555555555555556,0.22077922077922077,0.16883116883116883,0.17391304347826086,0.28409090909090917,0.2085889570552147,0.1696969696969697,0.22368421052631576,0.155688622754491
126,SP:4fc9a0b34192e1b3587c8e2128851c6aebddd26b,"This paper evaluates several transductive few-shot learning methods [15,28,20,21,23,17] on class-imbalanced FSL tasks. The class distribution in the query set follows a Dirichlet distribution. The support set is balanced. This work proposes a novel method based on $\alpha$-divergences addressing the imbalance problem. The novel method $\alpha$-TIM achieves competitive performance compared to several recent baselines.","The paper studies transductive few-shot learning in the more realistic setting with arbitrary class distributions within the query sets. In the proposed setting, authors use Dirichlet distribution to model the marginal probabilities of the classes in the query sets as random variables and generate random samples within the simplex. Focusing on two recent transductive methods PT-MAP and TIM, they first show that class-balance prior is encoded in these two methods and then extend TIM by generalizing mutual-information loss to \alpha divergences to more effectively deal with arbitrary class distributions. The experiments validate proposed \alpha-divergence approach on three standard benchmark datasets (mini-ImageNet, tiered-ImageNet and CUB) and show that the method outperforms inductive and transductive few-shot learning methods in the proposed setting.","This paper studies transductive evaluation in few-shot learning, which allows the model to use statistics of all the unlabeled query set examples when making class predictions. Models evaluated in transductive fashion typically attain higher metrics by using this extra information. Most work on transductive evaluation, however, assumes that the query set is evenly distributed across the possible classes. This is of course an unrealistic assumption for the real world and this paper specifically studies this aspect: (1) They evaluate in a more realistic transductive evaluation scenario where the class counts within the query set are sampled according to a Dirichlet distribution and show that most transductive evaluation-based methods perform worse in this setup; (2) They propose a modification of an existing model for transductive evaluation to work better in this scenario where the class counts can be varied in the query set.","The researchers use transductive inference and introduce the effect of arbitrary class distributions within the query sets of few-shot tasks, rather than using class-balanced tasks, by removing the class-balance artefact. They do this by modeling the marginal probabilities of the classes as Dirichlet-distributed random variables, rather than from a known and fixed uniform distribution. They assess their model by comparing transductive methods over 3 datasets, and show their experiment setting’s performance drops compared to inductive methods. They also propose a generalization of the mutual-information loss based on alpha-divergences, which is an extension of the Shannon mutual information and tolerates class-distribution variations more effectively.",0.2857142857142857,0.30158730158730157,0.25396825396825395,0.2109375,0.2578125,0.16666666666666666,0.140625,0.13194444444444445,0.14414414414414414,0.1875,0.2972972972972973,0.21621621621621623,0.18848167539267013,0.18357487922705315,0.1839080459770115,0.19852941176470587,0.2761506276150627,0.18823529411764706
127,SP:4ff82f679a321ed61e02c50d5997c4e179441a0e,"The paper describes some lower bounds for differentially private empirical risk minimization (DP ERM). The main results are:  * An $\Omega(\sqrt{p \log(1/\delta)}/\epsilon n)$ lower bound for unconstrtained ERM under approximate differential privacy. This improves on prior work of Bassily et al. in the presence of the $\log(1/\delta)$ term, and in the optimization being unconstrained.  * An $\Omega(p/\varepsilon n)$ lower bound for pure differential privacy.  The proofs use now standard techniques from the literature: fingerprinting codes for approximate DP and a packing argument for pure DP.","This paper presents tight lower bounds on differentially private ERM, a well-studied topic in the DP literature. It obtains tight bounds for both the constrained and unconstrained settings. The exact quantitative improvements are stated precisely in the abstract, and I won't repeat them here. The improvements are not staggering, but they are tight, and are presumably the final word on this topic.   ","This paper studies differentially private empirical risk minimisation (ERM) in the unconstrained setting. It gives tight lower bounds for approximate DP ERM for general loss functions, which also implies the same lower bound for the constrained case, which is an improvement over a classic lower bound by Bassily et al 2014. It also gives a lower bound for unconstrained pure DP ERM that recovers the result in the constrained case.","## Summary of Contributions  This paper studies the unconstrained empirical risk minimization (ERM) under differential privacy (DP). In this setting, there is a loss function $\ell: \mathbb{R}^p \times \mathcal{X} \to \mathbb{R}$ and we are given $x_1, \dots, x_n \in \mathcal{X}$; the goal is to output $\theta$ that minimizes the empirical loss $L(\theta; X) := \frac{1}{n} \sum_{i=1}^n \ell(\theta; x_i)$. The goal is to minimize the excess empirical loss $\mathbb{E}[L(\theta; X) - \min_{\theta^* \in \mathbb{R}^p} L(\theta^*; X)]$. We want our algorithm to satisfies $(\epsilon, \delta)$-DP. Recall that the case $\delta > 0$ is referred to as *approximate-DP* whereas the case $\delta = 0$ is referred ti as *pure-DP*. Here we assume that $\ell$ is 1-Lipchitz; the results easily extends to $C$-Lipchitz functions with an extra multiplicative factor of $C$ in the excess empirical loss.  The main contributions of the paper are: 1. In the approximate-DP setting, the authors show a lower bound of $\Omega\left(\frac{\sqrt{p \log(1/\delta)}}{\epsilon n}\right)$. This improves upon the best known bound of $\Omega\left(\frac{\sqrt{p}}{\epsilon n \log p}\right)$ in the unconstrained case from [Asi et al., 2021] and $\Omega\left(\frac{\sqrt{p}}{\epsilon n}\right)$ in the constrained case [Bassily et al., FOCS 2014]. The new lower bound also matches the known upper bound in both cases [Bassily et al., FOCS 2014]. 2. In the pure-DP setting, the authors show a lower bound of $\Omega\left(\frac{p}{\epsilon n}\right)$.  To prove 1., the authors reduce from the 1-way marginal problem (similar to previous work). Recall that in 1-way marginal, we are given $x_1, \dots, x_n \in \\{-1, 1\\}^p$ and the goal is to approximate $\frac{1}{n} \sum_{i=1}^n x_i$; a lower bound of $\Omega(\frac{\sqrt{p \log(1/\delta)}}{\epsilon n})$ is known for the problem [Bun et al., STOC 2014]. The authors use an $\\ell_1$-distance loss function, i.e., $\\ell(\\theta; x_i) = ||\\theta - x_i||\_1$. Notice that here the optimal solution is $\\theta^*\_j = sign(\sum\_{i} z\_{i, j})$. This in spirit is very similar to 1-way marginal but not exactly the same. Specifically, if $\sum_{i} z_{i, j}$ is roughly around zero, then taking $\theta_j = -1$ or $\theta_j = 1$ does *not* effect the loss too much. Therefore, a direct ""blackbox"" reduction from 1-way marginal does not seem to work. To overcome this, the authors observe that actually in the construction of [Bun et al., STOC 2014] most of the coordinates' means are not close to zero (formalized as ""biased mean"" property in the current paper) and thus the hard instance gives the desired lower bound for DP ERM.  To prove 2., the authors use a standard packing-style construction together with the $\\ell_2$-distance loss function",0.15217391304347827,0.21739130434782608,0.44565217391304346,0.265625,0.3125,0.45714285714285713,0.21875,0.2857142857142857,0.0823293172690763,0.24285714285714285,0.040160642570281124,0.0642570281124498,0.1794871794871795,0.24691358024691357,0.13898305084745763,0.25373134328358204,0.07117437722419928,0.11267605633802817
128,SP:5068e491ee0ae7282cd98ef966b471389e2ab069,"The authors identify and benchmark a plethora of recently proposed techniques for predicting neural network architecture performance on a task without doing a full training run.   They begin by grouping each technique into categories: model-based, learning-curve-based, zero-cost, and weight-sharing. When viewed like this it becomes clear that there is a need for a common benchmark to distinguish the performance of the different approaches.   To compare under one framework, evaluation is done with respect to two variables: initialization time (time spent preparing the model) and query time (time spent retrieving predictions from the model).  The authors then present their own method, OMNI, which performs NAS using a blend of model-based, learning-curve-based, and zero-cost predictors.  ",The paper give a large-scale comparison of performance predictors in NAS. The results act as recommendations for the best predictors to use in different settings. The author also show that certain families of predictors can be combined to achieve even better predictive power.,"Neural Architecture Search (NAS) aims to find the network architectures with the best accuracies -- or best size/accuracy tradeoffs -- from within a human-defined search space. The submission evaluates a number of different heuristics for ranking different candidate architectures in a search space. The submission claims to evaluate 31 different techniques which fall into several categories (e.g., early stopping, learning curve extrapolation, supervised learning, one-shot models, zero-shot metrics). The submission claims that different techniques work better for different compute budgets (e.g., low setup cost/low search cost vs. high setup cost/high search cost). It also claims that combining techniques from different families (e.g., supervised learning + zero-shot metrics) can lead to better results than using any one technique on its own. Experimental results are reported on NASBench-101, NASBench-201, the NASBench-301/DARTS search space, and NASBench-NLP.","This paper compares 3 kinds of performance predictors, model based predictors, learning curve based predictor and zero nas predictors. They benchmark the performance of all 31 predictors against 5 benchmarks: NAS-Bench101, DARTS, NASBench201 and NLP.  In addition to this, they also incorporate SOTL-E and jacobian variance as addition features to NGBOOST and SemiNAS predictors and demonstrate that it improves the performance over all 3 of them. ",0.08196721311475409,0.14754098360655737,0.13934426229508196,0.3181818181818182,0.25,0.06206896551724138,0.22727272727272727,0.12413793103448276,0.25,0.09655172413793103,0.16176470588235295,0.1323529411764706,0.12048192771084337,0.1348314606741573,0.17894736842105263,0.14814814814814814,0.19642857142857145,0.08450704225352113
129,SP:511226b467019dcd85e9ebf8b9b56f8f1b3ef889,"This paper provides an in-depth study of the properties and applications of the semantic alignment between the original parent StyleGAN model and its finetuned child model on another dataset. Specifically, the paper empirically demonstrates the semantical alignment of the two models. Then, based on the properties, the paper solves serval tasks like image translation, image morphing, zero-shot image editing and attribute classification. ","  The paper provides interesting analysis and leveraging of GAN’s model alignment (i.e., transfer learning). Without custom architectures and losses, it demonstrates impressive performance in a diverse set of tasks (image translation and image morphing). It also demonstrates promising results for zero-shot image recognition by leveraging the shared latent space of aligned models.  ","This work is about the task of transfer learning to tame a new ""child"" network using a pre-trained ""parent"" network. While the model and fine-tuning technology lack novelty, the shared semantic information in the generation network are interesting. Finally, the authors applied the proposed aligned model to multiple tasks, including image-to-image translation, cross-domain image morphing, and zero-shot classification and regression. The impressive results with shared semantic information are achieved. ","The paper undertakes extensive experimentation in the adaptability of latent space modifications from a base model, and a fine-tuned model for a secondary dataset. They demonstrate which areas of the models change most, and which information remains trained in the parameters, despite re-training. They go on to use their findings in downstream experiments, reaching state-of-the-art quality.",0.25,0.28125,0.203125,0.2727272727272727,0.18181818181818182,0.16,0.2909090909090909,0.24,0.21311475409836064,0.2,0.16393442622950818,0.19672131147540983,0.2689075630252101,0.2589928057553957,0.20799999999999996,0.23076923076923075,0.17241379310344826,0.1764705882352941
130,SP:5495d9168a8770eb2493e2d2bb6b68423e82b9e6,"This work propose to first search for the search space for transformers and then applies once-for-all NAS methods to search for architectures within the searched search space. In particular, the work proposes an E-T error metric to evaluate the quality of the search spaces. From the results of the whole search process, this work also summarizes some design guidelines for vision transformers. The experiments are extensive and they show the superiority of the proposed search method. ",This paper proposes a method for searching the search space of vision. The authors propose decomposing the search space by the search dimensions and evaluating them with the proposed E-T error to explore the optimal search space. Then the new subspace is formulated as Equation (8). The results reported in this paper have achieved state-of-the-art results.,"This work applies NAS to the vision transformer architecture. Besides searching the architectures, it also automatically adjusts the search space guided by the E-T error that combines the error distribution and top model errors to evaluate the quality of different choices in a search dimension. By fitting a linear function to approximate the E-T error, the new search space is updated through a simple formula. They used the supernet to efficiently compute the E-T errors, and use evolutionary search to search for the best algorithm in a given search space. The discovered architecture is shown to be competitive to the state-of-the-art models, and also generalize to other vision tasks. ","This paper introduces a neural architecture search method called S3 that first searches the search space before identifying good architectures from the discovered search space.  The authors focus on applying S3 to vision transformers and design a search space modeled after ViT.  They evaluate S3 discovered architectures on various computer vision tasks and show good performance on image classification, object detection, and semantic segmentation. ",0.25316455696202533,0.35443037974683544,0.20253164556962025,0.4166666666666667,0.23333333333333334,0.1565217391304348,0.3333333333333333,0.24347826086956523,0.25,0.21739130434782608,0.21875,0.28125,0.2877697841726619,0.288659793814433,0.22377622377622378,0.2857142857142857,0.22580645161290322,0.2011173184357542
131,SP:54a60315416c6e304f59741490c335fb1e2ce95d,The paper proposes MCM ~ algorithm to perform bidirectional compression in distributed setting. The authors claim similar convergence guarantees as vanilla setting.  They introduce the notion of sending different models to different clients while keeping the global model preserved.  ,This paper proposed a new method: MCM for bidirectional compression in federated learning. Convergence analysis showed that linear convergence can be achieved with the proposed method. Empirical results also show the method could achieve non-trivial performance with high compression rate.,"The paper proposes a bidirectional compression algorithm for parameter-server-based distributed training. Two variants, MCM and Rand-MCM, are proposed to achieve the same improvement as the uplink-compression-only compression algorithm, where the downlink-compression-only impacts local models while the global model is preserved. The Rand-MCM also introduces the concept of (simulated) random smoothing via model compression.","In this paper, the authors propose MCM, a new algorithm achieves bidirectional compression with better convergence rate. Theoretical analysis is provided. The empirical results show good performance. ",0.21052631578947367,0.39473684210526316,0.18421052631578946,0.1951219512195122,0.3170731707317073,0.09836065573770492,0.1951219512195122,0.2459016393442623,0.25925925925925924,0.13114754098360656,0.48148148148148145,0.2222222222222222,0.20253164556962025,0.30303030303030304,0.2153846153846154,0.15686274509803924,0.3823529411764706,0.13636363636363638
132,SP:54cdc6fe43ed138231f26daf699119f2a16473d0,The paper makes three contributions in particular: - A local version of randomized smoothing for multi-output classifiers. The authors suggest using a customized smoothing distribution for certifying each output of the multi-output classifier. The custom distributions allow them to produce tighter guarantees for each output. - A new analysis method of variance smoothing for discrete data uses the average softmax value instead of the majority vote as the prediction rule. The authors use the first and second-order statistics (mean and variance) to provide robustness guarantees in this method. - A collective certification strategy for multi-output classifiers using a common interface ($\ell_p$ norm ellipsoids) for base certificates for every output. The authors describe a common way of stating the base certified regions for every output. Then the multi-output certification problem can be expressed as a mixed-integer linear program to find a point inside the perturbation model that lies outside the base certified regions for the maximum number of outputs.,"This paper leverages the recent anisotropic certificates for randomized smoothing for certifying multi-output classifiers. In particular, it leverages anisotropic Gaussian  and Bernoulli smoothing for better collective robustness. Experimental evaluation was conducted on semantic segmentation and node classification to demonstrate the effectiveness of the proposed method.","**Summary**  The paper proposes a localized smoothing approach for certifying structured output models. The threat model of interest in this setting is the bounded input perturbation that results in the highest number of prediction flips per pixel. The paper proposes to utilize the idea that when certifying pixel predictions at location (i,j), one can smooth the input pixels (k,l) far away from (i,j) by larger noise magnitude (standard deviation) resulting in a higher certified radii as they perhaps may not play a significant role in predicting the label of (i,j). The paper formulates the problem of finding worst cases adversaries that result in misprediction to the task of finding adversaries outside the certified region with anisotropic smoothing over the input. The paper then lower bounds the binary objective with a box constraints and solve the problem using linear  programs. Experiments are conducted on image segmentation tasks along with node classification.","The authors consider tasks mapping a single input to multiple outputs and study robustness certificate against input perturbations. To achieve the goal, the authors propose a collective certificate where each output is dependent on the entire input but assigns different levels of importance to different input regions, and then derived the collective certificate based on localized randomized smoothing. The proposed collective certificate is evaluated on both image segmentation and node classification tasks.  ",0.08641975308641975,0.1728395061728395,0.09876543209876543,0.2826086956521739,0.21739130434782608,0.12337662337662338,0.30434782608695654,0.18181818181818182,0.2222222222222222,0.08441558441558442,0.1388888888888889,0.2638888888888889,0.1346153846153846,0.1772151898734177,0.13675213675213674,0.12999999999999998,0.1694915254237288,0.16814159292035397
133,SP:5630707c9d0d9e21fce2efddef874e373bfed026,"This paper proposes an automatic data augmentation approach. Different from existing works, they proposed to augment patches in the image rather than the whole image. The approach is formulated as a multi-agent reinforcement learning problem. They empirically show the effectiveness of their approach across several image classification datasets.","This paper target the task of automatically determining the best augmentation method to obtain improved accuracy. While the previous related studies focus on the image-level augmentation and ignore the semantic information of the augmented images, the proposed algorithm augments the grid-wise patches of the given input with the preserved semantic information. To overcome the enlarged number of combinations to consider all the patches, the algorithm utilizes the MARL algorithm with the unified reward function. By MARL algorithm, the number of parameters can be reduced and the training speed can be much improved, compared to the previous auto-augmentation methods. Through the image classification and fine-grained image recognition tasks, the proposed algorithm was validated, and it shows the state-of-the-art performance among the compared methods.","  The paper proposes an evolution of the traditional   pipeline of image data augmentation used to reduce ML   model overfitting.    Instead of applying transformations such as shear,   rotate, CutOut, etc. at the image level, the proposed   technique divides the images into a fixed grid and   applies a potentially different transformation to each   cell. The problem of selecting a transform for each cell   is cast as a multi-agent RL (MARL) task, and the agents   learn as the main network trains within a (multi-agent)   Advantage Actor Critic framework.    The agents use a shared reward mechanism, with the reward   defined as the difference between the loss on the   augmented sample and the original loss.    The regular grid is fixed for a dataset and   hyperparameters like the magnitude of the augmentations   follow a fixed schedule; the agents only pick _which_   augmentation to apply on a patch.    Experiments performed on CIFAR-{10,100}, ImageNet,   CUB-200-2011, Stanford Cars, and FGVC-Aircraft show   relatively small but very consistent improvements in   terms of image classification accuracy. Different design   choices (MARL vs. single-agent vs. random, grid size,   etc.) are ablated and discussed in detail. ","This paper proposed a fine-grained automated data augmentation approach, Patch AutoAugment (PAA), which tries to increase diversity in local regions by divide an image into a grid of patches and search for the joint optimal augmentation policies for the patches. The proposed PAA considers the task as a multi-agent reinforcement learning problem,  and adopt a multi-agent reinforcement learning algorithm to automatically search for the optimal augmentation policies by considering the contextual relationship between the patches. They verify the proposed method on many classification and fine-grained recognition dataset(CIFAR10, CIFAR-100, ImageNet, CUB-200-2011, Stanford Cars and FGVC-Aircraft). The experiments show a good result and visualization results provide some insights that the PAA  help the target network to localize more class-related cues. ",0.2653061224489796,0.3877551020408163,0.40816326530612246,0.24031007751937986,0.20930232558139536,0.20634920634920634,0.10077519379844961,0.10052910052910052,0.15625,0.164021164021164,0.2109375,0.3046875,0.14606741573033707,0.15966386554621848,0.22598870056497175,0.19496855345911948,0.21011673151750973,0.24605678233438488
134,SP:5676944f4983676b5ad843fdb190bf029ad647bb,"This paper introduces a dynamic normalization, named Dynamic Token Normalization (DTN), to replace the vanilla layer norm in ViT. It learns to normalize tokens in both intra-token and inter-token manners, enabling Transformers to capture both the global contextual information and the local positional context. Experimental results show that DTN can improve some Vision Transformers in ImageNet Classification and Long ListOps tasks. ","This work presents a token normalization method in replacement of Layer Norm (LN) and Instance Norm (IN) for vision transformers (ViTs). The motivation is that the authors find that the common normalization used in most existing ViTs, LN, will reduce the difference in token magnitude, and this may lead to failure of capturing positional context and other inductive bias. Hence, they propose a Dynamic Token Normalization (DTN) component by combining LN and relative positional embedding based transformation. DTN can be plug in varying ViTs e.g., ViT, Swin, PVT. The experiments are done on ImageNet, ImageNet-C, ImageNet-R, and ListOps in supervised learning setting, as well as self-supervised pre-training. ","This paper studies properties in layer normalization and instance normalization, and states that the two normalization operations have their own drawbacks in vision transformers: LN suffers from lacking inductive bias while IN may be affected by the different semantics within the tokens. Accordingly, this paper proposes a new normalization method called DTN and considers both inter- and intra- token normalization into vision transformers. According to the experiments in the paper, the proposed DTN normalization boosts the performance of various vision transformer models on classification with ImageNet dataset as well as robustness with IMAGENET-C and IMAGENET-R.",The paper first analyzes the limitation of LN in Transformers and then proposes DTN to capture both long-range dependencies and local positional context. DTN is a unified version that balances LN and IN. Extensive experiments show the effectiveness of the proposed DTN with some small/middle-scale Transformers on the ImageNet.,0.30158730158730157,0.2857142857142857,0.25396825396825395,0.21428571428571427,0.13392857142857142,0.15463917525773196,0.16964285714285715,0.18556701030927836,0.3076923076923077,0.24742268041237114,0.28846153846153844,0.28846153846153844,0.21714285714285714,0.225,0.2782608695652174,0.2296650717703349,0.18292682926829268,0.20134228187919465
135,SP:56a74403d4471cd95641dc669f5eac89a2c93144,"Inspired by ideas about how humans learn about objects, the authors detail a system to train a neural network to perceive generic objects using image triplets where objects move and the viewer also can move.  The viewer's motion is provided as an input. Object perception by parts of the network operating on the first two time points is rewarded by predicting what is seen at the third time point (the training signal). Having been trained, the object perception part of the system, which is a relatively basic neural network, can segment them from a single image.  This new setup requires different data that what has been used in this space, and the authors contribute a synthetic dataset as well. ","This paper presents an unsupervised object-centric scene representation technique that can decompose a scene into multiple objects (and segment the scene) and infer their 3D locations and pose. The overall setup is very similar to earlier models like MONet but this model works on sequences of images, more precisely on 3 consecutive images. It uses the first two images to infer the 3D position and pose of objects and combining this with known camera motion tries to predict the last (third) image. The main contribution here is an optical flow based method to warp the image at time t using the predicted object location/pose/depth to predict (some of) the pixels in image at time t+1.  In more detail, the object extraction network outputs the location and pose of each object. And a separate depth perception network outputs the depth for each pixel in the image. The location and pose of objects are used to estimate the velocity of each object (e.g., by subtracting the position at t-1 from position at t. note this requires matching each object at time t-1 to object in time t, which they do using a soft-matching approach). These along with the depth information are then used to warp the image at t to predict pixels in image at time t+1. This is possible only for a subset of the pixels so for the rest, they use a separate ""imagination"" network that takes in object information and predicts the color/depth and object masks at t+1. The predictions from warping and imagination network are then combined to form the final predicted color and depth images.  To train the model, they require images and camera motion, and use a combination of losses: reconstruction loss on predicted and ground truth image, self-supervised losses on object location, pose, and depth.  ","This paper presents a method to learn how to parse 3-frame videos into object-centric representations, which include segmentation masks and 3D positions and yaws for those objects frame-by-frame, as well as an image representation of the background, and an overall depth map. This is accomplished with a depth network, an object network with an LSTM at the bottleneck to iteratively pick out objects and their positions and yaws, and a decoder to provide segmentations, a warping/re-compositing operation that pastes the inferred objects at their estimated positions for the NEXT frame (i.e., with a constant-velocity assumption), and finally an ""imagination"" network which refines the estimated image. The model learns with a combination of 4 losses, which include image reprojection/prediction, depth consistency, a spatial term that includes consistency and randomness (though I have complaints about this), and finally a penalty term that discourages object probabilities from being zero. The paper also introduces a new synthetic dataset where prior methods do badly, and the proposed method does slightly better. The learned depth maps look good, but this is perhaps expected because camera poses are known. ","This paper studies the problem of predicting the segmentations and poses (position + yaw orientation) of multiple objects, given the image of a scene. The paper introduces a method that is trained without supervision for the segmentations, similar to several other recent object-centric models. In contrast to these existing models, the method proposed in this paper additionally estimates the 3D location of each object by predicting a depth map and classifies the yaw angle by representing the pose domain as equally-spaced bins. To do so, during training the method operates on a short clip of the scene recorded by a moving camera, and uses self-supervision by predicting the scene’s image at the next time step. At test time, the model is able to infer a representation of each object in the scene and segment them given a single image of the scene.",0.25,0.20833333333333334,0.21666666666666667,0.13141025641025642,0.14423076923076922,0.14136125654450263,0.09615384615384616,0.13089005235602094,0.1793103448275862,0.21465968586387435,0.3103448275862069,0.18620689655172415,0.1388888888888889,0.1607717041800643,0.1962264150943396,0.16302186878727634,0.19693654266958424,0.16071428571428573
136,SP:570149eb8fb97928f94312e40bdc48dfe9885848,"This paper proposes a method called Generative Planning (GPM), which aims to improve exploration for model-free RL. GPM learns a recurrent model to generate short term plans at each time step, and only decides to switch to the new plan if it is a lot better than the old plan. This encourages temporally extended explorations, and also does it in an adaptive manner. Experiments on a set of continuous control benchmarks show that GPM is able to converge faster than prior approaches, explore more effectively, and generate interpretable short-term plans. ","This paper presents a method called generative planning method (GPM) to improve exploration in RL. GPM performs exploration by learning a planner (a map from state to a sequence of action, aka a “plan”)) and performing MPC with a special rule for whether or not to use the latest plan or keep the old plan. The planner is an auto-regressive model (specifically, a stochastic RNN) and is trained to maximize an auto-regressive Q-function. Each time step, the plan from the previous time step is shifted forward by one time step and compared to the newly generated plan. The plans are compared using their predicted Q-values, and the policy switches plans with some probability that increases monotonically with Q(new plan) - Q(old plan). The exact likelihood is determined by a hyperparameter, l_commit_target, that, intuitively, sets a soft target for how long a plan is typically kept. The authors compare GPM to a variety of action-repeat-based exploration methods, from epsilon greedy policies to policies with learned action repeat counts (DAR & TAAC) and find that it is competitive with or outperforms these methods on various low-dimensional robot domains, as well as the image-based CARLA environment. The authors also visualize the trajectories and qualitatively show that GPM improves exploration.","A generative planning method is proposed, which sits somewhere between model-free and model-based methods. No explicit model is learned. However, an action plan is generated using a recurrent action-plan generator, paired with a similarly recurrent critic.  At each environment step, a new action plan is generated, and  the current action plan can be abandoned in favor of the new action plan if the estimated benefit is large enough. The method builds on SAC.  Overall the benefits of the temporally extended action plans are: (a) temporally-coordinated exploration; (b) more effective than action-repeat; (c) some degree of interpretability given that an action-sequence plan represents then intent of a policy in a given state.","This paper proposes a method for exploration called Generative Planning method (GPM), which generates a multi-step action sequence such that the exploration is more temporally consistent and ""intentional"" compared to regular single-step action noise exploration. The multi-step action sequence is output by a generator with an RNN structure, and the generator is optimized by maximizing the plan value function. The authors show that their method GPM performs better than some other methods in several continuous control tasks and present some interesting qualitative results (e.g. state trajectory) showing that the exploration is more effective.",0.358695652173913,0.25,0.2717391304347826,0.12962962962962962,0.1527777777777778,0.1794871794871795,0.1527777777777778,0.19658119658119658,0.25773195876288657,0.23931623931623933,0.3402061855670103,0.21649484536082475,0.2142857142857143,0.22009569377990432,0.26455026455026454,0.16816816816816815,0.2108626198083067,0.19626168224299065
137,SP:5739081ab7aaf71d389705c28f14a316fbb0a728,"This paper studies the problem of estimating the edge couplings in a Markov random field using ell-one regularized least-squares estimation. It is assumed throughout that all nodes have the same degree and the nonzero coupling have the same value. Using the replica method, the authors derive a set of couple equations of state (EOS), which depend on the underlying parameters of the model (e.g., the degree and coupling strength),  the ratio between the number of samples and the number of nodes, and the spectral density of the covariance of the resulting MRF. By analyzing the solutions to the equations, the authors provide numerical predictions of the reconstruction performance (e.g, precision and recall) in the limit of high dimension. Further analysis of these expressions gives a sharp predicition for the number of samples needed for exact recovery as a function of the the degree and the coupling strength. Additionally, the authors suggest a modification of the EOS that uses the observed data to produce more accurate predictions. Because the analysis depends on the replica method the results are not rigorous. The accuracy of the theoretical analysis is supported by numerical simulations. ","The submission considers the problem of recovering the underlying graph structure of an Ising model defined on N nodes, given M i.i.d. samples. More specifically, the authors focus on Ising models associated with typical random regular graphs in the paramagnetic phase. Ideas from statistical physics are borrowed to study the performance of an estimator derived from an $\ell_1$-regularized linear regression problem. It is claimed that using $M = \mathcal{O}(\log(N))$ samples is enough to obtain model selection consistency. Non-asymptotic results are also given, and some numerical experiments are presented to support the findings.","The manuscript contains a theoretical investigation of the Ising model selection problem.  The task is to recover the support (the graph topology) of the interaction network from observations of Ising configurations.   An objective function in the M-estimator family, named L1-LinR by the authors, is analyzed.  It contains the typical quadratic data-dependent loss from linear regression and an L1 regularization term,  therefore it is a mismatched objective compared to L1-LogR (pseudolikelihood) and the Interaction Screening. ","The authors study the Ising model selection problem and compute analytically via the replica method the typical performances of the $\ell_1$-regularized linear regression. They confirm the theoretical result by running a good number of numerical simulations. They demonstrate the sample complexity is such that O(log N) samples are enough to correctly identify the right neighborhood of a generic variable. The theory presented is able to predict the precision and the recall rates also for finite values of the system size and a small number of samples. Moreover, the theory seems to work fine also for graphs with many loops. The methodology of the present work can be extended also to other estimators.",0.0979381443298969,0.10309278350515463,0.16494845360824742,0.1919191919191919,0.24242424242424243,0.24358974358974358,0.1919191919191919,0.2564102564102564,0.2782608695652174,0.24358974358974358,0.20869565217391303,0.16521739130434782,0.1296928327645051,0.14705882352941177,0.2071197411003236,0.21468926553672316,0.22429906542056074,0.1968911917098446
138,SP:57ace99a05a76b7d7427619cb6881fc87d74160f,"This paper is situated in the context of CTDE for cooperative MARL. The paper proposes new forms of intrinsic rewards to improve multi-agent exploration and a policy regularization term for an agent to have more influence over others. Experiments were conducted on existing benchmarks, such as StarCraft micromanagement and scenarios in the multi-agent particle environment, and two sparse-reward gridworld settings designed by the authors. The proposed method was compared with selected MARL baselines and ablations.","This paper introduces two techniques for Centralized Training with Decentralized Execution (CTDE) MARL. Specifically: (1) It proposes an ""influence"" training objective by which one agent is encouraged to help the other agents reach target returns. (2) It introduces intrinsic motivation and intrinsic cost terms aimed at encouraging efficient joint exploration of the Markov game. It then goes on to give several empirical comparisons with baselines, showing the utility of the approach in Starcraft Multi-Agent Challenge (SMAC), two sparse reward settings, Multi-Agent Particle Environments, and OpenAI Gym continuous control environments. It also performs an ablation study in (SMAC), demonstrating the usefulness of each component of the proposed method.","This paper introduces the idea of estimating the influence an agent has on other agents' actions, in order to achieve better coordination among agents. An agent is chosen as the influencer, which estimates the gap between other agents' actions and their targets given its current action. The influencer is encouraged to minimize this gap to lead other agents closer to their target returns. The paper also proposes to learn an intrinsic reward for each agent to encourage agents to learn more diverse team behaviour. The proposed method was tested in a wide range of multi-agent tasks. ","This paper introduces a new exploration method for cooperative multi-agent reinforcement learning (MARL), which utilizes influence-based regularization and curiosity-driven incentives to encourage coordinated and diverse exploration. This paper formulates the dissimilarity between other agents' behaviors and their one-step TD targets as the influence metric and extends the random network distillation (RND) to the multi-agent setting for crafting a ""novelty"" metric. Empirical results show that this method achieves improved performances on a comprehensive set of challenging tasks.",0.2564102564102564,0.20512820512820512,0.1794871794871795,0.1743119266055046,0.1559633027522936,0.20618556701030927,0.1834862385321101,0.16494845360824742,0.1728395061728395,0.1958762886597938,0.20987654320987653,0.24691358024691357,0.21390374331550802,0.18285714285714283,0.1761006289308176,0.18446601941747573,0.1789473684210526,0.2247191011235955
139,SP:57f9812fa5e7d0c66d412beb035301684d760746,"The authors identify a potential failure in KL-regularized reinforcement learning in which small predictive variance of the target policy may lead to exploding gradients, possibly destabilizing gradient-based learning algorithms. The authors discuss an uncertainty collapse of parametric models under Maximum Likelihood estimation in OOD regions of the expert, resulting in the aforementioned pathology. Finally, they suggest to use non-parametric models of the behavior policy (in which uncertainty collapse is not present), showing improved performance on several continuous control benchmarks.",This paper presents a previously unrecognized pathology in KL-regularized RL with expert demonstrations. Specifically the commonly used parametric behavioral policies suffer from collapse of predictive variance at states far from the demonstrations. This collapse of variance hinders the algorithm’s ability to learn effectively. The authors then propose to use non-parametric behavioral policies and demonstrate its effectiveness in several control tasks. ,"The authors identify a problem with KL-based reinforcement learning from expert demonstrations. Poor out-of-sample variance predictions of NN expert policies ends up unreasonably penalizing RL exploration outside the expert data distribution, potentially causing numerical instability. To remedy this they propose to instead use non-parametric expert policies based on a scalable GP approach. ","This paper studies the problem of KL-regularized reinforcement learning for locomotion tasks, where the KL regularization penalizes deviations from expert demonstrations.  This work makes the observation that fitting expert demonstrations to a certain class of conditional neural density models (e.g. neural networks that output the parameters of a Gaussian distribution), results in policies whose variance collapses for states that are different enough from the expert data.   The paper argues that this collapse causes instabilities in learning KL-regularized policies: for Gaussian policies, the KL penalty grows quadratically to infinity as the expert policy variance goes to 0. To address this issue, the paper proposes to instead compute the KL penalty by fitting expert demonstrations to models that do not suffer from this collapse, in particular Gaussian Process regression models. Gaussian Process models result in variance that increases (depending on the choice of kernel) for states that are far enough from the expert data.  The paper provides some analysis on why the variance collapse affects the RL optimization process, experiments that  aim to show an empirical relationship between expert policy variance, KL penalty and magnitude of the policy gradients, and a comparison between various methods that use KL regularization with an expert policy. ",0.21951219512195122,0.2073170731707317,0.2682926829268293,0.23809523809523808,0.3333333333333333,0.375,0.2857142857142857,0.30357142857142855,0.10784313725490197,0.26785714285714285,0.10294117647058823,0.10294117647058823,0.24827586206896554,0.24637681159420288,0.15384615384615385,0.25210084033613445,0.15730337078651682,0.16153846153846155
140,SP:581faa3e1fd39ddefc2740985fa8f94cacdf2b64,"This paper studies edge independent graph models that covers a large family of graph models. The main result is that it shows under certain conditions, the edge independent models are limited to generate graphs with high triangle and other subgraph densities, which are commonly observed in real world networks. Based on this finding, this paper propose a way to generate graph which balances matching the original graph and memorizing the graph statistics. ","The main result is that certain structural properties of an edge-independent graph model, e.g. the expected number of triangles, are upper bounded by how strongly the graphs sampled from the model are expected to be overlapping.   Experiments on two datasets empirically confirm that several edge-independent baseline models require a high amount of overlap in their sampled graphs in order to match the number of triangles in the original graph dataset.",The authors in this work study the limitations of edge independent models which are an important class of graph generative models. Specifically the authors demonstrate that such models are inherently limited in their ability to generate graphs with high triangle and other subgraph properties which are typical in real-world graphs. The authors demonstrate the main results of their work via empirical experiments.,"In this paper, the authors analyze some properties of edge-independent generative graph models: given a fixed matrix of connection probabilities in [0,1], the edges are drawn independently. This is the basis for several methods to generate graphs that match certain statistics of a given input graph (*over the same set of nodes*). It has been observed that such models tend to produce less triangles (or generally k-cycles) than real graphs. In light of this, the authors show that the expected density of k-cycles can be bounded by the model *overlap*, which measures how much the model tends to memorize only one graph, as opposed to generating a large variety of graphs. The proofs are simple and are mostly given in the main paper. They also show that their bounds are tight, up to multiplicative constants, for Erdös-Rényi graphs, which are models with minimal overlap. Numerical experiments are performed to compare several edge-independent models, including a novel one introduced in the paper, and illustrate the (negative) theoretical findings.",0.2361111111111111,0.3888888888888889,0.3472222222222222,0.1917808219178082,0.3561643835616438,0.38095238095238093,0.2328767123287671,0.4444444444444444,0.14285714285714285,0.2222222222222222,0.14857142857142858,0.13714285714285715,0.23448275862068965,0.4148148148148148,0.20242914979757085,0.20588235294117646,0.20967741935483872,0.2016806722689076
141,SP:59ce2e6c3674157d6fa990316812d0823c1ec586,"This paper investigate the gap problem between cross-attention (CA) and dual-encoder (DE) models for document reranking. In general, cross-attention models perform much better than dual-encoder models while consuming more computational cost. The authors first leverage Mercer's theorem to prove that dual-encoders models are sufficiently expressive to model a broad class of scores using countably infinite dimension, which mainly the gap between CA and DE models should not exist from theoretical analysis. They further identified the gap is caused by the generalization ability of the DE models. To reduce the overfitting on the training data, they proposed to use distillation algorithms. Specially, they design the loss function which matches the margins between the teacher (CA) and student (DE) models. They also extend the margin to probability matching by using softmax cross-entropy loss. Through the experiments, they demonstrated that their proposed distillation approaches can further reduce the gap between DE and CA models.  The main contributions of this paper is that they show there should not be a gap between DE and CA models from theoretical analysis. They found that the gap is caused by the generalization ability of CA model and proposed distillation approach to reduce the gap.","This paper aims to narrow the performance gap between cross-attention BERT and dual-encoder BERT for re-ranking task. The authors empirically and theoretically analyze the underlying reasons of the performance gap. The gap could be mitigated by the proposed knowledge distillation method, where the original cross-attention BERT model acts as the “teacher” and the more efficient dual-encoder BERT model is used as student. Comprehensive experiments confirmed the effectiveness of the proposed KD method for various re-ranking tasks.   ","In information retrieval problems, there are two kinds of models: dual-encoder models and cross-attention models. Cross-attention models generally outperform dual encoder models by a large margin. Due to the latency requirement of real search systems, cross-attention models might not meet the requirements. So one ideal solution is to transfer the knowledge from cross-attention models to dual encoder models. This paper follows this line of research ideas by reducing the gap between the two models.   Firstly, the paper theoretically analyzes the effects with sufficiently large embedding dimensions. Secondly, the paper empirically verifies the theoretical results. Thirdly, the paper proposes a new method that further bridges the gap between the two methods. The writing is clear and well-structured. I like the idea and logic of the paper. I have some questions about the experiment part. ","The paper contrasts cross-encoder vs. bi-encoder architectures for re-ranking in information retrieval.  The authors argue that bi-encoders under-perform cross-encoders, not for lack of capacity, but due to poor generalization, and back this claim with some theory and empirical results.  The authors then propose distillation of the cross-encoder model into a bi-encoder, which is shown to improve bi-encoder results.  While this has been proposed before, the distillation loss function the authors propose is slightly different than the standard cross-entropy (which the authors show is closely related).",0.1568627450980392,0.17647058823529413,0.12745098039215685,0.24390243902439024,0.23170731707317074,0.12949640287769784,0.3902439024390244,0.2589928057553957,0.2736842105263158,0.14388489208633093,0.2,0.18947368421052632,0.22377622377622378,0.2099125364431487,0.17391304347826084,0.18099547511312217,0.21468926553672316,0.15384615384615383
142,SP:59e7ff1cdee42c9623615f6105c0e0f44e7b75a5,"This paper investigates the advantages of the Bures-Wasserstein (BW) metric over the widely used Affine Invariant (AI) one, for the manifold of symmetric positive definite (SPD) matrices. Rewriting the two metrics in a compatible manner reveals that for a wide variety of objectives, the BW metric is a linear function of the input matrix whereas the AI has a quadratic dependence. One exception is the log-det function which is included in the analysis. The paper experimentally validates that for ill-conditioned problems BW is a better choice compared to AI.","This paper provides several results about the Bures-Wasserstein  (BW) geometry on the positive definite matrices. A thorough comparison with the Affine Invariant (AI) metric is made. The main focus of the paper is on optimization: is it faster to optimize a function under the AI or under the BW metric? The authors argue that when the solution of the problem $X^*$ is ill-conditioned, then algorithms derived under BW metric have faster convergence theoretical convergence. This is due to the conditioning of the Riemannian Hessian at the optimum, which is better for the BW metric when $X^*$ has poor conditioning. The authors also demonstrate that a variety of functions that are convex for the AI geometry are also convex for the BW geometry. Experiments on an array of synthetic problems validate that both Riemannian gradient descent and Riemannian trust region methods are faster in the BW geometry.","This paper studies the Bures-Wasserstein (BW) geometry of symmetric positive-definite (SPD) matrices and compares it to the commonly-used affine-invariant (AI) geometry in the context of Riemannian optimization. It is shown theoretically that the BW geometry has several advantages. Particularly, it is shown that (i) the BW metric is linearly dependent on the SPD matrix, (ii) the condition number of the Riemannian Hessian of the BW metric is smaller than that of the AI metric, and (iii) the curvature constant of the BW metric is smaller than that of the AI metric. Based on these properties, it is shown that the convergence of Riemannian steepest descent and Riemannian trust-region based on the BW metric is faster than based on the AI metric. The theoretical results are demonstrated on simulations of six problems: weighted least squares, Lyapunov equations, trace regression, metric learning, log-det maximization, and Gaussian mixture model.",The authors consider the Riemannian optimization with Bures-Wasserstein (BW) geometry for symmetric positive definite (SPD) matrix manifold. The authors compare the proposed approach with its counterpart with the popular Affine-Invariant (AI) geometry.  The authors illustrate that the proposed approach (with BW geometry) is more suitable than its counterpart with AI geometry when SPD matrices are ill-conditioned. ,0.33695652173913043,0.33695652173913043,0.20652173913043478,0.3310810810810811,0.14189189189189189,0.1513157894736842,0.20945945945945946,0.20394736842105263,0.3220338983050847,0.3223684210526316,0.3559322033898305,0.3898305084745763,0.25833333333333336,0.2540983606557377,0.25165562913907286,0.32666666666666666,0.20289855072463767,0.21800947867298576
143,SP:5a10c13eb78d26a25dac74601419deb68c53cb75,"The paper describes a technique for evaluating GRU networks based on the multigrid reduction in time (MGRIT) technique. These techniques are not new, in general or to neural network training, but the contribution here is their application to GRU layers. After presenting some of the theory behind ordinary differential equation (ODE) representations of the GRU and laying out the mathematical framework for the MGRIT method, the method is evaluated on two datasets. The results show impressive scalability up to 32 processes (CPU-only) at the cost of a slight loss of accuracy compared to the traditional, sequential GRU","This paper aims to address the limitations of existing approaches for training Gated Recurrent Unit (GRU) given long sequence in terms of both training time and model accuracy. To tackle this challenge, author propose a novel parallel training scheme (called parallel-in-time) for GRU based on a multigrid reduction in time (MGRIT) solver.  Specifically, the key to achieving speedup is a hierarchical correction of the hidden state to accelerate end-to-end communication in both the forward and backward propagation. Authors gives experimental results on two public datasets to demonstrate the performance improvement in the long sequence scenario. ","The paper proposed to parallelize the inference and training of GRU networks (a type of recurrent neural networks) at the `time` dimension.  The main contribution is the application of multigrid reduction in time (MGRIT) solver, and a new GRU architecture (Implicit GRU) that handles the stiffness in the architecture. As a result, the evaluation shows 6.5 times faster training time.",The main focus/goal of the submitted paper is the parallelization of the Gated Recurrent Unit (GRU) (the authors focus on classification problems). The authors describe the incorporation of a multigrid reduction in time (MGRIT) solver to speed-up and better parallelize the  application of forward and back propagation of information. The proposed technique seems to provide a speedup of about an order of  magnitude (at most) when implemented on distributed/shared memory hybrid computing environments.,0.23469387755102042,0.17346938775510204,0.19387755102040816,0.18181818181818182,0.21212121212121213,0.3114754098360656,0.23232323232323232,0.2786885245901639,0.25,0.29508196721311475,0.27631578947368424,0.25,0.233502538071066,0.2138364779874214,0.21839080459770113,0.225,0.24000000000000002,0.2773722627737226
144,SP:5a6099feb5da2c35f99d4d76c7e0ff3cd3e9c196,"This paper proposes a heuristic graph based machine learning model, which can be used to solve the class of order fulfillment problems. The original mixed integer programming problem is known to be NP-hard, and thus computationally intractable. The authors treat each instance of the order fulfillment problems as a tripartite graph, which serves as the input of the proposed model.   The proposed model consists of three parts: a graph attention network (GAT) to embed the features, a feed forward (FF) layer, and an assignment layer generating the output. ","This paper presents a graph based deep learning model for solving the order fulfillment problem of assigning retail (sub-)orders to different warehouses for fulfilment. It formulates the problem as a Mixed Integer Problem (MIP, although I think it is fully integer) which can be solved using a (open-source) MIP solver such as SCIP. Following up on related work, the authors train a graph neural network, specifically designed to incorporate edge features and different types of nodes, to predict the optimal solution, using supervised learning on a dataset of 100.000 generated instances solved with SCIP. Empirically, on three different test problem sizes, the trained model provides solutions within 2-3% to the results found by SCIP, while being a 2-4 orders of magnitude faster. ","A Graph Neural Network (GNN) model is designed to enable the supervised learning of optimal solutions for an order fulfillment problem in supply chain management. This is a problem that must be solved in real-time, making a GNN whose forward computations are quick an attractive option.  The proposed GNN model has three types of nodes corresponding to orders, items, and warehouses. Edges between the nodes of different types represent relationships between them (e.g., an item is in an order). Nodes and edges may have feature vectors representing additional information, e.g., the identity of a warehouse or certain costs on the edges. The GNN is designed to take these features into account.  The model is trained using optimal solutions as labels in a binary classification formulation. Predictions are post-processed to guarantee a feasible solution in the constraints of the order fulfillment problem.  Experimental results on randomly generated instances show that a GNN trained on small instances of this problem performs well even on larger instances, yielding near-optimal solutions in a short amount of time compared to simple GNN solutions, a non-ML heuristic, and an exact solver.","Authors propose to solve an order fulfillment problem with imitation learning backed by GNNs. The specific version of the fulfillment problem is nontrivial: discussed has a hierarchy in order (an order can be decomposed into sub-orders), and items become forbidden over time. SCIP package is used as the expert policy, and authors employ behavior cloning. The specific model they use is a Graph Attention Transformer on the tripartite graph of orders, items, and warehouses. Compared to a reasonable heuristic, Pointer Network, and GAT which ignores edge features, the propose method yields much lower inference latency. ",0.30337078651685395,0.29213483146067415,0.23595505617977527,0.23622047244094488,0.14173228346456693,0.13089005235602094,0.2125984251968504,0.13612565445026178,0.21875,0.15706806282722513,0.1875,0.2604166666666667,0.25,0.18571428571428572,0.227027027027027,0.18867924528301888,0.16143497757847536,0.17421602787456444
145,SP:5c0114535065d5125349f00bafdbccc911461ede,"This paper proposes a new loss terms to encourage Transformer-based models perform ""explicit reasoning"", which is beneficial for transferring knowledge from oracle-trained VQA models to deployable ones. Specifically, the novel loss is similar with the one commonly used in modular network (e.g. converting questions to executable program/functions), but instead of relying on the predicted programs to assemble executable programs during inference, in this paper this loss is only used during training as an auxiliary regularization on top of regular Transformer-based VQA models. The authors provide theoretical proof about the benefits of using the additional loss, and empirically evaluated the advantage on GQA dataset (including out-of-distribution evaluation on GQA-OOD). The proposed system demonstrates not the best but still competitive performance especially considering the model size and training efficiency (without using extra data).  ========================================================================================  Thanks for the response from the authors. I've updated my rating after reading the rebuttal as well as the comments from other reviewers.","The paper studies the problem of training VQA models on noisy/imperfect visual input (e.g., representations from existing object detectors). It proposes using program supervision as an additional task to assist in transferring reasoning patterns learned through clean/oracle visual inputs to the imperfect inputs. The experiments are performed on the GQA/GQA-OOD benchmarks, and the empirical results show gains on both frequent (head) and infrequent (tail) classes, showing improvement in reasoning skills. Furthermore, the work theoretically shows that predicting programs can reduce sample complexity.","This paper describes a novel approach for doing more robust VQA that doesn’t just learn to exploit the biases in the data. They build on previous work which demonstrated that training a model on oracle visual data makes it less susceptible to learning biases in the data. While the previous work showed that the better reasoning skills learnt by the models trained on oracle visuals decline when transferred to the noisier visuals extracted by pre-trained vision models, here the authors demonstrate that adding the additional supervision step of making the model predict the reasoning program alongside the VQA answer makes it more robust to such distributional shifts. ","This paper targets on the application problem of visual question answering, whose recent state of the art was set by attention (typically transformer) - based deep neural networks. This work propose to use program supervision to introduce extra loss terms in the training of these transformer based models, arguing that the guidance from these signals improve the model's robustness against noise and fluctuations in the visual input during transfer learning. Theoretical analysis and experimental evaluations are conducted on program supervision, leading to plausible findings and results.",0.10429447852760736,0.13496932515337423,0.1165644171779141,0.19540229885057472,0.1839080459770115,0.14678899082568808,0.19540229885057472,0.2018348623853211,0.22093023255813954,0.1559633027522936,0.18604651162790697,0.18604651162790697,0.136,0.16176470588235295,0.15261044176706828,0.17346938775510204,0.18497109826589594,0.1641025641025641
146,SP:5d758b9125e716c92dde5cfcc8aad67adbd30ba0,"This paper proposes a novel 3D point cloud representation learning framework. At the core of this method, is a lifting scheme inspired by wavelet decomposition. The proposed method roughly splits the input data in half at each stage, producing a down-sampled approximation C and detail d. Then C is further processed by the next layer, forming a multiscale pyramid. In summary, the contribution of this paper is:  1. Proposed to use the lifting scheme in point cloud processing, using graph convolution networks and transformers as backbone.  2. Evaluated the method against state-of-the-art baselines and showed that the proposed scheme performs well. ","The paper presents a new deep neural network architecture for 3D point cloud representation learning, based on wavelet decomposition. In particular, the authors propose a data-driven adaptive lifting scheme that introduces non-linearity into wavelet. The original linear operators update(U) and predict(P) in wavelet decomposition are replaced by non-linear graph convolutional networks (GCN). Equipped with wavelet transform and Transformers, the proposed network aims to captures and refines the holistic and complementary geometry of 3D shapes to supplement neighboring local information. Experimental results on standard benchmarks (i.e., shape classification and part segmentation) show that it achieves state-of- the-arts or competitive performance.","This paper presents a novel framework for 3D shape representation learning, which is based on multi-scale wavelet decomposition. This is very different from existing works. A novel transformer-based neural network, AWT-Net, is also proposed.","This paper proposes a new 3D shape representation learning method using multi-scale wavelet decomposition. In particular, the authors introduce a neural network architecture that decomposed 3D shapes into sub-bands components at multiple scales. In particular, starting from a pointcloud the proposed model learns to decompose it into coarse (high frequency) and detail (low frequency) components using an adaptive lifting scheme, similar to the original lifting scheme introduced for defining second-generation wavelets. Subsequently, two transformer models are used to refine the coarse and approximate geometry of the 3D shape. The proposed model achieves state-of-the-art results on the shape classification task on the ModelNet40 and the ScanObjectNN dataset and on the part segmentation task on the ShapeNet Part dataset. The concept of using such an adaptive lifting scheme seems to facilitate learning and to the best of my knowledge is novel for the case of shape representation learning.",0.22857142857142856,0.13333333333333333,0.2761904761904762,0.12149532710280374,0.3177570093457944,0.40540540540540543,0.22429906542056074,0.3783783783783784,0.19078947368421054,0.35135135135135137,0.2236842105263158,0.09868421052631579,0.22641509433962265,0.1971830985915493,0.2256809338521401,0.18055555555555558,0.2625482625482625,0.15873015873015872
147,SP:5d94dbfd10dc2ef86415853cc41f414a24962d4f,"The paper shows that not all cases of heterophily are harmful for GNNs with aggregation operations. Based on a backpropagation analysis on an SGC-style GNN, it provides a new metric based on similarity matrix which considers the influence of both graph structure and input features. Observing that the diversification operation is able to address some harmful heterophily cases, it proposes the Adaptive Channel Mixing GNN framework which combines a high-pass filter, a low-pass filter and an identity channel.","In this paper, the authors first analyze the potential drawbacks of existing heterophily metrics, then propose an aggregated heterophily metric aiming to better estimate the ""harmful"" heterophily and utilize diversification operation to address certain harmful heterophily cases. Based on the analysis, the authors propose an adaptive channel mixing (ACM) framework to improve GNN model performance. Several experiments have also been conducted to evaluate the proposed model.","The paper starts from studying the question of how heterophily affects the learning effectiveness of Graph Neural Networks on node classification tasks and posits that heterophily may not be always detrimental to the task. Following this observation, it proposes a new architecture, called Adaptive Channel Mixing, which appropriately applies aggregation, diversification and identity channels in each GNN layer in order to tackle harmful heterophily. According to the experimental results, the proposed architecture is successful.",This paper proposes a new aggregation-based homophily metric for assessing the homophily of a graph. This metric complements the current metrics by considering unharmful heterophily cases. The second contribution is a new filterbank framework which uses the diversification operation to fight harmful heterophily information.,0.2345679012345679,0.2222222222222222,0.16049382716049382,0.22727272727272727,0.18181818181818182,0.17567567567567569,0.2878787878787879,0.24324324324324326,0.28888888888888886,0.20270270270270271,0.26666666666666666,0.28888888888888886,0.2585034013605442,0.23225806451612904,0.20634920634920634,0.21428571428571427,0.21621621621621623,0.21848739495798322
148,SP:5db39fbba518e24a22b99c8256491295048ec417,"This paper discusses how message passing and feature aggregation are affected by abnormal node features, and designs a message passing module called Adaptive Message Passing (AMP) with adaptive residual connection and feature aggregation. In this way, the module can adaptively deal with the node that is inconsistent with the local feature during the message passing. The paper proposes the AirGNN that is built up with AMP as the message-passing layer in GNN, the experiment result shows that the AirGNN gets a more robust result on both the abnormal node and normal node classification comparing with the baselines.",This paper examines the behavior of GNNs when node features are perturbed during test time.  AirGNN is proposed based on the observations from the standard GNNs. AirGNN adaptively computes the weight of the initial embedding and aggregated embeddings and is robust to random/adversarial noise keeping the standard classification performance.," This paper observed interesting phenomenons: feature aggregation helps smooth out abnormal features but would lead to over-smoothing for normal features, residual link helps adjust feature smoothness for normal features but may hurt the performance when we have abnormal features. Therefore, it points out that we need to tradeoff between normal and abnormal features while designing GNNs. Then, based on the observation, this work proposed the AMP method and the AirGNN model, which are shown by experiments to be effective under various abnormal feature scenarios.  ",This paper targets to design GNNs with stronger resilience to abnormal node features. Empirical examination is firstly conducted by replacing the features of randomly selected nodes with random Gaussian noise. The comparison between models with and without the residual connections show the helpfulness of residual connections on learning with abnormal features. ,0.1326530612244898,0.16326530612244897,0.1326530612244898,0.22,0.22,0.12941176470588237,0.26,0.18823529411764706,0.2549019607843137,0.12941176470588237,0.21568627450980393,0.21568627450980393,0.17567567567567569,0.17486338797814208,0.174496644295302,0.16296296296296298,0.21782178217821785,0.16176470588235298
149,SP:5eef907024017849303477eed92f317438c87a69,"This paper proposes an energy-based perspective on cooperative games that permits a gradient-based calculation of Shapley/Banzhaf values, as well as the definition of a new alternative value - the variational index. A quick summary of the paper's key ideas is:  - For a given cooperative game $F$, we can seek an entropy maximizing distribution over coalitions $p(S)$ that satisfies a constraint on the mean coalition value $\mu$ - Solving the entropy maximization problem via its Lagrangian yields the Boltzmann distribution $p(S) \propto \exp(F(S)/T)$, where the temperature $T$ has a one-to-one correspondence with the mean coalition value $\mu$ (this result is in the appendix). This distribution gives more probability mass to coalitions that achieve higher values - We can seek a simpler alternative to $p(S)$ by doing mean-field variational inference, i.e., finding a factorized surrogate $q(S)$ where each player's participation is determined by independent Bernoulli RVs. The result will intuitively assign higher probabilities to players that belong to high-value coalitions, so these probabilities can serve a function similar to Shapley/Banzhaf values - The VI approach suggests a KL divergence minimization (or ELBO maximization) objective for learning $q(S)$, which is parameterized by $x \in [0, 1]^n$. Doing gradient descent on this objective yields a relatively simple update rule, where we repeatedly set $x_i^+ = \sigma(\nabla_i f_{mt}(x) / T)$ for $i = 1, \ldots, n$ - The authors define the ""variational index"" as a function of the solution to the KL divergence minimization problem: $s^* = T\sigma^{-1}(x^*)$ - The authors find that the Banzhaf value can be found using a single-step update to a particular initialization of the KL divergence minimization problem (luckily the temperature $T$ is not important for single-step updates). Similarly, they find that the Shapley value is the average of the single-step update applied to different initializations (again, the temperature doesn't matter). Finally, the authors point out that any single-step update applied to a symmetric initialization will be a probabilistic value (a class of solution concepts in cooperative game theory, of which Shapley/Banzhaf values are special cases) - Lastly, the authors suggest a practical sampling-based approach to calculating the necessary gradients, which are just as difficult to calculate as the Shapley/Banzhaf values because they require calculating the value for every coalition $S \subseteq N$  The experiments compare the variational index to Shapley and Banzhaf values in data and feature removal tasks, finding that it performs quite favorably in the settings examined.",The paper studies valuation problems for cooperative games. It proposes a new valuation measure called Variational Index. The idea is to create a coalition probability distribution based on a maximum entropy criterion. Player valuations are then derived by creating decoupled surrogates of this distribution. The authors then present a gradient ascent algorithm to compute this decoupling. Classical valuation criteria like the Shapley value and the Banzhaf index can be recovered as special cases or modifications of the algorithms iterates.,"This paper studies valuation problems from cooperative game theory. There are $n$ agents and a valuation function $F: [n] \to R$ where $F(S)$ is the collective payoff of the coalition $S \subseteq [n]$. The goal is to use this function $F$ to define an importance vector $\phi(F) \in R^n$. Examples include the Shapley value and Banzhaf index.  The authors introduce a probabilistic treatment of this problem, where they use $F$ to define a probability distribution $p$ where $p(S)$ is the probability that coalition $S$ forms. They then phrase the problem of defining an importance vector $\phi(F)$ as a decoupling problem. Under $p$, the $n$ agents may be correlated in a complicated way, but to assign each of them an individual importance value, one must decouple their interactions, or simplify their correlations. The goal is then to find a product distribution $q$ that is as close to $p$ as possible under the KL divergence. Specifically, the authors define $q$ to be an $n$ independent Bernoulli distribution, where the probability that agent $i$ participates in the coalition is denoted $x_i$. The authors show how to optimize the probabilities $x_1, \dots, x_n$ using coordinate ascent. Finally, they define the importance score of player $i$ as $\log(x_i/(1-x_i))$ (ignoring a temperature $T$ term for simplicity). The authors show that the resulting importance vector satisfies many of the game-theoretic axioms that the Shapley value and Banzhaf index satisfy, like the null player, marginalism, and symmetric axioms.  In the experiments, the authors look at small instances with $n = 25$ where it is actually possible to compute the gradients exactly (as opposed to an approximate sampling method). The applications they look at are for data valuation and feature attribution in the context of machine learning. For these tasks, they show that their proposed approach performs about the same as the Shapley value and Banzhaf index, and sometimes a bit better.","Valuation criteria based on game-theory (e.g. Shapely value) have been used in the ML literature for analyzing feature importance and for data subset selection. These criteria serve as solution concepts for cooperative games and have been adapted by some works in ML for subset valuation problems.    The present paper presents a probabilistic treatment of cooperative games, and shows that two classical valuation criteria can be seen as a one-step factored approximation to maximum entropy solution to the game. They then propose a new valuation criterion ""Variational Index"" that uses a multi-step factored approximation and show it satisfies some common axioms for cooperative games. The paper also has experimental results on the proposed criterion.  ",0.06588235294117648,0.17882352941176471,0.07058823529411765,0.35443037974683544,0.21518987341772153,0.07975460122699386,0.35443037974683544,0.2331288343558282,0.2564102564102564,0.08588957055214724,0.1452991452991453,0.2222222222222222,0.1111111111111111,0.20239680426098536,0.11070110701107011,0.1382716049382716,0.17346938775510207,0.11738148984198646
150,SP:60ba9cb4c42cecde6379ec0279434dece822a2b1,"In this paper, the problem of forward knowledge transfer in continual learning settings is explored. The idea is to measure correlations between the learned tasks based on the notion of ""trust region"" which helps to identify the most similar learned tasks to the current task. The core idea is that frozen weights for similar past tasks can be relaxed to reuse them to learn the current task. Since task similarities are used for this purpose, this will not lead to catastrophic forgetting and at the same time helps to transfer knowledge. Experiments on four benchmarks are provided to demonstrate that the method is effective.","**Summary:** The paper focuses on gradient projection (GP) for incremental learning. The authors motivate their work by stating that while approaches based on GP lead to superior performance in overcoming catastrophic forgetting, they suffer from a significant drawback. In GP, once one calculates the subspaces spanned by layerwise inputs for a task, say Task A, the network is then forced to only update the weights in an orthogonal direction to these subspaces for learning a subsequent task, say Task B (hence keeping important parameters for Task A intact and overcome catastrophic forgetting). However, suppose Task B and Task A are similar (an extreme case is when Task B is the continuation of Task A!). In that case, we know that the essential parameters for Task A are likely to be important for Task B and that the network could benefit by continuing to update the important parameters for Task A, which is not allowed in GP algorithms. This behavior has two consequences: 1) intransigence, i.e., the network won't be able to learn Task B as effectively as possible, and 2) the network will have reduced backward transfer. The backward transfer issue is apparent in the extreme case when Task B is a continuation of Task A, and the network's performance on Task A would have improved if it was able to learn Task B using the important weights for Task A!  The paper addresses this issue, with a simple, yet practical, solution. For a new task, the authors calculate the correlation between the subspaces calculated for old tasks and the new task (layerwise), keep track of the most correlated previous tasks for each layer, and denote them as (layerwise) Trust Regions. Next, the authors propose a scaled weight projection that allows for unfreezing the important parameters in the Trust Region of the new task, while learning this task, and learn the corresponding weights as part of the optimization process. Finally, the authors report results on Permuted MNIST (PMNIST), CIFAR100 Split, CIFAR100 Sup, and a sequence of 5-Datasets with 10-class classification which includes CIFAR-10, MNIST, SVHN, not-MNIST, and Fashion MNIST, in comparison with GP methods, regularization-based methods, and memory replay method. They show consistent improvement in accuracy, and more interestingly in backward transfer.  ","Some existing methods put restrictive constrains on the optimization space of the new task to prevent catastrophic forgetting, which may lead to unsatisfactory performance for the new tasks. This paper aims to facilitate the forward knowledge transfer based on an efficient characterization of task correlation. Main contributions can be summarized as follows: 1.Introduce a novel notion of ‘trust region’ based on the norm of gradient projection onto the subspace spanned by task inputs to measure task correlation. 2.Proposed a novel approach for the new task to leverage the knowledge of the strongly correlated old tasks in the Trust Region through a scaled weight projection. 3.Developed a continual learning approach, trust region gradient projection(TRGP) based on the introduced Trust Region, scaled weight projection and a module to construct task input subspace. 4.Compared to related state-of-the-art approaches, TRGP achieves substantial performance improvement on all Benchmarks.","This paper proposes a continual learning method based on gradient projection memory (GPM) of Saha et al., which projects the gradient of each layer to be orthogonal to the input subspace of previous tasks. Motivated by the fact that the orthogonal projection can harm the performance by being too restrictive, the authors propose a heuristic algorithm to reduce the restriction. Specifically, the authors choose a subset of most ""correlated"" tasks and let the model change along the subspace of the correlated tasks.",0.2692307692307692,0.23076923076923078,0.18269230769230768,0.11052631578947368,0.07631578947368421,0.152317880794702,0.07368421052631578,0.15894039735099338,0.23170731707317074,0.2781456953642384,0.35365853658536583,0.2804878048780488,0.11570247933884296,0.1882352941176471,0.20430107526881722,0.1581920903954802,0.12554112554112554,0.19742489270386268
151,SP:60ce257ca7c1dbbc88e4f36bad40f7eeb133368a,"The paper proposes GraphFormer, an architecture built on top of GAT and Transformers for textual graphs. The motivation behind the architecture is that the existing techniques on textual graphs involve a cascade process where the nodes are first encoded using a language model after which a GNN is applied. With GraphFormers, the graph aggregation and encoding can happen iteratively, allowing for better contextualization of nodes with text like “notes on transformers” since transformers can be interpreted as ML models or electrical devices depending on the surrounding context.  The authors also lay out a 2-step training procedure whereby they first train on a version of the graph with injected noise via link prediction before training on the actual graph. The motivation behind this is given as the fact that training on the graph with injected noise makes the GNN component of the GraphFormer model more robust without which the transformer part will dominate.","This paper proposes GraphFormers for text graph tasks where GNNs are nested in each layer of the transformer-based pretrained language model. In this way, each node can appropriate graph structure to build semantic representations more effectively. Two-stage progressive training and unidirectional graph aggregation are further proposed to improve representation quality and avoid unnecessary computation. They use three real-world million-scale textual graph datasets (DBMLP, Wikidata, Product Graph) and demonstrate consistent improvements compared with traditional cascaded transformers-GNN models.","This paper proposes a means to encode text feature for nodes in textual graphs, where a layerwise aggregation module (implemented as a multi-head self-attention) is appended after each Transformer encoder block that aggregates the hidden representations of (sampled) neighbor nodes. The produced contextual representations are dispatched to each node for the computation of next Transformer encoder block. To mitigate the possible model degradation issue that the aggregation module might be insufficiently trained, the authors also propose a two-stage training schedule where the model is first trained on nodes with corrupted inputs until convergence and then trained on normal data.","For representation learning on textual graphs, Cascaded Transformers-GNN architectures have been used, as language models and GNN are independently and consecutively deployed. The authors' claims, however, this approach has a limitation of information fusion of text encoding and graph structures. The authors proposed GNN-nested Transformers named as GraphFormers, where GNN components are nested alongside the transformer layers of language models. Also, the author proposed a progressive learning strategy and unidirectional graph attention to effectively train the graph network modules. The efficacy of the GraphFormers is demonstrated on 3 textual graph datasets (DBLP, Wiki, and Product Ads datasets), achieving SOTA performance.",0.09803921568627451,0.1503267973856209,0.1437908496732026,0.18518518518518517,0.2222222222222222,0.16666666666666666,0.18518518518518517,0.22549019607843138,0.21568627450980393,0.14705882352941177,0.17647058823529413,0.16666666666666666,0.1282051282051282,0.1803921568627451,0.17254901960784313,0.1639344262295082,0.19672131147540983,0.16666666666666666
152,SP:6232d8738592c9728feddec4462e61903a17d131,This paper presents a method to detect adversarial examples by disentangling the representation. This is done by training autoencoders over both correct and incorrect class/semantic features. The authors claim that the proposed method outperformed other SOTA methods for adversarial detection and even works against adaptive adversarial attacks. ,"The paper propose a new defense mechanism against adversarial attacks based on an autoencoder architecture. The proposed architecture uses self-supervised learning to disentangle the semantic and class features, which enables the autoencoder to discriminate the clean and adversarial examples at test time. The approach was shown to outperforms other autoencoder approach in the experiments, when the attacker as access to only the victim model or both the victim model and autoencoder.",The paper proposes a novel adversarial example detection strategy based on disentangled latent representations. The main idea is that latents are separated into class features and semantic features. Detection then uses the fact that adversarial samples preserve semantics while at the same time changing class according to some victim model. Training is being performed through a sophisticated combination of losses accounting for begin examples and counterexamples. Experimental results on detecting various kinds of attacks on popular datasets show clear improvements over previous methods. The paper also presents an evaluation of an adaptive attack.,"This paper proposes to detect adversary using learned disentangled auto-encoder. The model can disentangle the class feature and the semantic feature. Class features are easily changed by adversary, while the semantic features cannot. Existing Auto-Encoder cannot disentangle those two types of features, and also has strong generalization ability---high performance on both the adversary and the benign examples, making it hard to detect. Using the proposed disentangled AE, benign example can be reconstructed faithfully while adversary examples cannot. Thanks to the tailored AE, the paper state-of-the-art adversarial detection performance on MNIST, Fashion-MNIST, and CIFAR-10 in most cases.",0.22916666666666666,0.2916666666666667,0.3125,0.2361111111111111,0.2361111111111111,0.17204301075268819,0.1527777777777778,0.15053763440860216,0.14423076923076922,0.1827956989247312,0.16346153846153846,0.15384615384615385,0.18333333333333335,0.19858156028368795,0.19736842105263155,0.20606060606060608,0.19318181818181818,0.16243654822335027
153,SP:625e3908502fd5be949bb915116ed7569ba84298,"This work proposed a neural reparametrization scheme to accelerate a large class of nonconvex nonlinear optimization problems. The proposed method is grounded on analysis that the dynamics of gradient ﬂow are related to the condition number of the system. More specifically, by reparametrizing the optimization problem with a graph convolutional network (GNN), the proposed method can modify the condition number and obtain convergence speed up, the acceleration is demonstrated on optimizing synchronization problems and persistent homology of point-clouds.","The authors derive a neural reparameterization of non-convex optimization problems in order to accelerate their convergence. They do this by deriving how the slowest components of the optimization variables can have their convergence rate improved by preconditioning with a NTK-based matrix. They make connections between this approach and Group Convolutional Networks. Experimentally, they show this approach improves upon baseline gradient-based optimization on a two datasets. ","This work studies the question to what extend a reparametrization of an optimization problem, i.e. representing the original parameters w to optimize for as a function of some other parameters theta, can accelerate the convergences of the gradient flow / gradient descent for nonconvex optimization problems. It studies the dynamics of the flow via eigenvectors of a matrix M formed as the expectation over the outer product of the gradient of the loss with itself to reveal 'slow' and 'fast' modes of the evolution. It subsequently derives sufficient conditions for the reparametrization (which is chosen to be linear but time varying) to balance the decay on all modes. After discussing an efficient approximation of the theoretically derived scheme, numerical results demonstrate the effectiveness of the proposed reparametrization in two exemplary applications.  ","This paper proposes a reparameterization of non-linear non-convex optimization problems. This reparameterization amounts to a linear map (i.e., ""optimization params = linear operation of a different set of parameters). These linear maps are interpreted as a graph convolution network. The experimental results are validated on ""Kuramoto models"" and ""persistent homology models"".",0.17721518987341772,0.3037974683544304,0.189873417721519,0.22058823529411764,0.16176470588235295,0.09923664122137404,0.20588235294117646,0.183206106870229,0.2830188679245283,0.11450381679389313,0.20754716981132076,0.24528301886792453,0.1904761904761905,0.22857142857142854,0.2272727272727273,0.15075376884422112,0.18181818181818182,0.14130434782608695
154,SP:639fd88482330389019fb5be7446a909b99a8609,"This paper focuses on improving the computational efficiency of the construction of a decision tree. The paper proposes a scheme that combines row sub-sampling and an adaptive column subsampling to reduce the computational cost of finding the best split at each internal node during the tree construction. The authors present some theoretical guarantees for the computational gain and the reduction in the loss function. The empirical evaluation compares the train/test error vs computational complexity tradeoffs of the proposed scheme against various baselines and demonstrate significantly better tradeoffs, at times demonstrating over 2-3 orders of magnitude speedups. The empirical evaluation also highlights an application where the number of columns are very high -- a decision tree for Haar features of images, again showing significantly better tradeoffs than considered baselines. ",The paper proposes a stochastic (i.e. approximate) algorithm to fasten node splitting during tree induction. The idea of the method is to filter out the most irrelevant features using a (growing) subsample of training examples and then to optimize the split using all samples only for the remaining features. The approach is shown to improve computing times for tree construction by several orders of magnitude without impacting accuracy. ,"The paper presents a simple iterative sub-sampling procedure that can substantially reduce the runtime required for finding a split on a numeric feature in axis-parallel decision trees. In each iteration of the algorithm, the lowest-ranked half of features is discarded from further consideration, and the amount of data used for computing the value of a feature is increased for the next iteration, starting with a very small initial set of data. Once a user-specified maximum number of iterations has been reached, standard split selection is performed based on the remaining features and the full dataset available at the node. The paper derives a bound on the runtime, proves that the algorithm will monotonically decrease an upper bound on the split metric, and shows that the probability of discarding a feature depends on the overlap of classes wrt the feature (in a binary classification scenario). It also shows that the algorithm produces a consistent estimator and that the objective function decreases monotonically as new nodes are added. Experimental results on six high-dimensional datasets show that the proposed method produces decision trees that are competitive with standard trees that are obtained after pre-filtering features using chi-square or mRmR, but at a substantially reduced training time. The paper also has an additional experiment on MNIST and F-MNIST looking at trees generated using Haar features.","The paper introduces an approach to decision tree induction based on sub-sampling and pruning heuristics.  Typically, inducing a tree involves O(D*N_j*log(N_j)) work at each node j, where D is the number of features and N_j is the number of observations at node j.  This is because we can find the optimal split at node j for each feature in O(N_j) time after sorting each of the N_j observations. The basic idea introduced in this paper is to reduce the computational complexity incurred at each node by starting with a subsample of N_j, pruning features with sub-par performance, increasing the subsample size, pruning sub-par performing features, .... until the best feature has been chosen on the full sample of N_j observations.  The idea is very simple and the authors attempt to bolster this simple idea with some theoretical results and a wide array of experiments. The method does appear to be effective for datasets with very large D, which for example occurs when constructing Haar features on image data.  ",0.13846153846153847,0.23076923076923078,0.2230769230769231,0.2463768115942029,0.2898550724637681,0.1572052401746725,0.2608695652173913,0.13100436681222707,0.15934065934065933,0.07423580786026202,0.10989010989010989,0.1978021978021978,0.18090452261306533,0.1671309192200557,0.1858974358974359,0.11409395973154361,0.15936254980079684,0.1751824817518248
155,SP:63bcbaf0c5644aaba863cf60fa10db763f382ee8,"The authors propose an objective for learning a diverse set of options in which the goal is to maximize the entropy of terminating states from any initial state while at the same time minimize the entropy of terminating states given a specific option.  Intuitively, this means learning options that tend to be deterministic, while at the same time are able to reach diverse terminating states. The authors then evaluate against other option learning frameworks in a series of diverse environments.","The authors propose an HRL algorithm that uses the VIC objective to discover the options, i.e., the termination condition of the options is trained to maximize the mutual information between the set of options and their terminating states. These options are first trained without access to a reward function in an unsupervised manner. Later on there are experiments that show how an RL agent can re-use these options in downstream tasks. ","This paper proposed an algorithm, termed infomax Termination Critic (IMTC) to learn diversified options in RL. This algorithm learns termination conditions of options by maximizing mutual information between options and corresponding state transitions. The experiments demonstrate the IMTC algorithm learns diversified options and can be reused in various tasks.","The paper presents an approach for learning diverse temporally extended and reusable options. It is based on the assumption that learning diverse options are generally useful for downstream tasks. Thus, the approach aims to discover options without making any assumptions about the downstream tasks. The main idea behind the approach is to discover options by maximizing the mutual information between the options and the corresponding state transitions and demonstrates the options discovered by this approach. Also, the paper demonstrates that these options are useful for faster learning in a downstream task.  ",0.2375,0.125,0.225,0.2191780821917808,0.3287671232876712,0.3877551020408163,0.2602739726027397,0.20408163265306123,0.1978021978021978,0.32653061224489793,0.26373626373626374,0.2087912087912088,0.24836601307189543,0.15503875968992248,0.2105263157894737,0.26229508196721313,0.29268292682926833,0.2714285714285714
156,SP:6495caf14ebb8b9c3cbf50a5f05ec1eb600864fe,"This paper studies a defense against poisoning attacks using an outlier detection technique they develop using a notion of ""self-expanding"" sets. They assume a number of properties for inliers and outliers and can classify them so long as those properties are satisfied. They run experiments based on an existing backdoor attack and they show that their technique could be successful.    ",This paper tackles backdoor attacks where an adversary performs targeted attack against neural networks by injecting some poisoned data into the training data without sacrificing prediction accuracy on clean data. The defense works by recovering the clean data from poisoned training data. The authors proposed an Inverse Self-Paced Learning (ISPL) algorithm to first find a set of homogeneous sets (pure clear or poisoned data) and use an ensemble of weaker learners to exclude poisoned data from training set. ,"This paper proposes a new defense to backdoor attacks. In practice, it is an iterative training procedure which aims to remove poisoned data from the training set. This happens in two phases: an ensemble of weak learners identifies distinct homogeneous sub-populations in the training se, and a boosting framework aims to exclude poisoned data and recover clean data. They compare their approach with a few other defenses on CIFAR-10 and on dirty-label backdoor attacks.",This paper proposes an iterative data filtering/expanding approach with a set of weak learners for backdoor defense. The core idea is that clean distribution and backdoor distribution is incompatible and thus making them seperable based on the expanding error of the training set with many smaller subsets. The weak learners refer to the snapshots of the classifier trained on different subsets and expanded sets.  It is an iterative approach with multiple (8) rounds of  ISPL (Inverse Self-Paced Learnin) and 24 weak learners (each trained for 40 epochs). The entire process is also repeated for 3 times. Theoretical formulation and jusstificaiton have been given along with some empirical verfifiction.,0.14754098360655737,0.19672131147540983,0.22950819672131148,0.22784810126582278,0.20253164556962025,0.22077922077922077,0.11392405063291139,0.15584415584415584,0.12727272727272726,0.23376623376623376,0.14545454545454545,0.15454545454545454,0.1285714285714286,0.17391304347826086,0.16374269005847955,0.23076923076923075,0.1693121693121693,0.18181818181818182
157,SP:64ce86f8bd8572f699809c808aea8364fbbe4ef3,"The paper considers the learning of fair representations (and classifiers). In particular, the paper considers the (group) sufficiency rule for fairness and optimization path alignment to achieve the fairness requirement. The paper considers a representation fair if the optimization path (gradient steps) of subgroups are equivalent / invariant, with respect to optimal subgroup classifiers. To this end, the paper introduces a bi-level optimization problem for a representation function, which is solved by approximating gradients with the implicit function theorem. The approximation error of the gradients and the convergence of representation function (appendix) are analysed. Their algorithm is then experimentally tested under various datasets and compared to various baseline approaches.","This paper considers the fair representation learning problem. In particular, the fairness notion utilized is (close to) the sufficient rule, and this paper proposes a bi-level implicit path alignment algorithm to (approximately) achieve it.  The contribution of the paper includes (1) an empirical algorithm for DNN to (approximately) achieve the sufficient rule, (2) theoretical analysis w.r.t. effectiveness of the algorithm as well as its application scope (classification and regression).","This paper considers fair representation as a bi-level optimization problem where the data representation is first learned then the fair predictors are optimized based on this fixed representation. A theoretical analysis on the the error gap of the implicit algorithm is conducted, and some positive results have been shown in the paper. The paper presents an interesting and topical problem and is easy to follow.   ","This paper proposes a methodology for fair representation learning. The core novelty of the proposal is the satisfaction of a sufficient rule. Experimental results claim good performance with respect to other approaches in terms of an accuracy-fairness tradeoff, in which a sufficient gap is measured.",0.22018348623853212,0.1834862385321101,0.11009174311926606,0.25,0.19444444444444445,0.19696969696969696,0.3333333333333333,0.30303030303030304,0.2608695652173913,0.2727272727272727,0.30434782608695654,0.2826086956521739,0.2651933701657459,0.2285714285714286,0.15483870967741936,0.2608695652173913,0.23728813559322037,0.23214285714285715
158,SP:66784b2f0f08680057670dfea49a4ae88f7a2b38,"A fast model editing method is proposed in this paper to edit pre-trained models at scale. The core component is a Model Editor Networks with Gradient Decomposition (MEND). The paper claims that the MEND method has reliability, locality, generality. MEND is empirically validated on some curated datasets.","While large pre-trained nlp models have achieved great performance on a variety of downstream tasks, the largest of these models still make errors. This paper deals with the problem of enabling both developers and end users of such models to correct inaccurate outputs while leaving the model otherwise intact. If presented with only a single problematic input and new desired output, fine-tuning approaches tend to overfit. To enable easy post-hoc editing at scale, this paper proposes Model Editor Networks with Gradient Decomposition (MEND), a collection of small auxiliary editing networks that use a single desired input-output pair to make fast, local edits to a pre-trained model.   The approach trains lightweight model editor networks to produce edits to a pre-trained model’s weights when provided with the standard fine-tuning gradient of a given correction as input, leveraging the gradient as an information-rich starting point for editing.   MEND leverages the fact that gradients with respect to the fully-connected layers in neural networks are rank-1, enabling a parameter-efficient architecture that represents this gradient transform.  It provides experiments with T5, GPT, BERT, and BART models showing that MEND is the only approach to model editing that produces effective edits for models with tens of millions to over 10 billion parameters.","This paper tackles the challenging model of editing a potentially very large pre-trained model in a way that is specific (local) yet comprehensive (general). Their method, called MEND, take as input the decomposed, information-rich gradient from fine-tuning to learn the parameters for their meta-networks that subsequently perform the updates. The authors show that it works better than existing model editing techniques on large models like GPT-Neo, GPT-J, T5-XL, T5-XXL, and some smaller models like BERT-base, and distilGPT-2.","This paper studies the problem of ""model editing"" - altering the model predictions on local examples without affecting the global behavior.  Authors tackle the challenge of editing very large models with billions of parameters, such as T5 or GPT-J, where editing is particularly important, as it is impractical to re-train such models from scratch to correct every mistake.  To address this challenge, authors propose MEND - a novel model editing method that differs from prior art in three ways: - it does not require computing higher-order gradients (and hence, requires less compute/memory) - it can be trained for a given model without the need to change the model parameters (and hence, can be applied to pre-existing models) - it has better asymptotic time & memory complexity in terms of model size (and hence can be used for very large models) ",0.4166666666666667,0.22916666666666666,0.22916666666666666,0.11059907834101383,0.12442396313364056,0.22988505747126436,0.09216589861751152,0.12643678160919541,0.07913669064748201,0.27586206896551724,0.19424460431654678,0.14388489208633093,0.1509433962264151,0.16296296296296295,0.11764705882352941,0.15789473684210525,0.15168539325842695,0.17699115044247787
159,SP:66c5acd36a5fb74478d3f5ecaffc8479868dbe81,"This paper introduces Variational Thompson Sampling, where rather than sampling from the posterior, which can be infeasible, a computationally tractable upper bound is optimized.  In the stochastic multi-armed bandit setting with subgaussian noise, they provide a general analysis for any algorithm that plays a ""stochastically optimistic"" policy, which includes vanilla TS and the new method.  The VTS algorithm is shown to have sublinear regret for the more challenging bilinear saddle point problems where it can be shown that TS fails.","This paper presents and establishes regret bounds for a new heuristic, inspired by Thompson sampling, for selecting actions in stochastic multi-arm bandit and bilinear saddle-point problems. As in Thompson sampling, a belief state is maintained over the values of each arm. However, instead of selecting an action according to its probability of optimality, one instead computes a policy (distribution over actions) that maximizes a novel objective. The objective can be understood as combining a ""pure exploitation"" term (the expected reward of the policy, under the current belief distribution), and an ""exploration"" term that depends only on the relative concentrations (and not the expected values) of the posteriors for each arm.",This work develops an analog to Thompson sampling by upper-bounding the expected regret in sequential decision-making problems. The two terms in the upper bound loosely resemble the evidence lower bound of variational inference: the first term encourages selecting arms with high expected reward; the second term depends on the inverse of the rate function and penalizes heavy tails and encourages exploration. The resulting variational Thompson sampling algorithm is evaluated on a random game and a constrained bandit problem.,"This paper proposes a novel class of policies for online learning, named variational Thompson sampling (VTS). VTS is derived from a family of stochastically optimistic policies, which includes standard TS as a special case. VTS is amenable to some interesting interpretations when compared with difference existing policies such as UCB and K-learning, and is shown to outperform TS consistently in two classes of problems, i.e., zero-sum two player games and constrained bandits.",0.19753086419753085,0.14814814814814814,0.20987654320987653,0.16964285714285715,0.11607142857142858,0.125,0.14285714285714285,0.15,0.22666666666666666,0.2375,0.17333333333333334,0.13333333333333333,0.16580310880829016,0.14906832298136644,0.21794871794871795,0.19791666666666669,0.1390374331550802,0.12903225806451615
160,SP:679e57a2027ff1855e5dc80bd3ec91f6489cc747,"The paper analyzes the connection between searching for an optimal behavior policy that minimizes variance, and policy improvement in RL. The paper shows a number of interesting theoretical results, such as non-negative policy improvements, convergence and connections to implicit trust regions between consecutive policy iterates. The paper also shows results in constrained case where the policy class is parameterized. Finally, the paper shows empirical results on a few low-dimensional RL examples.","Importance sampling (IS) is at the core of many off-policy reinforcement learning (RL) algorithms.  The common use of IS in RL is to consider a _fixed_ policy behavior, say $q$ and an optimization policy $p$. The samples are acquired with the behavior policy $q$. The objective is to maximize  $$ \max_p \mathbb{E}_{x\sim q}\left[\frac{p(x)}{q(x)}f(x)\right]. $$  This approach suffers from high variance.   This paper highlight that originally, IS was thought to _reduce_ the variance of Monte-Carlo estimates. The main idea of this paper is to use IS as a variance minimization technique. It is interesting to see that the $\min_q Var[p(x)/q(x)f(x)]$ leads to a _policy improvement_.  The paper is structured as follow:  1. The authors show in Equation 1 the _minimum variance_ $q^*$. This analytical result is (as far as I understand) purely theoretical and cannot be used directly in RL. Note that Equation 2 also shows how the minimum variance behavior is a policy improvement. The authors, therefore, suggest that one could simply iterate the minimum variance to consistently improve the policy. At this point, a few questions remain open: will this process converge to a local or global optimum? Can one quantify the divergence between consecutive distributions?  2. To study the convergence properties, the authors introduce an operator $\mathcal{I}$ which takes in input a distribution $p$ and produces the next _minimum variance_ distribution $q$, which will be the next behavior policy. The operator $\mathcal{I}$ has several fixed points: if $p$ is deterministic then $p = \mathcal{I}p$. Theorem 4.2 ensures that the iterated application of $\mathcal{I}$ converges to the optimum in the domain of $p$.  3. Often, trust-region methods are used in RL to ensure stability in the learning process. Theorem 4.3 shows that the application of $\mathcal{I}$ produces a policy with bounded Renyi divergence with the previous policy. Interesting, this result is valid for any order of the divergence. This result is stronger than many other results in RL, where the divergence is only bounded for a unique order (typically 2: KL divergence), leaving the divergence on other orders unbounded.   4. However, the finding $q^*$ is often not possible. In short, if we consider a generic set of distribution, the application of $\mathcal{I}$ to an element of the set, might produce a distribution not contained in the set. The authors need at this point to devise an _approximation_ of $\mathcal{I}$, and check again for the convergence properties.  The first approximation consists in minimizing a $\alpha$-Renyi divergence. The difference between classic divergence minimization (or constraint) approaches is that in this paper, the authors are constraining divergence between ""return distribution"" (even though modified by the monotonic function $h$) rather than the divergence between parameter distribution or state distribution like in REPS or TRPO (@authors: correct me if I am wrong on this point).  5. The authors analyze the policy improvement of this ""approximated"" operator. They notice that the policy improvement happens only for some particular choice of $h$    6. The authors prove in Theorem 5.4 that the approximated operator has a tighter trust region (for every single choice of $\alpha$) w.r.t. the unconstrained operator $\mathcal{i}$.  7. The authors provide a practical algorithm in Section 6, and evaluate its efficacy in four classic control benchmarks. They also provide an analysis of the hyperparameters used (monotonic function $h$ and batch-size).   To summarize, the paper proposes a different use of IS, which allows both for variance reduction and policy improvement.  The improvement scheme is new (although it can be related to other approaches) and sheds new light on policy gradient optimization.      ","This paper proposes a novel policy optimization algorithm called POPE that is based off of ideas about how to use importance sampling for variance reduction from the Monte Carlo estimation community. The paper proves that the policy that minimizes the variance of the evaluation step actually provides a policy improvement, and then creates an algorithm that repeatedly estimates the minimal variance policy as a policy improvement step. It concludes with some small-scale experiments with linear policy classes on low-dimensional control tasks and claims modest improvements over TRPO and POIS baselines. ","The paper proposes a policy improvement algorithm inspired by the minimum-variance policy of importance sampling (IS) technique in Monte Carlo simulation community. Properties of the new policy improvement algorithm such as convergence and implicit trust region are well studied in the paper. And in practical, the paper leverages a surrogate objective with the non-central alpha-moment as the finite sample objective for the algorithm. The new algorithm is used in the policy optimization in reinforcement learning. Empirical experiments on several continuous control demonstrate the benefit of the new proposed algorithm compared with existing trust region baselines, especially on the robustness to the small batch sizes.",0.3972602739726027,0.2191780821917808,0.2876712328767123,0.05305466237942122,0.06591639871382636,0.25,0.04662379421221865,0.17391304347826086,0.19626168224299065,0.358695652173913,0.38317757009345793,0.21495327102803738,0.08345323741007195,0.19393939393939394,0.23333333333333334,0.09243697478991597,0.11248285322359394,0.23115577889447234
161,SP:6805f2245484fc91b5c13aa5f09e5478b810f97f,"The paper presents adaptive and anytime methods for semantic segmentation and pose recognition. Both of these are ""pixel-level"" tasks where the model is expected to output a prediction for each pixel in the input image. Their adaptive method, that performs variable amounts of computation depending on the input and the budget, with the amount of computation devoted to a given pixel varying from one part of the image to another. This is done by adding early exits to the base model architecture, and modifying the convolution layers to perform sparse computation on a subset of locations followed by interpolation. Experiments are done on Cityscapes semantic segmentation and MPII pose estimation benchmarks.","The paper proposes a new task called ""anytime"" prediction, which requires a model to make a progression of predictions, which might be halted at any time. The authors then also introduce an end-to-end model for this problem. The main two components behind the proposed model are: (1) a cascade of “exits” enabling the model to make progressive predictions while taking into account accuracy vs computational cost tradeoff; (2) Confidence Adaptivity, which allows the model to focus on the less confident pixel prediction. The authors implement their approach using HRNet baseline and demonstrate improved performance and efficiency on semantic segmentation and pose prediction tasks.","This paper proposes an anytime method for pixel recognition tasks like semantic segmentation and human pose estimation. The key idea is to design a network with multiple ""early exits"", from which the network could make predictions using a corresponding prediction head. Thus, depending on criteria like budget or confidence, the network could decide where to exit and therefore leads to different accuracy-computation operating points (i.e. its anytime property).   The technical contributions are mainly on two parts. The first contribution is a re-design of prediction heads (denoted as RH in the paper) at each early exits, mainly to tackle the difference of granularity for different intermediate feature maps. Another contribution is a confidence-based adaptive filtering mechanism that decides how to allocate computation budgets across spatial regions. Specifically, at each exit, the max prediction scores are used as a measure such that all spatial pixels that exceeds certain score threshold will not be processed in later stages.   The authors evaluated their methods on two tasks (Cityscapes for semantics segmentation and MPII for human pose estimation) and demonstrated that their full-blown method yields a better accuracy-computation tradeoff, compared to variants of the proposed approach and several previous methods.    ------------------------- POST REBUTTAL -------------------------  The authors have addressed most of my concerns in their response. Also, after some discussions on the issue of lacking positive signals from wallclock time metric, I buy the arguments to position this work as a forward looking exploration in the alternative space to methods that are chosen in current hardware lottery. With this, I will raise my rating to positive inclined.    ","This work focuses on anytime pixel-level recognition (e.g., semantic segmentation). They propose to add intermediate exists in the architecture for anytime inference. They also consider spatial confidence adaptivity in their network, where they only execute subsequent layers on a small set of non-confidence pixels and obtain the features of other positions via interpolation. They apply the method to semantic segmentation and human pose estimation and demonstrate a reduction in FLOPs and good anytime performance for both tasks.   ",0.20535714285714285,0.3125,0.16964285714285715,0.24761904761904763,0.13333333333333333,0.09433962264150944,0.21904761904761905,0.1320754716981132,0.2375,0.09811320754716982,0.175,0.3125,0.2119815668202765,0.1856763925729443,0.19791666666666669,0.14054054054054055,0.15135135135135133,0.14492753623188406
162,SP:68dfc737a8ea591da2c7fe048a5b8995c89e1fec,This paper proposes a multi-modal pretraining method for document understanding. It involves a new hierarchical model architecture and three pretraining tasks. It compares with previous SOTA methods on three tasks and achieves stronger performance on two of them.   The major contribution is the newly proposed multi-modal pretraining tasks and the architecture is a combinations of models from both domains.,This paper proposes a self-supervised pre-training framework for document understanding called UniDoc. UniDoc takes multimodal data (image feature and text feature from document) as input and uses gated cross attention for learning cross-modal correlation. The superiority of UniDoc has been empirically shown on several downstream tasks.,"This paper presents UniDoc as a multimodal pre-trained model for document understanding. UniDoc consists of 3 layers: a feature extraction layer, which extracts sentence embeddings and region embeddings from the original document; a fusion layer, which fuses textual and visual information based on a gated cross-attention mechanism; a task layer, which uses 3 tasks as pre-training objectives to optimize model parameters. Evaluations are conducted on 4 downstream tasks, including Form Understanding, Receipt Understanding, Document Classification and Document Object Detection. Comparing to several SOTA baselines, UniDoc performs better on Form Understanding and Receipt Understanding, but worse on Document Classification. The paper also includes some ablation studies to investigate the effects of V/L/V+L, pre-training tasks and visual backbones, and some interesting future directions.  ","This paper proposes a new unified framework for pre-training image/text encoders for capturing both visual and textual features in documents. The main Transformer model is pre-trained with multimodal embeddings as input features (visual & textual features) through three pre-training tasks: Masked Sentence Modeling, Visual Contrastive Learning, and Vision-Language Alignment. The pre-trained model can be fine-tuned on downstream tasks on which the proposed UniDoc model outperforms baselines.",0.19672131147540983,0.2786885245901639,0.21311475409836064,0.3877551020408163,0.30612244897959184,0.125,0.24489795918367346,0.1328125,0.18055555555555555,0.1484375,0.20833333333333334,0.2222222222222222,0.21818181818181817,0.17989417989417988,0.19548872180451127,0.21468926553672316,0.24793388429752064,0.16
163,SP:698d6c344fe94ea4ec3ce54601f5976d82d00b85,"D2C proposes three main contributions to diffusion models, a recently popularized class of likelihood-based generative models based on parameterized Markov chains, particularly successful when viewed from a denoising score matching lens. First, this paper learns a prior for the *latent space* of an image VAE (NVAE), while past work mostly applied diffusion to the data space. Second, D2C adds a SimCLR-style contrastive loss to image latents in order to improve the quality of latent representations for classification. Third, binary attribute classifiers are learned conditioned on these latents and used to guide prior sampling with rejection sampling or Langevin dynamics. Experimental results qualitatively and quantitatively show some success in conditional image generation and manipulation.","The authors present Diffusion Decoding Models with Contrastive Representations (D2C) - a special VAE model with diffusion modeling in the latent space combined with contrastive learning based training of the inference network. The authors claim that D2C helps avoid the prior hole issue with VAEs due to diffusion modeling of the latent space, while contrastive learning helps learn meaningful representations allowing Few-Shot conditional generation. They further demonstrate that D2C outperforms NVAE and DDIM in few-shot conditional generation while remaining comparable to DDIM in unconditional generation. ","The paper describes two methods: 1) how to incorporate self-supervised learning in VAE to have a meaningful encoded representation, 2) how to use diffusion model over later to bridge the gap between posterior and prior for unconditional generation, 3) how to use the given setup to do few-shot conditional generation. Empirically, authors show highly improved representation quality, which can be attributed to the incorporation of self-supervised learning. Additionally, experiments show improved FID scores for face datasets, but not for CIFAR. Authors don’t show results on ImageNet. The authors show improved FID scores for the conditional generation task. There is no FID comparison against StyleGAN2 (or any other GAN-based model), even if the authors claim to come very close in sample quality to such models. The qualitative depiction of image manipulation tasks is very impressive.","This paper proposes to learn a joint model consisting of a VAE and a latent space diffusion. The training process is augmented with a contrastive loss.  The authors show that this approach makes possible a few-shot generation of images with desired properties. To achieve this,  one needs to train a classifier in the latent space and after that sample latent codes with high classifier scores in any way.  Also, the case of semantic editing is evaluated. The manipulation is implemented using a Langevin-like approach in the latent space.",0.14782608695652175,0.1391304347826087,0.1826086956521739,0.1744186046511628,0.19767441860465115,0.12949640287769784,0.19767441860465115,0.11510791366906475,0.23333333333333334,0.1079136690647482,0.18888888888888888,0.2,0.16915422885572137,0.1259842519685039,0.20487804878048782,0.13333333333333336,0.19318181818181818,0.1572052401746725
164,SP:6adcd2a71ce70922c4cbe155d49f105964faee8f,"This paper considers the problem of “one-shot” set/graph generation, which involves learning a probabilistic decoder that maps latent vectors to sets.  First, the authors extend the usual definition of equivariance for a function to a learning algorithm.  This definition is used to show that exchangeability is not useful in GANs and VAEs when used for set generation.  Next, the authors propose Top-n, which is a new set creation mechanism which learns to select the most relevant points from a trainable reference set, in a deterministic and non-exchangeable fashion.  Top-n can replace iid generation in a VAE or GAN.  Experimental results are provided for SetMNIST reconstruction and generative tasks for a synthetic molecule dataset for sets, and the QM9 chemical dataset for graphs, demonstrating that Top-n is competitive with or outperforms a number of existing generative approaches.","In this paper, the author proposes that exchangeability is unnecessary for the generative model in the domain of set and graph generation. The definition of equivariance is generalized to learning algorithms, which is appropriate for generative modeling. Then, a method called Top-N which can be used in classical generative models, such as VAE, GANS, is proposed. In the author's argument, the proposed method has the ability to extrapolate to larger sets than those seen during training as well as the ability to train easily, which are not satisfied by the previous generative models. ","This work proposes a new deterministic set sampling mechanism, Top-n. Top-n learns to select the best 'n' points from a trainable reference set. Unlike the previous set sampling mechanisms such i.i.d. sampling, First-n and MLP projection, Top-n do not suffer from collision problem and can generate sets of various sizes (unseen during training). Top-n can be incorporated for one-shot sampling in VAE and GANs like generative models. Experimental results on standard benchmark for set and molecular graph generation, suggest improved performance in comparison to prior sampling mechanism.","This submission discusses probabilistic models that generate sets and graphs conditioned on latent vector representations. The paper discusses the following different approaches to generate sets:   * i.i.d sampling of set element representations, concatenated with the set's vector representation followed by (possibly equivariant) networks. * First-n generation, using a learnable reference set of a maximum number of nodes/set elements, and concatenating the latent set representation to each element of this reference set, and then picking the first n element of this reference set as node representations.  * MLP-based generation of node representations that don't take into account invariance of the generator under node permutation.   	 The authors claim that i.i.d sampling has two problems: the additional stochasticity makes it harder to train, and if two points are sampled too close together the resulting node representations will be similar (dubbed the collision problem). Furthermore, MLP-based generators don’t explicitly take into account permutation symmetries, can't generalize to arbitrary number of nodes and first-n generation prioritizes learning of the first elements of the reference list as they will be used more often than the last elements of this reference list.   The authors propose a new definition of equivariance with respect to permutations: instead of talking about equivariance of functions, they propose to define equivariance for a learning algorithm, by stating that  ""a learning algorithm is equivariant to the action of a group if the training dynamics do not depend on the group elements that are used to represent the training data"".  They furthermore adapt first-n to the method top-n with differentiable sorting of the representations of the elements in the reference set based on cosine similarity with a vector that depends on the latent vector representation.   The following claims are made:  *  top-n is easier to train than i.i.d generation because it doesn't involve an extra sampling step * top-n  captures complex dependencies in data better.  The proposed method is benchmarked on set and graph generation tasks : SetMNIST, synthetic molecule-like 3D structures, the QM9 dataset.  ",0.16901408450704225,0.176056338028169,0.3028169014084507,0.16842105263157894,0.29473684210526313,0.29473684210526313,0.25263157894736843,0.2631578947368421,0.12427745664739884,0.16842105263157894,0.08092485549132948,0.08092485549132948,0.20253164556962025,0.21097046413502107,0.1762295081967213,0.16842105263157894,0.12698412698412698,0.12698412698412698
165,SP:6b19f16c429ffa7f613b57d082bde3794a8e29e0,"In this paper, the authors study counterfactual invariance in machine learning models---intervening on “non causal” parts of the input should not change model predictions---and its relation to how models generalize out-of-domain. The authors show that counterfactually invariant predictors rely more on “causal” features as opposed to spurious correlations, thus generalizing to unseen distributions where these spurious correlations may not hold. The authors further propose two regularization schemes (one if the underlying data generation mechanism is represented by a causal model and another if it is an anticausal model) using which we can train counterfactually invariant predictors without access to counterfactual examples---as opposed to most prior work in which researchers often seek to either automatically construct or crowdsource counterfactual examples. They show that these strategies are heavily dependent on the underlying causal mechanisms, and that applying the regularization scheme designed for a causal model could lead to worse performance if the underlying data generation mechanism was anticausal and vice versa. The authors offer empirical evidence on text classification tasks (review helpfulness classification and natural language inference) to support their theoretical findings. I have some concerns about the paper (as discussed below) but overall I believe this paper builds on an exciting line of work and offers good theoretical and empirical contributions.","The paper casts robustness to spurious correlations as a model's dependence on input data that is not stable under stress testing. Stress testing here corresponds to changing part of the input data that a practitioner believes should not change the prediction. The authors term this counterfactual invariance and consider two different causal structures that could've produced the data. Identifying distribution-level implications (as opposed to unit-level) of these causal structures, the authors propose regularization schemes to enforce when learning predictors to ensure that counterfactual-invariance holds (under certain assumptions). The effectiveness of these schemes is shown both theoretically and empirically.",This paper presents conditions that are necessary for counterfactual invariant predictions under certain assumptions. They derive these from causal graphs and show the connection to worst-case domain generalization. Experiments on natural language datasets show that the proposed conditions do lead to a decrease in worst-domain error.,"In this work, the authors focus on how to build models that are robust to spurious correlations. The focus of the paper is language datasets. The authors work with two types of DAGs -- a) association of the label with the invariant feature is anti-causal, and b) association of the label with invariant feature is causal. The authors propose a notion of invariance, which they call counterfactual invariance, that they require the predictor to satisfy. Under this notion, the prediction $f$ has to remain invariant under all the potential outcomes that $X$ can have under variations to the spurious feature $Z$. Since the notion of counterfactual invariance is hard to enforce as counterfactuals are not observed, the authors come up with necessary conditions that are then used as proxies to enforce counterfactual invariance. The necessary conditions are different in the two directions causal and anti-causal. Further, the authors analyze the relationship between best counterfactual invariant predictor on train domain and target domain (under appropriate notion of changes allowed in target domain). Finally, the authors establish conditions for min-max optimality of the predictor that is learned. ",0.12093023255813953,0.06511627906976744,0.17209302325581396,0.08737864077669903,0.2524271844660194,0.2916666666666667,0.2524271844660194,0.2916666666666667,0.19786096256684493,0.1875,0.13903743315508021,0.0748663101604278,0.16352201257861637,0.1064638783269962,0.18407960199004975,0.11920529801324502,0.1793103448275862,0.11914893617021274
166,SP:6ba17dd4b31a39478abd995df894447675f2f974,"The paper proposes a graph-learning model (HCM) for learning hierarchical chunks from sequential data. The paper first proposes an idealised HCM method, for which the paper provides learning guarantees via a proof by induction, and an online approximation to this idealised method, which is more computationally feasible and which is used to perform experiments in temporal, visual, visuotemporal and language sequential data domains. The paper demonstrates that the online method learns interpretable chunks at multiple levels of abstraction and demonstrates positive (and negative) transfer to other hierarchically structured environments with similar (and different) structures.","This paper proposes a method for learning representations of non- i.i.d. data in terms of hierarchical sets of chunks, inspired by cognitive theories of grouping by proximity. These sets are assembled over time from the initial set of primitive data points by finding correlations between temporally/spatially sequential primitives/chunks and appending to the set. The authors show that this learning method is tractable, has convergence w.r.t. hierarchically-decomposable problems, and learns intuitively and practically reasonable chunk sets.","This paper presents HCM, an approach for chunking a sequence of data into a hierarchical representation. More specifically, HCM learns a tree with atomic units (ie the low-level inputs, in this case integers representing things like text characters or quantized pixel values) as the leaves and increasingly complex groupings of them higher up the tree.   HCM learns by iteratively parsing the provided data (ie stream of tokens), in each pass computing marginals for the current set of chunks as well as transition frequencies between them. After updating its marginals and transition frequencies, the two chunks with highest joint probability are combined into one. The process continues until all pairs of chunks pass an independence test.   I believe the main contribution of this paper is in that it presents an idea for interpretable grouping based on the principle of grouping by proximity from cognitive science, and a largely qualitative proof of concept for it.","This paper proposes a non neural system of parsing natural language text by chunking sequences to form hierarchical structures. The algorithm strongly resembles classical parsing algorithms. Decisions about when to chunk a phrase into a constituent are based on chi^2 tests of independence, where a pair of chunks that are considered to be dependent are joined into a single constituent. They test this chunking algorithm on natural language data against an RNN,  concluding that the classical parsing algorithm is more sample efficient in achieving a low KL-divergence from the true sequence data. They also provide some examples of how this algorithm can be applied to temporal image data or video.",0.18947368421052632,0.17894736842105263,0.14736842105263157,0.23170731707317074,0.17073170731707318,0.11688311688311688,0.21951219512195122,0.11038961038961038,0.125,0.12337662337662338,0.125,0.16071428571428573,0.2033898305084746,0.13654618473895583,0.13526570048309178,0.16101694915254236,0.14432989690721648,0.13533834586466165
167,SP:6bc677d060ba4ab09f6da61458680e7a7976644b,"The work aims to provide a deeper understanding of a corollary in the lottery ticket hypothesis: how to know whether a winning ticket found in one task can be transferred to another task. The paper tries to combine the widely used pruning method in the lottery ticket hypothesis (iterative magnitude pruning) with a concept from statistical physics, which is the renormalization group. The authors show iterative magnitude pruning can be considered as a renormalization group operator. Experiments are conducted using the renormalization group to examine whether pruned models are in the same universality classes and extend the renormalization group to the Elastic Lottery Ticket Hypothesis.","Although Lottery Ticket Hypothesis has suggested an exciting corollary about the winning tickets, it is still unclear why winning ticket universality exists, or any way of knowing a priori. In this paper, authors make use of renormalization group theory to perform detailed understanding of this hypothesis. Authors find that the principle method iterative magnitude pruning (IMP) is directly related to the renormalization group (RG).    Based on the contributions of  RG leading to a first principled understanding of the universality in behavior near phase transitions, viewing the IMP from an RG perspective may lead to new insight on the universality of winning tickets. Such insight is build upon the experimental support of the theory in large scale lottery ticket experiments.","This paper tries to find a theoretical explanation of the transferability of lottery ticket used in similar tasks. Observing the similarity between the universality in renormalization group and the lottery ticket hypothesis, the author proposes that the iterative magnitude pruning, which is used to find the winning tickets, could be a renormalization group scheme. The authors also provide some evidence on their theory on vision model of ResNet families.","This paper seeks to explain empirical observations in the literature on the Lottery Ticket Hypothesis (LTH) based on ideas from renormalization group (RG) theory in physics. The authors focus on the particular case of iterative magnitude pruning (IMP) as the pruning scheme and view the flow on the space of model parameters during IMP as analogous to RG flow in the space of couplings of a Hamiltonian. The primary experimental evidence for the connection comes from two considerations. (1) Computing the percentage of all non-zero parameters that are in a particular residual block (of a ResNet model) at a particular time. For ResNet-50 models considered, this leads to four scalar, dynamical quantities. These four quantities are found to grow or shrink exponentially with time, suggesting they are eigenfunctions of the IMP map. For three different types of learning & dataset (ImageNet / simCLR / MoCo) and one architecture (ResNet-50), the authors find the same eigenvalue across the three settings, per block. From this, the authors conclude that this universality (shared exponent) across different settings is evidence for the analogy to RG. (2) Transferability of lottery tickets. The authors hypothesize that the relevance / irrelevance / marginality  of a particular block ought to be matched by the source and target architecture when transferring tickets. More specifically, there are three conclusions from the literature (#1-3 in Sec 3.2) that are explained (for example, why the smallest model has the weakest transferability). ",0.2571428571428571,0.23809523809523808,0.24761904761904763,0.17647058823529413,0.2605042016806723,0.3188405797101449,0.226890756302521,0.36231884057971014,0.1087866108786611,0.30434782608695654,0.1297071129707113,0.09205020920502092,0.24107142857142855,0.28735632183908044,0.15116279069767444,0.22340425531914895,0.17318435754189945,0.14285714285714285
168,SP:6c0b7cb37e285cb9342f049d7b61af4565fe01fd,"This paper aims to improve the density distribution of generative radiance field models by generating relitable reflectance fields and using different lightings to shade them during training. The core idea is that artifacts will arise when the generated density distribution (""shape"") is unnatural and is getting shaded by different lightings, which should be detected and resolved by a discriminator. Experiments verify that this shading regularization can yield naturally looking shapes induced by the generated radiance fields."," This paper proposes a GAN capable of generating a relightable radiance field when trained on an unlabeled dataset of face images. The relightable radiance field generator is conditioned on the 3D location and a latent code and outputs a volume density field (à la NeRF) and an ""albedo"" field (I strongly advise against the use of this word; see below) that allows explicit control of the light location. The training is conducted in the GAN style, where the training signal comes from only a discriminator loss. The intuition is that the generator is better off generating a meaningful 3D volume that, when rendered from a random camera view, generates a photorealistic face image that falls into the training data distribution, than generating some intricately-designed volume that gives faces of different identities when viewed from different angles.  The authors demonstrate that the model is able to learn 3D shapes in an unsupervised fashion from just 2D images. The work is mainly compared against pi-GAN, which is shown to suffer from what the authors call ""color-shape ambiguities."" For efficient rendering, the authors also propose an auxiliary network that predicts the surface location given the latent code and a viewing direction, such that the network can sample around the predicted surface to avoid expensive sampling of unoccupied space.","This paper proposes a new method to train generative radiance fields that can achieve more accurate 3D shape reconstruction. The key idea is that instead of directly synthesizing the images from different viewpoints through volume ray tracing, which may suffer from color-shape ambiguity, it synthesizes albedo and normal and then renders the image with different lighting conditions. This new method requires the synthesized albedo and normal to be able to render realistic images under various lighting conditions, providing an extra regularization for shape reconstruction. To further improve the rendering speed, it uses an extra 2D CNN to predict a depth map from latent code and camera poses so that it only needs to sample near the surface in volume ray tracing. Experiments show that the proposed method greatly improves shape reconstruction with similar image synthesis quality compared to state-of-the-arts.","The paper proposes a novel generative model for neural radiance field, where the key idea is to synthetically add shading effects based on random lighting configurations. Shading will make images unrealistic when the geometry is corrupted. Therefore, the system gets trained to generate accurate shapes in its implicit radiance field representation. The system makes convincing results both qualitatively and quantitatively.",0.27631578947368424,0.25,0.17105263157894737,0.14746543778801843,0.07834101382488479,0.1048951048951049,0.0967741935483871,0.13286713286713286,0.21666666666666667,0.22377622377622378,0.2833333333333333,0.25,0.14334470989761094,0.1735159817351598,0.19117647058823534,0.17777777777777778,0.12274368231046931,0.1477832512315271
169,SP:6dabaca9a77620b7c4019bf5f9c2a88628fc691c,"The work addresses an online learning problem, where the algorithm receives a sequence of points q_1,q_2,... in the d-dimensional unit ball, and must guess the label of each point among k possible labels {1,...,k}. The correct label corresponds to a nearest-neighbor partition of the ball, R_1,...,R_k determined by k unknown ""centers"" x_1,...,x_k. If the algorithm predicts that a point q has label i, it incurs a loss equal to the distance between q and the subset R_i of the points labelled as i. The goal is to minimize the total loss. The main result is a set of upper bounds on the loss in the form poly(d), or similar, but in any case independent of the length T of the sequence. The work considers also the special case k=2, the case of general convex classes (for which it gives lower bounds), as well as some other implications of the upper bounds. ","The paper considers the problem of online prediction of which region of a k-centred nearest-neighbour partition (where “nearest” is defined via a “distance” function) a point is in. The feedback on each trial is which region the point is in - as in standard online learning. However, instead of bounding the number of mistakes, this paper seeks to bound the total loss, where the loss on a trial is the “distance” of the point to the region predicted (see the main review for a confusion I have here). The paper considers two types of “distance” - the (negative of the) inner product and the p-norm. The paper gives loss bounds that are independent of the total number of trials and don’t have a “margin” term.","This paper proposes an online multiclass classifier that minimizes the distance between a point and the correct class partition for that point’s label. They argue that this loss function punishes more harshly points that are classified as extremely wrong versus almost correct. They propose an algorithm where in every round, their classifier predicts a point and suffers the distance based loss between their predicted point and the correct class region. ",The author proposed an online learning setting where the true label is determined by which of k centers is the closest from the query point for a distance function.  The goal of the learner is to minimize the total distance from each query to the region corresponding to the correct label. The authors then propose learning algorithms for these settings and provide a theoretical analysis of the proposed algorithms. The authors also show interesting properties of this learning setting that could not be achieved in the traditional multiclass learning setting.,0.19879518072289157,0.10843373493975904,0.1927710843373494,0.12598425196850394,0.2125984251968504,0.15492957746478872,0.25984251968503935,0.2535211267605634,0.35555555555555557,0.22535211267605634,0.3,0.12222222222222222,0.2252559726962457,0.1518987341772152,0.25,0.1616161616161616,0.24884792626728114,0.13664596273291924
170,SP:6e54083a06942f2c41e1796a9f911d3dd9bab0cc,"In this paper, the authors study some simple convolutional kernels. They show that two or three layers convolutional kernels with polynomial kernels on the higher level and Gaussian pooling provide similar performance as the much more complicated state-of-the-art convolutional kernels (e.g., Myrtle kernel). Motivated by these good performance, they proceed to characterize the RKHS of these kernels, and describe how extra layers and pooling allows to capture interaction between patches with more or less spatial dependency. They then use the RKHS norm and some standard bound on the generalization error to show how choosing an architecture adapted to the target function can improve the statistical efficiency.","This paper analyzed convolutional kernels. Experimentally, the authors showed that convolutional kernel formed by shallow convolutional neural networks (2 - 3 layers) performs as well as the state of art deep convolutional kernels (e.g., Shankar et. al.). The authors also provide an exact description of the RKHS functions and their norm, of the one layer convolutional kernel and two layer convolutional kernel with low degree polynomial activation function on the top layer. Finally, assuming that the target function has a specific form, then theoretical results can give the generalization upper bound of kernel ridge regression in terms of the sample complexity; this shows that using a proper architecture can same the sample complexity by a polynomial factor of the input size.  ","The paper studies the RKHS and generalization properties of convolutional kernel networks, the NNGP corresponding to CNNs and the class of kernels that achieve state of the art performance on image classification. Among the theoretical contributions are analysis of the regularization induced by pooling, and interactions between patches captured with iterated convolutions. The paper also includes experimental results on CIFAR10 matching the prior state of the art for a kernel method while using a shallower architecture, as well as some ablations on the size of the convolutional kernels, the size of the pooling filters, and the number of layers. ","The paper aims to study properties of deep convolutional models via the surrogate of simple hierarchical kernels with convolution and pooling layers. The authors characterise the underlying RKHS and their norms, and provide generalization bounds. The results imply that convolution operations such as pooling and patches, if present in the data, lead to improve guarantees. An empirical study both justifies studying these simple kernels in the first place, and illustrates the obtained theoretical results. ",0.2545454545454545,0.20909090909090908,0.17272727272727273,0.21487603305785125,0.1487603305785124,0.1919191919191919,0.23140495867768596,0.23232323232323232,0.25675675675675674,0.26262626262626265,0.24324324324324326,0.25675675675675674,0.2424242424242424,0.22009569377990432,0.20652173913043476,0.2363636363636364,0.18461538461538463,0.21965317919075145
171,SP:6eb5ce1d85928a3af759d75016089c535941d0b0,"The authors derive exact and non-asymptotic (in size or time) expressions for the expected test loss of a linear model during the SGD training dynamics. The time-dependent average test loss is given as function of the eigenvectors and the eigenvalues of the covariance matrix of the features when the features are Gaussian. For non-Gaussian features also forth-moments are involved and the Gaussian formula can be adapted to provide an upper bound. In any case, the formula for the  Gaussian case seems to match very well the experiments on real world datasets. The formalism covers both the online setting and the setting of multiple passes over a fixed training set. ","The paper studies the learning dynamics of stochastic gradient descent on simple linear models with structured features. In particular, the paper discussed the influence of data structure on learning dynamics and optimal batch sizes. In practice, this model seems to be able to predict the training/test error of small neural networks on real data. ","This paper addresses the problem of characterising analytically the dynamics of stochastic gradient descent (SGD) in problems where the data have features with arbitrary covariance structure. All the results are valid for linear models (e.g., random features, neural tangent kernel) trained on the mean-squared error loss. First, the authors consider one-pass SGD and Gaussian features, and derive an exact closed-form expression for the time-evolution of the expected test loss, i.e., averaged over the data distribution and the sampling sequence. Then, they turn to generic non-Gaussian feature maps with arbitrary covariance and similarly compute an exact closed-form expression for the expected test loss of one-pass SGD. They also derive an upper bound that only depends on the features covariance, provided that a regularity condition holds on the fourth moments of the features. This result shows that non-Gaussian effects are negligible in the settings under consideration. Finally, they extend their results to multi-pass SGD and provide expressions for the expected test and training losses over time. The authors apply their theoretical findings to derive heuristic estimates of the optimal batch size and learning rate and study their dependence on the features and target distributions. The numerical simulations show a good agreement with the theoretical predictions.","This paper studies the relation between the test error of stochastic gradient descent on training linear models and the structure of the data distribution, the iteration number, and the batch size. The analyses are first done on one-pass SGD and then extended to multi-pass SGD. Results in theory and real data experiments are presented to illustrate the learning curves of the SGD algorithm.",0.12389380530973451,0.30973451327433627,0.1592920353982301,0.38181818181818183,0.34545454545454546,0.14485981308411214,0.2545454545454545,0.16355140186915887,0.27692307692307694,0.09813084112149532,0.2923076923076923,0.47692307692307695,0.16666666666666669,0.21406727828746175,0.20224719101123595,0.1561338289962825,0.31666666666666665,0.2222222222222222
172,SP:6ec2c8456ab95f7d028d00b591dab3eadc549eb8,"This paper proposes a new architecture for learning visual analogies, based on Gentner’s Structure Mapping Theory for how humans might draw analogies. Gentner’s theory proposes representing the relationships between objects explicitly, so that this relational structure can be reused in new domains (and suggests that this commonality in structure is what permits analogies to be made between perceptually dissimilar objects). The authors propose a neural network model architecture and test it on the Raven’s Progressive Matrices dataset. The proposed architecture first splits a series of ‘source’ visual scenes into objects, attributes and the relationships between those scenes, before feeding just the relationship head into a second network. The second network then switches between two different architectures (depending on the relation fed in). The architecture in the second network (whichever is chosen) receives the ‘source’ relationship and two ‘target’ scenes before trying to predict which of a set of 4 candidate  ‘target’ scenes completes the visual analogy between source and target. The authors test their architecture on the generalisation splits in the RPM dataset and compare test accuracy results to the baseline models used by Hill et al, 2019. The authors show that their model (which builds in additional architectural structure) performs better at a subset of tests than more general architectures.","This paper targets the problem of abstract reasoning, with a special focus on the task of learning visual analogies. The authors propose a multi-stage neural network (Neural Structure Mapping, NSM) for decomposing the problem into vision relationship recognition and concept inference. They tested their model on an existing RPM (Raven's Progressive Matrices) based visual analogy benchmark that contains different systematic generalization tests and outperformed existing models. The authors made further discussion on these experimental results to support their proposals on model designs.","A model called Neural Structure Mapping (NSM) is introduced to solve the task of abstract visual analogy making. The NSM model consists of a visual relationship encoder and an analogy inference engine. The visual relationship encoder extracts the visual domain elements, including object, attribute, and relation, while the analogy inference engine is a neural modular architecture that constructs the model layout based on the relation and predicts the final answer. On the dataset proposed by Hill et. al., the NSM shows better performance than other baselines.","This paper tackles the problem of analogical reasoning. In particular, it presents a framework for learning the Raven Progressive Matrices (RPM) task, an abstract analogy task.  In the RPM task, a sequence of three images from a source domain are given. There is some relationship that holds for the sequence, e.g. the third image is the union of the first two. Then, given an incomplete sequence of two images from the target domain, the third image must be chosen from a list of four possible candidates.  The proposed Neural Structure Mapping (NPM) system consists of two pieces. The first piece is the Visual Relationship Encoder. Given the source sequence of images, the encoder predicts the type of relationship exhibited in the sequence. This information is passed to the second piece, the Analogy Inference Engine. The architecture of the engine is assembled dynamically, according to the predicted relationship. The assembled network takes the target sequence and the candidate matrices as input and selects the completion of the sequence from among the candidates.  The encoder is trained with the ground truth relationship labels. The engine is trained using the ground truth candidate labels.   The paper presents an experiment to test systematic generalization, in which particular attributes are held out during train time. The NSM system is found to achieve better performance.  In contrast to Hill 2019, which presents the model with semantically-contrasting alternative candidates at train time, NSM achieves good performance even when the alternative candidates are not necessarily semantically related.  ",0.1308411214953271,0.14018691588785046,0.18691588785046728,0.19047619047619047,0.2857142857142857,0.3372093023255814,0.3333333333333333,0.3488372093023256,0.1593625498007968,0.18604651162790697,0.09561752988047809,0.11553784860557768,0.1879194630872483,0.19999999999999998,0.17204301075268816,0.18823529411764706,0.14328358208955222,0.17210682492581603
173,SP:6ed1637ac697821931f685db0d476b9f7b56971a,"The main contribution of this paper is SimplEx: a method that aims to explain a test prediction by decomposing it into the weighted sum of nearest neighbors (from a user-specified corpus) in the latent space. They claim this method is more precise and robust than existing explanation-by-example-comparison methods (namely, KNN-like explanations) and distinguish this method from (often gradient-based) feature saliency methods. The paper builds off work that try to explain model predictions that use comparisons in latent variable space (e.g., Concept Activation Vectors, Deep KNN). SimplEx works PROVIDED the model has a hidden layer that linearly maps to the output (this is the layer that is used for explaining predictions).   - This constraint is significant for accurate weighting of examples. Note that if this is not satisfied by the model (say for instance a softmax is added right before the final output), it is possible to consider an earlier layer that satisfies the linear-mapping constraint as the output  Note too that if the latent space spanned by the user uploaded corpus does not contain the latent representation of the test prediction of interest (it likely will not), the test's latent is projected into the space. This means that it is possible to use this method on any input, regardless of the corpus. We can then measure how good this approximation is by calculating the residual between the corpus span and the test latent.  We can then ""interpret the shift in latent space as resulting from a shift in the input space."" This is the crux selling point of the paper, and it is accomplished using integrated (and projected) jacobians. This technique also lets us see the contribution of a feature from a corpus example.  The authors then proceed to (briefly) characterize their model on MNIST against dKNN inspired methods, and show a use case on a clinical risk dataset.  They claim the following novelty:  1. Freedom to choose comparison corpus, no need for this to equal the training set 2. The instance-level decompositions are valid in latent and output space, and are more robust than other explainability methods","This paper introduces SimplEx, a method that approximate the hidden representation of a test example using the linear combination of hidden representations of examples in a pre-defined corpus. Further, the hidden representation of the test instance is associated with each input feature of these examples in the corpus. The paper demonstrates the quality of approximation of SimplEx (Sec 3.1, SimplEx outperforms kNN baselines) and demonstrate its practical utility in the experiment of predicting cancer mortality (Sec 3.2) by allowing model users to interpret model predictions and determine whether to trust the prediction. Detailed math derivation and additional experiments with MNIST and time series data are included in the appendix.","This paper introduces SimplEx, which is a method to post-hoc explain a model prediction by computing from a user-defined set of examples, how the set of examples decompose the prediction. For example, the clinical risk prediction of a patient can be explained by a set of other patients and how much each patient influenced the prediction.   Contributions -	A post-hoc explanation method, called SimplEx, were users can choose the set of examples from which the explanation should be constructed. This is advantageous when the training set is not available or a user wants customized explanations, e.g. a doctor wanting to understand a predictions in terms of other patients she/he knows. -	SimplEx combines the ideas of feature saliency and example-based explanations. For each example, the method computes the percentage of this example contributing to explaining the prediction as well as which features of this example were important. -	A latent-space generalization of Integrated Gradients [10] -	Experimental evaluation that show SimplEx to be more precise and robust compared to baselines. ","In this paper, the authors propose to explain the behavior of black-box neural networks by approximating a top-level hidden layer (no non-linearities afterwards, except possibly softmax) as a linear combination of the hidden layers obtained from different examples. These examples may come from the training data, but it is not required. The contribution of each input feature is also evaluated using an extension of integrated gradient across multiple dimensions.",0.11204481792717087,0.12885154061624648,0.06162464985994398,0.24107142857142858,0.14285714285714285,0.12138728323699421,0.35714285714285715,0.2658959537572254,0.3055555555555556,0.15606936416184972,0.2222222222222222,0.2916666666666667,0.17057569296375266,0.17358490566037732,0.10256410256410257,0.18947368421052632,0.17391304347826086,0.17142857142857143
174,SP:6ff26839a14991597555ead4c82eb6ddb61e4dbc,"The submission tackles the problem of training GANs with limited data. It proposes to do so by adaptively regularizing GAN discriminators by replacing some real data by generated samples when the discriminator overfits, accoding to an overfitting heuristic.  A study of the proposed objectives show that the method keep the classical GAN optima while alleviating overfitting of the discriminator.  The method is tested on various datasets with limited data, and compared, using a StyleGAN-2 backbone, against other state-of-the-art methods for training with limited data.","This paper introduces a data augmentation strategy for training GANs, which proves to be especially effective in the low data regime. The essence of the idea is to replace real images by images produced by the generator, using an adaptive replacement probability in the discriminator training step. By classifying some fake images as real, discriminator overfitting is prevented, which normally leads to very poor training performance and instability when very little data is available. The proposed method (named APA) improves over or can be combined with another recent data augmentation scheme for the low data regime (such as ADA).","This paper tries to solve discriminator overfitting problem.  The authors propose adaptive pseudo augmentation (APA). * APA employs the generator itself to augment the real data distribution with fake images.   * i.e., Fake images are presented as ""real"" instances to the discriminator. * Adaptiveness comes from measuring overfittingness of the discriminator. * APA has similar theoretical properties to the original GAN, but with $(1-\alpha)p_\text{data} + \alpha p_g = p_g$.  APA improves systnesis quality in the limited-data regime. * on FFHQ, AFHQ-Cat, CUB, Danbooru The authors provide a theoretical analysis of APA, similar to the original GAN. ","This paper proposes a new method for GAN training with limited data, named adaptive pseudo augmentation (APA). The key idea originates from the empirical observation that, when trained with limited data, the GAN discriminator tends to overfit easily, leading to poor generation performance. Therefore, the authors propose to adaptively add generated samples to the training data, so as to counteract the overfitting during training. Accordingly, they present the APA that is developed based on the output of the discriminator. Empirical experiments demonstrate the effectiveness of the proposed APA.",0.19318181818181818,0.19318181818181818,0.23863636363636365,0.20202020202020202,0.1919191919191919,0.20408163265306123,0.1717171717171717,0.17346938775510204,0.23863636363636365,0.20408163265306123,0.2159090909090909,0.22727272727272727,0.18181818181818182,0.1827956989247312,0.23863636363636365,0.20304568527918782,0.20320855614973263,0.2150537634408602
175,SP:702029739062693e3f96051cbb38f20c53f2a223,"This paper investigates emergent language research, whose goal is to study language as it emerges from the inherent properties of the environment, language, and agents. This is typically in the context of reinforcement learning such that the phenomena arises from the pressure to maximize reward. This paper considers two types of rewards. One directly comes from the task. The other is shaped rewards which makes learning easier. This paper mainly investigates the impact of shaped reward on the emergent language. Using a simple sender-receiver navigation game, the paper shows that shaped rewards can explicitly bias the semantics of the learned language, change the entropy of the learned language, and mask the potential effects of other environmental variables of interest.","This paper studies the effect of auxiliary rewards in an environment for a multi-agent communication task. The task is for a sender to navigate a receiver to the center of some environment, in worlds with varying sizes, and must develop a discrete language (consisting of 1 of 64 symbols) to solve this task.  The authors argue that seemingly innocuous (""unbiased"") shaped rewards in the environment have significant, measurable effects on the learned languages, both qualitatively and quantitatively. The authors both visualize the effect of shaped rewards on the emergent language, and measure the change in entropy to the languages caused by the denser shaped rewards. For example, adding denser rewards increases the entropy (and presumably information) communicated by the language; denser rewards prevent a decrease in entropy for languages generated for larger (more difficult) tasks, suggesting that information bandwidth remains high in the shaped reward setting.  Finally, the authors suggest an alternative hypothesis for the differential entropies observed by language in this game: the size of the experience replay buffer in their PPO algorithm, which roughly corresponds to the frequency of updating the semantics of the learned language. There is some connection drawn here to a batched CRP with varying batch size, though it's not super clear to me how strong this connection is to PPO learning and why this CRP is the right way of thinking about experience buffer sizes.  The core argument of the paper, then, is to more carefully consider how environmental rewards might change the semantics of the learned language and mask other factors of interest in studies of emergent communication.","The paper examines the effect of reward shaping in the context of emergent communication, and specifically with respect to the Shannon entropy of the emerged language. The paper showed that shaping the rewards can lead to higher entropy than training with just the base reward. We care about the entropy because this relates to how expressive the language is. There are existing works, such as Kharatinov et al 2020, which examine the relationship between task/environment and the resulting language entropy.  The paper uses PPO as its main outer-level optimizer, which it uses because the task is a multi-step task which might be challenging to learn with other algorithms such as REINFORCE.  The paper provides theoretical analysis of the effect of reward shaping on entropy by considering an analogy between PPO learning and a process they call the 'Extended Chinese Restaurant Process' (ECRP), where at each step, the customer can be divided into beta parts, and each of the beta bits of customers (which are now each of size 1/beta) takes a seat. This is presented as being analogous to how PPO first store multiple episodes over a buffer, and then updates the weights of the network, using the results of the whole buffer; then rinses and repeats.  Experimental results are presented which show that making the task harder/longer (by increasing the world radius) results in a fall of the entropy of the resultant emergent language, but not in the presence of reward shaping.  The paper asserts that whilst multi-step tasks, and PPO, are not much used in emergent communication literature currently, but it seems plausible that this will become the case in the future, and this paper is a small exploratory step in that direction.",The paper looks into finding effects of reward shaping on emergent language learning with RL. The work shows the difference via analyzing entropy and the behavior of the learned agent. It draws similarities between the Chinese restaurant process and RL. And shows that some of the differences between the behavior of the agent learned with and without reward shaping can be explained by experience buffer size used in the RL learning algorithm. The work uses a new simple navigation task for the study.,0.35833333333333334,0.3,0.16666666666666666,0.20224719101123595,0.09737827715355805,0.12027491408934708,0.16104868913857678,0.12371134020618557,0.24096385542168675,0.18556701030927836,0.3132530120481928,0.42168674698795183,0.2222222222222222,0.17518248175182483,0.19704433497536947,0.1935483870967742,0.14857142857142858,0.18716577540106952
176,SP:713c57555a88d922516f42e7ff0ddd5bfbd90a24,"The paper ""Towards demystifying representation learning with non-contrastive self-supervision"" presents and analyzes a family of algorithms, DirectSet(\alpha), for non-conctrastive self-supervised learning with only positive pairs. The theoretical analysis assumes linear layers, and focuses on a special data distribution assumption, where the input space is separated in two linear subspaces, one invariant under the 'data augmentations', the other being the complement. It is then shown that the proposed algorithm converges to the projection matrix onto the invariant sub-space. Further it is shown theoretically that this has the down-stream advantage reduced sample complexity for learning on this representation. Empirically, it is shown that the method performs on par, or sometimes slightly better, than the previously proposed closely related method DirectPred (Tian et al. 2021). ","In this paper, the authors make theoretical progress on understanding non-contrastive self-supervised learning (ncSSL).  ncSSL has previously demonstrated strong empirical performance, even outperforming contrastive learning, but the theory behind it is still unclear.  In this work, the authors build off of prior analysis by [1], and showcase the role weight decay has in learning a desirable representation; it acts as a threshold that discards noisy features with high variance introduced by the data augmentation, and keeps stable features with low variance.    [1] Tian et al., Understanding Self-Supervised Learning Dynamics without Contrastive Pairs, 2019.","The paper provides a new method for self-supervised learning (SSL), called DirectCopy, based on a previous work DirectPred. An important contribution of this work is theoretical analyses on DirectSet($\alpha$), a theoretical model on linear neural networks that work with arbitrary $\alpha$. The authors prove that the weight decay coefficient $\eta$ has the ability to filter out unnecessary features and hence helps learn useful representations. DirectCopy is a special case of DirectSet($\alpha$) when $\alpha=1$. The new algorithm does not require to compute the burdensome eigen-decomposition while enjoying similar empirical performance as DirectPred. ","This paper attempts to investigate the reasons behind why non-contrastive SSL methods such as BYOL and Sim-Siam do not collapse to trivial solutions, how they learn representations that are related to the data distribution and augmentation process, and how these reduce the sample-complexity of downstream tasks. It heavily draws upon DirectPred (Tian et al) and generalizes the method for directly setting the weights of the predictor network via a parameter that seems to be tied to the strength and distinguishability of the learnt features.   Tian et al: https://arxiv.org/abs/2102.06810",0.15503875968992248,0.15503875968992248,0.13953488372093023,0.15625,0.1875,0.10416666666666667,0.20833333333333334,0.20833333333333334,0.1875,0.15625,0.1875,0.10416666666666667,0.1777777777777778,0.1777777777777778,0.15999999999999998,0.15625,0.1875,0.10416666666666667
177,SP:7260bd50f600a481ec7710792b63f518218e0eaf,"This work investigates the optimality of random permutation as a scan ordering for SGD. They found that for general strongly convex functions with Lipschitz Hessian, random permutations are optimal in high dimension but not optimal in 1-dimension. For general convex quadratics, random permutations are also not optimal. Finally, the authors introduced a new technique termed FlipFlop that works by reversing the permutation of the previous epoch at every even epoch, and at the odd epochs the algorithm just follows its original permutation, whether it be random or cyclic. FlipFlop has been proven to improve the convergence of random reshuffling, single shuffle, and incremental gradient descent on quadratic functions. Experiments on a 100-dimensional quadratic objective and 1-dimensional logistic regression demonstrate that FlipFlop indeed converges faster than random reshuffling.","This paper studies theoretical properties of permutation-based fixed-step size SGD for finite sum optimization. The main theorems state that  * For 1-d Hessian-smooth functions, there exists permutations such that the convergence rate is exponential in the number of iterations $K$ * For higher dimension, there exists strongly-convex function such that the convergence rate is at best $O(1/K^3)$ * If some component in the strongly-convex objective function is nonconvex, the convergence rate has a lower bound $O(1/K^2)$  The paper then proposes the algorithm FlipFlop, where adjacent epochs use permutations of reverse order. The algorithm is proved to improve upon random permutation when all the component functions are quadratic. The results are corroborated with simulations.","This paper is motivated by the observed phenomenon that, in stochastic gradient descent (SGD), without-replacement sampling (random permutation) gives faster convergence than with-replacement sampling. The paper studies whether random permutations are optimal among permutation-based SGD, by considering different deterministic, random, or hybrid ways of generating permutations of input points.  Focusing on optimizing convex functions at a constant step size, the paper shows that: 1. there exist optimal permutations which converges exponentially faster than random permutations for 1-dimensional functions. 2. such improvement is not possible in higher dimensions or for strongly convex objectives, where random permutations are optimal. 3. by reversing the permutation every other epoch (flipflopping), convergence on quadratic functions improves for three permutation-based methods: Incremental Gradient Descent (deterministic), Random Reshuffle (random), and Single Shuffle (hybrid).","# === Update ===  I have decided to maintain my score. This is very strong submission in my opinion. A brief overview of my thoughts is the following:  **Topic**: This paper comes in the middle of a wave of interest in permutation SGD and similar variants. It is highly topical and a good fit for the conference.  **Theoretical Results**: It is easy focus on FlipFlop and its limited analysis (i.e. quadratics only), but there are many other highly interesting results in this submission. Theorems 1 and 2 show that one-dimensional functions are ""easy"" for permutation-based optimization, while quadratics in higher dimensions are hard. Yes, this result requires a fixed step-size, but it is also the first analysis of permutation SGD conducted on the level of specific permutations. Theorem 3 refines these results to show that the improvement in Theorem 1 is specific to quadratics. The proof technique for these results is elegant (continuity of the composition of updates + IVT) and, as a I noted in my review, leads to useful intermediate results.  **Experiments** The experiments are limited to synthetic data, but this is a theoretical paper with ~37 pages of appendices already. I think that an intensive evaluation of FlipFlop is beyond the scope of this submission. Moreover, since the analysis considers quadratics only, I think that the nice results on logistic regression in Appendix H are quite encouraging.  # ======  This submission analyzes the effects of permutation choice on the convergence of the random-reshuffling, shuffle-once, and incremental variants of stochastic gradient descent (SGD). In particular, the goal of this work is to determine settings whether specific permutations can give faster convergence than random reshuffling and in what settings.  The authors show that, in the case of one-dimensional finite-sum functions with smooth Hessians, there exists a sequence of permutations for which SGD converges at a linear (i.e. exponential) rate.  However, they also prove that this phenomenon is specific to one-dimensional functions using a new, dimension-dependent lower bound for any permutation-based SGD method. This lower bound is tight with the known convergence rate of random reshuffling, implying this method is optimal in the general setting. Finally, the authors restrict themselves to finite-sums of convex quadratics and develop a simple heuristic permutation ""schedule"" giving provably faster convergence.  This approach, which is called FlipFlop, alternates between fresh permutations and the previous permutation, but in _reverse_ order. Small experiments confirm that FlipFlop accelerates convergence of permutation-based SGD variants. ",0.18461538461538463,0.2153846153846154,0.3,0.1885245901639344,0.28688524590163933,0.2595419847328244,0.19672131147540983,0.21374045801526717,0.09443099273607748,0.17557251908396945,0.0847457627118644,0.08232445520581114,0.19047619047619047,0.21455938697318006,0.143646408839779,0.1818181818181818,0.1308411214953271,0.125
178,SP:729c3e22a6f0170bdc8e1f511812dc9e9a4fd4a8,"This paper replaces the self-attention layer with a global filter layer:  a sequence of a 2D FFT, einsum, and then 2D inverse FFT. A benefit of the approach is that the attention computation is reduced from O(n^2) to O(nlogn) time-complexity. This demonstrates that GFNet is Pareto optimal versus other Transformer-like models on ImageNet on an accuracy-FLOPs basis.","This paper proposes Global Filter Networks (GFNet), which make use of discrete Fourier transform (DFT) and a global filter layer to mix the input tokens. According to the convolution theorem, the combination of the two operations can be regarded as a depthwise global circular convolution, and thus this work can be viewed as a kind of convolutional network with global convolutional layers. By taking advantage of FFT, the proposed method is faster than previous methods such as self-attention and spatial MLP with comparable performance on ImageNet.",The work proposes a new attention-based architecture for computer vision (classification). The proposed architecture replaces the self-attention layer in ViT with computational blocks involving the Fourier transform. Due to the efficiency of FFT and IFFT the quadratic complexity of self-attention can be reduced to a log-linear computational cost. In order to boost performance distillation of taken features from a pre-trained vision transformer is used. Results are presented on ImageNet as well as on some transfer learning experiments. ,"This paper proposes to replace self-attention in vision transformer by a *Global Filter Layer* consisting of a 2D fast Fourier transform (FFT), a point-wise multiplication with learned weights and an 2D inverse FFT. The proposed layer compares favourably with (a) self-attention as it replaces the quadratic dependency on compute cost and memory footprint with a log-linear rate and (b) variants of MLP-mixer as the number of weights scales linearly with the number of pixels instead of quadratically. The performances of the proposed model are convincing both for supervised classification on ImageNet and fine-tuning to downstream task on smaller datasets.",0.25,0.265625,0.359375,0.1724137931034483,0.2413793103448276,0.23170731707317074,0.1839080459770115,0.2073170731707317,0.21904761904761905,0.18292682926829268,0.2,0.18095238095238095,0.2119205298013245,0.23287671232876714,0.27218934911242604,0.1775147928994083,0.21875000000000003,0.20320855614973263
179,SP:72e0cac289dce803582053614ec9ee93e783c838,"Min-wise hashing (MinHash) is a fundamental and popular algorithm in machine learning. This paper proposes Circulant MinHash (C-MinHash) to approximate the Jaccard similarity in massive binary data. Compared with MinHash, C-MinHash only requires two (or maybe one in practice) random permutations in a circulant manner for approximation. The authors also systematically demonstrate that the C-MinHash can provide a smaller estimation variance than MinHash. Extensive experiments validate the effectiveness of C-MinHash.   ","The paper designs an improved version of the classical MinHash data structure for calculating the Jaccard similarity between two binary strings.  The classical MinHash data structure generates K hash values for a binary string by generating K independent random permutations of the binary string and taking the index of the first “1” in the permuted string as the hash value in each random permutation. The estimator is then the percentage of equal hash values out of K hash values for the two binary strings. This is an unbiased estimator.  This paper’s algorithm, called C-MinHash, first applies a random permutation to the binary string. For the K hash values, instead of using K independent random permutations, the new algorithm uses the same permutation but shifted by 1, 2, …, K positions. The estimator is the same as that of the MinHash and can be easily shown to be unbiased. The paper then shows that this simple scheme, with only two independent random permutations, yields a smaller variance than that of MinHash. ","This paper proposes C-MINHASH to improve vanilla MINHASH. Instead of using K random permutations to generate K hash values, C-MINHASH requires only two permutations. Theoretically, C-MINHASH provides unbiased estimate, and its variance is smaller than MINHASH. Extensive empirical experiments verify the theoretical analysis. ","This paper proposes an effective approach for MinHash by permutating data vectors. It first randomly shuffles the data to break structures exhibited in the original data and then performs permutation K-times to obtain K hash values. Besides, this paper proposes an approach that performs only one permutation to compute hash values. This paper shows the theoretical approximation error of the proposed approach. By using text and image datasets, it shows that experimental results follow the results of the theoretical analysis.",0.32,0.26666666666666666,0.18666666666666668,0.1111111111111111,0.15204678362573099,0.32608695652173914,0.14035087719298245,0.43478260869565216,0.1728395061728395,0.41304347826086957,0.32098765432098764,0.18518518518518517,0.19512195121951217,0.3305785123966942,0.1794871794871795,0.17511520737327188,0.20634920634920634,0.23622047244094485
180,SP:73e6281bf556a6ae92bdcf8d68e6e8973bc8b56b,"The paper ""Robust Predictable Control"" describes a method to learn policies with an information bottleneck between environment observations and a latent representation which is used for decision-making. The bottleneck is posed between *sequences* of observation and latent states, in contrast to prior work, where a bottleneck is posed between individual states and latents independently. A crucial component of the proposed method is an action-conditioned dynamics model in the latent space. The proposed training objective is jointly minimized by training an encoder, latent dynamics model, and policy.  First, this encourages the encoder and dynamics model to yield good predictions of encoded environment observations.  Furthermore, by training the policy, the expected task return is maximized while ensuring that the behaviour of the agent is well-predictable.  The latent dynamics model formulation can be exploited to derive theoretical guarantees between open-loop and information-constrained closed-loop control.  Several experiments show that the information-constrained agent performs well in the nominal case but also on distorted environments and offers smooth transitions between different behaviours depending on the tightness of the information constraint.","This paper proposes the method ""robust predictable control (RPC)"". It proposes to use a information bottleneck between inputs and actions. However, in contrast to previous works, it does so not on a per-observation basis, but on a per-sequence basis using a latent model. Similar to some of the prior work, the encoding cost is not added to the loss, but subtracted from the reward. ","In this paper, the authors propose a new reinforcement learning method, called robust predictable control, that optimizes a policy under limited representational resources. Similar to previous methods, they formalize the problem in a fully observable MDP and learn a compressed representation of the state space using a variational information bottleneck constraint. Crucially, they apply the bottleneck on sequences of states. Through a clever factorization of the variational prior, they obtain a prior dynamics model in the compressed state space. This dynamics model is predictive and the agent only needs additional representational resources if the encoded states are not predicted by the prior dynamics already. Using a set of standard RL benchmarks, the authors show that this set up improves compression and enforces predictable and robust behaviors . ","This paper introduces a novel approach to learning parameterized policies. The way I understand the work: it jointly optimizes a policy objective, a probabilistic dynamic model objective, and a compression objective. A bottleneck encoding distribution is key here, which is subject to both model predictions (given previous action and encoding) and compression (given current observation). In spirit, it is similar to variational information bottleneck (VIB) but can be seen as an extension of that to RL. So, the interpretation of this optimization is that the agent is aiming for encoding distribution that is maximally predictive of the next observation and maximally compressive of the observation while the agent is maximizing the future aggregate reward. The idea is neat. The optimization can also be easily put together. Some theoretical and empirical results are given. ",0.13812154696132597,0.1878453038674033,0.1712707182320442,0.3333333333333333,0.2878787878787879,0.20634920634920634,0.3787878787878788,0.2698412698412698,0.23308270676691728,0.1746031746031746,0.14285714285714285,0.19548872180451127,0.20242914979757087,0.22149837133550487,0.19745222929936307,0.22916666666666666,0.19095477386934673,0.20077220077220076
181,SP:7656b0bd5eb7e46359d8111e5534a07744f5d7ae,"This paper proposes CAT-RS which combines two novel losses for training base classifiers for randomized smoothing. The two novel losses aim at preserving clean accuracy for hard samples and improving the certified radius for easy samples. In term of ACR (average certified radius), the method achieves state-of-the-art on MNIST and CIFAR-10.",This paper proposed new loss functions during the training of classifiers for certified robustness via randomized smoothing. The new loss function treat samples with different confidence level differently. The main idea is to prioritize samples with high confidence because it provides more additional certified radius when its confidence grows.,"This paper proposes a loss function for training the base classifier for randomized smoothed classifiers. Specifically, the loss distinguishes between training examples with high prediction confidence and those with low confidence. Several empirical tricks are applied to design the loss function to optimize the performance. Experiments on MNIST and CIFAR-10 show that the proposed training method is superior to existing state-of-the-art randomized smoothed classifiers, especially when the radius r is large.","The paper studies certified robustness via randomized smoothing (RS). RS has a fundamental accuracy and robustness tradeoff. The authors aim to enhance such tradeoff through a sample-wise control of robustness over the training samples. In particular, the authors investigate the correspondence between robustness and prediction confidence of smoothed classifiers and design a new loss function. The proposed method is evaluated on MNIST and CIFAR10.  ",0.23214285714285715,0.30357142857142855,0.25,0.2857142857142857,0.24489795918367346,0.21333333333333335,0.2653061224489796,0.22666666666666666,0.2153846153846154,0.18666666666666668,0.18461538461538463,0.24615384615384617,0.24761904761904763,0.25954198473282447,0.23140495867768596,0.22580645161290322,0.2105263157894737,0.22857142857142856
182,SP:76b64e6b104818ed26e9331d134df0125d84291c,"This works suggests a novel method on how to harness the representational power of models such as CLIP. In this task the input is distorted with a known function, such as missing pixels or gaussian noise, blurring. This works proposes training a student contrastively by matching the clip representation of the clean image with student's representation of the distorted image. They propose different matching functions and explain why in their setup it will not collapse if one does not compare against every other.  The experimental setup is extensive and shows much better label efficiency when working with distorted images, they also show better noise extrapolation and better dataset tranferability in compare to baseline. Their baseline is a resnet pretrained on imagenet supervised trained on the distorted imagenet-100.","This paper proposes an inversion method that exploits a pre-trained representation for regularization. Given only corrupted images, instead of working in the pixel space, it attempts to find their matching representation pairs in the pre-trained representation space that is trained with clean images by using contrastive objective function. Assuming that one has an access to a powerful representation that is already available, this method provides a robust representation against various types and levels of distortions and train/test time discrepancy. ","This paper proposes to recover the feature representation, pre-trained from CLIP, of a clean input image given a corrupted version of the image. They propose to recover the features, instead of the clean image, as their goal is to use these features for downstream tasks, e.g. classification. Furthermore, this allows for the transfer of knowledge from CLIP, which reduces the amount of labeled data needed for the downstream task. To recover the features, they train a student model via L2-Loss and contrastive loss given the clean feature from CLIP. Empirically, they evaluate on a subset of ImageNet (with only 100 classes) and consider three types of corruptions, random masking, Gaussian noise, and Gaussian blur. Empirical comparison with a baseline method, trained end-to-end with corrupted images, demonstrates their approach is more robust and requires less labeled data.","The paper consider the following problem: Let $x$ be an image, let $R$ be a feature map or representation obtained through the CLIP network, and let $A$ be a distortion process. The representation takes as input an image and yields a feature representation. The distortion process also takes an image as input, and yields a distorted image (e.g., it deletes pixels).  The paper assumes we are given the representations and corresponding distorted images, i.e., a collection $\{A(x_i), R(x_i)\}$, and the goal is to learn a student function $S(A( \cdot ))$ that is equally useful as the original representation $R(x)$ for a classification task. The paper learns the student $S$ by minimizing a contrastive loss, and the utility of the representation obtained by the student $S$ is measured by the performance on a supervised learning task, specifically on ImageNet-100.   The paper provides simulation results demonstrating that the method learns useful representations. The paper's main result is to show that the method 'outperforms a pretrained ResNet, of the same size as the robust encoder, fine-tuned end-to-end on labeled distorted images'. In addition to this result, the paper provides a number of ablations studies and compares to two reasonable baselines.",0.11627906976744186,0.17054263565891473,0.20155038759689922,0.21951219512195122,0.25609756097560976,0.24822695035460993,0.18292682926829268,0.15602836879432624,0.12440191387559808,0.1276595744680851,0.10047846889952153,0.1674641148325359,0.14218009478672985,0.16296296296296298,0.15384615384615383,0.16143497757847533,0.1443298969072165,0.2
183,SP:76e858a6ef79a3bd861803395e25d7f65fd29a00,The authors present a method for iterative small molecule generation based on an autoencoder framework with graph neural networks. The method specifically focuses on the ability to extend molecular scaffolds (predefined subparts of a molecule) with structural motifs and individual atoms. The authors show results on unconstrained molecular optimization tasks as well as tasks in which a scaffold is given. ,"The paper study the problem of fragment-based molecule generation.  They propose a model MoLeR which consists of an encoder of molecular graph using Graph convolutional neural network (two GNN, one for complete molecules and one for partial molecules), a MLP decoder layer to predict the fragment, the attaching atom on the fragment, and the bonding atoms on the partial molecule. To train the model, it includes three losses for multi-task learning: a KL term between variational posterior and prior of latent representation, a self-reconstruction loss, and a property prediction MSE loss.  The paper conduct experiments on GulcaMol and show that it is able to generate molecules similar to the training molecule distribution, from scratch or from a given scaffold. It shows better results than LSTM, JTVAE, CDDD-MCTS, etc.  ","This work proposes a generative model for molecules. The approach uses a library of motifs, extracted from the training data by breaking down molecules through acyclic bonds adjacent to a cycle. The model can also sample individual atoms. The method is set up in an autoencoder fashion with a GCN encoder that learns node and edge embeddings that are projected into a vectorial latent space. The nodes are initialized with features representing which motif they belong to, so the latent space is motif-aware. The generative approach from the latent space is reminiscent of autoregressive models, but it does not marginalize over the full sequential set of steps, and only through single-step transformations. Seeded with a starting atom or motif, the model sequentially samples the motif (or atom) to be connected and which atoms will be connected. Because more than one path can connect an initial and a final product, it is important to train over multiple paths","This paper proposes a graph-based generative model for molecule generation. The proposed framework MoLeR can use scaffolds as the initial seed to incrementlly generate molecules motif by motif or atom by atom so that the generated molecules consist of the specified scaffold. Experiments show the proposed method performs comparably to existing methods on unconstrained molecular optimization tasks and outperforms these methods on scaffold-based tasks. At the same time, the proposed method is more efficient due to that it is not conditioned on the generation history.",0.2833333333333333,0.23333333333333334,0.2833333333333333,0.16666666666666666,0.13636363636363635,0.12578616352201258,0.12878787878787878,0.0880503144654088,0.19540229885057472,0.13836477987421383,0.20689655172413793,0.22988505747126436,0.17708333333333334,0.1278538812785388,0.23129251700680276,0.15120274914089346,0.16438356164383564,0.16260162601626016
184,SP:7782a99e3c41ff523c0c56bfbe399c855a77acf2,"This paper presents a new conditional variational autoencoder (VAE) approach to learn a low dimensional embedding of neuropsychiatric disorders from resting-state functional connectivity data. The proposed approach uses diagnostic information in 2 ways: 1) to cluster samples from the same disorder together via the conditional VAE model, and 2) to separate the clusters using contrastive learning.  The method is compared against other dimension reduction approaches on both a synthetic dataset and 2 real datasets which showed a consistent nosological relation among 3 disorders. ","The authors present a new conditional VAE, which uses diagnostic information as a supervision signal in two different ways, with the aim to identify small latent embedding spaces for functional brain connectivity that preserve diagnostic information. The authors have potential applications in computational psychiatry in mind, which are indeed extensively discussed in the field (transitioning to a continuous nosological approach to mental illness). They compare their approach on synthetic data to various related approaches and ultimately apply it to two functional connectivity datasets from patients with different psychiatric diagnoses, which appears to give good results.  Their main contribution is technical and lies in defining a new cost function to learn the embedding space. Their work incrementally builds on existing work, but appears to be sufficiently novel. There is also a moderate empirical contribution as they analyze the relationship between MDD, ASD and SCZ.","This paper presents a method to reduce the dimensions of resting-state functional connectivity for psychiatry disorders such as autism spectrum disorder, major depressive disorder, and schizophrenia. Their method is based on a conditional variational auto-encoder that utilized the diagnostic label. They evaluated their method on two neuroimaging datasets and obtained clustered 2-dimensional representations of multi psychiatry disorder.","The authors propose a novel variational autoencoder to utilize functional connectivity (FC) features from resting state fMRI (rs-fMRI) scans in order to uncover latent nosological relationships between diverse yet related neuropsychiatric disorders. The autoencoder  seeks to learn a mapping  from high-dimensional FC space to a low-dimensional embedding space that is constrained to preserve pairwise relationships between the diagnostic attributes of the disorders. From a clinical standpoint, this work studies the nosology of complex disorders using a continuous dimensional characterization rather than using categorical and discrete diagnostic labels for supervision.    The authors validate their framework on both synthetic data and two separate clinical rs-fMRI datasets consisting of patients diagnosed with Autism Spectrum Disorder (ASD), Major Depressive Disorder (MDD) and Schizophrenia (SCZ). Their experiments evaluate the consistency of uncovered pairwise nosological relationships inferred from their latent representations.  They demonstrate that their model is capable of reliably inferring a dimensional characterization of multiple brain disorders beyond diagnosis labels. ",0.2261904761904762,0.21428571428571427,0.2619047619047619,0.09090909090909091,0.1958041958041958,0.31666666666666665,0.13286713286713286,0.3,0.13836477987421383,0.21666666666666667,0.1761006289308176,0.11949685534591195,0.1674008810572687,0.25,0.1810699588477366,0.12807881773399016,0.18543046357615894,0.1735159817351598
185,SP:779821ed85084f8bf1b29d8822b312989b186ee9,"The paper proposes a GNN-based extension of transformers, which have been shown to be effective for reaction prediction etc. before. In particular, the GNN-based embedding of molecules in the reaction embeddings overcomes the artificial bias inherent in the often applied sequence embeddings. The experiments show that there are sometimes increases of performance in reaction and retrosynthesis prediction - and the approach could be applied to similar problems.   ",This paper proposes a graph-to-sequence architecture called Graph2SMILES for the retrosynthesis and the reaction outcome prediction. Graph2SMILES uses an attention-augmented D-MPNN encoder to capture the local information and a global attention encoder with graph-aware positional embeddings to capture the global information. Experiments show that Graph2SMILES is competitive with Transformer baselines but does not outperform state-of-the-art methods on tasks of the one-step retrosynthesis and the reaction outcome prediction. ,"The authors proposed a new method for retrosynthesis, which does not require the mapping numbers and extracting templates from the literature.  Basically, the model consists of a graph-based encoder and a sequence based encoder. The encoder consists of local aggregation from neighbors and global attention using a new positional method. The decoder is a Transformer model with relative positional encoding.  The method achieved promising results on several retrosynthesis datasets. ","This paper proposes a graph-to-SMILES framework, which incorporates several recently developed engineering techniques from the community, for synthesis planning and reaction outcome prediction tasks. The proposed method leverages graph neural networks and Transformer attention model to encode the graph inputs and then utilizes a Transformer decoder to generate the SMILES string as outputs. Experiments on benchmark retrosynthesis and reaction prediction tasks show that the proposed approach outperformed the vanilla SMILES-to-SMILES transformer baseline, but obtained inferior results than some other advanced methods. The paper is interesting, but both the technical novelty and the experimental studies are weak to me.",0.2647058823529412,0.16176470588235295,0.23529411764705882,0.23684210526315788,0.35526315789473684,0.2,0.23684210526315788,0.15714285714285714,0.1568627450980392,0.2571428571428571,0.2647058823529412,0.13725490196078433,0.25,0.15942028985507248,0.18823529411764706,0.2465753424657534,0.3033707865168539,0.16279069767441862
186,SP:7997a1b59ef2fc0127c3fff02d191f5d655168f8,"This paper addresses the problem of sound event detection and localization from multi-channel raw audio waveforms. Essentially, a different audio front end feature extraction scheme - ""Synperiodic Filterbank"" if proposed which can be parameterized and jointly learnt along with a backbone classifier in an end-to end manner. The feature extraction scheme proposed is grounded in principle of uncertainty governing time-frequency resolution. Through ablation studies the authors show that this feature extraction scheme performs better (albeit marginally) compared to some of the traditionally used feature extraction schemes like MFCC, and LFBEs. Subsequently, the authors show that combining the proposed ""Synperiodic Filterbank"" with a significantly large model (~3X times the closest baseline model in terms of No. of trainable parameters) based on the Transformer Architecture offers a dramatic improvement in the results compared to the baseline. ","This paper proposes a method for improving the detection and the localization of several sound sources using a 4-channel signal (The addressed configuration, determined or under-determined case is not detailed in this paper). The proposed approach is based on a so-called ""synperiodic"" filter bank representation that is used as the input of a deep convolutional neural network. Finally method proposed by the authors is comparatively evaluated on the DCASE2020 dataset and pretends to provide the best results.  ","This submission addresses the sound source detection problem. Compared to previous method, the authors propose a novel synperiodic filerbank compared to syndistance bank, which learns a data-dependent time-frequency resolution map. Experiments show that the proposed method achieves state-of-the-art performance on several datasets. ","I am sorry, by mistake I posted the incomplete review version!   This manuscript proposes a sound source localization method utilizing ""synperiodic filter"" bank representation as to the input of a deep convolutional neural network.  Authors claim that the convolution of the proposed filterbanks with the raw waveform helps to achieve multi-scale perception in the time domain which results in performance improvement.  The authors demonstrated that their method could outperform some of the state-of-the-art similar work.   ",0.14705882352941177,0.1323529411764706,0.13970588235294118,0.15,0.275,0.3191489361702128,0.25,0.3829787234042553,0.24050632911392406,0.2553191489361702,0.27848101265822783,0.189873417721519,0.18518518518518517,0.19672131147540986,0.17674418604651163,0.1889763779527559,0.27672955974842767,0.2380952380952381
187,SP:79da8f6cacc8386e02bab32154e7eaefbe2c683c,"The paper proposes a novel method for open-world object detection, where instances of unknown categories need to be identified and annotated data for such new categories need to be integrated into the model in an incremental fashion. Prior work typically separate this problem into two tasks, out-of-distribution detection and incremental/continuous learning. This paper proposes to use fixed semantic anchors for each category, which are embedding vectors from a language model (or randomly generated vectors) for each category.  Importantly, when new data arrives, new embeddings are added while previous ones do not change. This encourages the feature representation to be compact (discriminative) and consistent (over time).","This paper proposes a semantic topology embedding for Open-World Object Detection (OWOD) where an object detector identifies objects of unknown classes and incrementally learns to classify them assuming that their annotations are progressively given by humans. To maintain discriminative and consistent relationships among object classes, the authors introduce a semantic topology for the feature space of the detector by constructing pre-deﬁned anchors for categories using a pretrained language model. During training, it enables the detector to distinguish unknown objects out of the known categories and also makes learned features of different classes undistorted during incremental learning. Eperimental results show that the semantic topology improves state-of-the-art open-world object detectors and help the open-world detectors preserve a discriminative and consistent feature space.","This paper extends the recently-introduced object ORE object detector [A]. ORE is trained in an incremental fashion and was shown to minimize the confusion between classes presented in different task sets by adding a contrastive objective to the model training, that pushes features, representing different semantic classes, far away in the feature space. This paper presents the case that combining ORE detector with large-scale language models, trained by aligning textual queries and images, can significantly improve ORE detector results. This is not surprising: it was already shown that such multi-modal language models could be successfully used for detection of novel classes in a zero-shot setting (in which names of target classes are given, but image data for these classes is available during the model training, see [D]). To the best of my knowledge, the core idea of semantic anchoring using language models was introduced in [C] (see Sec. 3.3 Densely Sampled Embedding Space). In this work, the semantic embedding space is extended with additional data from external sources that contain semantic information about the unseen classes via language embedding to image embedding alignment. This method suggests using CLIP [B] instead (which makes sense and simplifies the knowledge transfer!)  [A] Joseph et al., CVPR’21 [B] Radford et al., ICLR’21 [C] Bansal et al., ECCV’18 [D] Xian et al., CVPR'17 [E] Zheng et al., CVPR’21","The paper addresses the problem of open world object detection - a lifelong learning system where the object detector is required to detect objects belonging to all classes known so far and “unknown” objects. Unknown objects are annotated and available for training at a later stage. At a given time only a specific set of training data is available with annotations that involve all known classes so far, while all past training data are unavailable.    The proposed approach builds upon recent work [A], by incorporating a semantic topology in the object feature space (RoI features in Faster R-CNN). While [A] use contrastive clustering to group object features based on object category, and energy based out-of-distribution detection to detect “unknown” objects, the authors propose an end-to-end trainable approach. The proposed approach uses RoI features in two parallel streams, with one focused on clustering operation, while having object category prediction loss on both streams of features. Features are clustered around predetermined semantically meaningful object representations. Pre-trained word embeddings from a language model (CLIP) are used for this purpose. The use of pre-defined semantic anchors allows learned features of known objects to form consistent clusters during incremental learning.    Experiments are conducted using 80 categories from PASCAL-VOC and MSCOCO datasets. The problem is designed as 4 sequential tasks each involving 20 object categories (same setup as in [A]). Detection performance is measured using mAP, as well as measures that focus on unknown object detection.  [A] Towards Open World Object Detection, Joseph et al., CVPR 2021  ",0.23853211009174313,0.1651376146788991,0.26605504587155965,0.2421875,0.3046875,0.15879828326180256,0.203125,0.07725321888412018,0.1124031007751938,0.13304721030042918,0.1511627906976744,0.1434108527131783,0.21940928270042193,0.10526315789473685,0.15803814713896458,0.17174515235457063,0.20207253886010362,0.15071283095723012
188,SP:7a04efbf835c238bbdf70a8b8decee4ec2907a3a,"This paper proposes two methods for estimating smoothness of the prediction function learned by a deep network :   1/ an extension of Rahaman et al. to multiclass classification, by adding a sine of desired frequency to the target vector, then training on the new target vectors. The ""effective noise fitting"" is then the difference between the validation loss on clean targets and the validation loss on modified targets.  2/ a measure of change of the logits in l2 norm between 2 training examples, averaged over many examples  Using their methods, they show the effect on the smoothness of the predicted function of:  - varying the number of parameters  - adding explicit regularization (weight decay/mixup)  - distillation","This paper proposes a set of tools to probe spectral bias of deep neural networks, that is their tendency to learn low frequency, simpler functions earlier in training, whereas high frequencies are fit later. Authors propose adding noise to the labels through a target function via label smoothing, where the frequency and direction of the target function in image space can be varied. Moreover, they introduce a linear interpolation technique on validation data to probe the smoothness of the learned function along paths connecting natural images. Authors perform extensive experiments to demonstrate how the proposed tools can be used to investigate the spectral effect of training parameters such as model size and different forms of explicit and implicit regularization.","This work presents an empirical study of how different training aspects affect the spectral bias of neural networks in practice. To that end, the authors propose to inject label noise of different frequencies to the CIFAR10 dataset and suggest that the time a neural network takes to start overfitting this noise is a good metric of its spectral bias. They also propose to measure the variability of the loss landscape in the linear interpolation path between two images as proxy for spectral bias. Experiments on the effect of model architecture, explicit regularization, and data augmentation suggest that deep neural networks exhibit a strong spectral bias, in practice, which can be modulated by different design factors.","The paper focuses on the spectral bias of neural networks, i.e., their tendency to learn first the low-frequency information. The goal of this work is to extend the spectral bias into practical image recognition networks, i.e., beyond the fully-connected nets and the NTK regime that it was previously studied. To that end, the paper conducts a number of studies in both explicit and implicit regularization schemes used in practice, and provides links for the success of model distillation. ",0.24778761061946902,0.19469026548672566,0.1504424778761062,0.23529411764705882,0.21008403361344538,0.1826086956521739,0.23529411764705882,0.19130434782608696,0.2073170731707317,0.24347826086956523,0.3048780487804878,0.25609756097560976,0.2413793103448276,0.19298245614035092,0.17435897435897438,0.23931623931623933,0.24875621890547264,0.2131979695431472
189,SP:7aa09356b2c85d54933c0d0d89a3f8fe2e37b27b,"Motivated by the literature showing that the primate visual system has two pathways (""ventral"" and ""dorsal"") with distinct functional specializations, the authors try to build a model of a visual system that also has representationally and functionally distinct regions. They turn to calcium imaging data from mouse visual cortex, which allows them to compare learned representations of ANN to the neural representations found in distinct cortical areas of animals shown the same images. (There aren't publicly available datasets with simultaneous recordings from ventral and dorsal areas in primates, as far as I know.) The authors find that a two-pathway 3D ResNet model trained on the contrastive predictive coding (CPC) task and a dataset of natural videos has layers that overall explain more of the mouse calcium responses in more cortical areas than several baseline models, including the same architecture trained on the same dataset but with a supervised task. In addition, the two pathways in the self-supervised model transfer better to different tasks (object recognition and motion discrimination), which is reminiscent of some of the functional specialization between primate ventral and dorsal pathways.","The authors show that a two-stream 3d ResNet architecture trained using a contrastive predictive coding (CPC) objective develops a representation that resembles the ventral/dorsal stream split in visual cortex. Although I do not buy into all claims of the paper, I am generally very supportive of it. The authors make their main point and the paper is well written. With some adjustments of the claims it will be a great paper.   ### Update after discussion period  Given the results from the control experiments reported by the authors, I support publication of the paper and increased my score to 7. I do ask the authors to include those results in the final version of the paper.","The authors address the challenge of training a single deep neural network, using a single loss function, which captures the specialized neural responses in both the ventral (“what”) and dorsal (“where”) pathways of mouse visual cortex. The authors demonstrate that a deep neural network comprised of two parallel convolutional hierarchies, trained with a single self-supervised predictive objective, captures neural responses in the ventral and dorsal streams better than a neural network comprised of a single convolutional hierarchy (which captures only the ventral stream), and better than either architecture trained with a supervised objective. Furthermore, by evaluating their two-pathway model on “what” and “where” downstream tasks —object categorization and motion detection— the authors show that their model has learned functionally specialized representations in each of its two pathways.","The paper uses contrastive predictive coding to train deep networks on UCF101 videos. It then uses representational similarity to compare their representations to those of mouse calcium-imaging data from the Allen Brain Observatory. The key result is that a two-stream network is a good match to mouse visual cortex data. One of the (structurally identical) streams develops a representation that is a good match to dorsal mouse areas, and the other matches ventral mouse areas. Deeper areas correspond better to areas higher in the mouse visual hierarchy. Furthermore, the ventral stream of the model better supports transfer to CIFAR-10, while the dorsal stream better supports discrimination of random-dot motion direction. ",0.13440860215053763,0.1989247311827957,0.13978494623655913,0.19827586206896552,0.1724137931034483,0.16279069767441862,0.21551724137931033,0.2868217054263566,0.22807017543859648,0.17829457364341086,0.17543859649122806,0.18421052631578946,0.16556291390728475,0.23492063492063492,0.1733333333333333,0.18775510204081636,0.17391304347826086,0.17283950617283952
190,SP:7abf578ef0d3b67d87437bdd1cff129f72c102c6,"The paper addresses the extrapolation problem where a test sequence longer than training sequences is given and proposes Attention with Linear Biases (ALiBi) that adds a penalty linear to the distance between a query and a key to the attention scores. ALiBi shows remarkable input length extrapolation ability while computationally efficient with almost marginal overhead compared to the standard transformer. Moreover, ALiBi does not induce any additional parameters and generalizes well to a billion scale language model.","This paper studies input length extrapolation for Transformer language models; i.e., how Transformer LMs perform on test sequences that are longer than training sequences. The paper finds that how positions are encoded plays a crucial role for input length extrapolation. Models with sinusoidal and rotary position embeddings do not extrapolate well, while T5’s position-dependent attention mechanism (dubbed T5 bias) enables better extrapolation. The paper then proposes ALiBi, another attention mechanism that also allows extrapolation while being computationally more efficient than T5 bias. These results are empirically confirmed on two datasets.","This paper investigates the extrapolation capability of transformer-based language models. The authors observed that existing positional encoding methods (e.g., sinusoidal embedding, relative positional embedding) fail to generalize to longer sequences in language modeling tasks. Therefore, they introduce a new positional encoding method called ALiBi, which adds temporal bias to the multi-head attention to penalize attention score proportional to token distances. Experimental results show that ALiBi has significantly stronger extrapolation capability compared to other positional encoding methods.","The submission proposed an effective approach to allow pre-trained transformer-based language models to extrapolate beyond the maximum length used in training, which potentially reduces the training time as extrapolation is empirically guaranteed. The proposed method adds fixed biases to the dot-product values between queries and keys that linearly decays w.r.t. the gap between two positions. Empirically, the proposed method indeed successfully allows pre-trained models to be evaluated on sequences that are multiple times longer than the training ones without significant loss.",0.2077922077922078,0.19480519480519481,0.15584415584415584,0.16129032258064516,0.10752688172043011,0.16455696202531644,0.17204301075268819,0.189873417721519,0.13793103448275862,0.189873417721519,0.11494252873563218,0.14942528735632185,0.18823529411764708,0.19230769230769232,0.14634146341463414,0.17441860465116282,0.11111111111111112,0.15662650602409636
191,SP:7b04b45c4dd237d69321d280dcdcbc89fb362015,"This paper presents a novel imputation method for high-dimensional datasets that typically serve as benchmarks for machine learning methods. This method (EMFlow) innovates by training a normalizing flow network to map input data samples to a multivariate Gaussian, where imputation is performed via an online version of expectation-maximization (EM), which is commonly used for missing data imputation. EMFlow is applied across regression tasks in datasets in the UCI machine learning dataset repository, and standard image classification datasets, MNIST and CIFAR-10. Empirical results show strong performance for missing data imputation, as well as downstream classification from these imputations, and the model design choices and well-constructed architecture make for easy training and fast optimization convergence.","The authors propose a novel architecture EMFlow for missing data imputation. The authors also show the results of various experiments with multivariate and image datasets. Finally, the authors report the accuracy of post-imputation classification on image datasets.","The paper aims at imputing missing data which are MCAR and MAR.  For modeling the observed data distribution, it utilizes the framework of normalizing flow of which the latent variable/source variable space is Gaussian. By assuming the consistency of inter-feature dependencies in the latent/source variable space, it applies online EM for the imputation of the latent space variables. In the experiments, the proposed method, EMFlow is compared with GAIN, MisGAN, and MCFlow.","This paper presents a model named EMFlow, which performs data imputation in the latent space using the online EM algorithm together with the normalizing flow models. The normalizing flow models aim to capture the complete data density $p_X$ and the bidirectional mapping between the data space and the latent space, even when the data is only partially observed. The parameters in the latent space are updated using an online EM algorithm. Thanks to the feature-wise mapping, the dependency between features in the data space is carried over to the latent space and hence the imputation can be done. Evaluation using ten UCI datasets, MNIST, and CIFAR-10 datasets show impressive improvement against baseline models and the convergence is faster than MCFlow.",0.1111111111111111,0.1111111111111111,0.2222222222222222,0.2631578947368421,0.3157894736842105,0.3333333333333333,0.34210526315789475,0.17333333333333334,0.21138211382113822,0.13333333333333333,0.0975609756097561,0.2032520325203252,0.16774193548387095,0.13541666666666666,0.21666666666666665,0.17699115044247787,0.14906832298136646,0.2525252525252525
192,SP:7bee8d65c68765cbfe38767743fec27981879d34,Experiments are convincing. The provided code is helpful for researchers who need a fast computation of NTK. Though the ideological (mathematical part) is very simple.,"This work aims to solve the computation problem of the finite-width neural tangent kernel (NTK), which is a central object in deep learning. The authors analyze the computation and memory requirements for finite-width NTK and propose two novel algorithms that can improve efficiency. Open-source has been provided by the authors.","This paper studies the practical compute and memory requirements to computing the Neural Tangent Kernel (NTK), introducing two new approaches to doing so for standard NN primitives (structured derivative and NTK-vector product) which each have advantages (in terms of variables like batch size, output dim) over the naive jacobian contraction method. The authors provide experiments demonstrating the advantages of their approaches across a range of architectures and hardware. The authors provide open-source code which seems to be integrated neatly with the JAX and Neural Tangents frameworks.","This paper studies an in-depth analysis of runtime and memory requirements for computing the finite-width NTK. The authors analyze computing costs of Jacobian-vector products (and vice versa) for both fully-connected and convolutional neural networks. They also improve the NTK computation cost by leveraging the structure of neural networks resulting in drastic speedup and memory saving. Finally, they make all their implementations open-source based on the JAX library.",0.24,0.2,0.2,0.3018867924528302,0.3018867924528302,0.2159090909090909,0.11320754716981132,0.056818181818181816,0.06944444444444445,0.18181818181818182,0.2222222222222222,0.2638888888888889,0.15384615384615385,0.08849557522123894,0.10309278350515463,0.22695035460992907,0.25599999999999995,0.23750000000000002
193,SP:7d5ec55a01247b65e4a8f1973d448214585d6baa,"This paper is about two things: understanding learned NLP representations by finding and understanding similarities between *different* representations; and comparing these representations to fMRI data. The paper takes a new approach to both problems, and finds a dimension in the space of language representations that seems to correspond to something like ""semantic depth of analysis."" Furthermore, the comparison with fMRI data shows that many representations are correlated with brain activations.","The submission proposes to study similarities between language representations in different language models. First, a similarity measure between language representations is derived. Then, the matrix of similarities is reduced to two dimensions using MDS. The authors then discuss qualitatively this low-dimensional space of language representations. Then, linear encoding models of fMRI recordings are used to project the first MDS dimension on the cortical surface of a subject listening to narratives. Finally, the matrix of similarities is used to predict the performances of linear encoding models. ","* The authors propose ""representation embeddings"" (vectors describing transfer performance from task i to task j) as a low-dimensional space describing the content of language representations. * They analyze the derived representation embedding space and claim that the main principal component tracks ""depth"" (line 192) of linguistic processing. * They conduct a brain encoding task and evaluate 100 models on this task. They claim that a model's brain encoding performance tracks with its behavior in this low-dimensional representation embedding space, and that this low-dimensional behavior also maps intuitively onto the spatial representation of language in the brain.","Using a slightly modified (but relatively straightforward version) of an analysis that’s recently been called ‘neural taskonomy’, this work attempts to use the relationships (read: transfer learning affinity) among different natural language representations to assess the organization of linguistic representation in human fMRI data via regularized linear encoding models.  The authors first contribute a modified version of a previously used encoder-decoder framework to create a 'task' (linguistic representation) affinity matrix that demonstrates conspicuous, interpretable structure when reduced via MDS to fewer projections. They then use the structure discernible in this matrix to ascertain whether similar structure (specifically, a hierarchy of representation from word embeddings to later, deep transformer layers) is evident in the brain. ",0.21428571428571427,0.18571428571428572,0.22857142857142856,0.18604651162790697,0.23255813953488372,0.17346938775510204,0.1744186046511628,0.1326530612244898,0.13793103448275862,0.16326530612244897,0.1724137931034483,0.14655172413793102,0.19230769230769232,0.15476190476190477,0.17204301075268816,0.17391304347826086,0.19801980198019803,0.1588785046728972
194,SP:7e35e4e610e75c922f2b5219ce625e417f010eeb,"This paper proposes to learn GMM priors in autoencoders, to be able to construct a generative model. They compare the generation quality of the proposed approach with VAEs using the FID metric. One interesting contribution seems to be the fact that they are using a loss function other than the GMM likelihood to fit the GMM. ","The paper proposes a deterministic autoencoder that imposes a GMM on the latent space to learn a complex multimodal latent space. The model uses two regularizers including the KS distance and a constant covariance term to minimize the discrepancy between the marginal CDF of the empirical posterior and the given GMM prior. The authors provide multiple experimental results to show the reconstruction and sampling quality, deep clustering, and the latent model capability to generate discrete and complex structures such as chemical molecules.","The paper builds on the ideas of Ghosh et al 2020 [10] pointing out the equivalence between Gaussian prior VAEs and deterministic AEs with noise inputation and further equivalence with regularized deterministic AEs. Since the original paper [10] needed to estimate the latent distribution ex post with a GMM, the authors here propose to use GMM prior directly and translate this into a deterministic AE with more complex regularization derived from univariate Kolmogorov-Smirnof distance combined with another term for covariance matching.They further propose a heuristic to find a reasonable heyper-parameter setting for the two regularizers. Finally, they show on a battery of experiments (generations over standard image datasets, unsupervised clustering, generating discrete structures) the favourable properties of their method.","The author(s) motivate and derive a novel deterministic auto-encoder loss that naturally encourages a multi-model latent representation that follows a GMM. The author(s) motivate and derive this loss using the KS test metric and generalize it to multiple dimensions and multiple modes. Their experiments, whereby they ancestrally resample latent codes show an impressive ability to generate realistic samples.",0.25,0.23214285714285715,0.16071428571428573,0.2073170731707317,0.2073170731707317,0.09836065573770492,0.17073170731707318,0.10655737704918032,0.14516129032258066,0.13934426229508196,0.27419354838709675,0.1935483870967742,0.20289855072463767,0.14606741573033707,0.15254237288135594,0.16666666666666663,0.2361111111111111,0.13043478260869565
195,SP:7effe51275b9b2e14b3e099533e410e09f5b7c5a,"In this paper, the authors focus on a more general cross domain imitation learning problem where only expert demonstrations from one domain is available. To solve such a problem, the authors use the Gromov-Wasserstein distance to align and compare states between tasks from different domains and propose a Gromov-Wasserstein Imitation Learning (GWIL).  They also show theoretically the possibilities and limitations of GWIL. ","This paper presents a method to transfer policies between different MDPs based on the minimization of Gromov-Wasserstein distance. This distance provides a pseudo-reward that can be used to learn via RL the optimal policy in the target MDP given an optimal policy in the original MDP. The method is optimal if the MDPs can be mapped into each other through an isometry, but works also empirically in other cases. ","This paper frames cross-domain imitation learning as an optimal transport problem using the Gromov-Wasserstein distance. This problem is highly relevant to imitation learning settings where there is often substantial domain mismatch between action and state spaces, eg. a humanoid robot learning to walk from a human demonstrator. The paper introduces a reward function that can be optimised and proves that this is equivalent to minimising the Gromov-Wasserstein distance between state action occupancies of an agent and expert. Substantial discussion/ proofs are included to show that minimising the Gromov-Wasserstein distance is equivalent to recovering an optimal policy up to an isometry. This is both a blessing and a curse, as it allows for optimal policies to be recovered under extreme changes in domain or differences, but does mean that recovered policies could be entirely unsuitable due to isometry.   The paper is well written and concisely written, although does get excessively mathy at times, when a figure could be more helpful. Experimental results corroborate the proofs and propositions, and highlight the value of the proposed approach.   ","A method is proposed for cross-domain imitation learning, without resorting to any form of correspondence. This is done using a Gromov-Wasserstein distance between policies (in practice, Euclidiean distances on collected state-action pairs,  within a given domain), which finds isometric transformations that best preserve distance measures, between the two domains. Given an imitation domain and an expert domain with example trajectories, a pseudo-reward is computed  based on the degree to which the distances from a state to its neighbors in the imitation domain, are preserved in the expert domain. Given these pseudo-rewards, as computed for collected episodes, SAC is used as an RL algorithm to optimize the policy.  The paper contributes both a theoretical analysis and experiments with:  U-maze, pendulum-to-cart-and-pole, and half-cheetah-to-fallen-walker. ",0.15625,0.359375,0.265625,0.30985915492957744,0.2676056338028169,0.16292134831460675,0.14084507042253522,0.12921348314606743,0.1259259259259259,0.12359550561797752,0.14074074074074075,0.21481481481481482,0.14814814814814814,0.1900826446280992,0.1708542713567839,0.17670682730923692,0.18446601941747573,0.18530351437699683
196,SP:7f2640f18294519a5abb1daaa226800d2377a5e0,"# === Update === #  I appreciate the thorough response to my review provided by the authors. I have looked through the newest version of the submission and find it to be much improved. The inclusion of repeats for the train loss / test accuracy comparison is great to see and the updated figures are mostly fixed. The experiments now make a meaningful argument for Eigencurve as a (non)-convex optimization method.  I have increased my score to 6 to reflect the improvements and the author response.   # ====== #  This submission proposes a new step-size schedule for stochastic gradient descent which leverages the eigenvalue spectrum of Hessian to speed-up convergence.  The approach, dubbed Eigencurve, is shown to achieve the minimax optimal convergence rate for stochastic gradient descent on quadratic functions under an additional condition that the eigenspectrum of the Hessian decays according to a power law. When this decay condition is not satisfied, Eigencurve still improves upon the popular step-decay schedule; in this case, it is sub-optimal by a factor of $\\log(\\kappa)$ rather than the $\\log(T)$ factor of step-decay. A lower bound is also provided verifying that the $\\log(T)$ sub-optimality of step-decay is tight and cannot be improved for quadratics. The submission concludes with an empirical investigation of Eigencurve for (non-convex) optimization of several popular neural network architectures on the CIFAR-10 and ImageNet datasets.","This paper studies the convergence of SGD with different learning rate schedules and aims to achieve minimax optimal convergence rates on quadratic objectives. To this end, this work proposes a new learning rate schedule (named Eigencurve) based on the Hessian spectrum and provides an optimal last-iterate convergence rate. The proposed Eigencurve gives rise to slightly improved performance on image classification tasks with deep neural networks. In addition, the introduced schedule is similar to the popular cosine learning rate schedule on some problems, which, to some extent, justifies the effectiveness of the cosine schedule.","The authors propose Eigencurve, a new approach to learning rate scheduling that utilizes information form the eigenvalues of the Hessian. They show that this scheduler obtains the minimax optimal rate on the noisy quadratic problem. Empirically, this scheduler demonstrates faster convergence on CIFAR-10 and ImageNet, especially when the number of epochs is small.","The paper studies convergence rates of SGD with different stepsize schemes, in the context of linear regression. For convergence of the last iterate of SGD, the best known result still misses a $\\log T$ factor compared to the minimax rate. This work aims to fill this gap with an improved stepsize scheme that utilizes the eigenvalue distribution of the Hessian. When the true Hessian is known, the proposed method successfully fills the gap, provably and in the sense of the worst problem instances. When the true Hessian is hard to know, practical variants are also proposed, and are shown to be comparable to the state-of-the-art stepsize schemes in standard deep neural network benchmarks.",0.1091703056768559,0.09606986899563319,0.13973799126637554,0.14893617021276595,0.24468085106382978,0.2962962962962963,0.26595744680851063,0.4074074074074074,0.27586206896551724,0.25925925925925924,0.19827586206896552,0.13793103448275862,0.15479876160990713,0.1554770318021201,0.18550724637681157,0.18918918918918917,0.21904761904761905,0.18823529411764706
197,SP:7fda4f67daf3eb27cdfafe8f8a3f8d719da956c3,The paper proposes an approach for few-shot imitation learning that jointly meta-learns a high-level policy and a set of low-level policies from a diverse set of demonstrations (multi-task). It then finetunes both high-level and low-level policies on few demonstrations of the target task for imitating them. In experiments on the MetaWorld50 task suite the proposed approach outperforms few-shot imitation methods that either don't use hierarchy or don't use meta-learning. ,The paper describes Dual Meta Imitation Learning (DMIL) which is a hierarchical meta imitation learning method where the high-level network and sub-skills are iteratively meta-learned with model-agnostic meta-learning (MAML). The DMIL is a hierarchical extension of MAML-based imitation learning (IL) and is a meta-learning extension of Hierarchical Imitation learning (HIL).  The authors provide theoretical proof of the convergence based on the connection with the Expectation-Maximization (EM) algorithm. They showed the state-of-the-art few-shot imitation learning performance on the meta-world benchmark.,"This paper proposes a meta imitation learning framework aimed at learning long-horizon robot control tasks with fast adaptation capabilities. Specifically, the proposed approach adapts the model-agnostic meta learning framework for learning a hierarchical policy, where both levels of the hierarchy are meta-trained and fine-tuned at test time to learn new tasks. On the metaworld benchmark, the proposed method outperforms prior work, and qualitative analysis shows that the method discovers meaningful skills.",The authors propose to learn a high level policy network that picks a subskill policy to predict actions with meta imitation learning (DMIL). Both the high and low level policy networks are fine-tuned at meta-test time. They show that DMIL converges and evaluate their approach against several ablations on the ML10 and ML45 setting of the meta-world benchmark.,0.2625,0.2,0.1875,0.20652173913043478,0.20652173913043478,0.21333333333333335,0.22826086956521738,0.21333333333333335,0.2459016393442623,0.25333333333333335,0.3114754098360656,0.26229508196721313,0.24418604651162792,0.20645161290322583,0.21276595744680848,0.22754491017964074,0.2483660130718954,0.23529411764705885
198,SP:801a61d01d3b159f301013b182150a80fbfe8fa2,"This paper releases a new dataset with human and machine generated contradictory contexts for QA pairs from SQuAD 1.1. Amazon Mechanical Turk Workers are shown a paragraph and are asked to make edits such that it contradicts the original paragraph with respect to elements such as time, outcome, purpose, location, etc. In addition, the authors fine tune BART on a collection of masked constituent parses of Wikipedia sentences and it is then trained to fill the mask with an alternative phrase.  To automatically generate contradictory contexts, the authors use this fine tuned the BART model on the masked constituency parse of paragraph sentences. A dataset of 10,000 paragraphs from SQuAD  are transformed (once by mechanical Turk workers, and the rest by three different transformations by the BART Model). The paper presents experiments on this dataset for the task of QA -- specifically, machine reading comprehension. In one experiment, the QA system is first trained to predict which of the 5 (1 real + 4 contradictory) is correct. Then an off-the-shelf span based passage reader returns spans as answers. In the second experiment, the performance of QA models is compared on the unmodified SQuAD dataset as well as a version where a distracting passage is also added to context (by randomly choosing a different passage). The authors experiment using BERT, ROBERTA and SPAN-BERT and report a drop in performance in both experimental settings. In addition experiments reveal that models return worse performance on the subset of the data created by human workers ( perhaps unsurprising). Additional studies on the nature of edits have also been presented.   Overall a well written and easy to read paper. However, I am not sure I am clear about the goals of the paper -- I elaborate further in the rest of the review.    ","The authors studied how contradictory information affects the accuracy of QA systems.  They created a new dataset of ~10k from SQuAD with added contradictory paragraphs (context). The contradictory data was generated by two different ways. The authors employed Amazon Mechanical Turks raters to rewrite the original context. They also designed a BART-FG model to automatically produce such context by replacing spans with model generated value.   The authors also proposed a way to help QA systems avoid contradictory information. A RoBERTa-based model was used to classify a context into trustworthy or not, with an accuracy around 80%. The trustworthiness score is then used to weigh the final result, together with existing scores of quality/confidence.  Evaluation was done with the new dataset. It showed that 1) adding contradictory information hurts QA performance badly, 2) the RoBERTa-based classifier can help regain some of the loss, but not all of them. The authors also measured the effectiveness of contradictory information creation, where human takes the top place by producing the least altered context with largest effect on final outcome.  The authors promised to share the dataset and the source code / weights of the proposed models. They further discussed the potential ethical impact of releasing the data, arguing that it's net beneficial. ","This paper addresses the problem of deriving the correct answer when contradicting examples are presented to the model. First, it introduces a dataset for the task. The dataset, ContraQA is built on SQuAD, and it contains contradicting contexts produced by humans and neural-models. Then, it presents a model for generating contradicting examples. The model, BART-FG, generates fake contexts by iteratively modifying and original input paragraph. The procedure starts by applying a constituency parsing to extract constituency spans from the input sentence. Then, it randomly masks some of these constituency spans, that are eventually fill by a BART model fine-tuned on Wikipedia dump. To study how QA models behave with contradicting examples, this work evaluates the performances in a scenario where the correct and the fake contexts are presented to the model. In order to make the QA system robust to fake contexts, it proposes a misinformation-aware framework that combines the score of the model with a trust score outputs by a fake detector, which is a transformer-based model trained to classify if a context is real or fake. The results show that under this setting, model performance decreases, and that the reduction can be mitigate by applying the fake detector model. Finally, it shows a comparison, between BART-FG and GPT-2, to identify which of the two can generate more impactful fake contexts for the QA model. ","The work investigates closed-domain Question Answering under contradicting contexts by introducing a new task ContraQA—an extension of SQuAD1.1—which includes contradicting contexts for the SQuAD articles, produced by both humans and neural models. The work also proposes a neural framework, BART-FG, to automatically generate these contradicting contexts by iteratively modifying constituency spans on the original context. Finally, the work gives a brief analysis on how SOTA QA systems perform on the new task, ContraQA, and proposes a misinformation detecting system which when unified with a Machine Reader performs significantly better than SOTA systems over ContraQA.",0.15384615384615385,0.16722408026755853,0.07692307692307693,0.1792452830188679,0.10377358490566038,0.1459227467811159,0.2169811320754717,0.2145922746781116,0.23232323232323232,0.1630901287553648,0.2222222222222222,0.3434343434343434,0.18003913894324855,0.18796992481203006,0.11557788944723618,0.1707865168539326,0.1414790996784566,0.20481927710843373
199,SP:80346eeafb0a6d1d556c304a3f8753aff037469b,"One of the weakness of traditional DCNNs is it needs large clean-labeled dataset. In this paper, the author proposes SVMNET, a deep learning architecture which includes a layered structure of Support Vector Machine (SVM) ensembles. The result shows that the SVMNET outperforms other deep convolutional neural networks such as ResNet-50 with less training time for cases in which the number of labeled training samples is small.  ","The paper argues that one of the major drawbacks for deep convolutional neural networks (DCNNs) is the need for large annotated training sets. To address this drawback, the paper proposes a new architecture, SVMnet, which is designed to achieve relatively high accuracy  (compared to DCNNs) in settings of small training sets.  The SVMnet architecture is composed of one or more stacked ""SVM layers"". Each SVM layer is composed of a set of independent svm classifiers, where the input to each svm is a patch in the image and the output is a probability estimate for the image class. For example, given a grayscale h X w input image, an SVMnet with 5x5 kernels and stride of 5 would have a 2d “array” of h/5 X w/5 svms, each one trains on a patch of 5x5 pixels whichis flattened to a vector of 25 features. Each svm i is then trained directly to predict the class of the image, based on the 5x5 window, thus resulting in a probability vector for the 5x5 patch. From the validation step, we also obtain the average accuracy of each svm i, denoted by Ai, from which we can compute its weight when aggravating the predictions from all svms. In general each svm layer will have k X l svms, and thh output of the svm layer can be formulated either as a 2d array of k X l predictions or a 3d array of k X l X c of probabilities for each class (where c is the number of classes). When stacking a second SVM layer, the input to that layer is then the k X l X c probability map which is treated as a feature map. Finally, the predictions from the last SVM layer are tallied to produce a majority vote for the image. When training on images with multiple channels, for example RGB images, the channels are flattened to one vector, for example: a 3x5x5 patch would be flattened to a 75 dimensional feature vector.   To demonstrate the applicability of the proposed method, the paper compares SVMnet to ResNet on 3 publicly available datasets and claims to achieve superior performance to resnet in settings of limited training data as well as faster training time. ","The paper describes the use of SVM classifiers trained independently using local receptive fields of images, outputting class probabilities for each pixel that are organized in channels. New independent SVM classifiers are trained over the class probabilities and their results are combined using voting to perform the final prediction. The authors report experiments on two datasets, comparing with versions of ResNet trained from scratch, in which SVMNet shows better results than ResNet when using fewer training examples, For one of the datasets, the ResNet could not converge, while SVMNet obtained up to 80%.",In this work auhors proposed an ensemble of multiple SVM classifiers setup in a hierarchical way (somewhat similar to neural networks). Each SVM is trained is a small patch or window from the input imagery and some classifiers can be eliminated from contributing to the predictions. Results show better performance by the model in the small data regime compared with larger convolutional neural networks trained from scratch.,0.36764705882352944,0.17647058823529413,0.1323529411764706,0.07712765957446809,0.06382978723404255,0.16129032258064516,0.06648936170212766,0.12903225806451613,0.13432835820895522,0.3118279569892473,0.3582089552238806,0.22388059701492538,0.11261261261261261,0.14906832298136644,0.1333333333333333,0.12366737739872068,0.10835214446952596,0.1875
200,SP:80614db60d27a48c3c1b1882844e298666b798d4,"This paper discusses a very interesting question: what is the relationship between adversarial robustness and cross-domain transferability. The previous studies show that a more robust model can transfer better. This paper argues that the true reason for the cross-domain transferability is not the adversarial robustness, but the effect of regularization, which can also be achieved through other methods, such as data augmentation.","This paper aims to convey the message that adversarially robust models may not have better transferability in terms of transfer learning in vision tasks. However, this message is unclear due to the mixture of data augmentation, regularization, robustness in the presentation. Some definitions are confusing and the conclusion seems to contradict with existing literature, e.g,  Salman et al arxiv: 2007.08489.  --post rebuttal--  Thanks to the authors for the draft revision and the additional appendix C. Now the paper is clearer to me: the general idea is to say that robustness is a type of regularization, and this regularization is the key to generalization. However, I think the former is not novel as already mentioned in Roth et al NeurIPS 2020. Besides, there are still many presentation issues even after the revision, e.g.: 1) Prop 3.1 mentioned the relative domain transferability, but the formal definition is introduced later in Def 1. 2) The added paragraph is confusing to me. How would you define the input space ${\cal X}$? Is it a compact set? Why would the perturbation go outside ${\cal X}$? Shouldn't $x + \delta$ still be in the domain of $f_c^{{\cal D}_{\cal S}}$ as otherwise it is not well-defined? 3) Def 2 should be compared with existing metrics.  4) In Section 3.4 the authors use the squared loss but in experiments they use cross entropy loss.  Despite the interesting topic, I would keep my current score until a more well-written version is presented.  ","This paper aims to investigate the theoretical connection between domain generalization (aka Transferability) and adversarial robustness in some general settings. Authors claim that a thorough theoretical treatment of this problem has not been given yet, and therefore set out to establish a number of fundamental relations.  A simple example has been proposed which shows the existence of cases, where adversarial robustness and transferability can be independent (or even negatively correlated). Also, paper proves that adding more restriction (tighter regularization) on the feature extractor stage of a learning algorithm gives better domain generalization. Additionally, some intrinsic and fundamental measures have been defined to bound the domain generalization error for transferring a learned model from domain S to domain T. In this regard, uniform convergence bounds have been derived to show the gap between empirical and statistical versions of such measures remain small or even converge to zero when sample size asymptotically increases.  Finally, a number of experimental results have been shown to support the above theoretical achievements. I have not gone through the experimental parts nor the proofs, yet.","This paper studies theoretically how adversarially trained models can transfer better, and disentangle the robustness and accuracy on the target domain. They claim that the main reason is more of regularisation rather than robustness. Corresponding examples and theory are presented.",0.296875,0.265625,0.171875,0.11857707509881422,0.05928853754940711,0.06179775280898876,0.07509881422924901,0.09550561797752809,0.275,0.16853932584269662,0.375,0.275,0.1198738170347003,0.14049586776859502,0.21153846153846156,0.13921113689095127,0.10238907849829351,0.10091743119266056
201,SP:80b8488b5a7c29014b0fefbc16698afac42250a0,This paper uses graph-based clustering methods to identify the humanly comprehensible modularities in the neural network. They thoroughly discussed how they construct the graph of neurons and carry out the clustering. They also proposed the fisher bates p-value for testing the significance of identified modules.,"In this work the authors provide an empirical definition of 'modularity' of trained deep networks. Their definition is ‘automated’ in the sense that is does not require any human identification and evaluation of the specific modules.  The proposed definition consists in the following steps.  1) Construct an undirected graph from the nodes and weights of the trained-network  2) Divide the graph in groups using a spectral clustering algorithm  3)  Define ‘subclusters’ as the sets of all nodes that are in the same cluster and in the same layer  4) For each subcluster compute an *importance* score and a *coherence* score  5) If the sub-clusters are more important and coherent than randomly defined groups of nodes of the same size than the network is said to be ‘modular’.  The *importance* of a group of nodes is computed as the loss of accuracy that results from masking to zero that group of nodes. The greater the loss of accuracy, the greater the importance of the group.  The *coherence* of a group of nodes is computed as the maximum magnitude of the pre-activation weights of the group that is achievable with a single image.  By numerical testing, the authors find that it is indeed possible to find groups of nodes that are more important and coherent than random in a statistically significant way for different architectures. ","This work aims to quantitatively evaluate modularity in neural networks (although, as detailed below, I am not sure this is what it ends up doing). To this end, the authors propose to cluster the neurons of the network using spectral clustering applied to a graph that is weighted by similarity between the neurons. They consider two approaches for defining similarity - either via similar responses to inputs (using Spearman or Pearson correlation) or by taking into account the weighted connections (leveraging the learned network weights) between them. There are some technical details regarding the division to layers, exact implementation, and especially the consideration of convolutional architectures (where certain neurons are grouped together along channels), which I will not refer to much here for brevity, but suffices to say that these are all well described in the manuscript and make sense without much need for justification. Once the clusters (or ""subclusters"" when divided to layers) are obtained, they are evaluated empirically to related them to the network's task. This portion is mostly based on image classification tasks, so clusters are evaluated based on their impact on classification accuracy, their importance in identifying each class, or coherence of visual features captured by them. The results are somewhat inconclusive (more details below), but it is appreciable that the authors do acknowledge this in a well balanced discussion section.","This paper seeks to answer whether modern DNNs are modular and proposes statistical methods to quantify modularity. In this study, DNNs are described as modular to the extent they agree with 2 prototypical conditions of modularity, namely importance (which is mainly captured by the drop in accuracy after removing a candidate module) and coherence (which would measure the extent to which neurons in a single module are .activated by similar features). These proxies for modularity, importance (with respect to task performance) and coherence (degree of specialization), are computed based on two well-known DNN interpretability techniques, namely lesions and feature visualization.  ",0.2978723404255319,0.2978723404255319,0.2553191489361702,0.168141592920354,0.08849557522123894,0.10222222222222223,0.061946902654867256,0.06222222222222222,0.1188118811881188,0.1688888888888889,0.19801980198019803,0.22772277227722773,0.10256410256410256,0.10294117647058823,0.16216216216216214,0.16851441241685144,0.12232415902140674,0.14110429447852763
202,SP:81db7f494ca61d3586adb505bf5d2e6e9e2c2bd0,"This paper covers a somewhat interesting and novel problem: learning constraints of an auction setting, and then learning to run an auction that respects those. Fairness in allocations is cited as a justification for this in practice, where it may be beneficial to learn from a human’s choices to determine how to decide who should win in the auction, for example to determine similar allocations across demographics.  The approach builds on the RegretNet line of work that learns to run revenue maximizing auctions. The approach broadly is to learn a feasibility constraint that is scored by human subjects, and then use that in the RegretNet architecture to learn to generate auctions that respect that constraint. In this work, the motivating constraint is a binary classification of fairness. The implementation is a composition of learning then constraint, followed by leveraging RegretNet to learn according to that function.   To support the approach, the authors first compare PreferenceNet to RegretNet trained with the exact constraints, and see similar performance suggesting some amount of usefulness. The authors then train PreferenceNet on responses from human subjects in two surveys regarding the binary qualitative fairness of allocations.  ",This paper describes how we can encode socially desirable constraints in auction mechanisms learned using the regretNet framework.   The paper has two main contributions:   (1) A metric that quantifies how well these mechanisms adhere to these constraints.   (2) An neural network and a training procedure (called the preferenceNet) that can encode these constraints using exemplars of the desired allocation  The authors demonstrate the efficacy of the proposed approach by showing that it can match the performance of standard approaches on both synthetic preferences as well as human preferences,"This paper proposes PreferenceNet, which is a new deep learning approach for designing revenue-maximizing auctions that encodes human preferences. PreferenceNet extends RegretNet [Duetting et al. 2019] with modified loss functions and additional constraints. ","This paper studies of designing revenue optimizing auctions subject to the an additional constraint on fairness. Instead of algebraically capturing the fairness of an allocation, the authors train a neural network termed PreferenceNet that assign a fairness score to any allocation. The neural network for strategy-proof, revenue maximizing auction (RegretNet) is trained to trade-off revenue and regret with the preference loss.  The author evaluate this on small instances (2-4 bidders and 2-4 items) and contrast revenue, regret and fairness of simple RegretNet with one constrained to produce fair allocations.   One primary motivation for a deep neural network based PreferenceNet is that it can capture real world preferences from humans. To explore this aspect, the authors conducted real world studies to get user preference samples and trained a PreferenceNet on that. They also correlate preference learned from humans with mathematical models of preference. ",0.11979166666666667,0.07291666666666667,0.16666666666666666,0.07954545454545454,0.20454545454545456,0.3235294117647059,0.26136363636363635,0.4117647058823529,0.2191780821917808,0.20588235294117646,0.1232876712328767,0.07534246575342465,0.16428571428571428,0.12389380530973451,0.18934911242603553,0.11475409836065574,0.15384615384615385,0.12222222222222222
203,SP:825a254c0725008143b260ead840ae35f9f096d1,"This work seeks to probe large pre-trained language models for indications that they have induced a conceptual space that is structured similarly to one constructed from interactions with the real-world, despite being trained solely from text.  Most of this probing takes the form perceptual tasks -- can LMs quickly learn new concepts related to navigation, or colors, if given some related examples from the space?  In both navigation and color tasks, this seems to be the case, and a consistent trend emerges where larger LMs perform better on this task.  The experiments seem well-controlled, and the authors present an insightful discussion of model performance.  ","The authors present their work looking at how text-only input can create what they call a ""grounded"" model. The results support the idea that a small input of text-only data can give rise to the models behaving in such ways as to generalise over things like left and right or colours. This indicates, the perhaps unsurprising conclusion, that language is able to, on a meta level, encode some aspects of groundedness, as defined by the authors, and the perhaps more impressive conclusion that the models can indeed tap into the latent structure found in language using a few examples and thereby generate useful isomophisms. ","This paper investigates the question of whether language models encode, in some way or another, conceptual spaces that are analogous or isomorphic to grounded ones. The paper reports on the design and results of several experiments to test this in domains which can be easily serialized as text (and hence made accessible to an LM) — spatial directional terms, cardinal directions, and color terms. The ability of a language model to correctly ground conceptual terms is tested in a limited set of unseen environments and for unseen terms, where supervision is provided in the few-shot learning paradigm popularized with GPT-3. The largest tested model, GPT-3 175B, performs appreciably in all settings.","This works aims at investigating whether large language models (LMs) pretrained only on texts, can implicitly learn grounded concepts of the world beyond texts. Specifically, the authors test whether the LMs can map some conceptual domains (e.g., direction, color) to grounded world representations (e.g., textualized grid world, RGB representation of colors). The authors give a rather small number of examples as prompts to the models in an in-context learning setup. They find that large LMs like GPT-3 can often output the correct concept for the grounded world representations, even though it’s likely that the model hasn’t seen the grounded world representations in its pretraining. They conclude that the text-only LMs may already learn the grounded representations implicitly, without explicit from-scratch training like in the visual language models. ",0.1509433962264151,0.1792452830188679,0.16037735849056603,0.1792452830188679,0.2169811320754717,0.18584070796460178,0.1509433962264151,0.168141592920354,0.1259259259259259,0.168141592920354,0.17037037037037037,0.15555555555555556,0.1509433962264151,0.1735159817351598,0.14107883817427383,0.1735159817351598,0.1908713692946058,0.16935483870967744
204,SP:82b7860146bf3f772bdcd5b448d62136ff6d5177,"This paper presents a deep neural net-based dereverberation algorithm that uses both audio and video modalities. Based on the observation that a visual scene captured by a camera conveys information that is related to room characteristics, the authors propose a visually informed audio dereverberation method that aims to extract clean, anechoic speech from reverberant speech. In doing so, they first construct a large audio-visual dataset synthesized using a 3D simulator for real-world scanned environments and LibriSpeech data. They then train deep neural networks that take as inputs both visual data (RGB and depth images) and audio data (reverberant speech), and output clean speech. When training, two types of losses - one for clean speech spectrogram estimation and the other for reverb-visual matching - are used. Through the experiments with several downstream tasks for speech, they showed that the proposed audio-visual dererberation method outperforms the baseline models, both for synthetic and real-world test data.","This paper examines an audio-visual approach for dereverberation, where dereverberation of speech is conditioned on RGB and depth images (either field-of-view or panoramic). It proposes a dataset for this task based on real-world 3D scans of homes, using Librispeech data. Real data is also used The model is based on a U-Net conditioned on embeddings extracted by a ""visual acoustics"" network doing direct spectrogram prediction. The method is evaluated using PESQ, WER for speech recognition, and EER for speaker verification on synthetic and real data. The audio-visual method is found to perform marginally better than an audio-only version of the model. The audio-only version of the model outperforms several baselines from the literature on synthetic data, with mixed results on real data.","This paper describes an audio de-reverberation method which integrates (panoramic, rgb+depth map) visual input with a spectrogram U-Net to estimate a de-reverberated spectrogram and recover the original clean signal. The proposed method is compared to several baseline (audio-only) de-reverberation methods on speech enhancement, recognition, and speaker verification problems using both synthetic and real data. Several ablations of the proposed method are compared, along with some quantitative error analysis investigating the effects of distance and environment. The proposed method appears to consistently improve on speech recognition and speaker verification, and perform about on par with prior work on speech enhancement (as measured by PESQ).  ","In this paper, the authors introduce a novel audio-visual dereverberation approach. They propose a Visually-Informed Dereverberation of Audio (VIDA) model for dereverberation. The authors also create synthetic/simulated datasets and real-world data for experimentation. Finally, they show the impact of the proposed VIDA model on several speech tasks including, recognition, enhancement, and speaker verification. The results are encouraging. The main contribution of this work is the use of visual information as an auxiliary input for dereverberation.     ",0.17834394904458598,0.14012738853503184,0.1592356687898089,0.23846153846153847,0.17692307692307693,0.2018348623853211,0.2153846153846154,0.2018348623853211,0.31645569620253167,0.28440366972477066,0.2911392405063291,0.27848101265822783,0.1951219512195122,0.16541353383458648,0.21186440677966104,0.2594142259414226,0.2200956937799043,0.23404255319148937
205,SP:83b82c145f446c1a29e863362c6ceed018e93e2b,"In a regime where we have access to offline behavior data from a suboptimal policy (a heuristic, human demonstrations, etc) we can use that data and ""trust region""-based methods (TRPO in this work) to nudge exploration in the right direction by keeping the learnable policy close to the behavioral one. While there are several works exploring this general idea, the particular way presented in this work is novel and achieves better results on a set of MuJoCo environment. The approach is also shown to work on a physical robot (Turtlebot).","This paper considers the problem of reinforcement learning with sparse reward functions obtained from offline demonstration. The authors propose a trust region policy optimization based algorithm with offline demonstration data for guidance. The proposed LOGO algorithm is proved to be efficient by a theoretical analysis showing the lower bound on the performance improvement. On benchmark datasets, the proposed algorithm performs better than the state-of-the-art approaches. An illustration is shown by implementing LOGO on a mobile robot for trajectory tracking and obstacle avoidance.","This paper presents ‘LOGO’, an extension of the TRPO algorithm which enables additional learning guidance from offline, sub-optimal (possibly incomplete observation) demonstration data. By annealing away the learning contribution from the sub-optimal guidance policy data during training (with a learning schedule and corresponding hyper-parameter), LOGO utilises this data for guidance, rather than directly imitating it. Furthermore, because LOGO utilises the trust-region methodology, the authors are able to provide a theoretical analysis and lower bound on performance improvement each episode. The method shows promising performance on several MuJoCo continuous control tasks, as well as in a Gazebo TurtleBot simulation, which is also able to be transferred to a real-world robot.","This paper aims to improve the performance of RL in sparse reward settings via the guidance of sub-optimal demonstrations. The idea proposed by this work is to modify TRPO to restrict its updates to remain close to the behavior policy that generated the offline dataset, while decaying this constraint over time to enable the RL policy to improve upon the behavior policy in an online fashion. The authors do so by minimizing the KL divergence between the current RL policy and the behavior policy while not moving too far away from the current RL policy. The method is evaluated on openAI gym style tasks as well as a real robot navigation task. ",0.1978021978021978,0.2087912087912088,0.21978021978021978,0.3176470588235294,0.25882352941176473,0.21929824561403508,0.21176470588235294,0.16666666666666666,0.17699115044247787,0.23684210526315788,0.19469026548672566,0.22123893805309736,0.20454545454545453,0.18536585365853658,0.19607843137254902,0.271356783919598,0.22222222222222224,0.22026431718061673
206,SP:841b12443d0274e34b78940f220b17d36798899b,"UPDATE:  I acknowledge that I've read the author responses as well as the other reviews.   While the authors added further analysis on the proposed method, I am skeptical of the performance of the method since the proposed method requires validation OOD data to achieve SOTA performance and the runtime is much larger than MSP and energy baseline. However, I think the rebuttal clarified much of my concerns and therefore raise the score to 5 weak reject.   =================================================================  The paper proposes an out-of-distribution detection (OOD) metric based on Fisher-Rao distance which can be applied for pre-trained classifiers. First, the authors derive a Fisher-Rao distance applied to the distribution of softmax. Also, they motivate a toy example where the FIsher-Rao distance outperforms the conventional OOD metric, Mahalanobis distance. Furthermore, they formulate the Fisher-Rao distance-based framework, IGEOOD on Black(Grey)-Box, where we can only get access on the logit of the network output, and White-Box, where we can get access to intermediate feature layers. Finally, the authors compare IGEOOD against conventional OOD metrics on various out-of-distribution data and in-distribution data.","The paper proposes to use a score based on the Fisher-Rao information metric (the Riemannian metric in the space of probability distributions) for the detection of out-of-distribution samples input to a trained DNN. While the output SoftMax probabilities are used in the black-box and grey-box scenarios, the learnt features in the intervening DNN layers are additionally used in the white-box scenario. The approach models each sample as providing posterior probabilities – (a) the SoftMax probability in the label space, and (b) class-conditional PDFs over the corresponding feature spaces for each DNN layer. These latter are modeled as multivariate Gaussian distributions with diagonal covariance matrices. Extensive experiments on existing benchmarks are conducted for comparative results against the state of the art demonstrating promising results. ","The paper proposes a new group of methods for supervised OOD detection. In particular, the authors propose to use the Fisher-Rao distance between output distributions on the in-distribution data and test samples to detect OOD. The authors additionally propose to use Fisher-Rao distance in the hidden layer feature space, when possible (white-box setting). The method achieves strong empirical performance, improving upon standard baselines (such as Odin, Mahalanobis), especially in the white-box setting.","This paper presents IGEOOD, a new method for detecting OOD samples by using geodesic (Fisher-Rao) distance in confidence scoring. It further combines confidence scores from the logit outputs and the layer-wise features of a deep neural network. The method is validated under various testing environments such as the availability of OOD data or the accessibility of latent features of a deep network. The idea of using Fisher-Rao distance for OOD detection seems novel and interesting. ",0.12631578947368421,0.12105263157894737,0.10526315789473684,0.17829457364341086,0.12403100775193798,0.22077922077922077,0.18604651162790697,0.2987012987012987,0.2564102564102564,0.2987012987012987,0.20512820512820512,0.21794871794871795,0.15047021943573669,0.17228464419475653,0.14925373134328357,0.22330097087378642,0.15458937198067632,0.21935483870967745
207,SP:8433900e40c5c5df1f003dd1d4fb08c7aafd51f8,"The authors propose a new RNN architecture, long expressive memory (LEM), motivated by a system of ODEs with multiple time constants. They prove that it can avoid the vanishing gradient problem while retaining the flexibility to approximate a broad class of dynamical systems. They report comparable or improved prediction performance of LEM-based sequence models across a very wide variety of tasks, as compared to several recent alternatives.","This paper introduces Long Expressive Memory (LEM; Eq. 3) which is a new architecture for recurrent networks derived from discretization of a particular system of multi-scale ODEs (Eq. 2). It presents interesting theoretical results that characterize the properties of the proposed architecture (Sec. 4). The key results are that under certain conditions (e.g. fine discretization) the gradient propagation through time is well behaved, and that it is expressive enough to represent general dynamical systems, and multiscale dynamical systems in particular. Empirical results are presented on the adding problem, a synthetic two-scale dynamical system, sequential MNIST/CIFAR-10 classification, EigenWorms classification of very long time series, heart rate prediction, the Speech Commands dataset, and character-level modeling on Penn Treebank (Sec. 5). Across all tasks, the authors report best performance using the proposed architecture when compared to various methods in the literature. ","The paper tries to propose a new recurrent architecture that could address the well-known issues (of recurrent models) like vanishing gradient and exploding gradient. The architecture is a kind of a realization of numerical discretization of ODEs using an implicit-explicit time-stepping scheme. The authors clearly explain why the designed model architecture can capture multiscale data. The authors also connect the proposed method to vanilla LSTM, Hodgkin-Huxley equations, and heterogeneous multiscale methods for ODEs.  The authors also try to provide theoretical evidence that the proposed methods mitigate the gradient exploding and vanishing issues (under some conditions) while learning informative representations for long/short sequence data.  The authors tried on many tasks across different domains, and the proposed methods consistently outperformed the baselines. The benefits are pretty significant in some tasks.","This paper introduces a model class of neural networks whose architecture is defined by a circuit that computes a discretized step of a pair of multi-scale ODEs. The key novelty is on the multi-scale aspect of this system. In terms of representation capability, this class can represent LSTMs (and vice-versa) and can also approximate any dynamical system from a very broad class of dynamical systems (as well as multi-scale dynamical systems). The authors also prove upper bounds in l_infinity on both the hidden state values and the gradients, and argue this addresses the exploding gradient problem. They also address the vanishing gradient problem by arguing that the magnitude of the partial gradient corresponding to the contribution that depends on step k of the circuit does not depend on k.  They then present a variety of experiments using this new architectural component for modeling several sequence problems. They include a small study on the multi-scale nature of the datasets they study as well.",0.29411764705882354,0.23529411764705882,0.3088235294117647,0.18055555555555555,0.20833333333333334,0.21804511278195488,0.1388888888888889,0.12030075187969924,0.125,0.19548872180451127,0.17857142857142858,0.17261904761904762,0.18867924528301885,0.15920398009950248,0.17796610169491525,0.18772563176895304,0.1923076923076923,0.1926910299003322
208,SP:8475e89f143c727e33147b652c2d0b3cdb420382,"The current leading theory of what contrastive losses are doing and why they work interprets contrastive learning as balancing alignment with uniformity, as proposed in [2].  This paper seeks to augment that understanding of contrastive learning using a new perspective, focusing on the role of data augmentation.  It is well-known that contrastive learning techniques are highly sensitive to the data augmentation schemes used, most notably discussed in [1].  In this work, the authors interpret augmentation as a way to connect different intra-class images together.  Then, the contrastive loss is seen as a way to gradually cluster intra-class samples together by aligning augmented views, producing representations that are class-separated even in feature space.  On top of introducing a new lens with which to understand contrastive learning, the authors also provide proofs on performance guarantees, as well as a new evaluation metric.  The metric is inspired by their augmentation-oriented understanding, and was also found to align well with downstream performance.  The authors provide a scenario where alignment and uniformity are satisfied, but fails to translate well to downstream classification accuracy.  This suggests to them that the instance discrimination task alone cannot guarantee the learning of class-discriminative features that would enable better downstream classification, and directs their attention to the other important component of contrastive-learning to help explain the story: augmentation.  They then build off the analytical work of [3] to prove guarantees for the downstream performance with a relaxed assumption.   [1] Chen et al., A Simple Framework for Contrastive Learning of Visual Representations, 2021.  [2] Wang and Isola., Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere, 2020.  [3] Saunshi et al., A theoretical analysis of contrastive unsupervised representation learning, 2019. ","The paper proposes a new theory for understanding contrastive representation learning. The novelty is the focus on the interplay between alignment and augmentation. Prior work has identified alignment as one of the factors of contrastive learning, but have not investigated how different types of augmentations may affect the learned embeddings. This work adds that missing piece. The results intuitively make sense, showing that proper amount of augmentation (that connects samples of the same class) has positive effect on downstream classification. Empirically, the authors verify that too weak or too strong augmentation harms performance. Based on observations, the authors define a metric on ratio of positive pairs among nearest (embedding) neighbors, and found the change of this metric throughout training positively correlate with performance.","This paper aims to provide theoretical understanding for contrastive learning where ""similar pairs"" of points $x$ and $x^+$ are encouraged to have similar representations through an InfoNCE inspired objective function. Some prior works show the benefit of learned representations for linearly classifying downstream classes, by making conditional independence like assumption on the similar pairs or positive samples, i.e. $x$ and $x^+$ are (approximately) conditionally independent given downstream label $y$. This work argues that these assumptions are quite strong for contrastive learning with data augmentations, and aims to show guarantees under the following weaker and more realistic assumption: support of augmentation distribution of different inputs from the same class overlap to form a ""connected graph"" of inputs within a class, whereas support of augmentations of inputs from different classes do not overlap. Lower and upper bounds using this and some other assumptions, connecting the downstream performance of representation function to the contrastive loss. Some simulation experiments are presented to support some aspects of the theoretical analysis.  Using the insights from the analysis, the paper proposes an ""Average Confusion Ratio (ACR)"" metric that can be used to predict the ranking of downstream performances of different augmentations **using only unlabeled data**. Experimental evidence is provided on CIFAR and STL datasets to verify the efficacy of this metric for some practical augmentations.    While there are some interesting aspects in the paper (especially the ACR metric), the theoretical analysis seems to have raised many questions and concerns that I have summarized below (details in main review).   - **Soundness of assumptions**: Assumption 4.6, which is crucial, seems questionable and may not be coherent or appropriate to make in this setting. More on this in point (W2) of main review  - **Deeper dive into theoretical results**: There is a lack of discussion about the (non-)vacuousness of the bounds in the main results Theorem 4.2 and 4.8, that puts the interpretation and significance of the result in question. More on this and related issues in point (W2) of main review.  - **Comparison to prior work**: The work of HaoChen et al. in particular is not adequately compared to, especially since some of the points being addressed here are covered through a different kind of analysis in that paper. More on this in point (W3) of main review.","The authors provided a new understanding of contrastive learning from the perspective of data augmentation for intra-class samples. In particular, the authors proposed to understand the role of data augmentation as to create certain ``chaos'' between intra-class samples so to encourage the clustering of intra-class samples and also the learning of class-separated representations. Additionally, a new metric ARC is proposed to evaluate the downstream performance. The conclusion is validated via both synthetic and real-world datasets. ",0.1111111111111111,0.1736111111111111,0.1111111111111111,0.25203252032520324,0.17073170731707318,0.06561679790026247,0.2601626016260163,0.13123359580052493,0.4,0.08136482939632546,0.2625,0.3125,0.15571776155717762,0.14947683109118087,0.1739130434782609,0.12301587301587301,0.20689655172413796,0.10845986984815618
209,SP:84c9eb6623e7950585d80a664dd51b3ecc356dea,"This paper presents a new network architecture called FitVid to perform the task of video prediction, i.e. the task of predicting future frames from previous frames. Previous methods for this tasks usually suffer from ""underfitting"", while FitVid is able to overfit on major benchmarks without increasing the number of parameters, thanks to a better model architecture. What is more, FitVid is much easier to train than previous work, without using any bells and whistles in training. As a result, FitVid achieves state-of-the-art on four challenging video datasets across a wide range of metrics.","This paper discusses a new framework named FITVID to handle the problem of video prediction. FitVid is built on existing modules like Sikp connection, Sequeeze and Excite, and LSTMs. FitVid only needs a simple training strategy and begins overfitting the datasets. Data augmentation is also adopted to handle the problem of overfitting. ","The paper proposes a simple and scalable variational video prediction model FitVid, which attains a better fit to video prediction datasets even with a similar parameter count as prior models. The author has observed that previous methods suffer from underfitting on these datasets, directly applying FitVid actually results in overfitting. The FitVid uses a set of existing image augmentation techniques to prevent overfitting so that it can achieve state-of-the-art results on several prediction benchmarks. FitVid's architecture is based on SE-UNet LSTM, which seems to be the common backbone for the stochastic video prediction task. ","The paper studies conditional video prediction. In particular, it focuses on the problem of current models underfitting and not scaling to datasets. The paper proposes an architecture that is claimed capable of using parameters more efficiently in order to overfit. Then, data augmentation is introduced to improve performance for generalization. They evaluated their method against baselines on 4 datasets (Human 3.6M, KITTI, Robonet, BAIR pushing dataset). They also showed experiments to support overfitting/underfitting claims.  ",0.18556701030927836,0.2268041237113402,0.15463917525773196,0.2692307692307692,0.23076923076923078,0.13131313131313133,0.34615384615384615,0.2222222222222222,0.19736842105263158,0.1414141414141414,0.15789473684210525,0.17105263157894737,0.2416107382550336,0.22448979591836735,0.17341040462427748,0.18543046357615894,0.18749999999999997,0.14857142857142858
210,SP:862d6d76692aee384adc70fd845f0b89cfda93d3,"The authors propose a text-based video retrieval method based on context from weakly related user comments. Evaluation shows that considering user comments improves the retrieval performance. Compared to conventional methods, the proposed method shows similar performance with CLIP4Cilip. However, CLIP4clip needs a much larger-scale training, so the superiority of the proposed method is claimed.","This paper proposes the use of user comments in addition to videos and titles to learn better representations for retrieval. Since user comments may be loosely related to the video, they use an attention-based mechanism to ignore irrelevant user comments. Their experiments show that using comments using this mechanism leads to better contextualized representations, which lead to competitive results on standard benchmarks.","The paper looks into video-text retrieval problem.  The claim is that existing works mostly rely on titles and captions.  The paper argues for using user comments.  The challenge is that not all user comments are meaningful or relevant.  Therefore, the paper looks into attention mechanism to filter out the irrelevant content. The main contribution is a context adapter module based on transformer that allows relating visual input and textual input. The base architecture seems to have been derived from CLIP.","In this paper, the authors proposed a multi-modal video representation learning and retrieval method based on it, taking advantage of visual and text embeddings, particularly users' comments on video sharing platforms. For this, the authors present a new component called Context Adapter Module (CAM), applying skip connections to selectively reflect user comments in the model. According to their experiments, the authors claim that the proposed method outperforms baseline models, by providing more contextualized representations of videos. ",0.14285714285714285,0.25,0.30357142857142855,0.20634920634920634,0.23809523809523808,0.1728395061728395,0.12698412698412698,0.1728395061728395,0.22077922077922077,0.16049382716049382,0.19480519480519481,0.18181818181818182,0.13445378151260504,0.20437956204379562,0.2556390977443609,0.18055555555555555,0.21428571428571427,0.1772151898734177
211,SP:8648453f5a7c5e9b99a8fdbaa340f4e2b4d048d0,"- This paper tackles the out-of-distribution problem for node-level prediction on graphs from the invariance perspective. It presents a novel approach in which the whole graph is divided into n ego-graphs where n is the number of nodes. All the ego-graphs can be treated as a set of IID. Based on this division, the model can be trained to defend the adversarial attack from multiple environments.  - This work extends the discussion of the OOD problem to node-level tasks on graphs. A new learning approach is proposed and theoretically proven to be correct. In empirical experiments on multiple datasets, the approach (EERM) outperforms its counterpart baseline ERM. ","The paper adapts a recently proposed invariant risk minimization approach for tackling distribution shift on node-level predictions on graphs. The main idea is to decompose the graph into a set of ego-graphs rooted at each node, thus incorporating structural information. Since the learning objective requires data from different environments they authors introduce auxiliary context generators trained to maximize the variance loss. ","This paper studies the problem of distribution shifts as out-of-distribution generalization. Specifically, it formulates the OOD problem as invariant risk minimization under different environments. The relation between these two has been extensively discussed in the paper. Multiple environment is done by graph editing using policy gradient. Experiments on three different kind of distribution shifts are presented and the effectiveness of EERM is validated.","The importance of out-of-domain (OOD) generalization has emerged, and there is much research for out-of-domain generalization [1,2,3,4]. However, the related works on the graph-structured datasets are not explored well. OOD generalization of the graph is not trivial because the graph has the interconnection among nodes and the existence of structural information. To solve the problems, this paper proposes a new method, multiple contexts explore that are adversarially trained to maximize the variance of risk. The proposed model is validated on many diverse datasets, Cora, Amazon-Photo, Twitch-explicit, and so on.  [1] Arjovsky, Martin, et al. ""Invariant risk minimization."" arXiv preprint arXiv:1907.02893 (2019).  [2] Sagawa, Shiori, et al. ""Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization."" arXiv preprint arXiv:1911.08731 (2019).  [3] Krueger, David, et al. ""Out-of-distribution generalization via risk extrapolation (rex)."" International Conference on Machine Learning. PMLR, 2021.  [4] Creager, Elliot, Jörn-Henrik Jacobsen, and Richard Zemel. ""Environment inference for invariant learning."" International Conference on Machine Learning. PMLR, 2021.",0.16216216216216217,0.15315315315315314,0.2072072072072072,0.14285714285714285,0.25396825396825395,0.26153846153846155,0.2857142857142857,0.26153846153846155,0.12637362637362637,0.13846153846153847,0.08791208791208792,0.09340659340659341,0.20689655172413793,0.19318181818181818,0.15699658703071673,0.140625,0.13061224489795917,0.13765182186234817
212,SP:870cd8794f7ff48fbed71c2abc9fb7dad51bd343,"This paper proposes InfoTS, a method for learning augmentations that improve contrastive learning of time series data. The core contribution is a learnable augmentation strategy that uses Concrete/Gumbel-Softmax distributions. The paper shows empirical results on several time-series forecasting and classification benchmarks.","The authors propose a contrastive learning framework for time series data, where data augmentations are adaptively being selected, given a fidelity and variety criterion. Additionally to these two criteria, a contrastive learning objective is applied both on local and global level. The model is tested on a time-series forecasting and classification task on multiple datasets.  ","This is a very good and solid paper that has both empirical elements and theoretical basis. As mentioned in the paper data augmentation in time series is a notoriously difficult problem for the simple reason that it can distort the time series completely - which in contrast to images - humans cannot verify this as it can happen with images. So the contribution of this paper is a new data augmentation approach based on information theory, a meta learning approach and an approach to select optimal data augmentation for contrastive learning.","This paper describes an information-aware approach to representation learning for time series. The formulation focuses on how to obtain effective data augmentations and addresses the underlying problem from information-theoretic viewpoints, leading to the two optimization criteria, namely, high fidelity and high variety. The experimental results on several time series datasets for forecasting and classification show improvements over the methods in comparison. Detailed comments are listed below.",0.3181818181818182,0.3181818181818182,0.36363636363636365,0.19642857142857142,0.32142857142857145,0.11235955056179775,0.25,0.15730337078651685,0.23529411764705882,0.12359550561797752,0.2647058823529412,0.14705882352941177,0.28,0.21052631578947367,0.2857142857142857,0.15172413793103448,0.2903225806451613,0.12738853503184716
213,SP:878325384328c885ced7af0ebf31bbf79287c169,"Private multi-winner voting is the task of revealing k-hot binary vectors that satisfy a bounded differential privacy guarantee. They propose three new mechanisms. 1. Binary voting operates independently per label through composition. 2. \tau voting bounds votes optimally in their l2 norm.  3. Powerset voting operates over the entire binary vector by viewing the possible outcomes as a power set. They prove that Powerset voting requires strong correlations between labels to outperform Binary voting.  They also use these mechanisms to enable privacy-preserving multi-label learning. They empirically compare techniques with DPSGD on large real-world healthcare data and standard multi-label benchmarks. Their techniques outperform all others in the centralized setting, and show that mechanisms can be used to collaboratively improve models in a multi-site (distributed) setting.  ","The authors study differentially private multi-winner voting, which is designed for multi-label learning subject to a privacy constraint meant to limit information leakage about training data to an adversary. They propose three mechanisms: Binary, which essentially runs an existing differentially-private election for each label independently, \tau voting, which works with votes that have bounded \ell_2 norm, and powerset voting, which explicitly encodes each possible subset of winners as an alternative in an election and then votes over them. They show that Binary voting (the naive approach) generally outperforms powerset voting as long as there aren’t strong correlations between votes. Lastly, they show that they can use these multi-winner DP techniques to a extend single-label technique, PATE, and empirically demonstrate the effectiveness of their approach.","This paper considers the design of differentially private multi-label mechanisms. In particular, the authors employ multi-winner voting protocols to existing differentially private single-label learning algorithms e.g. PATE. They consider three multi-label voting protocols -- binary voting, $\tau$-voting and powerset voting.   Binary voting works by independently applying majority voting on each coordinate. It is obvious that such an aggregation mechanism has does not provide a better privacy guarantee than $k$ applications of binary voting. The privacy guarantee can be improved when the coordinates are dependent. This motivates the authors to consider $\tau$-voting where the $\ell_2$-norm of each ballot is bounded by $\tau$. Finally, in the powerset voting, the voting is done over the universe of all subsets of the alternatives i.e. $2^k$ alternatives.  The authors make two important observations through experiments. First, when there is high consensus and $\sigma_G \rightarrow 0$, binary voting performs best. On the other hand, $\tau$-voting outperforms with lower consensus and larger values of $\sigma_G$. Second, even in centralized setting, multi-label methods outperform existing benchmarks.","This paper considers differentially private multi-winner voting. This problem is a generalization of single-winner voting, which is widely used in PATE type private semi-supervised learning. The authors give three private mechanisms and perform empirical comparisons on multi-label semi-supervised learning settings.  Specifically, when there is no total votes constraint on each ballot, the binary mechanism performs report noisy argmax for each candidate. When there is a total votes constraint, the tau mechanism performs l2 clipping first and then performs report noisy argmax for each candidate as the binary mechanism. The powerset mechanism converts the multi-label problem into a single label problem and performs regular report noisy argmax. ",0.26717557251908397,0.20610687022900764,0.1450381679389313,0.21374045801526717,0.1984732824427481,0.14285714285714285,0.26717557251908397,0.14835164835164835,0.16964285714285715,0.15384615384615385,0.23214285714285715,0.23214285714285715,0.26717557251908397,0.17252396166134185,0.15637860082304528,0.17891373801916932,0.21399176954732513,0.17687074829931973
214,SP:892558b9f4fb53ed5ca2a7ee440b7d728b1886d6,"The paper extends the recently introduced SAU measure from bandits to RL. The idea is to enhance the exploration of action based on an approximation of the estimation error. More precisely, the bonus is proportional to the average squared temporal difference error. This uncertainty measure is easy to compute and can be integrated into both tabular and continuous methods (e.g., Q-learning and DQN).",This paper extends a recently-proposed exploration method called SAU in bandits to the RL problem. They combine this exploration approach with the standard Q-learning algorithm. Their experiments show that this approach can obtain better performance compared with the Q-learning algorithm with eps-greedy exploration.,"This paper studies exploration bonus in practical deep RL based on Sample Average Uncertainty (SAU) and upper confidence bound (UCB). SAU is a recently studied novel uncertainty quantification that works for rather arbitrary estimators. Previous paper has studied how to use SAU to derive UCB-type bonus in multi-armed bandits and proved that it could achieve optimal regret.   This paper successfully extends the SAU-UCB-type exploration bonus from bandits settings to RL and most importantly, deep RL settings, and shows how to incorporate SAU-UCB-type bonus in (deep) RL. This paper conducts various experiments for RL and deep RL, demonstrating the advantage of their algorithms over the standard benchmarks.   The paper is generally well-written.","This work introduces $\delta^2$-exploration for reinforcement learning (RL), which aims to incorporate sample average uncertainty (SAU) into RL exploration. The authors discussed the background and formulation of SAU. The authors further propose $\delta^2$-exploration, which incorporates SAU into Q-learning and compare such exploration with value-uncertainty exploration (UCB-type exploration) and $\epsilon$-greedy. The author then proposes to incorporate $\delta^2$-exploration into DQN and conduct experiments to compare $\delta^2$-exploration with SOTA exploration algorithms. Empirical results show that $\delta^2$-explorations attain comparable results to bootstrapped DQN.",0.2,0.2153846153846154,0.16923076923076924,0.2765957446808511,0.2553191489361702,0.16101694915254236,0.2765957446808511,0.11864406779661017,0.11827956989247312,0.11016949152542373,0.12903225806451613,0.20430107526881722,0.23214285714285715,0.15300546448087432,0.13924050632911392,0.1575757575757576,0.1714285714285714,0.18009478672985782
215,SP:8a78fee6173dc6639dfd9e33a10d0c8432a08512,"They authors show that for linear optimization on strongly convex domains the lazy subgradient method achieves a best-of-both-worlds behavior of a expected regret bound of O(\sqrt{N}) and O(\log(N)) for adversarial and stochastic adversaries, respectively. The best known prior bounds were O(\sqrt{log(N)}) adversarial regret with a more complicated algorithm (based on A,B-prod). On a technical side, the analysis for lazy subgradient is much more intricate and considers loss contributions in the direction of the comparator separately from other directions, and may be of interest in other online linear optimization settings.  Simulations are provided that convincingly argue for the proposed method in the proposed setting.  ","The authors study online linear optimization in the case of known strongly convex constraints sets. They show that the simple method of online subgradient descent, which is known to achieve $O(\sqrt{N})$ regret in the adversarial case, also achieves $O(\log(N))$ regret in the i.i.d. case. This result is more elegant than and slightly improves upon existing results in the strongly convex case.","The paper considers the problem of online linear optimization under a full information feedback setting on a strongly convex domain. In this setting, the authors show that the classical lazy subgradient method is optimal both in the adversarial and stochastic settings.  In particular, the authors show that for the same choice of hyper-parameters, the lazy sub-gradient method achieves O(\sqrt{N}) regret in the adversarial setting and O(log{N}) regret in the stochastic setting. To show this result, the authors have to do a more nuanced analysis of the algorithm (that relies on differential geometry methods) than existing works.   ","The authors study online linear optimization and show that the Lazy Online Subgradient Algorithm is *universal* on strongly convex domains: it simultaneously achieves the optimal $O(\sqrt T)$ regret when the costs are adversarial and the optimal $O(\log T)$ regret when the costs are stochastic. This result is a clear improvement over previous work of Huang et al, who described a more complex algorithm and didn't achieve the optimal rates. The proof is based on an intricate analysis, drawing from differential geometry, of the points selected by the algorithm, and in particular, how much of each step affects regret against the minimizer $x^*$ and how much is orthogonal to the direction $x_i - x^*$ and doesn't affect regret. This technique may be of interest to the broader ML community.",0.19130434782608696,0.22608695652173913,0.2782608695652174,0.43283582089552236,0.417910447761194,0.29411764705882354,0.3283582089552239,0.2549019607843137,0.24242424242424243,0.28431372549019607,0.21212121212121213,0.22727272727272727,0.2417582417582417,0.23963133640552992,0.25910931174089075,0.3431952662721893,0.2814070351758794,0.25641025641025644
216,SP:8aa471b92e2671d471107c087164378f45fb204f,"This paper focused on a classical federated learning setting where data was non-iid partitioned in different local servers, and came up with a method named SDA-FL, which combined GAN-generated data, differential privacy to both address the non-iid problem and keep local data privacy. The main contributions are: 1. Utilized differential private GAN-generated data to solve the non-iid and local data privacy problems; 2. Designed a label updating mechanism to increase the model performance; 3. Tested the algorithm on CIFAR-10, MNIST, fashion-MNIST to confirm the performance of the algorithm.","This paper proposes a new framework for federated learning to resolve the non-IID issue by sharing differentially private synthetic data. Each client pretrains a local GAN to generate synthetic data and upload the data to the parameter server. To effectively use the synthetic data, the server performs pseudo labeling and shares this information with the clients along with the global model parameters, to let the local data, including both the real and synthetic data, approach an IID distribution. The interplay between model training and synthetic data updating improves the convergence of the local models and resolves the non-IID issue. ","The paper proposes a new federated learning algorithm called SDA-FL, which utilizes GANs to generate synthetic data for federated training on non-IID data. Specifically, each client pretrains a GAN to generate synthetic data and send them to the server. In each round, the server sends the global model and the synthetic data to the clients. Then, the clients update the label of the synthetic data and use both the local data and synthetic data to update the local model. The local models are sent to the server, which further averages the models and uses the averaged model to label the synthetic data. Experiments show that SDA-FL significantly outperforms the other federated learning approaches on non-IID data. The paper also studies the influence of different privacy budgets on the performance when applying differentially private GANs.","Federated learning suffers from non-IIDness across clients. A line of previous works address the non-IID problem by data sharing which violates the privacy requirement. In this paper, the author propose SDA-FL. Compare to the most basic FedAvg, the additional components includes  - *Image synthesis with DP-GAN*. Before training classifier, each client trains a differentially private GAN to generate synthetic data, and upload these data to PS for future data-sharing.  - *Synthetic image labeling*. Similar to self-training, PS use local models to assign pseudo-labels to unlabeled data. These labels are updated during training, i.e., interplay between model training and synthetic dataset updating.  - *Mixup*. For each clients, private data and shared synthetic data are mixed to alleviate non-IIDness.  - *ServerUpdate*. Data sharing also makes it possible for the server to conduct gradient descent.   This paper also empirically evaluate their framework under supervised and semi-supervised learning settings. Moreover, this paper study the sensitivity to privacy budget and effect of synthetic data. ",0.25,0.2916666666666667,0.25,0.45544554455445546,0.33663366336633666,0.2463768115942029,0.2376237623762376,0.2028985507246377,0.14545454545454545,0.3333333333333333,0.20606060606060606,0.20606060606060606,0.24365482233502536,0.23931623931623933,0.1839080459770115,0.38493723849372385,0.2556390977443609,0.22442244224422445
217,SP:8ad1b170f0392a132a3816c9cd28fb7332343e65,"The paper provides DEGREE, which decomposes the feedforward propagation mechanism of a GNN to understand it. They give realistic decomposition techniques for those typically used layers in GNNs after presenting the key guidelines for developing decomposition-based explanations. They also devise an approach for providing subgraph-level explanation via agglomeration, which makes effective use of graph topology. DEGREE surpasses baselines in terms of fidelity and can capture important structures in graph data, according to experimental results.","This paper proposes a decomposition-based explanation method for graph neural networks. The motivation of this paper is that existing works based on approximation and perturbation suffer from drawbacks. To address the issue of existing works, the authors directly decompose the influence of node groups in the forward pass. The decomposition rules are designed for GCN and GAT. Further, to efficiently select subgraph groups from all possible combinations, the authors propose a greedy approach to search for maximally influential node sets. Experiments on synthetic and real-world datasets verify the improvements over existing works. ",The paper aims at tackling the black-box nature problem of GNN by introducing a new type of explainable GNN framework called DEGREE (Decomposition based Explanation for GRaph nEural nEtworks). There are mainly two innovations. The first one lies in its ability to track contribution of components in the input graph. The second one is the algorithm for subgraph-level explanation via agglomeration. The model achieves a good compromise between performance and time efficiency.,"This paper proposes a decomposition-based explanations method for graph neural networks.  In detail, the authors design a subgraph level interpretation algorithm to reveal complex interactions between graph nodes, so as to achieve the faithful explanation for GNN predictions. They demonstrate the effectiveness of the proposed method on synthetic and real-world datasets.   ",0.13157894736842105,0.2236842105263158,0.14473684210526316,0.2127659574468085,0.24468085106382978,0.20270270270270271,0.10638297872340426,0.22972972972972974,0.20754716981132076,0.2702702702702703,0.4339622641509434,0.2830188679245283,0.1176470588235294,0.22666666666666668,0.17054263565891475,0.23809523809523814,0.3129251700680272,0.2362204724409449
218,SP:8b233a2a5049ccda84e8840b97b800ffc5862e16,"The paper studies the trade-off between minimizing the mean-square error (distortion) and the Wasserstein-2 distance (perception) when constructing an estimator \hat{X} of a random variable X. In particular, the work analyzes a distortion-perception function, which given the radius P of a Wasserstein-2 ball around X returns the minimal mean-square error of \hat{X} constrained to be in that ball.   The analysis reveals that for estimators with small Wasserstein-2 distance to the true distribution, one can often obtain an estimator with much better mean-square error by a simple linear interpolation in the output space.   The theoretical results are applied to deep image super-resolution, where it is shown that the perceptual quality of an estimator with small MSE can be improved by interpolating it with one having small Wasserstein-2 distance. Remarkably, the interpolated estimator often still has low MSE, while significantly lower Wasserstein-2 distance, leading to a greatly improved perceptual quality. ","The paper characterizes the perception-distortion in the context of (image) enhancement tradeoff, complementing prior results. For example, in image super resolution, this tradeoff describes the relationship between the point-wise reconstruction quality (distortion) and how well the distribution of the reconstructions match the distribution of the high-resolution data one aims to reconstruct. The paper derives this general tradeoff explicitly and characterizes certain aspects specifically for Gaussian distributions. The theoretical results are illustrated with experiments.","Distortion-perception is the phenomenon that the better a criterion (on natural images) is optimized (for instance SNR in image denoising) the stronger the deviation to the distribution of the space of natural images. This paper characterises this phenomenon under Mean Squared Error (MSE) as the criterion under a Wasserstein bound on the conditional distribution. The result is an interpolation between the distribution of the conditional distribution and the distribution itself. An explicit formulation of the optimizer of their Distortion-perception function is provided. Simulations are made, showing that interpolating different models that approximates the quantities in their theoretical formulation can achieve better visual aspect.","The authors of this paper suggest that they show four novel contributions to the image compression literature that hold true for MSE distortion and Wasserstein-2 perception index: 1. They prove that the Distortion-perception function is always quadratic in the perception constraint P regardless of the underlying distribution 2.  The show that it is possible to construct estimators of the Distortion-perception curve from estimators at the two extremes of the tradeoff (the one that globally minimizes MSE and the one that minimizes MSE under a perfect perceptual quality constraint). 3.In the gaussian setting, they provide a closed form expression for optimal estimators and the corresponding Distortion-perception curve, they also show that this is a lower bound on the curve of any distribution having the same second-order stats. 4.They illustrate their results conclusively with super-resolution as their test case.",0.13043478260869565,0.13664596273291926,0.19254658385093168,0.23684210526315788,0.2894736842105263,0.23809523809523808,0.27631578947368424,0.20952380952380953,0.21379310344827587,0.17142857142857143,0.15172413793103448,0.1724137931034483,0.17721518987341772,0.16541353383458646,0.20261437908496735,0.19889502762430936,0.1990950226244344,0.19999999999999998
219,SP:8bc53935566be2b70403f4b46fe94686d5eae1a1,"This work builds on the previous work[1] and extends it for a more general scenario, where per sample intermediate distribution shift is hard to quantify. They propose gradual feature interpolation for the case where samples from intermediate distribution are missing. Iterative self-training fails in such cases. The work presents results on synthetic and natural distributions to evaluate their claim.  [1] Ananya Kumar, Tengyu Ma, and Percy Liang. Understanding self-training for gradual domain adaptation. arXiv preprint arXiv:2002.11361, 2020.","This work proposes an iterative self-training approach for unsupervised domain adaptation. In particular, the authors aim at gradually adapting a model trained on a source domain to the target domain. Based on the claim that previous work on this setting assumed that samples from distributions that represent gradual changes from the source to the target domain are available when adapting the model, the authors proposed a strategy to generate such intermediate samples for cases where they are not available. The introduced approach, named GIFT, consists in performing manifold mix-up between representations of examples from the source and target domains considering an increasing value of the hyperparameter that accounts for the weight of the representation of the target domain example. By doing so, the authors claim an automatic curriculum is introduced in the training. Moreover, since labels from the target domain examples are not available, the authors also introduced a heuristic to pair the examples that are mixed. GIFT was empirically validated on experiments with synthetic domain shifts on the CIFAR-10 dataset and was shown to outperform iterative self-training in terms of target accuracy. Experiments on two datasets presenting natural domain shifts were also performed.  ","This work adddresses domain adaptation (DA) by a GIFT method. GIFT consists of a manifold mixup technique, which generates virtual samples by mixing up the features of source and target domains samples. The mixup coefficient is annealed over time to bias towards the target domain. Another ingredient of GIFT is a co-teaching strategy that lets two networks teach each other. On a few small synthetic as well as natural image datasets, GIFT outperforms a few baseline methods.",This paper introduces how to deal with the situation when intermediate distributions are unavailable. The basic idea is to create virtual samples by interpolating source and target representations. The effectiveness of the proposed method is evaluated and results in several interesting conclusions.,0.25609756097560976,0.12195121951219512,0.0975609756097561,0.12121212121212122,0.08585858585858586,0.1282051282051282,0.10606060606060606,0.1282051282051282,0.19047619047619047,0.3076923076923077,0.40476190476190477,0.23809523809523808,0.15,0.125,0.12903225806451615,0.17391304347826086,0.14166666666666666,0.16666666666666666
220,SP:8cfafcf0de6de33a8fd298593eeea82376b4697a,Recent successful model-based offline RL techniques have relied on heuristics to penalize rewards according to the uncertainty of the estimated MDP. This paper reviews the different penalties that have been designed in the literature. The impact and importance of associated hyperparameters such as the planning horizon and the number of models in the ensemble are also evaluated. The author show that the selection of the best penalty and best hyperparameters lead to stronger performance.,"The authors present an empirical study of several uncertainty quantification heuristics applied to model learning in offline model-based reinforcement learning. Specifically, they consider the basic architecture of MOPO, in which an uncertainty based state-action penalty function is applied on top of the standard reward to construct a pessimistic MDP. Within this set up, the authors perform a empirical study of several different uncertainty penalties, exploring their correlation to the prediction error of the model, as well as in terms of their ability to detect individual transitions with high-percentile prediction errors. Finally, the authors perform Bayesian optimization over the choice of the uncertainty penalty as well as other hyperparameters such as number of ensemble elements, planning horizon, penalty weights, etc. They present these results showing that the optimal choice of penalty and penalty weight can vary significantly, not only between environments, but also within an environment as a function of offline dataset.","The paper provides an evaluation of many of the design choices and hyperparameter decisions made in offline model-based reinforcement learning methods which have emerged recently. Particularly, the empirical study looks at uncertainty penalties used in these methods, as well as hyperparameters such as ensemble size, penalty weighting, and rollout horizon. The authors find that offline MBRL methods are quite sensitive to each of these parameters. They compare the “optimized” version of MOPO with hyperparameters tuned using Bayesian optimization, and find that it leads to statistically significant performance improvements over the version of MOPO presented in the original paper.","Model-based offline reinforcement learning algorithms typically involve constructing a pessimistic MDP, which is implemented based on an uncertainty estimation of the learned model. This paper conducts empirical analysis to compare different design choices of the uncertainty estimation in practice. In more details, the authors compare different approaches in terms of the correlation between the estimated uncertainty and ground truth model error. They also use bayesian optimization to search the best hyperparameter configuration that achieves strong empirical performance. ",0.3466666666666667,0.24,0.25333333333333335,0.18831168831168832,0.14285714285714285,0.1717171717171717,0.16883116883116883,0.18181818181818182,0.24358974358974358,0.29292929292929293,0.28205128205128205,0.21794871794871795,0.2270742358078603,0.20689655172413793,0.24836601307189546,0.2292490118577075,0.1896551724137931,0.192090395480226
221,SP:8f0eb77f64b185627b7a82005e0b9e368197c8cd,"The authors provide a new scalable listwise loss for ranking called PiRank.  The loss is defined as 1 - a differentiable relaxation of NDCG metric with temperature-controlled inspired by NeuralSort algorithm. In this setting, the difference with the NDCG is negligeable the temperature tends to 0.  Scalability is ensured by a divide-and-conquer strategy where the sorting relaxation is applied to sub-lists of smaller size and then propagate only on the top items from each sub-list to merge the sort.","This paper proposes a new class of surrogates called PiRank for ranking metrics optimization. In particular, it uses temperature-controlled relaxation to the permutation matrix to derive a continuous and differentiable sorting operator. Due to the large size of the ranking lists in practical applications, this paper also proposes a divide-and-conquer method to reduce the complexity of metric computation. Experiment results show that the proposed methods can achieve promising results on two popular benchmarks.  ","This paper tackles the problem of Learning To Rank. The idea is to relax ranking metrics (eg DCG), that are non differentiable with respect to the model parameters theta. Originally, these metrics measure how well the predicted ranking (induced by the scores predicted by the model) places high items with high true relevance labels. The relaxed metrics replace the permutation matrix in the original metrics by a relaxed, unimodal matrix, parametrized by a temperature.  This relaxation was proposed in Neuralsort, but is of quadratic complexity in the total number of items L at each time. In this paper, the authors propose a computationally efficient version of Neuralsort, truncated at the k first items, relying on a divide and conquer strategy. This approach is of much lower complexity, expressed in terms of L,k, and d the depth of the tree.  Finally, the method is evaluated on benchmark learning to rank datasets. It obtains similar or better performance than sota methods.","This paper proposes PiRank, a new neural learning-to-rank (LTR) approach based on differentiable sorting operators. The authors present empirical results against two widely used LTR data sets (MSLR-WEB30K and Yahoo! C14).",0.20481927710843373,0.26506024096385544,0.060240963855421686,0.3026315789473684,0.14473684210526316,0.05625,0.2236842105263158,0.1375,0.14705882352941177,0.14375,0.3235294117647059,0.2647058823529412,0.21383647798742136,0.18106995884773666,0.08547008547008546,0.19491525423728812,0.2,0.09278350515463918
222,SP:8f28988012f8dca74c90316f7feeda15d49af2c5,"This paper studies the domain generalization (DG) problem setting and proposes a modification to the stochastic weight averaging (SWA) method that leads to better performance on DG testbeds. The proposed method, named SWAD, uses the same basic idea as SWA but samples parameters more densely and uses additional techniques for preventing the incorporation of overfit parameters. Some theoretical analysis supports the general thrust of seeking flat minima for generalization. The focus of the paper, however, is empirical, as SWAD demonstrates improved performance compared to many prior methods on several DG testbeds. Furthermore, SWAD can be combined with these prior methods, such as CORAL, for even better performance.","This paper studies how to improve the model performance on domain generalization problems from the perspective of loss landscape. Specifically, the authors propose to apply weight averaging (WA) to find flat minima and demonstrate this could lead to improve model generalization on unseen domains. Theoretically, the authors provide generalization bounds for the proposed method. Empirically, the proposed method achieves better performance on benchmark datasets compared with previous methods.","This paper extends the idea of Stochastic Weighting Averaging (SWA) to the context of Domain Generalization (DG). Theoretically, the authors argues that finding a flat minima improves domain generalization. Algorithmically, the authors modify SWA by increasing the number of samples being averaged (e.g. per iteration) and by selecting the interval from where the samples are taken. The proposed algorithm  is evaluated in  the DomainBed benchmark and shows compelling results.","This paper provides some theoretical justification that so-called flat solutions exhibit better domain generalization along with a strongly performing training methodology, SWAD, for improving domain generalization. They first prove theorems bounding the domain generalization performance in terms of robust risk minimization. Then they propose an approach to finding flat solutions related to previous work on in distribution generalization, and they thoroughly explore this method with regard to the flatness of solutions found, its domain generalization performance, and also perform ablations of the method.  ",0.17757009345794392,0.1588785046728972,0.1308411214953271,0.25,0.23529411764705882,0.17142857142857143,0.27941176470588236,0.24285714285714285,0.16666666666666666,0.24285714285714285,0.19047619047619047,0.14285714285714285,0.2171428571428571,0.192090395480226,0.14659685863874344,0.24637681159420288,0.21052631578947367,0.15584415584415584
223,SP:8fa76926a21fb41c5c9fd357246c06a42ae26b9f,"This paper investigates the application of transformer for image generation by replacing the convolutional layer with MLP(including attention). The motivation provided is the limitation of the style-based generator (e.g.,  StyleGAN), which leverages the single style over multiple image regions, allowing a less entangled representation learning in W and visible artifacts in generated images.  To combat this shortcoming, authors introduce transformer-liked architecture, and successfully generate high-quality images (1024*1024) without touching any convolutional layers in generator.","This paper proposes to adopt the StyleGAN architecture (i.e. individual styles that affect image generation) to a transformer based model for image generation. The model generates individual image patches which are modulated by style tokens through an attention mechanism. The model is evaluated on the FFHQ and LSUN Church datasets and performs similarly well or better than StyleGAN2 based on FID, precision, and recall.","In this paper, a transformer-based modification of the generator in the ubiquitous StyleGAN model is proposed (named _TokenGAN_). More specifically, the entire generator is replaced by a convolution-free architecture where style-content modulation is now modeled using cross-attention layers instead of AdaIN. Similar to the original StyleGAN, this should allow for decoupled control of style and content at different granularities. The paper shows that the proposed approach achieves FID scores comparable to or better than StyleGAN2 for unconditional image synthesis on FFHQ and LSUN-Churches.","This paper proposes TokenGAN for unconditional image synthesis. The token-based generator takes the learned content tokens and style tokens from the latent space as input, and output a series of style-modulated content tokens, which are then concatenated and reshaped to get the output image. Cross-attention is used to assign the styles to different content tokens. The model adopts the style-based generator idea from StyleGAN and introduces the recent token-based structures and transformers to the traditional style-based generator. Experiments are conducted on FFHQ and LSUN CHURCH datasets and the quantitative results are slightly better than StyleGAN2.",0.175,0.175,0.2125,0.27692307692307694,0.3230769230769231,0.23863636363636365,0.2153846153846154,0.1590909090909091,0.16831683168316833,0.20454545454545456,0.2079207920792079,0.2079207920792079,0.19310344827586207,0.16666666666666666,0.18784530386740333,0.23529411764705882,0.25301204819277107,0.22222222222222224
224,SP:8fdfed1c38ae00a0063ab41f72fa26826f5f4570,"This paper proposes efficient packing methods for training sequences of BERT, such that the 50% of the padding tokens in the Wikipedia dataset is avoided to speed up the training. These methods include shortest-pack-first histogram-packing (SPFHP) and non-negative least-squares histogram-packing (NNLSHP) algorithms, which are shown to be straightforward to implement and have little impact on the performance. Empirical studies show that a near 2x speedup over the vanilla BERT training is achieved by the proposed methods. ","This paper proposes two packing algorithms for bert pretraining, shortest-pack-first histogram-packing (SPFHP) and non-negative least-squares histogram-packing (NNLSHP). 2* speed was achieved under datasets such as Wikipedia for bert-large training.These packing algorithms packed Wikipedia’s 16M sequences in 0.02s (SPFHP). Packing depths of from 1 to 16 were testified. This paper also has 20 pages near appendix telling about packing algorithms, packedBERT of model changes and hypermeter adjusting, and detailed experiments such as bin-packing algorithm comparison, scaling analysis and technical background on packing with some core codes attached as well.","This paper profiles the training data of BERT and finds out opportunities to reduce padding thus saving computation. By packing multiple sequences into one fixed length sequence (two packing algorithms were used), the author shows that a BERT training speed-up of 2x can be achieved without loss of quality. The author also studies how to correspondingly change the modeling configuration and optimizer configurations to compensate the effective batch size increase due to the packing.","The paper proposed to pack sequences instead of padding to reach the max sequence length for each sample for BERT pretraining. To achieve similar optimization results with the original training process, the proposed method modified positional embedding, additional attention masks and other optimization hyper-parameters. On the Wikipedia dataset, the proposed method can achieve 2x speedup while achieving similar training loss.  ",0.2926829268292683,0.1951219512195122,0.17073170731707318,0.12121212121212122,0.0707070707070707,0.13333333333333333,0.24242424242424243,0.21333333333333335,0.22950819672131148,0.16,0.11475409836065574,0.16393442622950818,0.26519337016574585,0.20382165605095545,0.1958041958041958,0.1379310344827586,0.0875,0.14705882352941174
225,SP:8ff52b027a3c2a464b2c2fedb768c092b0fc6ca5,"This paper connects the architecture of a resnet and a gaussian injected resnet to that of a transport equation and a diffusion equation respectively. They show that under this framework they provide robustness guarantees of their PDE framework scale with the $\sigma$ parameter of the diffusion equation. Furthermore, they also show that larger the $\sigma$ better the generalization gap for the resulting neural network.   They introduce a learning algorithm, to attain the network from their PDE framework starting from an initial NN (also a resnet).  They also empirically verify that their method improves robustness, and that it performs better than Gaussian Noise injection.","This paper considers DNNs as transformations of the input data which can be seen as discrete approximations to the solutions of PDEs. It reformulates a point of view which has been trending for a few years now. More precisely, this paper represents the network as an operator, which is actually solution to a PDE, which acts on a ""base classifier"". Using facts from PDE theory, they then propose improvements for standard neural architectures.","This paper studied a ResNet-like DNN model that can be expressed as a discretization of PDEs. First, this paper showed that, under some assumptions, any adjust operator is a solution of a second-order convection-diffusion PDE (Theorem 1). ResNets and ResNets with Gaussian noise injection are special cases of this theorem. Next, for the specific PDE (Eq. (7)), this paper derived generalization guarantees regarding the Rademacher complexity. This paper also derived robustness guarantees in terms of input perturbations. Finally, this paper analyzed the predictive performance of ResNet trained by the proposed method with various model hyperparameters on clean and adversarial datasets.","The authors cast DNN classifiers as the push-forward of a base classifier under a flow map at some fixed final time. Under some reasonable assumptions on the flow, they show that, given any base classifier, the flow map can be obtained as the solution to a convection-diffusion equation. They show that ResNets and Gaussian noise injection can be viewed as special cases of their model and give a robustness guarantee for any  classifier defined as a solution to their PDE. Experiments on the 2-d half moon data set as well CIFAR 10 and 100 show better robustness  of their model to adversarial attacks  when compared to standard ResNet(s).",0.13592233009708737,0.18446601941747573,0.20388349514563106,0.2054794520547945,0.2191780821917808,0.2524271844660194,0.1917808219178082,0.18446601941747573,0.1875,0.14563106796116504,0.14285714285714285,0.23214285714285715,0.1590909090909091,0.1844660194174757,0.1953488372093023,0.17045454545454544,0.17297297297297298,0.24186046511627907
226,SP:903545b1b340ec5c13070e0f25f550c444de4124,"This paper presents a new Shortest Distance Query technique, namely, the Betweenness Centrality-based Distance Re- sampling (BCDR). The objective of this technique is overcome some drawbacks of traditional embedding-based distance prediction methods based on truncated random walks and point-wise Mutual Information (PMI). These drawbacks are a limited distance exploration and the lack of preservation of the shortest distance relation due to local optima. BCDR uses betweenness centrality to create a random walk that occupies a wider distance and uses Distance Resampling (DR) instead of PMI to preserve the relationship of distances. The paper presents theoretical guarantees of performance to address the exploration range and the intractability of shortest distance on paths. The experiments on three real datasets and simulated datasets show the performance evaluation with respect to baselines, exploration distance, and preservation of distance relation (and violation of the probability distance relation)","In this work the author(s) proposed a framework for node embeddings that captures better shortest path distances for undirected graphs. Specifically, the authors propose a new random walk framework based on betweenness centralities, and a distance resampling strategy that uses the shortest path distances, and the betweenness centrality scores and is shown to capture well shortest path distances (Proposition 1).  The authors also evaluate their framework experimentally, verifying that compared to other popular node embedding methods, it can be used to represent shortest path distances faithfully. ","This paper proposes a method to construct graph embeddings that are well-tuned for answering shortest-distance queries (SDQs), based on betweennes centrality distance sampling. The main idea appears to be that the betweenness centrality measures helps identify nodes that could serve as landmarks for distance calculations, hence an distance-oriented embedding anchored on betweenness centrality is bound to perform well. Distance are resampled from walk paths, and a step of other methods based on pointwise mutual information (PMI) optimization is abandoned.","This paper proposed a new graph shortest distance embedding method. This method uses a betweenness centrality based random walk to sample paths in the graph and distance resampling step before optimization. They show that the estimated distance after embedding has a linear dependence with the original distance in the graph They also show that this embedding method preserves the shortest distance relation between points. In experimental results, they show that this algorithm achieves better accuracy than previous algorithms.  ",0.1310344827586207,0.13793103448275862,0.15862068965517243,0.19540229885057472,0.20689655172413793,0.17073170731707318,0.21839080459770116,0.24390243902439024,0.2948717948717949,0.2073170731707317,0.23076923076923078,0.1794871794871795,0.16379310344827588,0.1762114537444934,0.2062780269058296,0.2011834319526627,0.21818181818181817,0.17500000000000002
227,SP:91fd4189bf04aca4ccd1288ec8459e1edb29d378,"The authors propose ROSA, a reward shaping method that trains a separate “Shaper” policy to learn how to generate reward bonuses. The problem is formulated as a two-player Markov game, in which the environment dynamics are only affected by the primary, “Controller” policy, but the Shaper and Controller policies each optimize their own rewards. The Controller’s reward is the normal reward plus a potential-based reward that depends on the previous two states and the previous two Shaper actions. The shapers’ reward is a combination of the Controller’s reward, a penalty for switching often, and a count-based exploration bonus. The shaper’s rewards are also automatically gated by a switching function, which is hard-coded to switch with higher probability whenever a curiosity-metric increases (in this case using the RND exploration metric). The authors extend the standard reward shaping result to show that including the actions of the Shaper in the potential-based reward shaping term maintains policy invariance. Then, the authors empirically demonstrate on a grid-world environment that ROSA provides useful reward shaping that guides the agent towards the goal and ignores irrelevant parts of the state space. The authors also compare to and find that ROSA outperforms ICM, RND, PPO, and LIRPG on Gravitar, and gets similar performance on Solaris and Super Mario as some of these prior works.","The key idea in this paper is to jointly train a pair of agents, namely a Controller that performs the RL task and a Shaper agent that shapes the Controller's reward function to better its performance. Rather than shape all states, Shaper learns ""switching controls"" to determine states on which to place its modeling effort on, and ablation experiments suggest this works well compared to the straightforward approach of shaping all states. A natural concern is whether this Markov game approach results in stable training and convergence. To address this issue, the authors provide theoretical convergence results which show ROSA convergences to a Nash Equilibrium (NE) with weakly higher total return. Experiments on several domains suggest the method works well relative to alternative reward shaping approaches (e.g., those based on curiosity or bi-level optimization). ","This paper introduces a new algorithm to solve the sparse reward settings. The algorithm tries to find an optimal policy by leveraging two learners: a controller and a shaper. The controller learns to maximize the environment reward signal plus the signal provided by the shaper. The shaper learns a potential-based reward shaping function by maximizing the controller's objective plus a cost penalty for providing reward feedback and an exploration bonus. The shapers reward function is created using a randomly generated state function and a dot product based on the shaper's ""action."" Some theoretical analysis is provided along with experiments demonstrate to demonstrate that this algorithm is helping solve sparse reward problems.  ","The paper proposes to frame the reward shaping method as a Markov game between two players. In this setting the first player is learning to act in the environment while the second player is learning to provide reward shaping to the first player. Additionally, the authors propose to use a switching scheme that indicates whether or not to perform reward shaping for the first player. The authors provide some theoretical guarantees under a large set of assumptions. The authors perform experiments on a set of toy tasks and a few more complex domains.",0.12334801762114538,0.15418502202643172,0.14096916299559473,0.1678832116788321,0.15328467153284672,0.18421052631578946,0.20437956204379562,0.30701754385964913,0.34408602150537637,0.20175438596491227,0.22580645161290322,0.22580645161290322,0.15384615384615385,0.20527859237536655,0.20000000000000004,0.1832669322709163,0.1826086956521739,0.20289855072463767
228,SP:931661154975d94fc5ba1bc89d7a7fdf643df8f2,"The paper works on a novel problem of interpreting and explaining structured output models. The paper utilizes an energy based model to account for correlations between structured outputs and learns an interpretability block which given as input an image learns to mask it such that the energy based model would assign a similar score to the ground truth output and input as well as the ground truth output and perturbed input. In essence, the energy based model is a proxy for the actual deep neural network performing the structured prediction task. Results on a couple of datasets demonstrate that the work does better than baselines like LIME which do not utilize the correlations in the outputs modeled by the energy based model.  ","The authors propose an energy-based training method for achieving model interpretability, which performs instance-wise feature selection. The proposed model adopts a similar approach of one of the pre-existing interpretable methods by calculating feature-level importance score with regard to each instance. The authors validate their method on synthetic and public datasets.","The paper proposes a technique for identifying what input variables are most relevant for determining the value of a single, given output variable in structured-output (MAP) inference.  The idea is to learn an energy model that predicts which input variables are relevant to a particular structured prediction (x, y).  The authors propose to implement the energy model using a neural network followed by a Gumbel-softmax activation, and to train it by maximizing a structured hinge loss.  The proposed approach is evaluated on three datasets and compared to standard attribution techniques (LIME, SHAP, L2X).","This paper propose a method for interpreting structured output model. The key idea is to find an ""interpretation"" which explains an ""target"" output random variable based on subset of rest of the output variables. The training objective is on finding a small subset which keeps the target output random variable invariant. The proposed methodology is applied to explain a synthetic energy function and structured prediction energy networks. ",0.13114754098360656,0.20491803278688525,0.1721311475409836,0.24074074074074073,0.2222222222222222,0.18947368421052632,0.2962962962962963,0.2631578947368421,0.31343283582089554,0.1368421052631579,0.1791044776119403,0.26865671641791045,0.18181818181818182,0.2304147465437788,0.22222222222222224,0.17449664429530204,0.1983471074380165,0.2222222222222222
229,SP:943b0a3f94ba270bb7c0dc1e1f363e53bc5cf8ae,"This work investigates the use of Deep MARL in order to study economies. They propose and implement an RBC (economic market model) and propose a reward shaping schedule to bias agents into learning non-degenerate joint-strategies. In their setting, low-welfare equilria and provide a brief quantitative and qualitative analysis of discovered equilriba in open- and closed-economies. ","This paper proposes a multi-agent deep reinforcement learning (DRL) method to compute general equilibria in economics. The main contribution is hence algorithmic. The method uses a combination of DRL, structured learning curricula, and suitable annealing of action space and of some penalty coefficients (which are useful to make some problems easier to solve, but distort the computed solution). The authors apply their method to an example of real-business-cycle model, which involve three types of agents: firms, consumers and a government. The problem is of Stackelberg (or leader-follower) type, with the government acting as a leader. Computing equilibria for such problems is generally very challenging. ","This paper proposed a deep reinforcement learning framework for finding dynamic general equilibrium, which is one of the most fundamental problems in economics. As the dynamic general equilibrium is a special case of the Markov game, this problem has also been recognized as a significant topic in machine learning. The proposed scheme is tested in a real-business-cycle model with 100 worker-consumers, 10 firms, and a government, the scale of which is much larger compared with the numerical examples in most related works.",This paper uses a multi-agent reinforcement learning algorithm to simulate an economic environment and solve the general equilibrium of the induced game. The authors propose to use a structured learning curriculum that runs only on GPUs. The authors conduct experiments to show that their algorithm converges fast and that the solution represents an epsilon Nash equilibrium.,0.1694915254237288,0.1694915254237288,0.1694915254237288,0.25,0.17592592592592593,0.18823529411764706,0.09259259259259259,0.11764705882352941,0.17543859649122806,0.3176470588235294,0.3333333333333333,0.2807017543859649,0.11976047904191617,0.13888888888888887,0.17241379310344826,0.27979274611398963,0.23030303030303026,0.22535211267605634
230,SP:95ed80753116005f1f7bae24c855d350f4af85a1,"The paper presents a collection of somewhat disjoint contributions to outlier detection. First, the authors propose Species - a novel OOD test dataset. The main advantage of this dataset is being disjoint from ImageNet-22k. Second, the authors propose to detect outliers according to the max-logit criterion. The authors claim that max-logit is especially suitable for OOD detection in multi-label environments. Third, the authors propose two novel datasets for dense outlier detection. StreetHazards is especially interesting since it allows proper rendering of introduced outliers. ","This work explores out-of-distribution (ODD) detection in three large-scale settings: multi-class OOD detection, multi-label OOD detection and anomaly segmentation. To facilitate large-scale experiments, it introduces a novel species dataset and a road anomaly dataset for multi-class OOD detection and anomaly segmentation respectively. It also demonstrates a new setup for multi-label OOD detection. In addition, this work establishes a new baseline via a simple detector based on the maximum logit in all the three large-scale settings.","The authors extend the out-of-distribution (OOD) detection from not-seen in small-scale settings to large-scale multiclass and multi-label ones. They provide large-scale benchmarks for evaluating ODD detectors on classification as well as segmentation. Additionally, they propose a simple yet strong baseline for this practical problem.","The paper presents the negative of the maximum unnormalized logit (MaxLogit) as an anomaly score for out-of-distribution (OOD) detection. Also, it introduces a large-scale setup for ODD. The proposed metric shows promising results compared to the maximum softmax probability (MSP) in the proposed setup (in-distribution ImageNet-1K and out-distribution Places365). For the multi-label experiment, the PASCAL VOC and MS-COCO are in-distribution and ImageNet-22K out-distribution. The proposed MaxLogit works better than in this MSP too. Finally, the proposed metric shows promising results in the  CAOS benchmark.",0.13953488372093023,0.12790697674418605,0.16279069767441862,0.19047619047619047,0.21428571428571427,0.2549019607843137,0.14285714285714285,0.21568627450980393,0.14736842105263157,0.3137254901960784,0.18947368421052632,0.1368421052631579,0.1411764705882353,0.16058394160583941,0.15469613259668508,0.23703703703703702,0.20111731843575417,0.1780821917808219
231,SP:96e1da163020441f9724985ae15674233e0cfe0d,"This paper studies a networked MARL problem based on the model in [Zhang et al 2018], where each agent can observe the global state, take local action and observe local rewards. The key difference in setting from [Zhang et al 2018] is that [Zhang et al 2018] assume the global action can be observed, but in this paper, only local action is known to each agent. To deal with this, an additional consensus loop is added to estimate the average TD error, which can be used to estimate the advantage function. Further, compared to [Zhang et al 2018], a finite time error bound is provided.  ","This paper considers cooperative multi-agent reinforcement learning (MARL) for average reward MDPs with fully decentralized actor-critic methods. In particular, the authors make some progress on top of existing works in this direction, and in particular (Zhang et al., 2018). More precisely, the authors remove the assumption in (Zhang et al., 2018) that the joint actions are observable to all agents, and propose to modify the actor updates with mini-batch TD sharing to accommodate the scenario where each agent only observes its own action. The authors then establish a finite-sample bound for the proposed algorithm in terms of convergence to stationary points under linear value function approximation. Numerical experiments are also provided to showcase the benefits of the modifications over the algorithm in (Zhang et al., 2018). ","This paper establishes the first finite-time convergence result of the actor-critic algorithm for fully decentralized multi-agent reinforcement learning (MARL) problems with average reward. It focuses on the practical setting where the rewards and actions of each agent are only known to itself, and the knowledge of joint actions of the agents is not assumed. The established finite-sample complexity matches that of the state-of-the-art single-agent actor-critic algorithms.","This paper studies the cooperative average reward fully decentralized multi-agent reinforcement learning (MARL) problems, where the agents interact with their neighbors over a communication network. It proposes a consensus-based actor-critic algorithm and shows its convergence to the stationary point. The convergence rate and sample complexity of this algorithm are provided and comparison with existing algorithm is shown in the numerical experiments. ",0.2857142857142857,0.13333333333333333,0.12380952380952381,0.19230769230769232,0.16153846153846155,0.28,0.23076923076923078,0.18666666666666668,0.203125,0.3333333333333333,0.328125,0.328125,0.25531914893617025,0.15555555555555556,0.15384615384615385,0.2439024390243902,0.21649484536082478,0.302158273381295
232,SP:96f4f90488c15167d85261a883cd70fc15e06bb9,"Learning to discover novel class is a very challenging task and a new research topic in recent years. In this task, a known-class dataset can be used to help cluster novel classes. “Novel” means that there are no overlaps between novel classes and known classes. This setting looks ill-defined since it is not clear why we need known classes to help cluster novel classes. However, this paper answers this question based on a novel concept: K-epsilon separation and points out when this setting is ill-defined/well-defined. Based on this contribution, this paper is above the acceptance borderline.   Nevertheless, the presentation should be polished. Although I understand the setting and the contribution in the end, the presentation flow is not smooth. Some typos make me struggle when I read this paper. Besides, the connection between sampling process and Assumption (D) should be discussed deeply. I find that they might not the same thing.  ","The authors provide a formal definition of the L2DNC task, give proofs of useful insight into the problem, and empirically demonstrate novel methods. The authors use several baselines and ablation to demonstrate the value of the proposed methods. The theoretical insights suggest that common high level features are necessary to solve the L2DNC task, which motivates the proposed approaches based on low dimensional non-linear projection of the data. Similarly the clustering-centric definition of the problem motivates the similarity learning methods presented.","The paper introduces meta-discovery as a way to adapt meta-learning strategies to the problem of learning to discover novel classes. It also presents a formalization of the problem that helps shading light on the conditions under which it is learnable.  The authors additionally introduce a novel sampling strategy aimed at sampling data having the same ""dominant"" view to help the following clustering procedure.  The experimental evaluation shows advantages over existing recent competitors when the number of examples for unseen classes is small.","This paper considers the problem of novel class discovery (NCD). Different from the original setting of NCD, this paper reconsiders the assumptions behind NCD and defines a new yet more practical setting, which can significantly reduce the number of unlabeled data needed for training novel classes. In addition, this paper presents a meta-learning-based approach to address the new setting, which achieves consistent improvements on four public datasets. They also provide a theory to reveal why we need to assume that known and novel classes should share semantic features.",0.08917197452229299,0.09554140127388536,0.10828025477707007,0.1927710843373494,0.14457831325301204,0.16666666666666666,0.1686746987951807,0.17857142857142858,0.18888888888888888,0.19047619047619047,0.13333333333333333,0.15555555555555556,0.11666666666666667,0.12448132780082989,0.1376518218623482,0.19161676646706585,0.13872832369942195,0.16091954022988508
233,SP:97f618558f4add834e5930fd177f012a753247dc,"This paper proposed a new subset selection method with novel diversity objective and balancing constraints. The authors prove that the proposed sample selection criterion is a submodular function. Hence its greedy algorithm comes with approximation guarantees (Nemhauser et al., 1978). The experiments on popular image classification datasets show that the proposed method is slightly better than k-center (Sener & Savarese, 2018). ","This paper presents a new method for batch active learning in deep neural networks, which aims to progressively construct compact subsets from a training dataset that maximize accuracy of learned models. The authors propose to address this problem by optimizing a submodular set function, expressed as a weighted combination of uncertainty, diversity, and triplet objectives, under a set of balancing constraints on classes and decision boundaries. The resulting optimization problem is solved using a greedy algorithm. Experiments are conducted on CIFAR10, CIFAR100, ImageNet and CIFAR100-LT datasets, where the proposed approach outperforms all baselines, with larger improvements under a class-imbalanced setting (CIFAR100-LT).","The paper describes a new active learning algorithm for classification problems. The authors propose a selection criteria that trades off between uncertainty and diversity. The novelty lies in formulating a three-pronged approach to diversity that still manages to maintain submodularity of the selection criteria: 1) a traditional diversity metric based on cosine similarity of embeddings 2) a new triplet/clique ""loss"" that penalizes the selection of nearby points 3) class-balancing and boundary-balancing constraints  By showing this three-pronged approach maintains submodularity, they are able to devise a greedy algorithm that still has a constant approximation guarantee to the optimal selection subset (as measured by the selection criteria). Experiments on CIFAR-10, CIFAR-100, ImageNet, and CIFAR-100LT (long-tailed) suggest that the new approach performs as well as or better than state-of-the-art methods.","Most batch-mode active learning strategies involve maximizing a sub-modular score function of the value of each image to be labeled. This paper demonstrates that current methods fail to sample diverse classes or images near decision boundaries, arguably one of the most important regions to obtain label information. The paper proposes a technique for incorporating class-balance and boundary-balance constraints to the sub-modular optimization problem and performs experiments over several image data sets to show the value of class and boundary balancing. ",0.3114754098360656,0.32786885245901637,0.16393442622950818,0.2403846153846154,0.16346153846153846,0.1366906474820144,0.18269230769230768,0.14388489208633093,0.11764705882352941,0.17985611510791366,0.2,0.2235294117647059,0.2303030303030303,0.19999999999999998,0.136986301369863,0.205761316872428,0.17989417989417988,0.16964285714285715
234,SP:99a36b28752bfc101877bfd0da436e6fb19c69d3,"This paper studies the ability of shallow and deep neural networks to approximate Korobov functions, and analyses their representation power in terms of the number of used parameters. The authors first show that 2 layers neural networks using common activation functions can approximate any Korobov function within eps error in infinity norm using O(eps^{-1/2} (log 1/eps)^{3(d-1)/2}) parameters. This result improves the existing upper bound of O(eps^{-d/r}) parameters for approximating Sobolev functions in W^{r,p} using 1-hidden layer neural networks, hence reducing the curse of dimensionality.  For deep neural network, this paper provides a new result, showing that neural networks with depth O(log d) and O(eps^{-1/2} (log 1/eps)^{3(d-1)/2}) approximate Korobov functions in X^{2, infinity} within eps error in L-infinity norm. This improves the result of [1], by requiring a depth independent of the required accuracy. However, the current work requires a C^2 and non-linear activation function, and is hence not applicable to ReLU.  Finally, the authors show that any continuous function approximator for the Korobov space requires O(eps^{-1/2} (log 1/eps)^{(d-1)/2}) parameters for achieving error eps, hence matching the previous upper bound for NNs up to factor (log 1/eps)^{d-1}.  [1] Hadrien Montanelli, Haizhao Yang, and Qiang Du. Deep ReLU networks overcome the curse of dimensionality for bandlimited functions","This paper studies what function class can be efficiently approximated by neural networks. This paper focuses on a special function class, namely the Korobov function space $X^{2, \infty}$, which contains function with $L^\infty$ bounded weak $\alpha$-order derivative, where $\| \alpha \|_\infty \le 2$. This is a local constraint on the local smoothness of the function space. This paper shows that shallow (with depth=$2$) and deep neural networks can efficiently approximate $X^{2, \infty}$, and the number of parameters does not scale with $\varepsilon^{-\mathrm{poly}(d)}$, thus efficiently escaping the curse of dimensionality. Furthermore, this paper shows the optimality of their parameters bound by showing a matching lower bound. ","This paper studies approximation capabilities of neural networks for the purpose of approximating Korobov functions which are multivariate functions of bounded second mixed derivatives.  The paper presents a complete study for approximating such functions with NNs: they study shallow nets and show that 2 layers with ReLUs and total #neurons of O(1/eps log^{1.5d}(1/eps))  where d is the dimension, can \eps-approximate  Korobov functions. Moreover, by allowing larger depths, close to logd, they can get a better depedence on \eps. Finally, they prove that any continuous function approximator requires a #params close to their upper bound in order to approximation Korobov functions.  This gives a complete picture for how Korobov functions behave wrt to function approximation with shallow or deep nets.  ","The paper proves upper and lower bounds on neural networks for approximating Korobov functions $X^{2, \infty}$. Upper and lower bounds match for approximation in $L^\infty$ norm sense, and the rate is free of the curse of data dimensionality. The scope of network architectures discussed is extensive, including shallow (2-layer) networks, deep networks with ReLU(-like) activation functions and Sigmoidal activation functions.",0.128099173553719,0.1652892561983471,0.09090909090909091,0.1875,0.16071428571428573,0.1111111111111111,0.2767857142857143,0.31746031746031744,0.34375,0.16666666666666666,0.28125,0.21875,0.17514124293785308,0.21739130434782608,0.1437908496732026,0.17647058823529413,0.20454545454545459,0.14736842105263157
235,SP:9ba33d09bd68d8598e2aff428ecca5060922a4dc,"This paper presents a theory of domain generalization based on statistical learning theory (Rademacher complexity) and demonstrates a trade-off between training loss and model complexity. They show that existing methods are actually controlling the model complexity. Based on the analysis, the authors argue that proper model selection is critical for complexity control. Instead of hyper-parameter search, they propose to use domain-wise cross-validation as the model selection strategy. Experiments on the DomainBed benchmark show the effectiveness of the proposed method.","This paper has several findings taking a bias-variance trade-off of DG performance, that explains existing DG algorithm performance variability. (ii) The complexity control strategy used to determine bias-variance trade-off is crucial in practice, with peak DG performance achieved when optimizing model complexity based on domain-wise validation. (iii) Regularisation required for optimal DG is greater than for conventional optimization for within-domain performance.","This paper considers the problem of domain generalization (DG), wherein predictors are trained on a related set of training domains and evaluated on an unseen test domain.  The authors first present a learning-theoretic bound on the performance of an average-case formulation for DG, and then present a set of experiments that consider the trade-off between complexity and out-of-distribution (OOD) performance.  The experiments indicate that such trade-offs exist for linear models, and there is also evidence that the trade-off persists for shallow neural networks.","In domain generalization, recent work has shown that ERM has comparable out-of-domain accuracy to state-of-the-art DG methods. The paper aims to explain this result through model complexity. The main argument is that ood generalization requires a smaller model complexity. The upshot is that DG methods can utilize cross-domain validation to obtain better generalizing models.",0.18072289156626506,0.21686746987951808,0.1566265060240964,0.19402985074626866,0.14925373134328357,0.13333333333333333,0.22388059701492538,0.2,0.21666666666666667,0.14444444444444443,0.16666666666666666,0.2,0.19999999999999998,0.20809248554913298,0.18181818181818182,0.16560509554140126,0.15748031496062992,0.16
236,SP:9bd0a519881297066ee60ccf62ee27e4c109047d,"This paper considers design choices involved in offline RL approaches that consider a reduction to weighted/conditional behavior cloning, a class of approaches referred to as Reinforcement Learning via Supervised Learning (RvS). The paper studies various issues including expressivity of policy architecture, regularization, choice of conditioning variables (e.g. based on goals or rewards).   At a high level, the takeaways are (a) RvS approaches are successful when using neural networks with appropriate capacity and regularization, (b) with appropriate conditioning variables (goal/reward based conditioning), (c) RvS can obtain policies that exhibit compositional behavior by conditioning on appropriate events.",The paper investigates different variants of behavior cloning. The methods investigated aim at achieving better policies for use in offline RL through supervised learning than the average behavior contained in the data. The methods studied are categorized and examined against three to four benchmark problems.,"The paper studies the behavior cloning based strategies of offline RL algorithms in different type of environments and reports that performance primarily depends on model size and regularization. The results contradict some of the earlier claims, and the authors conjecture that model size and regularization characteristics can explain past results. The paper also discusses additional insights such as the importance of conditioning and simple validation based evaluation fail to generalize. ","This paper studies the importance of the design decisions for supervised learning type reinforcement learning algorithms. Through extensive experiments, find that more complex design choices, such as the large sequence models and value-based weighting schemes used in some prior works, are generally not necessary. Our results show that carefully designed RvS methods can attain results that match or exceed the best prior methods across a range of different offline RL benchmarks, including datasets with little or no optimal data.",0.10204081632653061,0.12244897959183673,0.11224489795918367,0.24444444444444444,0.2222222222222222,0.18571428571428572,0.2222222222222222,0.17142857142857143,0.1375,0.15714285714285714,0.125,0.1625,0.13986013986013987,0.14285714285714288,0.12359550561797752,0.19130434782608693,0.16,0.17333333333333334
237,SP:9c399331a3b4a55d7e1ff9298f82a38b75b4f87d,"This paper presents a new distributed learning framework exploiting the vision transformer for various image processing applications. It gives impressive quantitative and qualitative results on multiple image restoration tasks meanwhile keeping privacy. Specifically, it employs a task-agnostic vision transformer to learn universal representation at the server, and several CNN-based task-specific heads and tails to handle different image restoration tasks at the client side. It also gives a training strategy to learn this model.","The paper presents an architecture for image processing tasks that splits up a network into three subsequent parts: head, body, and tail. Head and tail parts are CNN-based and can be trained on multiple client devices using federated learning (FedAvg), while the body part of the architecture is transformer-based and is trained on a central server. Head and tail parts are trained for specific tasks, while the body part is trained in a task-agnostic manner by selecting clients from each task for loss optimization. Experimental results show benchmark and convergence results that are comparable or favorable to non-distributed models, as well as comparison results to purely FL and SL approaches with a very small nr of clients.","In this work, the authors present a multi-task distributed learning framework called TAViT. The task-specific head CNN and the tail CNN are distributed to clients with their data connected to a standard Transformer body placed in the server. With an alternating training scheme, the heads and tails on client sides are trained by task-specific learning, while the body is trained by task-agnostic learning. Experiments on four different image processing tasks show the success of task-agnostic learning of the Transformer body and its synergistic improvement with the task-specific heads and tails.","This work is aimed at distributed ""privacy preserving"" training of neural networks for image processing tasks like deblocking, denoising, deraining, and deblurring. ""One of the most important contribution"" [pg 2] is breaking down the neural network model into task-specific convolutional head and tails (trained on ""clients""), and a common shared (across tasks) Transformer based feature backbone, which is trained on the server. The heads/tails and the transformer backbone are trained in an alternate manner by assuming the other model to be fixed.    The proposed is similar to the method ""Splitfed"" (Thapa et al., 2020) but is extended for different tasks (as described above).  Experimental results demonstrate:  (i) successful training of the neural network models with the proposed method. (ii) better/comparable performance to prior works on distributed/privacy-preserving methods. (iii) better performance using the Vit backbone as compared to CNN backbones, and also with the proposed multi-task vs. single-task setting.",0.21052631578947367,0.2631578947368421,0.23684210526315788,0.19008264462809918,0.2644628099173554,0.3020833333333333,0.1322314049586777,0.20833333333333334,0.11612903225806452,0.23958333333333334,0.2064516129032258,0.1870967741935484,0.16243654822335027,0.23255813953488372,0.15584415584415584,0.21198156682027652,0.2318840579710145,0.2310756972111554
238,SP:9d326254d77a188baf5bde39229c09b3966b5418,"Recently, vision Transformers have been popular and achieve SOTA performance on various tasks. This paper proposes a more simple variant of the Transformer architecture, *i.e.*, connect image patches via simple linear/MLP layers. The proposed ResMLP shows promising performance under various setups, *e.g.*, supervised and self-supervised training, and on different datasets. It is a new exploration of the methodology of neural architectures.","The paper proposes an MLP-based architecture for sequence-based DL tasks (vision, NLP). Basically, the idea is to remove the self-attention primitive from Transformers. Instead, it transposes the seq and channel dimension and applies linear projections. This is the same idea as the concurrent work MLP-Mixer from Google. The spatial mixing is now basically a DWise Conv with full receptive field shared across channels. Paper shows reasonable results on ImageNet benchmark (needs a lot more params and worse on throughput for the same top-1 accuracy wrt ViT, and also worse than pure ConvNet baselines). Paper has plenty of ablations on the training recipe and architecture. NLP tasks only include short sequence benchmarks like Translation, with promising signs.","This paper introduces Residual Multi-Layer Perceptrons (ResMLP): a purely multi-layer perceptron (MLP) based architecture for image classification. ResMLP takes image patches as input, projects them with a linear layer, and sequentially updates them in turn with two residual operations: (i) a simple linear layer that provides interaction between the patches, which is applied to all channels independently; and (ii) an MLP with a single hidden layer, which is independently applied to all patches. At the end of the network, the patches are average pooled, and fed to a linear classifier. The main change from ViT to ResMLP is replacing the self-attention layer with a simple linear layer cross patches. There are some other small changes, like pooling layers and normalization layers.  The claimed contributions are: 1. despite their simplicity, Residual Multi-Layer Perceptrons reach surprisingly good accuracy/complexity trade-offs with ImageNet-1k training only. From this point of view, this ""surprisingly good"" is very subjective. At least, from the results, the accuracy/complexity trade-offs is not as good as Vision Transformers (DeiT models). 2. The ResMLP benefits from distillation methods and also works well with self-supervised method DINO. 3. The simple linear layer enables observations on the spatial interaction that the network learns across layers. 4. The authors also try ResMLP on machine translation.   ","This paper introduce the ResMLP architecture. An alternative to self-attention layer in vision transformer using simply a $T \times T$ linear layer to allow long-range communication between tokens. They show that the performance of this model is comparable to the original vision transformer in supervised, self-supervised and transfer learning as well as knowledge distillation. ",0.18461538461538463,0.26153846153846155,0.16923076923076924,0.21487603305785125,0.08264462809917356,0.09090909090909091,0.09917355371900827,0.07727272727272727,0.19298245614035087,0.11818181818181818,0.17543859649122806,0.3508771929824561,0.12903225806451613,0.1192982456140351,0.18032786885245902,0.15249266862170088,0.11235955056179775,0.1444043321299639
239,SP:9d6202ab0010166f383d6d064aebe02ae97a1dfc,"This work gives algorithm for the soft k-means problem using similarity queries. This means that the clustering algorithm is allowed to make queries of the form ""how similar are two data items"". There have been recent results in this setting for k-means/median problems. This paper extends such results in the context of the soft k-means problem. The soft k-means problem is a generalisation of the k-means problem where assignment of a point to a center is on 0/1. So, the goal is to output k centres and membership weights U_{ij} (the degree of assignment of point i to center j).","Given a set of points X in R^d, a fuzzy k-means clustering of X is an assignments of k weights in [0,1] to every point in X, so that the i-th weight represents ""how much"" that point belongs to the i-th cluster. The problem tackled in this work is to recover a hidden fuzzy k-mean clustering of a given set X by querying some similarity oracle that answers queries in the form ""how similar are the weight vectors of these two points?"". The authors develop an algorithm to approximately recover the hidden clustering (in a well-defined optimization sense) with a number of queries that grows as log |X|, and that depends polynomially on other parameters (including the desired accuracy).  EDIT: I think the authors for their response; after reading it, I will leave my rating unchanged.","The paper extends the results in Ashtiani, Hassan, Shrinu Kushagra, and Shai Ben-David. ""Clustering with Same-Cluster Queries."" NeurIPS.2016 into the context of fuzzy clustering where assignments of points to centers are fractional. Analogous to the same-cluster queries proposed in the original paper, this paper proposed using similarity queries which would return a scalar reprenseting the similarity of the memberships of any two points (this is the inner product of the two membership vectors). The work first states that the similarity queries and the seemingly stronger membership queries (which directly return the fractional assignment of a point to a cluster) can be converted into each other. Then, given any ""true"" underlying fuzzy clustering solution, the paper formally defines how to measure the proximity of any fuzzy clustering solution to this underlying benchmark, and relates this proximity to a previous proposed performance metric for fuzzy clustering. The main body of the paper focuses on developing a fuzzy clustering algorithm based on similarity queries and providing theoretical guarantees of the number of queries needed, and how close the constructed fuzzy clustering is to the underlying clustering.","The authors study the fuzzy k-means problem in the presence of a cluster membership oracle. For the fuzzy k-means problem this is an oracle that returns the i-th entry in the j-th cluster membership vector. The authors aim at polynomial time algorithms that in the presence of a  sublinear number of membership queries.   Under some assumptions on the structure of the input instances they give different algorithms that achieve this goal. The first algorithm takes a uniform random sample and then queries the oracle for cluster memberships. Based on the results, it guesses the cluster centers. Then it exploits the structural assumption and the oracle to compute the clusters. A second algorithm is given that uses a different method to compute the clustering. It also samples a subset of the input data and then queries the oracle to identify the cluster with the largest membership weight in the sample. Then it computes the cluster of this center. After identifying the first cluster the algorithm iterates a sequential algorithm that always predicts the center of the next largest remaining cluster.",0.2037037037037037,0.25925925925925924,0.25925925925925924,0.2097902097902098,0.23776223776223776,0.20320855614973263,0.15384615384615385,0.1497326203208556,0.15300546448087432,0.16042780748663102,0.18579234972677597,0.20765027322404372,0.1752988047808765,0.18983050847457625,0.19243986254295534,0.18181818181818182,0.2085889570552147,0.20540540540540542
240,SP:9d8b57d60a0e59f9d9a90605094e8ef895f1c7de,"The paper investigates the topic of generalization in reinforcement learning. It starts from the observation that even in fully observed environments, a kind of partial observability arises from epistemic uncertainty about the true MDP the agent faces at test time. This observation gives rise to what the paper calls an epistemic POMDP, which has the property that expected return in the epistemic POMDP equals test time agent performance in expectation over the posterior distribution of MDPs (given the prior is accurate).   This insight is used to point out that, being a POMDP, the optimal policy of the epistemic POMDP is in general memory-based, and in the class of memoryless policies, stochastic. In the same vein, even a policy that is optimal on all training contexts can have very poor generalization performance, and the optimal policy for the epistemic POMDP might not be optimal in any single MDP.  The remainder of the paper focuses on memoryless policies in the empirical epistemic POMDP, where the latter arises from a finite number of samples from the MDP posterior. It is shown that one can lower-bound the performance of an ensemble of policies on the empirical epistemic POMDP by their average performance minus a disagreement penalty to the combined policy \pi and that optimizing this bound gives the optimal policy for the empirical epistemic POMDP.  Based on these insights, the paper proposes a practical algorithm (LEEP) that combines bootstrap sampling from the training contexts, choosing max for combining the individual policies, PPO as the base algorithm, and the derived regularization term into an algorithm where the n individual policies are optimized in a round-robin fashion.  The proposed algorithm is evaluated on the procgen benchmark, where it compares favorably to pure PPO and Distral. The algorithm is also shown to avoid overfitting when only few training contexts are available. Finally, an ablation study shows that both the agreement penalty term and the max link function contribute to the performance advantage of LEEP.","* This paper uses the insights from Bayesian RL to solve the problem of generalization in RL. In particular, the authors formulated the generalization problem as an epistemic POMDP and proposed an ensemble-based algorithm, called LEEP, to approximately solve the problem.  * The authors empirically demonstrated the performance of LEEP in generalizing to unseen contexts in the Procgen benchmark. They have shown that:   * LEEP performed better than PPO in Maze, Heist, BigFish, and Dodgeball.   * An ablation study to show that the better generalization performance of LEEP does not only come from the use of ensembles.   * An ablation study to show that the better generalization performance of LEEP does come from the inductive bias conferred by the `max_i π_i` link function proposed by the authors. ","The paper considers the problem of generalization in contextual MDPs, where the state of the environment consists of a context that remains fairly constant throughout an episode of the MDP. In such a setting the problem of generalization arises when an RL agent learns on a set of training MDPs (MDPs with contexts sampled from a set of training contexts) and then this trained RL agent must perform well on an unseen context test MDP. This paper underlines the challenges of learning and generalizing in such settings, namely, that methods that are not able to handle epistemic uncertainty fail at generalizing in such settings. The paper formulates this problem of generalization as a special POMDP and proposes a new sampling-based method for approximately solving this POMDP. Experimental results show that the proposed methods beats other baselines on a benchmark suite. ","The paper addresses the problem of a reinforcement learning generalizing across different dynamics (with the same state/action space). Specifically, the paper frames this problem as a POMDP, where the properties of the environment are encoded as hidden information. An algorithm LEEP is introduced, which learns different policies for different sampled environments, regularized to make the policies similar, and then combines the policy into a single, hopefully robust policy. ",0.11818181818181818,0.11515151515151516,0.07878787878787878,0.20161290322580644,0.13709677419354838,0.1347517730496454,0.31451612903225806,0.2695035460992908,0.37681159420289856,0.1773049645390071,0.2463768115942029,0.2753623188405797,0.17180616740088106,0.1613588110403397,0.13032581453634084,0.18867924528301885,0.1761658031088083,0.18095238095238098
241,SP:9dd460c3506a9a508b92baa63dff6b487e0eeca0,"This paper introduces VRDP, a method that learns to answer predictive, counterfactual, explanatory and descriptive yes or no questions about videos depicting the evolution of physical scenes. VRDP is composed of three modules, an object detector, a neuro-symbolic concept learner and differentiable physics model.  The paper shows that VRDP surpasses or closely matches the SOTA on two datasets, a synthetic dataset CLEVERER and a dataset of real world videos.","This paper proposes to do visual question answering by learning a differentiable physics model from video. The pipeline is divided to several modular and interpretable parts. Firstly, a visual perception module parse the input video to several objects’ features. Secondly, a concept learner parse the input questions to a set of programs and grounded concepts. The output of the above two modules are used to estimate the objects’ physical parameters and fed to a differentiable physical engine, which is used to do future trajectory predictions. Finally, a symbolic execution part is used to output answer given the future predictions.","The paper tackles the problem of dynamic visual reasoning by using a modular unit that is composed of three components: visual perception, concept learner, and physics simulator. This system is capable of performing system identification from visual and language modalities with assistance from a physics model. Having access to the physics model, it can produce imagined future trajectories for reasoning about possible future outcomes. This gave them an advantage of outperforming competitive methods, especially in counterfactual tasks. ","- The paper presents a composite model for visual question answering in scenarios which require dynamic visual reasoning. - The model pipeline consists of a visual perception module which extracts object-centric trajectories from an input video, a concept learner which parses the natural language QA pair into a set of neurosymbolic concept vectors, a differentiable physics model which can simulate the evolution of rigid body trajectories and a neurosymbolic program executor which computes or selects the answer for a given input question using the intermediate concept and trajectory features. - As a contribution over prior art in neurosymbolic visual reasoning, the authors propose the utilisation of a differentiable physics module for accurate trajectory prediction based on physics parameters which are inferred from the video input sample. Since explicit physical properties like mass, friction and restitution are inferred in the model, the computed output is very amenable to human interpretation. - The proposed approach is comprehensively evaluated on the synthetic CLEVRER and realistic Real-billiard benchmarks and compared against related neurosymbolic and end-to-end VQA models. It demonstrates remarkable boosts in data efficiency on CLEVRER, sets a new state-of-the-art performance on counterfactual questions and significantly outperforms prior art in a few-shot setup where generalisation to modified physical attributes like 'heavier' or 'lighter' is required.",0.21428571428571427,0.2,0.2714285714285714,0.16161616161616163,0.32323232323232326,0.2727272727272727,0.15151515151515152,0.18181818181818182,0.08837209302325581,0.2077922077922078,0.14883720930232558,0.09767441860465116,0.17751479289940827,0.1904761904761905,0.13333333333333333,0.18181818181818185,0.20382165605095542,0.14383561643835616
242,SP:9eadc19f7f712c488cf50d091f372092f6352930,"The paper proposes a simple attention-based model for conversational and multi-hop QA tasks. The model use BERT-like pre-trained LM ETC separately encodes questions and paragraph (i.e., a collection of sentences). Besides the encodings on sentence-level, the final context encodings also contain extra paragraph embeddings, which are a weighted sum of sentences’ encodings using a simple dot product attention. For the QA interaction, the models use a hard attention mechanism to select an entry representing either a sentence or a paragraph.  The experiments on two extractive QA datasets HYBRIDQA and QASPER show the model performs worse than the MATE model on HYBRIDQA but marginally better than other baselines on QASPER. On multi-hop QA and conversational QA tasks, the model performs marginally better than baselines on *expanded dataset*, but authors do not provide results on original datasets. ","This paper introduces DocHopper, a new model for complex question answering over long documents (e.g., multi-hop QA over multiple paragraphs, conversational QA, reasoning over scientific documents). DocHopper is based on ETC (Ainslie et al., 2020) and DocHopper extends the existing hierarchical attentions from ETC with a new approach to update query representations in latent space. Their model does not jointly encode a question and context and does not require re-encoding of queries as in prior work, which leads to their effectiveness at inference time. They evaluate DocHopper on four different datasets: ShARC, QASPER, HotpotQA, and HybridQA. The proposed method achieves strong performance on those datasets, reducing the computational cost at the inference time. ",The paper proposes an iterative approach for multi-hop question answering. At high-level the proposed model breaks a question into multiple sub-questions and then adds information relevant to each sub-question to the query vector for the next step retrieval. At each iteration an ETC encoder is used to encode the document and a sub-question; the vector corresponding to sub-question is then updated iteratively by contextualizing it over sentences and paragraphs in the document and is used to extract the final answer using a subsequent BERT reader. Evaluation results show improvements on 3 of 4 datasets. ,"This paper provides a novel MRC model (DocHopper) for multi-hop QA over long structured documents. In multi-hop QA, the evidence necessary to answer a user's question is spread across different parts of the long document. Previous approaches find the evidence by iteratively updating the user's query. The problems in these previous approaches are 1) computational efficiency and 2) ineffective modeling strategy for figuring out the relations of the evidence. DocHopper resolves these problems with a hierarchical attention mechanism. Hierarchical attention mechanism provides two types of embedding vectors for a single paragraph: 1) local sentence vectors and 2) global context vector of the paragraph. DocHopper computes the similarities between the query vectors and these sentence/paragraph vectors and selects the proper evidence. Since the sentence/paragraph vectors can be pre-computed, the only inference time required for this method is the time for question embedding, and this brings drastic improvement in the computational efficiency. This paper uses four types of datasets for evaluation: 1) conversational QA (ShARC), 2) TableQA (HybridQA), 3) QA on academic paper (QASPER), and 4) multi-hop factual QA (HotpotQA), and shows the QA performance and computational efficiency of their model.",0.14084507042253522,0.1619718309859155,0.24647887323943662,0.15517241379310345,0.2672413793103448,0.21,0.1724137931034483,0.23,0.17766497461928935,0.18,0.15736040609137056,0.1065989847715736,0.15503875968992248,0.19008264462809918,0.20648967551622419,0.16666666666666666,0.19808306709265178,0.1414141414141414
243,SP:9ec000cd9c15e3c9988a41921c465b42e7d41877,This paper presents a hierarchical cross contrastive self-supervised learning framework for learning visual representation. This paper proposes to project the representations of an image and its augmented version to multiple latent spaces and also make predictions on each of the latent spaces.  A contrastive loss between the features of different projection levels is minimized to learn the parameters. Experiments on the image classification and detection benchmarks are evaluated. A comparison between important existing methods is also done in the paper. ,"In this paper, the authors proposed a Hierarchical Cross Contrastive Learning (HCCL) method for Self-supervised learning (SSL) of visual representation. The proposed method include a design of a hierarchical projection network that produces multi-level latent representations. A cross contrastive loss is also introduced to learn invariant visual representations. HCCL is validated on several downstream tasks including classification, segmentation and object detection.","The authors propose an extension to the contrastive learning based representation learning approach. The proposed method, call Hierarchical Cross Contrastive Learning(HCCL), leverages the features in different levels and views for more consistent features over the standard CL using only the features of the final layers. The effectiveness is validated in classification, detection, segmentation, and few-shot learning tasks. ","This manuscript proposed a new contrastive self-supervised learning approach (HCCL). Compared with exiting work such as BYOL and SimSiam, HCCL introduced (1) multiple hierarchical projectors and predictors; and (2) contrastive loss calculated across different layers of projectors. HCCL is empirically evaluated on several self-supervised learning benchmarks (e.g., iNat18, Place-205, and COCO instance seg, etc.) and achieves noticeable improvement compared with previous states of the art (e.g., SWAV, BYOL, SimSiam, and Barlow Twins). Additionally, ablation studies are provided to verify the significance of hierarchical projectors, cross contrastive loss, and higher learning rates of the predictor.",0.2962962962962963,0.1728395061728395,0.2222222222222222,0.2698412698412698,0.2698412698412698,0.2033898305084746,0.38095238095238093,0.23728813559322035,0.18181818181818182,0.288135593220339,0.1717171717171717,0.12121212121212122,0.3333333333333333,0.19999999999999996,0.19999999999999998,0.2786885245901639,0.20987654320987653,0.1518987341772152
244,SP:9ef61f2064db8ac3b01b16694a744b274bdbbe83,"This paper proposes a mode-switching strategy for the exploration/exploitation dilemma instead of monolithic behaviour policies in order to obtain more diverse behaviour. Different granularities for the timing of the switches as well as different switching mechanisms are investigated (blind vs. informed switching). The focus for exploration is not on how, but when. For exploration, they both use Random Network Distillation (RND) as well as a uniform policy. Their experiments are conducted on the Atari Learning Environment (ALE), where they provide performance and diversity results.","This paper proposes to study exploration at different levels of granularity. Current methods either explore at the level of individual steps (e.g., \epsilon-greedy), or at the level of experiments (e.g. first a reward-free exploration phase, followed by a task-dependent learning phase using the gathered data). This paper proposes to study exploration at the intra-episodic level, i.e. where the agent switches between exploration and exploitation within the same episode.   They discuss various design choices to perform exploration at this level, for example switching after a certain number of steps or with a certain probability, or switching based on the discrepancy between the predicted value and actual experienced value.   The experimental results show that including intra-episodic exploration gives a modest benefit over other exploration schemes when using an R2D2 base agent. Other insights are also included, which show that the proportion of exploration does change throughout the learning process, indicating that different degrees of exploration are useful at different stages. They also show that the informed switching component learns switching behaviors which are non-uniform throughout the episodes.","This paper studies switching between exploit and explore modes in reinforcement learning. It discusses switching mechanisms based on time (""blind switching"") and based on state (""informed switching""). Studying seven Atari games, an empirical analysis of different switching mechanisms is performed.","This paper investigates when to switch between exploitation and exploration and how long to stay in each exploration mode during RL learning. It proposes new ways to explore the subject, especially with intra-episodic exploration variants. It presents a large body of study results (10 pages of appendices!!), and concludes with very thought-provoking suggestions and discussions.",0.20930232558139536,0.11627906976744186,0.10465116279069768,0.06521739130434782,0.08695652173913043,0.2,0.09782608695652174,0.25,0.15789473684210525,0.3,0.2807017543859649,0.14035087719298245,0.13333333333333333,0.15873015873015872,0.12587412587412586,0.10714285714285714,0.13278008298755187,0.16494845360824742
245,SP:9f09449a47464efb5458d0732df7664865558e6f,"The paper proposes a continual learning algorithm that enforces the convolutional filter in each layer to a low-rank filter subspace defined by a small set of filter atoms. For each task, each convolutional layer is defined by a new filter subspace but subspace coefficients are shared among the tasks. The algorithm is validated on multiple benchmark datasets. ","This paper tackles the continual learning via enforcing a low-rank filter structure to each CNN layer.  They first perform atom-coefficient filter decomposition and then learn each task with a new filter subspace, so that the method only needs to save the new filters for each task. The contribution of this paper includes the low-rank filter scheme and the designed intra-task and inter-task model ensemble performing on the filters. The proposed method also achieves SOTA performance on several datasets with tiny size of model memory. ","This paper introduces a model for continual learning based on the decomposition of linear filters into low-rank components, called atoms. Specifically, the authors decompose convolutional filters shaped (c,c',k,k) into two components: i) alpha, shaped (c,c',m) and D, shaped (m,k,k). The former is learned on the first task and then frozen, whereas for D every task has its own and they are do not conflict during optimization. On top of that, the authors envision two different ensembling schemes that improve performances. i) First, in task-incremental settings, they retrieve atoms from the task of interest and from similar tasks as well (based on SVD decomposition of D matrices and Grassman distance) and ensemble them. ii) Furthermore, in class-incremental settings, they explicitly setup multiple atoms per task, building a task ensemble. During inference, all ensembles are queried and their predictive variance is used to ""discover"" the relevant task, for which a prediction is carried out. Experiments are carried out on 3 datasets in both task-incremental and class-incremental learning settings. ","The paper, motivated by the task subspace modeling literature, enforced a low-rank filter structure to each CNN layer across time in continual learning. It not only ensures that the knowledge of the past tasks is not lost but also saves a lot of computing memory. Meanwhile, the paper proposes novel intra-task ensembles and inter-task ensembles for class-incremental settings and task-incremental settings, respectively. ",0.3103448275862069,0.29310344827586204,0.20689655172413793,0.24719101123595505,0.23595505617977527,0.11235955056179775,0.20224719101123595,0.09550561797752809,0.1791044776119403,0.12359550561797752,0.31343283582089554,0.29850746268656714,0.24489795918367344,0.1440677966101695,0.192,0.1647940074906367,0.2692307692307692,0.16326530612244897
246,SP:a0112febb28e518e87142e7cbb7e3586d06cae0b,"# Summary  The manuscript ""On the role of population heterogeneity in emergent communication"" addresses the question why results on population size in deep language emergence have not, so far, mirrored the effects it is claimed to have on natural language. In a nutshell, in natural communication population size correlates with simpler grammars and less idiosyncratic languages. The effects of population size on the structure of neural emergent communication is less explored and has hitherto not reflected what we know about natural language. The authors argue that one of the reasons for this is that artificial populations are often homogeneous. They show, through simulations, that introducing asymmetries in the training speed of speakers and listeners leads to trends that are more in line with natural language: the size of heterogeneous populations weakly correlates with more aligned languages; higher neg-entropy; better generalization when communicating about novel objects; and more compositional languages (measured as topographic similarity). ","The authors refer to prior work in sociolinguistic literature to state that larger communities create more systematic languages. However, they point out, this apparent correlation between language structure and population size has evaded machine learning practitioners studying language emergence. This paper claims that populations explored in machine learning have largely been homogeneous and that population heterogeneity is key to the emergence of structure in artificial agents. The authors reproduce the failure to achieve systematic languages by scaling up the population and then show that they can indeed get more structure by introducing heterogeneity. They explore introducing asymmetry between the speaker and the listener based on model capacity and learning speed, leading to an increase in language structure when the speaker is faster or has more capacity. The authors note that this effect only depends on the relative differences between the speaker and the listener and not the absolute values (there is a correlation with absolute values but the variation has a low magnitude). Further, they show that larger networks need fewer epochs to reach similar training accuracy, concluding that network capacity is a confounding factor of training speed in their setting. Finally, the authors create a population of heterogeneous agents by imbibing them with different learning speeds by updating an agent $i$ with probability $p_i$ after each round of the Lewis game. Four properties of the emergent language are studied: speakers synchronization, (negative) conditional entropy given an object, topographic similarity, and generalization. These metrics either improve or remain approximately at the same values as the population size increases.","This paper analyzes the structure of emergent languages in signaling games played by _populations_ of agents, motivated by a rich body of socio- and psycho-linguistics results showing that languages spoken by more people (and with more second-language learners) tend to be grammatically simpler (e.g. have a more impoverished morphology).  The authors measure various properties of the emergent languages as proxies for how ""systematic"" a language is, and show several things.  First, increasing the number of agents does not correlate with any of their measures, so population size alone does not suffice.  Second, in a minimally small population, various measures of ""diversity"" of the two agents do correlate with their systematicity measures.  Finally, in a large but diverse population, we do see correlations with population size and _some_ of the systematicity measures.  The paper is interesting and timely, and reports on a large number of experiments.  While I think the experiments could be more closely linked to the hypotheses from the sociolingusitic literature, the paper will be of interest to many researchers in emergent communication, NLP, and cognitive science, and could spur future work in this intersection.","This paper aims at solving a conflicting empirical observation and the present-state models for emerging languages. It has been observed that larger populations produce more structured languages. However, the state-of-art neural-based models have not been able to generate languages with such characteristics. This paper shows that a key ingredient is to allow population heterogeneity in the neural models rather than the current identically distributed specifications. ",0.22875816993464052,0.16339869281045752,0.0784313725490196,0.13127413127413126,0.06563706563706563,0.08465608465608465,0.13513513513513514,0.13227513227513227,0.17391304347826086,0.17989417989417988,0.2463768115942029,0.2318840579710145,0.1699029126213592,0.14619883040935672,0.10810810810810811,0.1517857142857143,0.10365853658536583,0.12403100775193797
247,SP:a0ee0e08b4bb578836fd5e0781e8713f254569fb,This paper adapts transformer to multi-agent motion forecasting. The attention layers are applied on time and agent axis to capture motion and social information. A latent variable is introduced on the output to capture discrete motion for each agent. Extensive experiments are conducted on various dataset with good performance. The training time of the proposed model is significantly faster than previous methods.,This submission presents a Transformer-based architecture for trajectory prediction tasks involving multiple agents. Such a multi-agent setting requires learning of spatio-temporal representations capturing both the long-term temporal dependencies as well as the social interactions between the agents. The paper formulates the task as modeling of the sequence of sets where every entry in the set corresponds to an agent’s observation. The proposed architecture augments the set transformer with a discrete latent variable to be able to make multiple predictions into the future. The given seed sequence is first encoded into a context representation which is later used to make future predictions with a decoder where the agents can be modeled jointly or independent from each other. ,"The paper tackles the multi-agent trajectory prediction problem, primarily for autonomous driving, but experiments also include TrajNet and predicting Omniglot strokes. The authors claim their contributions in a very general sense:  - Novel method on modeling sequences of structured continuous variables and capture multi-modal distributions. - Strong results on nuscenes, argoverse, trajnet and omniglot stroke prediction.  The key modeling novelty is the latent variable sequential set transformer which applies self-attention across different agents and across time in the scene.   I mostly agree with the authors' assessment in general. I have some concerns on the claimed novelty (see below).","This paper proposes a transformer-based VAE model for motion prediction that can output multi-model and scene-consistent predictions. Specifically, the transformer is employed for modeling both social and temporal information (but separately, first temporal and then social and repeat). The proposed method achieves state-of-the-art performance on the nuScenes dataset and top performance on the Argoverse dataset",0.20634920634920634,0.2222222222222222,0.20634920634920634,0.09917355371900827,0.12396694214876033,0.13131313131313133,0.10743801652892562,0.1414141414141414,0.21311475409836064,0.12121212121212122,0.2459016393442623,0.21311475409836064,0.14130434782608695,0.17283950617283947,0.20967741935483872,0.1090909090909091,0.16483516483516486,0.16250000000000003
248,SP:a151ae8afae0a073b0df83a74fd084dfe3753a48,"The authors of this paper proposes a DLGN (Deep Linear Gated Networks), a novel class of deep networks, inspired by a recent dual view where the computation in DNNs is broken into two parts: learning in the gates and learning in the weights. The DLGN disentangles the computations into 2 parts: (1) ""primal"" part between input and the pre-activations in the gating network, and (2) ""dual"" part in the weights network, conditioned on inputs and gates. DLGN’s performance recovers 83.5% of SOTA DNNs. This development may lead to more interpretable deep network models that are also highly performant.  ","The paper extends the framework of the deep gated network to the deep linearly gated network. Based on this extension, the paper investigates the separate part of the network. By theoretical analysis and empirical experiments, the paper argues that the neural network is learned path-by-path instead of layer-by-layer. Also, it present that the neural path kernel has several interesting properties.","This paper deals with the entanglement in the DNN through two steps. First, replacing the rectified linear units (ReLU) in the traditional DNN with Deep Linearly Gated Network (DLGN). Second, demonstrate the weighted network is disentangled in the path space.","This paper proposed deep linearly gated networks (DLGN) for interpreting DNNs with ReLU activations based on a dual view. The proposed framework is able to completely disentangle the ‘gating network’ and the ‘weight network’. Finally, the experiment results demonstrate that DLGN can achieve good performance for classification on two benchmark datasets compared to the existing DNNs.",0.1485148514851485,0.12871287128712872,0.18811881188118812,0.203125,0.21875,0.225,0.234375,0.325,0.3392857142857143,0.325,0.25,0.16071428571428573,0.1818181818181818,0.1843971631205674,0.2420382165605095,0.25000000000000006,0.23333333333333334,0.1875
249,SP:a18f4697f350a864866dac871f581b8fc67e8088,"The paper considers the problem of distributed training for graph learning tasks, under a setting where data privacy is significant for each individual machine and communication to/from a central parameter server is expensive. To preserve privacy each machine has only access to a distinct partition of the overall graph. The central server has access to the full graph. In the LLCG algorithm that the paper proposes, each machine trains on its local graph partition for some time before sending the parameters to the server. The server averages the received parameter, but additionally also does its own training using the full graph available to it. Theoretically the authors show that the proposed method avoids an error gap in the gradient norm that would exist if server correction is not performed. Experimental results show the proposed scheme performing similarly to GGS albeit with much lower communication costs.  ","Training GNNs is challenging due to high communication costs or large memory overheads. This paper proposes a communication-efficient distributed GNN training technique named Learn Locally, Correct Globally (LLCG) to periodically model averaging on the server using locally trained models.  It also applies global server corrections to refine the locally learned models and solve the irreducible performance degradation caused by ignoring node dependency. This paper provides the convergence analysis and shows the proposed method can address the residual error. The experimental results show significant improvement compared to existing methods.","This paper deals with the problem of distributed training of GNNs. Existing methods are either communication-intensive (sampling) or do not achieve good performance (averaging).  The authors propose a novel method, dubbed ""LLCG: Learn Locally Correct Globally"". Essentially, this method captures the idea of transmitting only local averages but adds a centralized step on the server to account for global structural information lost in the subgraph partition.  The authors further provide theoretical convergence guarantees. They both show that just averaging leads to an insurmountable residual error that explains the poor performance of averaging methods, as well as prove that this residual error disappears when adding the global correction step.","This paper proposes a distributed training technique for GNN. This technique includes local computations done in parallel by several machines and a correction phase done by a centralized server. A theoretical analysis is given for this technique, showing that the server correction phase reduces some irreducible error that happens due to splitting the graph and doing a local computation on each subgraph. Several experiments are made on real datasets which show the merits of this technique over previous techniques in terms of performance, communication steps, and size.",0.14383561643835616,0.17123287671232876,0.13013698630136986,0.2247191011235955,0.16853932584269662,0.1559633027522936,0.23595505617977527,0.22935779816513763,0.21839080459770116,0.1834862385321101,0.1724137931034483,0.19540229885057472,0.17872340425531913,0.19607843137254904,0.1630901287553648,0.20202020202020204,0.17045454545454547,0.17346938775510204
250,SP:a1cb0ca55bc919125f4dad5bcc6e0ad6c2527c1e,"This paper suggests representing policies for MDPs as programs, and shows how to learn programmatic policies effectively from a mix of trial-and-error experience on specific tasks and a pre-training phase to create an embedding that maps programs producing similar behaviors into similar points in the embedding. The authors compare their method to a variety of baselines, including both ablations of their own model as well as more traditional approaches from hierarchical RL, deep RL, and a standard program synthesizer on the KAREL domain. The authors find that the programmatic policy representations perform better overall for many of the KAREL tasks, and generalize better to larger problems than they were trained on.","This paper presents a two-stage method for reinforcement learning via program synthesis, and shows that is is effective compared to baselines for a number of simple gridworld tasks, deminstrating a degree of out-of-distribution generalisation. The first stage of the process learns a latent space for encoding programs, and the second stage searches that latent space for programs that maximise reward in  the task. Overall a very nice paper that deserves publication.","- Authors use a two-stage method for program synthesis: (1) They   create an embedding space capturing the semantics of the program   space; (2) They search through the embedding space for a program   that solves the task in hand. - They use variational autoencoder to encode programs, where an   encoder maps a sequence of program tokens into a latent space, while   a decoder maps from the latent space into the original program. They   use a combination of loss functions to ensure that not only   syntactically similar programs are close to each other in the latent   space but also semantically close programs. Loss function consists   of a composition of 3 losses. (A) Classical beta VAE loss which   helps that syntactically close programs are close to each other. (B)   Program behavior reconstruction loss which helps that semantically   similar programs are close to each other in the embedding space. For   example, two syntactically different programs can make same actions   under same situations. This loss is defined by difference between   execution traces of original and reconstructed program. (C) Latent   behavior reconstruction loss. This loss is constructed by learning a   policy to predict correct action based on the latent program   embedding. This allows for backpropagating gradient through the   policy and the encoder to ensure better quality of the embeddings. - After training the variational autoencoder, authors use a cross   entropy method to find a program with maximal performance using the   latent space. They sample the latent space distribution of programs,   decode the latent program into an actual program, execute the   program to obtain corresponding rewards, and repeat the process. - Authors did a nice ablation study where they measured effectiveness   of their individual design decisions: evaluating each of the loss   function components, as well as the effectiveness of the latent   space search method for finding the best performing program. They   compared they method to other methods, deep reinforcement learning   (DRL) based as well as Viper which is a decision tree policy   obtained by mimicking a DRL agent.",The paper approaches reinforcement learning from a program synthesis perspective and proposes a new method - LEAPS - that is capable of learning programmatic policies just from weak reward signals. This is achieved in two steps: i) learning a smooth latent manifold where similar programs are encoded close to each other; ii) searching the latent space efficiently via the Cross Entropy Method. LEAPS is applied to several tasks in the Karel domain where it successfully can learn programmatic policies that are even transferable to larger state spaces for zero-shot learning.,0.15789473684210525,0.24561403508771928,0.14912280701754385,0.3783783783783784,0.24324324324324326,0.08231707317073171,0.24324324324324326,0.08536585365853659,0.19101123595505617,0.08536585365853659,0.20224719101123595,0.30337078651685395,0.19148936170212766,0.12669683257918551,0.16748768472906403,0.13930348258706468,0.22085889570552147,0.12949640287769784
251,SP:a35eb46f391e1a1e347e7243245ca69f4c0f129f,"This paper adopts the framework of unsupervised skill learning. To solve the problem that the discriminator will have low confidence in the unseen data thus providing a low intrinsic reward, the paper derives an information gain auxiliary objective that involves training an ensemble of discriminators and rewarding the policy for their disagreement. The paper conducts extensive experiments on tabular grid world and 57 games of the Atari Suite. ","The paper proposes a novel unsupervised skill discovery algorithm. Beginning with the family of such methods (DIAYN, VIC, etc) that learn a discriminator to distinguish skills given some observations of the trajectory and a policy that executes a skill conditioned on the (discrete) skill random variable Z, the premise of the paper is that such methods on their own will fail when new states are encountered during the skill learning process as the discriminator would not have had sufficient data to learn to distinguish novel states. The paper proposes a novel reward bonus that works in addition to a base method such as DIAYN (DIversity is All You Need), such that this bonus “reimburses” the policy for visiting states where the discriminator uncertainty is high (measured using a form of disagreement across an ensemble of discriminators). Experiments on the pedagogical 4-room environment and the Atari suite of environments demonstrates the benefit of the proposed reward bonus in not only learning more skills (or “empowerment” in the VIC nomenclature), but the learnt skills are also superior for downstream tasks (external reward) and lifetime state coverage.",This paper identifies a source of pessimism in DIAYN-style methods for exploring new parts of the state space. They argue that this issue is due to using a single point estimator as a discriminator and that capturing the epistemic uncertainty of the discriminator could serve as an additional signal to guide exploration. They achieve this by using an ensemble of discriminators and incorporating the epistemic uncertainty across the ensemble into an additional intrinsic reward to the diversity of skills reward (through a mixing parameter $\lambda$). They examine this method against DIAYN-style methods and count-based methods and show that this new approach broadly outperforms both these classes of methods.    ,"This paper is concerned with unsupervised RL where an extrinsic reward signal is not available. The objective is for the agent to master the environment by exploring it while learning a diverse set of skills. This is done by simultaneously training a policy conditioned on a latent variable and a discriminator that tries to infer the latent variable from trajectories. The authors identify that the intrinsic rewards used in skill discovery result in the agent being pessimistic towards exploring novel parts of the environment. To alleviate this, the authors propose a new auxiliary objective, which results in a bonus based on the disagreement of an ensemble of discriminators. Empirical results on the grid world and Atari show improvements in skill discovery and solving downstream tasks compared to baselines.",0.3382352941176471,0.2647058823529412,0.3235294117647059,0.15135135135135136,0.1891891891891892,0.2072072072072072,0.12432432432432433,0.16216216216216217,0.171875,0.25225225225225223,0.2734375,0.1796875,0.18181818181818185,0.20111731843575417,0.22448979591836735,0.1891891891891892,0.22364217252396168,0.19246861924686193
252,SP:a472784ddb36f88e6e468f282fbd7ad74f8f7d75,"In this paper, the authors propose a new method for single view 3D reconstruction.  A conditional (image feature prior) implicit representation framework is proposed to reconstruct 3D scene from a single view. In this paper, the authors propose that feature gradient is essential for watertight reconstruction and propose a differentiable gradient sampling method for the formulation. Experiments have been performed on both synthetic and real datasets. Superior results have been presented.","This paper describes novel loss functions for learning to predict an implicit 3D scene representation from a single image.  They argue that when working with real scan data of scenes (rather than single objects) it is difficult to generate accurate occupancy or signed distance function (SDF) ground truth as would be required for supervised learning.  Instead, they propose to only use occupancy or SDF supervision near the surfaces of objects; elsewhere, they rely on constraints on the gradient of the occupancy or SDF adapted from Gropp et al. 2020.  They perform a thorough evaluation on several benchmark datasets and compare against state-of-the-art competing methods.  They show that they outperform competing methods, even though in some cases their method has access to less supervisory data.    They also perform an ablation study to show the importance of various parts of the loss function.  ","This paper presents a new method to learn implicit 3D scene reconstructions from single image input. The main improvement is a closed-form Differentiable Gradient Sampling.  By taking spatial gradient into  consideration, the proposed method can apply back-propagation of the  loss on spatial gradients to feature maps and allow the training for the case of without dense 3D supervision.   ",This paper presents a method for 3D scene reconstruction from a single image using implicit surface representations such as occupancy or SDF. The authors propose to incorporate loss functions on the spatial gradients to provide dense supervision in the 3D space in the case where 3D labels may be incomplete (e.g. open 3D meshes) or not well-defined everywhere. Experiments are performed on ShapeNet and ScanNet show that the proposed method can achieve competitive performance on single-image scene reconstruction tasks.,0.23943661971830985,0.28169014084507044,0.2535211267605634,0.125,0.1597222222222222,0.3333333333333333,0.11805555555555555,0.3333333333333333,0.21951219512195122,0.3,0.2804878048780488,0.24390243902439024,0.15813953488372093,0.3053435114503817,0.23529411764705882,0.17647058823529413,0.20353982300884957,0.28169014084507044
253,SP:a530dd966911e387a90e3cbf9f51c8cab6152723,"Many theoretical works have studied SGD, but they commonly rely on restrictive and unrealistic assumptions about the noise. In this work, the authors construct example optimization problems illustrating that, if these assumptions are relaxed, SGD can exhibit many strange behaviors, including (1) SGD can converge to local maxima, (2) SGD may escape saddle points arbitrarily slowly, (3) SGD can prefer sharp minima over flat ones, and (4) AMSGrad can converge to local maxima. Therefore, the authors conclude that in the most general nonconvex case, many counter-intuitive phenomena of SGD may arise and contrast to the commonly held presumptions.","This paper demonstrates on several fairly simple (e.g. 1-dimensional quadratic) objectives that stochastic gradient descent may easily have very poor behavior: it could converge to a maximum, or diverge even in convex settings if the learning rate is too high.  Specifically, it is shown that for any learning rate, there is is a distribution over quadratics whose expectation is $-rx^2$ for some $r\ge 0$ such that SGD will converge to the maximum at 0, and there is also a distribution whose expectation is $rx^2$ such that SGD still diverges even on this convex loss. Note that the distribution (and $r$) depend on the learning rate. Further, there are distributions such that SGD must escape a saddle point very slowly, and a distribution over 2-dimensional quartics such that SGD will converge to a ""sharp"" rather than a ""flat"" minimum. It is also shown that AMSgrad must converge to a local maximum on some non-convex distributions.  The results are proven by choosing a particular distribution over quadratic objectives that causes the logarithm SGD's iterate to be a sum of i.i.d. random variables. Then the central limit theorem provides an understanding of the limiting behavior of these iterates.  The results are augmented by empirical studies verifying the theorems directly, along with a very simple small neural network experiment.","## Post-discussion reassessment  See relevant post below.  ## Summary of contributions  In this paper, the authors examine whether SGD with a large, constant step-size avoids local maximizers (or, more generally, undesirable saddle points of the underlying minimization problem). More precisely, they focus on the algorithm $$ w_{t+1} = w_t - \lambda \hat g_t $$ where $\lambda>0$ is the algorithm's step-size, and $\hat g_t$ is a (stochastic) gradient of the (stochastic) loss function $\hat L(w;x)$, with $x$ a random variable.  The paper's results can be summarized as follows:  1. If $\hat L(w;x) = (x/2) \cdot w^2$, the authors identify a range of values of $\lambda$ (which depends on the distribution of $x$) such that, in probability, $w_t$ converges to $0$ – which, under the specified distributional assumptions for $x$, is the global maximum of $L = \mathbb{E}[\hat L]$. This is made precise in Propositions 1 and 2, and Corollary 1.  2. They provide a quartic loss function under which SGD converges to the function's sharper minimizers (as measured by the trace of the Hessian at said points). [Proposition 4]  3. They provide a specific range of parameters under which the AMSGrad algorithm converges to the undesirable maximizer of item (1) above.  [In the supplement, the authors also provide an analysis of a gradient-like diffusion (Appendices B and C), which they discuss as a continuous-time model of (SGD). This part is not directly connected to the rest of the paper, so I am not including it in my evaluation below.]","The paper provides several artificial examples on which SGD has an unintuitive behavior. This includes: (1) SGD converges to a local maximum; (2) If the learning rate is not fixed then SGD takes arbitrarily long time to escape saddle points; (3) SGD may prefer sharper minima (in contrast to several hypotheses regarding the implicit bias of GD); (4) Adaptive methods may also converge to a global maximum. Several experiments are made, varifying the theoretical results.",0.25252525252525254,0.23232323232323232,0.21212121212121213,0.1511111111111111,0.1111111111111111,0.06870229007633588,0.1111111111111111,0.08778625954198473,0.28,0.1297709923664122,0.3333333333333333,0.24,0.15432098765432098,0.12742382271468145,0.2413793103448276,0.13963039014373718,0.16666666666666666,0.10682492581602375
254,SP:a5945ec13e2f362df03b42511d44827ef081f4c3,This work analyzes the convergence rates of several existing prior guided zero order optimization methods. The zero order methods studied use statistical methods to provide an estimate of the function gradient using only function evaluations. This works studies the trade offs between gradient estimation accuracy and function evaluations as well as the effect of learning rate selection for the various estimates. This work also proposes a prior guided version of accelerated random search and studies its convergence. All algorithms studied are then numerically compared on benchmark functions as well as black box adversarial attacks. ,The paper provides a convergence analysis of a class of prior-guided zeroth-order (ZO) optimization algorithms and gives a prior-guided variant of the accelerated random search (ARS) algorithm.  Contribution: 1. Provides a complete convergence on some prior-guided zeroth-order optimization algorithms. 2. Provides a prior-guided variant of the ARS algorithm.,"This work studies the problem of zeroth-order optimization in a deterministic setting, where the exact function values at each point can be observed but the gradient of the function is not accessible. The main approach for solving such problems is first to estimate the gradient of the function and then apply a gradient method by plugging in this estimator instead of the gradient. The main goal of this work is to provide convergence guarantee for prior guided zeroth order algorithms.  Assumptions on the objective function: The objective function is (strongly) convex and smooth.  Approach: Since the paper assumes a deterministic setting, at each step by two function evaluations and choosing a small enough perturbation term, roughly they can observe the gradient of the function towards any arbitrary direction (page 3, equation 1). Also, they assume that at each step they can observe $q$ random directions of the gradient, which are given priorly (or they estimate them by $2q$ additional function evaluations). Then, they take advantage of these $q$ given directions and $g_{I}$s to construct $g_t$, where $g_t$ is the gradient estimator at step $t$, and $I<t$.   Contributions: Their results in Theorems 3 and 4 outline the convergence guarantees and robustness to the learning rate.",This paper studies the performance of zeroth order optimization algorithms where gradient estimation uses prior information. They provide a bound on the convergence analysis for such prior guided algorithms in terms of expected alignment of the gradient estimates to the true gradient. They use this framework to study historical priors and show that such a prior may counter the effects of using a sub-optimal learning rate. They also provide a new accelerated random search algorithm that incorporates prior information. ,0.18085106382978725,0.2765957446808511,0.22340425531914893,0.3333333333333333,0.2962962962962963,0.12857142857142856,0.3148148148148148,0.12380952380952381,0.2625,0.08571428571428572,0.2,0.3375,0.22972972972972974,0.17105263157894737,0.24137931034482757,0.13636363636363638,0.23880597014925375,0.18620689655172412
255,SP:a64b26faef315c3ece590322291bab198932c604,"This paper proposes a meta-learning approach based on MAML but with a task-conditioned initialization, which takes into account both information extracted from the features of the task (support set) data, as well as geometric information from a “rehearsed” learning path (obtained by gradient descent on the support set). The geometric information includes the parameters, loss, gradient and fisher matrix for different steps of the task adaptation process. They propose a GRU architecture to process this information across the different steps, yielding a path embedding. For both the feature and path embeddings, they utilize clustering, motivated by the need to share knowledge across similar tasks. They then combine the path and feature embeddings via an additional neural network to generate the final task-specific initialization. Computing the path embedding requires rehearsing which is computationally expensive (it’s akin to training twice on the task, once for rehearsing, and then training from the task-conditioned initialization). To amend this at inference time, they meta-learn a ‘tunnel’ connection that predicts the path embedding from the feature embedding. At test time, only a forward pass through this meta-learned connection is required instead of rehearsing. They experimentally evaluate their approach on few-shot classification and cold-start recommender problems and show performance gains over similar MAML-based approaches. They also perform ablation studies and analyses to understand the contribution of different parts of this system to downstream performance.",This paper proposes a clustered task-aware meta-learning algorithm. The proposed algorithm firstly collects the learning path and uses the path to train a sequence module. The sequence module output is combined with the task feature. The task feature is derived from the weighted sum of the soft cluster centers. The combination is then used as initialization of the model parameter for task adaptation. Experiments on image classification and cold-start recommendation demonstrate superior performance compared to baseline algorithms.,"This paper looks at the problem of meta-learning with heterogeneous tasks. This is an interesting problem that is receiving increasing attention in the past two or three years. Several previous work address this problem by clustering task representations and let the meta-learner exploit the information about task cluster. The authors claim that, for the first time, they cluster tasks not only at the level of the input representation (features), but also at the level of the optimization trajectory in parameter space. ","The paper proposes to apply the task-aware-modulation on global Meta-learning for learning on tasks sampled from heterogeneous distributions. This paper builds on prior work on the feature-based task characterization to integrate the rehearsed task gradient descent trajectory into task representation. As the rehearsal task is computationally expensive, the authors learn a different network to estimate the rehearsed task-trajectory characterization from the feature representation. The proposed framework is tested on few-shot image classification (meta dataset and miniimagenet) and cold-start recommendation tasks. ",0.13924050632911392,0.09282700421940929,0.1350210970464135,0.2125,0.3125,0.20481927710843373,0.4125,0.26506024096385544,0.367816091954023,0.20481927710843373,0.28735632183908044,0.19540229885057472,0.2082018927444795,0.1375,0.19753086419753083,0.2085889570552147,0.2994011976047904,0.2
256,SP:a69b894166482ccd7a3a9db53e0f5a7e6ecff89a,"The paper presents an image coding approach based on 2qubit gates. Downsampled MNIST digits are classified using standard components. Additionally, the experimental section studies the use of reduced codes.","The paper study the problem of quantum neural networks for classical image classification. It leverages the FRQI framework to enable larger size of the input image instead of using very small 4x4 input image. They construct parameterized quantum circuits using XX and ZZ gates (called ""CRADL"" and ""CRAML"") to perform the transformations of quantum state and perform classification.","In this paper, the authors propose a certain type of trainable quantum procedures used as machine learning models for the classification of images. They explicitly show how to implement the circuits for loading compressed images into quantum states that are fed to a trainable unitary that writes the predicted label into a readout register. They conclude on numerical simulation performed on the MNIST dataset.","This paper studies the image classification problem on quantum computers. The prior of image classification by Farhi et al. can only work on 4-by-4 input images, while this paper conducts experiments on 16-by-16 images for the MNIST dataset.",0.20689655172413793,0.13793103448275862,0.1724137931034483,0.1896551724137931,0.15517241379310345,0.171875,0.10344827586206896,0.0625,0.11904761904761904,0.171875,0.21428571428571427,0.2619047619047619,0.13793103448275862,0.08602150537634408,0.1408450704225352,0.18032786885245902,0.18000000000000002,0.20754716981132074
257,SP:a8057c4708dceb4f934e449080043037a70fabf7,"This work proposes to improve model-based RL methods by enforcing the model and value learned to be “self-consistent”, i.e. to satisfy the Bellman eqn where both the Bellman operator and value function are defined in terms of the model. The motivation behind this is that in classic MBRL, the model and value function are learned separately, and the value function is typically learned to be consistent with the model but not vice versa. The authors propose to enforce self-consistency by adding a ""self-consistency loss” (L_SC) to the base loss and jointly learning the model and value functions. L_SC is the squared TD error with a stop-gradient on the target and is calculated using rollouts from the model. Although in MuZero/Muesli the model and value functions are learned jointly as well, the targets of the loss are all defined with respect to data from the true environment and there is not term enforcing the “self-consistency” between model and value functions. Experimental evaluations on a selection of Atari games show that jointly updating the model and value functions with the additional L_SC term leads to improved sample efficiency as compared to Dyna and a model-free baseline. The authors also empirically study the search control problem.","This paper presents the idea of self-consistent reinforcement learning.  By enforcing ""consistency"" between value functions and approximate  (learned) models, a model-based learning algorithm can use synthetic experience generated from the approximate model.  This paper presents a few minor variations on this theme, and explores the results empirically.","This paper considers the problem of model-based reinforcement learning. The authors introduce a new loss, which drives the learned model and value function to be consistent with each other, rather than the model being consistent with the real environment and the value function being consistent with the real environment and the learned model. Experiments show that the proposed self-consistency loss can improve the sample efficiency and final performance. ","The paper offers a new approach to model-based reinforcement learning with their idea of self-consistency of models and values. In the traditional (Dyna) model-based RL paradigm, an approximate model is learned with ground truth transitions and values (and policies) are updated to be consistent with the learned model. The proposed self-consistency update however, formulates a new update to the approximate model so as to make it consistent with the current value estimates using completely imagined (virtual) experience. This update is inspired by minimizing the bellman residual of the approximate model and value for a given policy (since for the true value and model, the bellman equation is satisfied and the residual is zero). An empirical analysis is presented for a set of variations of the proposed self-consistency objective with two types of (grounded) model updates -- MLE (maximum-likelihood) and value equivalent updates. In both tabular and function approximate settings, improved sample efficiency is demonstrated by the proposed method.",0.07906976744186046,0.15813953488372093,0.2186046511627907,0.2653061224489796,0.3673469387755102,0.4142857142857143,0.3469387755102041,0.4857142857142857,0.2883435582822086,0.18571428571428572,0.11042944785276074,0.17791411042944785,0.12878787878787878,0.23859649122807017,0.2486772486772487,0.21848739495798322,0.169811320754717,0.24892703862660945
258,SP:a9a2c21110e00f19882d27bef0063c422a15e576,"The paper addresses the problem of action set selection, i.e. identifying which actions should be available to a RL agent, during training. The set of available actions can influence the RL agent's performance or even hinder it to reach its goal. A method to evaluate action sets is introduced and a case study is performed on a resource tuning example on cloud infrastructure.","This paper focuses on Reinforcement Learning (RL). RL methods often entail a potentially large action space exploration to find a good policy. This work proposes a method to reduce such action space exploration. The method separates actions into two categories: dispensable (the action can be ignored) and indispensable (the action must be taken). Dispensable actions are also ranked according to their importance with respect to the final policy. The method is data driven and operates by looking at the global reward returns obtained when a certain action is removed from the action space. The method is evaluated on a case study simulating cloud infrastructure workload optimisation, i.e. the task of reducing high CPU utilisation by allocating more resources.","The paper proposed a data-driven method for optimal action space selection in a reinforcement learning problem. Given a set of training state, action pair, the proposed approach first filters out the set of indispensable action set and then rank the other action set according to their cumulative reward values. To further improve the efficiency, a Monte Carlo sampling method is proposed for cut-off cardinality computation for action space. An action update rule is devised by computing optimal step size. Finally, a case study on a cloud environment is illustrated to select the optimal set of resources (#vCPUs and Memory size) to optimize the CPU utilization rate. The case study demonstrates that the Monte Carlo sampling based algorithm reduces action search space by 81% and then it creates a list of ranked action set. It is shown that a large action space does not necessarily lead to better performance for an RL agent. ",This paper empirically considered the impact of training action space for reinforcement learning in a case study. Understanding the impact of training action space is a valid and important problem. An empirical study of this problem appears to be the main contribution of this paper.,0.27692307692307694,0.3230769230769231,0.16923076923076924,0.2605042016806723,0.1092436974789916,0.11038961038961038,0.15126050420168066,0.13636363636363635,0.24444444444444444,0.2012987012987013,0.28888888888888886,0.37777777777777777,0.19565217391304346,0.1917808219178082,0.20000000000000004,0.22710622710622713,0.15853658536585366,0.1708542713567839
259,SP:a9e5d81f7ba88f4052730f255cf48cb40ed80942,"This work explores adversraial robustness from a frequency perspective. While this had been done before, this work challenges the commonly held notions, that adversarial perturbations are mainly a high-frequency phenomenon. The authors provide insight that the frequency properties of adversarial examples are dependant on the underlying training dataset. Additionally, the impact of different frequency properties in adversarial training is explored.","In the paper, authors investigate the questions of adversarial robustness through the lens of spatial frequencies. In particular, contrary to a popular misconception, adversarial examples are not always related to high frequency components. Instead, they can encompass a wide range of spatial frequencies and are largely dataset dependent. The paper further studies adversarial training using different frequencies to better understand an accuracy vs robustness tradeoff. Carefully crafted experiments validate their findings and suggest a more effective approach for adversarial training.  ","This paper presents a frequency-based understanding of adversarial examples in deep neural networks. The main observation is that the adversarial examples are neither in high-frequency or low-frequency components but dataset dependent. The authors also analyse the properties of training robust models with frequency constrains, and propose a frequency-based explanation for accuracy and robustness tradeoff.",This paper intends to investigate and analyze the phenomena of adversarial examples through the perspective of Fourier Analyses. It claims three major contributions: 1) they claim that adversarial examples is neither high frequency nor low frequency. 2) they adapt adversarial training with Fourier Analyses 3) they provide a new framework to measure robustness,0.21311475409836064,0.26229508196721313,0.16393442622950818,0.2125,0.2125,0.3103448275862069,0.1625,0.27586206896551724,0.18867924528301888,0.29310344827586204,0.32075471698113206,0.33962264150943394,0.18439716312056736,0.26890756302521013,0.17543859649122806,0.2463768115942029,0.25563909774436094,0.3243243243243243
260,SP:aa4d44b283ef4fea4335847c89fc7b5874169850,"This paper conducts a large-scale empirical study of ERM models tested in the domainBed pipeline, together with the empirical results tested, the authors also investigated the numerical values of several different measures, which allows the authors to further study the relationship between multiple different measures and the empirical performances. The empirical study suggests that there may be interesting connections between the target-entropy and the empirical performances. The paper candidly acknowledges its limitations of being purely empirical. ","The paper provides a large scale study of out-of-distribution generalization measures. It first provides a brief introduction of the core theoretical foundations of the field. Then the paper trains 12000 models on DomainBed Benchmarks to examine the predictive power of theory-based measures. The paper argues that theory based measure fail to accurately capture the OOD generalization behavior. Following the methodology of Jiang 2019 et al, the paper examine how various empirical measures (sharpness, entropy, Fisher, ...) predict OOD generalization behavior. In the end, the paper lists the predictive measures and examines their performance details.","Prior work has found that empirical risk minimization performs unexpectedly well on domain generalization tasks. In this work, the authors try to explain this phenomenon. First, they use theory developed for domain adaptation to bound the target domain test error, finding that theory-based measures do not adequately predict the target domain test error, especially when target domain labels are not available. Next, the authors try a variety of empirical measures. They find that some measures outperform the theory, especially when combined with the in-domain test error.","This work tried to find that which measures can be used for predicting the the out-of-domain generalization for the deep neural networks trained using Empirical Risk Minimization (ERM). The authors started from the domain adaptation theory of Ben-David, 2007 but found that it had limited ability. Then they explored many other measures, including Fisher based measures, Jacobian Norm based measures, Mixup based Measures, and entropy on target data. They found the best single factor led to around 0.70 Spearman's ρ and the joint use can boost the value to 0.80.",0.28205128205128205,0.16666666666666666,0.19230769230769232,0.15625,0.16666666666666666,0.18181818181818182,0.22916666666666666,0.14772727272727273,0.15789473684210525,0.17045454545454544,0.16842105263157894,0.16842105263157894,0.2528735632183908,0.1566265060240964,0.17341040462427743,0.16304347826086957,0.1675392670157068,0.17486338797814208
261,SP:aacc31e83886c4c997412a1e51090202075eda86,"This paper first proposes to take a probabilistic program and compile it into a sequence of invertible transformations. Applying this invertible transformation to a standard normal distribution produces a sample from the probabilistic program. This ""compiled"" invertible transformation is then used as a layer with learnable parameters, called a structured layer, in a larger flow model which can also have unstructured layers. The paper argues that this enables the ability to incorporate domain knowledge via these structured layers while enabling deviation from this domain knowledge via the unstructured flow layers. The paper performs experiments on multimodal, hierarchical, time series and variational inference data. ","This paper tries to bridge the concept of the normalizing flow model with the structured layers that encode the domain knowledge (also called inductive bias here). The general idea is to first notice that many existing probablistic programs (such as univariable RV) can be converted to flow under some special $f_{\phi}$.  Furthermore, the authors proposed a gated layer to allow the model to switch between user-specified model and the learned MAF. Extensive experiments in toy multimodality distributions, hierarchical gaussian, timeseries models and variational inferences are conducted to compare the proposed method with other baselines.  ","The authors describe how to translate a probabilistic program into a normalising flow layer which maps samples from a unit Gaussian into samples from the probabilistic program. They propose a ""gating"" mechanism for this layer so that it can learn to interpolate between this transformation and the identity transform. They then show how this layer can be combined with generic normalizing flow layers, and in this way add an inductive bias to a normalising flow without sacrificing its expressivity. In the experiments, they provide various examples using probabilistic programs in this way to construct normalising flows with helpful inductive biases.","This paper proposes a new type of normalizing flow for incorporating inductive biases into the model architecture. To this end, the authors introduce a so-called “structure layer”, aiming to transform a spherical Gaussian variable into a pre-defined probabilistic program. A modified version, called “gated structured layer” is proposed in order to skip problematic parts of the model. The model is evaluated on a variety of different datasets and outperforms the masked autoregressive flow baseline.",0.18446601941747573,0.22330097087378642,0.20388349514563106,0.15625,0.19791666666666666,0.15,0.19791666666666666,0.23,0.27631578947368424,0.15,0.25,0.19736842105263158,0.19095477386934673,0.22660098522167488,0.23463687150837992,0.1530612244897959,0.22093023255813954,0.17045454545454547
262,SP:aaea75b9c614f77e8025922780f9a8dd9c9d4aab,"This submission gives an algorithm in the MPC model (Massively Parallel Computation) for computing independent random walks from a single vertex or a subset of vertices. A previous paper, [LMOS], on the topic gave an algorithm that allowed for generating random walks from all vertices simultaneously (more specifically, the starting points of generated walks were from the stationary distribution). Suppose that you want to generate many random walks from a single vertex. [LMOS] would require generating lots of random walks from all vertices and because of that would require a lot of total space available in the system, and therefore, it would require a lot of machines. The algorithm introduced in this submission can be significantly more space efficient in this type of setting. ","This paper studies the problem of implementing random walks in the Massively Parallel Computation (MPC) model using logarithmic number of rounds.  Very recently, [LMOS20] develop a MPC algorithm that using O(log \ell) rounds, computes (independent) random walks of length up to \ell from every node in an arbitrary undirected graph. Their main idea is based on stitching: In order to compute walks of length r, we can stick together two walks of length r/2 and we can do this stitching procedure recursively using O(log \ell) rounds. One issue with this algorithm is that if we want to do a few walks starting from a few nodes, this algorithm still implements all walks for all nodes. In particular, for say B walks of length \ell, the algorithm of [LMOS20] uses O(mB) total memory (for all machines).  The current paper claims to solve this issue by proposing a MPC algorithm that uses a total memory of O(mn^{eps}\ell^4+Bn^{eps}\ell) and computes B walks of a given vertex r. The number of communication rounds of this new algorithm is slightly more and is O(log(\ell)/eps).",The paper introduces an algorithm for computing multiple random walks in parallel. Performance is measured using the MPC model. The authors also show an interesting application to local graph clustering. The proposed algorithm improves upon memory requirements of SOTA methods at the cost a logarithmically many more iterations.,"This paper studies the problem of parallelizing single-source and subset random walk computations. The primary technical contribution is an algorithm with the number of parallel rounds that depends on the length of the random walk in a logarithmic way. In addition, two applications are discussed in the article: personalized PageRank estimation and local clustering.",0.25806451612903225,0.11290322580645161,0.12903225806451613,0.07772020725388601,0.09844559585492228,0.16666666666666666,0.16580310880829016,0.2916666666666667,0.2909090909090909,0.3125,0.34545454545454546,0.14545454545454545,0.20189274447949526,0.1627906976744186,0.1787709497206704,0.12448132780082988,0.1532258064516129,0.15533980582524268
263,SP:ab0d024d4060235df45182dab584c36db16d8e31,"This paper proposes conformal methods for learning a predictive confidence set that is guaranteed to be nearly the most statistically efficient among predictive confidence sets produced by a parametric class of learning algorithms. To explain, many wrapper methods for assessing prediction uncertainty assume that the user has already committed to one particular learning algorithm and aim to quantify uncertainty in predictions produced by using *this* learning algorithm. By contrast, the methods proposed here assume that the learning algorithm has been specified only up to some parameter $\theta$ so that it makes sense to optimize $\theta$ with respect to the precision of the resulting predictive confidence sets.  From the point of view of practical implementation, the obvious challenge is the optimization of $\theta$ (as well as the associated prediction sets) over an arbitrary and large space of possible values of $\theta$, as the optimization involves a hard constraint. Thus, one main contribution of the paper is to propose a differentiable proxy that allows an efficient search over the parameter space. Regarding theoretical guarantees, the paper proves that the original, non-differentiable formulation of one of the methods achieves approximately valid coverage and nearly optimal statistical efficiency when the class of learning algorithms has a low capacity.","This paper generalizes the standard conformal prediction calibration setup to a constrained empirical risk minimization problem. Specifically, this work seeks to optimize some efficiency loss, while satisfying coverage constraints. This formulation allows for the introduction of multiple, learnable parameters which can help find a better set-based predictor. The paper explains the implications of this approach by analyzing the generalization error that may occur when transferring the solution learned by constrained ERM to a test population. It also explains practical ways of learning this problem via differentiable surrogate losses and Lagrangians. Contributions-wise, the paper contributes validating theoretical analysis that proves that this method can achieve approximate coverage and near optimal efficiency for certain set-function classes. It also empirically shows that the proposed method can improve over baselines that are not directly optimized for efficiency. ","This paper introduces an extension of conformal prediction with a different formulation. Instead of guaranteed coverage for a finite calibration set, the authors solve a constrained optimization problem where the length of prediction intervals is minimized subject to a coverage constraint. As a result, coverage is only asymptotically guaranteed, but instead the length of the prediction intervals can be shortened compared to traditional split conformal prediction.   The authors present theoretical results in the form of coverage bounds for specific function classes.   In the experiments classical regression datasets are analyzed, as well as multi-output regression problems and one multi-class classification problem. ","This paper considers to improve efficiency of conformal prediction (measured in the ""size"" of prediction sets). To this end, this paper uses multiple-learnable parameters by generalizing single-parameter conformal prediction. By doing so, the paper demonstrate that the proposed approach can improve the efficiency of conformal predictors while satisfying valid coverage. The efficacy of the proposed approach is demonstrated over regression, multi-output regression, and classification.",0.13658536585365855,0.11707317073170732,0.1024390243902439,0.16911764705882354,0.14705882352941177,0.21568627450980393,0.20588235294117646,0.23529411764705882,0.31343283582089554,0.22549019607843138,0.29850746268656714,0.3283582089552239,0.16422287390029328,0.15635179153094464,0.15441176470588236,0.19327731092436976,0.19704433497536944,0.26035502958579887
264,SP:ab5a8934846776a7be7d0ac1973d41fd6aae89fc,"This paper proposes a setup to perform Direct Speech To Speech Translation from one language to another. As such, we could view this as a voice conversion setup, but with the additional task of rendering translated voice in a different language. It seems to be created on the same lines as Translatotron, but with improved performance from different modeling choices. The architecture is encoder-decoder, with the decoder side consisting of two parts - a spectrogram synthesizer, and a phoneme/language translator. The phoneme translator is needed because it also needs to translate to a different language.   The experimental evaluation is quite good, consisting of cases to translate with the same voice as source (voice retention); samples with speaker turns (augmented dataset with two different speaker utterances concatenated together), and cross language translation. The paper shows improved results over Translatotron in all cases.   Aside from the results, the contributions seem to be incremental. They also emphasize a modified training setup that does not use speaker embeddings to avoid antispoofing. ","This paper advances the previously proposed direct speech-to-speech translation (S2ST) model Translatotron.  Direct S2ST has many benefits but has quite limited research effort. The major contributions of this work can be summarized as follows. Translatotron 2 addressed the following major problems of Translatotron: 1)  translation quality, naturalness and robustness of predicted speech are significantly improved and now comparable to a cascaded system (i.e., speech-to-text translation followed by TTS).  2) Similar to Translatotron, the model retrains source speaker’s voice in predicted speech, but Translatotron 2 achieves this without relying on any explicit speaker embedding or speaker ID, hence Translatotron 2 will not be able to generate speech in a different speaker’s voice. This will address some ethical concerns for deployment. Translatotron 2 achieved these improvements through the following approaches. Compared to the previous Translatotron model,  1) Translatotron 2  uses  the output from the auxiliary target phoneme decoder as an input to the spectrogram synthesizer;  2) the spectrogram synthesizer is duration-based, while still keeping the benefits of the attention mechanism.","The paper proposed a speech-to-speech translation (S2ST) model that is and improvement to a previous work. The model is trained end-to-end from speech to speech, along with an auxiliary speech-to-phoneme task. The relevance of the two tasks is further exploited beyond parameter sharing, by feeding phoneme decoder's hidden layer output to spectrogram synthesizer's input, and the synthesizer is duration based. To retain speakers' voice from source language to target language, two data-centric approach is proposed. First, a zero-shot voice-transfer TTS model is trained, to transfer target speech into source speaker's voice. Second, to enforce local voice similarity within an utterance, two samples from two distinct speakers are randomly selected and concatenated to create new training samples. Experiments are conducted on speech-to-text translation (ST) datasets with TTS synthesized target speech, and are measured by both objective (BLEU) and subjective (MOS) metrics. The proposed model's translation quality is much closer to the cascaded ST+TTS oracle than baseline S2ST models, for both bilingual and multilingual S2ST tasks. Its generated speech is also more natural and more resembles the source speech.","Translatotron2, a speech-to-speech neural based translation system is proposed. The work is a modification of Translatotron (also a speech-to-speech translation system) and tries to address some of the  issues in Translatotron:  a) Translation quality fairly below a cascaded baseline  b) synthesized translated speech suffers from robustness issues, such as babbling and long pauses  c) Voice retention relies on explicit speaker embedding which can potentially be misused for generating spoofing audio with arbitrary content  Compared to Translatotron, following modifications are made in Translatotron2. The experiments suggest that Translatotron 2 significantly outperforms Translatotron, and is comparable to a cascaded system, in terms of translation quality, speech naturalness and speech robustness.  a)  Output from the auxiliary target phoneme decoder is used as an input to the spectrogram synthesizer   b) Conformer Encoder with SpecAugment is used, which is known to improve the speech to text performance  c) Duration-based  Spectrogram synthesizer is used. Specifically, architecture and hyperparameters similar to Non-Attentive Tacotron spectrogram synthesizer is used, which is known to improve the robustness issue in Tacotron2   d) To retain speakers’ voices across translation, Translatotron2 is trained on parallel utterances with the same speaker’s voice on both sides. This enables the trained model to restrict generation to only the source speaker’s voice, mitigating the risk of potential misuse for creating spoofing audio artifacts. Parallel utterances are synthesized using a TTS model with cross lingual voice transfer capacity.  In addition, authors also trained Translatotron 2 with examples that contain two speakers’ voices in both the source and the target. The experiments suggest that Translatotron 2 has capability to retain voice identity when the input contains speaker turns.  For multilingual experiments, Translatotron 2 was trained with 4 high resource languages  and BLEU scores suggest Translatotron2 outperforms Translatotron in multilingual settings as well.  ",0.17857142857142858,0.20238095238095238,0.24404761904761904,0.17613636363636365,0.2727272727272727,0.24352331606217617,0.17045454545454544,0.17616580310880828,0.1362126245847176,0.16062176165803108,0.15946843853820597,0.15614617940199335,0.1744186046511628,0.18836565096952906,0.17484008528784645,0.16802168021680214,0.2012578616352201,0.1902834008097166
265,SP:abbab40e40ef09c8dccd16661af3c2a4461ebb1a,"A tournament is made by choosing a direction for each of the edges in a complete graph. A tournament can be induced of by skew symmetric matrices M where entries M_{ij} > 0 if and only if (i,j) is an edge. A tournament on n edges can be represented by a set of d-dimensional vectores {h_1, … , h_n} if (h_j)^T A h_i is not zero iff (i,j) is an edge (and A is an appropriate matrix).  The authors address two questions:  1) What structurally characterizes the class of tournaments that can be represented in d dimensions?  2) Given a tournament T on n nodes, what is the minimum dimension d needed to represent it?.  The first question is answered by considering structures the authors called forbidden. The authors provide a characterization of these forbidden structures as a union of certain equivalence classes.   They answer the latter question in part by providing bounds on what said dimension d should be.   ",This paper provides fundamental theories of tournament representations. The authors study two main questions. First they characterize the class of tournaments that can be represented in d dimensions. Second they give lower and upper bounds on the minimum dimension needed to represent a tournament on n nodes. ,"The paper studies the relationship between dimensional representation of tournament and their structural characterization. In particular, a relationship is established between rank d tournament and their forbidden configurations in terms of flip classes, introduced by Fisher&Ryan(1995) as a way to partion the set of tournaments of a given order. In addition, the problem of bounding the minimum possible dimension of a representation of a tournament is also investigate and lower and upper bounds are given.","This paper studies the theory of tournament representations, i.e. low-rank matrices $M$ whose sign agrees with the sign matrix of a tournament $T$.  The authors show several properties of such representations, reducing the study to so called $R$-cones, i.e. tournaments where one vertex beats all others. They also characterize completely rank 2 tournaments, and provide a forbiden class for rank $d$ tournaments.  Finally, they provide an upper bound on the minimum dimension of a presentation of any tournament $T$, in the for of a bound involving minimum feedback arc sets of $T$, and show how this can be extended to sign matrices.",0.1566265060240964,0.1144578313253012,0.13253012048192772,0.2553191489361702,0.425531914893617,0.2597402597402597,0.5531914893617021,0.24675324675324675,0.20754716981132076,0.15584415584415584,0.18867924528301888,0.18867924528301888,0.24413145539906103,0.15637860082304525,0.16176470588235295,0.1935483870967742,0.26143790849673204,0.21857923497267762
266,SP:acb1e0dc8d6ef5607e7d3ec9893b5364b9a6e831,"In this paper, the authors proposed a novel method to accelerate the QP  (quadratic programming) solving process using reinforcement learning. In their formulation, the state of the problem is the internal variables of the QP solver (in this case, OSQP's internal variables), the action space consists of the ADMM step size vector rho. Since the vector/matrix sizes of the QP can change between different problems, the authors used a multi-agent single-policy formulation, so each agent only observes the internal states that are relevant for a single coefficient in the step vector rho. The authors demonstrated that after training, their RLQP can converge faster for different classes of problems in the benchmark suite, especially if the problem is cold-started. ","The authors use RL to tune the step size policy of OSQP, an ADMM-based solver for quadratic programming problems. They develop policies for both a scalar and a diagonal step size. Numerical tests are performed on the suite of benchmark instances used by the OSQP developers and external benchmark sets (QPLIB, Netlib, and Maros-Meszaros). The results show clear gains within specific problem classes and some evidence of generalization across problem classes.","The paper introduces a RL formulation and training pipeline to accelerate quadratic programming. The main idea is based on learning the multi-dimensional multiplier for ADMM. The paper shows some empirical performance gains over a number of setups.  ====== updated post rebuttal =====  Thanks the reviewers for the replies. Aggregating all information so far, I think the major weakness of the paper is the following:  It does not provide a clear reason why formulating the whole problem as a multi-agent Markov game is useful (I am not fully convinced by the rebuttal). The algorithm I can see is still essentially a single agent policy but with a specialized architecture.  Now that after exchanging messages with the author a few times, I have understood how the detailed architecture works, and I am convinced about my point above. I would suggest that the authors present their architecture (such as mapping from R^6 to R^1) in a clear way, so as to make the architecture easier to understand. Also, I would like to see less stress on the Markov games, because it does not provide much gains -- once again, I'd like to frame it as a form of specialized architecture for single agent problem.  The paper is overall an application of RL to optimization problem. The gains are ok though the idea-wise contributions are not very novel (no extremely novel architecture or formulation).  Still, overall I think the paper provides a solid contribution and I am willing to raise my eval to 6.",This paper presents a method to improve QP solving speed via reinforcement learning.  A regularization parameter used in OSQP solver is being updated via a RL policy. The approach is shown to improve OSQP performance on various benchmarks. ,0.13821138211382114,0.2601626016260163,0.12195121951219512,0.2465753424657534,0.0821917808219178,0.043478260869565216,0.2328767123287671,0.12648221343873517,0.39473684210526316,0.07114624505928854,0.15789473684210525,0.2894736842105263,0.173469387755102,0.1702127659574468,0.18633540372670807,0.11042944785276072,0.10810810810810811,0.07560137457044673
267,SP:acf3825e96d1b7c66cdc339fc5de77330b8e8e90,"This work proposes a novel framework to train a face recognition network for the federated learning setting. The proposed method leverages differentially private local clustering mechanism to allows to securelly share the information of class centers of a local client to other clients.   In addition, the consensus-aware recognition loss encourages the global consensuses for the learned face embeddings among clients and significantly improves the performance as compared with the plain federated learning baselines. ","This paper proposes a FL strategy for face recognition. Despite the wide investigation of FL, very littler research discusses the use of FL for face recognition.  The proposed of PrivacyFace can: (1) distill sanitized clusters from local class centers using Differentially Private Local Clustering (DPLC) . (2) use a consensus-aware recognition loss to  encourage global consensuses among clients, leading to a more discriminative feature learning.  ","The main contribution of the paper is a slight modification of the standard federated learning (FL) framework for the purpose of improving face recognition performance. Since face recognition involves learning a deep neural network for feature embedding and the weight vectors for mapping the feature embedding to an identity, the standard FL framework (e.g. FedAvg) that updates only the feature embedding network does not suffice. Hence, the paper proposes to cluster the weight vectors of each participant and transmit the cluster centers (albeit with Gaussian noise to ensure differential privacy) to the other participants. Once the cluster centers of other participants are known, the local weight vectors can be learned to avoid overlap with those received clusters.","The aim of this paper is to tackle the privacy leakage issue when using federated learning technique to train deep face recognition models. The naïve practice, broadcasting the last fully connected layer, results in the leakage of ID features. But if we cut off the broadcast of $W^c$, it will cause the issue of overlapping IDs among the clients, which harms the training of face recognition model. Therefore, the authors propose their method, DPLC, to tackle this problem. The idea is to conduct online clustering, gaussian perturbation, combined supervision of cluster and local data.",0.2972972972972973,0.25675675675675674,0.21621621621621623,0.2153846153846154,0.2153846153846154,0.17796610169491525,0.3384615384615385,0.16101694915254236,0.16666666666666666,0.11864406779661017,0.14583333333333334,0.21875,0.3165467625899281,0.19791666666666666,0.18823529411764706,0.15300546448087432,0.1739130434782609,0.19626168224299068
268,SP:ad28c185efd966eea1f44a6ff474900812b4705a,"The authors present a variational-autoencoder-based model for learning and generating graph structures. In particular, they propose a multiresolution graph network (MGN) that encodes a given graph in a hierarchical manner, i.e., at different levels of resolution.  Training the nodes of the coarsened graphs as latents of a variational auto-encoder allows for sampling a new graph in a bottom-up manner, i.e., at increasingly higher resolution.  The main contribution of this work consists of developing the framework of MGNs and demonstrating that they can be trained as hierarchical variational autoencoders, which yields the multiresolution graph variational autoencoders (MG-VAEs). The MG-VAEs represent generative models that allow for generating graphs in a multiresolution and equivariant manner. The authors evaluate these models in a range of different settings, from unsupervised representation learning, over link prediction on citation graphs, to the generation of molecules or graph-based image generation. ","This study presents a multi-scale graph VAE with hierarchical graph coarsening. The Gumbel-max trick is employed to generate learnable hard partition for clustering, and permutation equivariant tensor operations are used (Kondor et al., 2018) to construct the group equivariance network. The authos find that the proposed framework exhibits competitive performance in graph generation, molecular generation, molecular representation learning, link prediction and graph-based image generation. Given the applicability of this framework, this proposed framework may be interesting to the graph generation community, and this paper is well-written.","This submission considers a multiresolution graph auto-encoder framework which is equivariant with respect to node permutation. Contrary to prior work on multi-scale graph generation, in this work the hierarchical structure in the encoder is learned through differentiable graph coarsening and is encouraged to produce balanced partitions of the graph. In the decoder, the graph is generated hierarchically and at each level, local adjacency matrices need to be predicted, rather than directly predicting the adjacency matrix of the full graph.  The method is evaluated on molecular generation, community graph generation,  and citation network generation in the main paper. The supplementary materials contains additional experiments on link prediction, unsupervised and supervised molecular property predictions, and graph-based image generation.   ","This paper proposes Multiresolution Equivariant Graph Variational Autoen-coders (MGVAE) which can learn and generate graphs in a multiresolution and equivariant fashion. Higher order message passing is used to encode the graph while maintaining learning mutually exclusive clusters so that coarsening into a lower resolution, thus creating a hierarchy of latent distributions. The model also maintain an end-to-end permutation equivariant with respect to node ordering. Their experimental results show that MGVAE achieves competitive re- sults with several generative tasks and graph link prediction etc.",0.12582781456953643,0.17218543046357615,0.11258278145695365,0.2777777777777778,0.15555555555555556,0.12605042016806722,0.2111111111111111,0.2184873949579832,0.19767441860465115,0.21008403361344538,0.16279069767441862,0.1744186046511628,0.15767634854771784,0.1925925925925926,0.14345991561181434,0.23923444976076552,0.1590909090909091,0.14634146341463414
269,SP:ad5b98e656cac6eb931f80d852c397d117cf1609,"This work addresses the question of how finite-width DNNs differ from their infinite-width GP limit. This is important since, while the GP limit is a powerful theoretical tool for studying DNNs, it's unable to capture _feature learning_, a key behavior of DNNs. The authors show that the mean predictor of a finite DNN trained with noisy gradients and weight decay corresponds to GP regression on a shifted target, and give an approximation for the shift based on the cumulants of the prior. They apply the framework to two toy models. In particular, for a two-layer linear CNN they derive a large-training-set approximation for the cumulants, and show that as the width increases, there is a phase transition out of the feature-learning regime.  ","This paper introduces a new perspective on feature learning in wide neural networks by demonstrating that DNNs trained with noisy gradients and large training data converge to solutions with mean predictions dictated by GP regression with shifted targets when using MSE loss. The authors then consider two toy CNN models, one linear and one non-linear, where they are able to make more analytic progress with their theory, and demonstrate the benefits of of feature learning of wide enough but finite NNs compared to their corresponding infinite-width GPs in terms of sample complextiy on student-teacher tasks. Finally, they also show a phase transition (marked by a critical value of width/channels) in the distribution of eigenvalues empirical covariance of weights in the first layer of a CNN instantiation, between a feature learning regime (where there is an outlier eigenvalue that is associated to the teacher's features) and a non-feature learning regime. Experimental evidence is provided to support these claims. ","The authors develop a self-consistent Gaussian Processes (GPs) theory to account for finite size effects in Deep neural networks (DNNs). The equivalence between GPs and DNNs is a well known result that holds in the so called ‘lazy learning’ regime in the infinite width/channel limit, where the DNNs’ parameters don’t change during training. In practical contexts, however, DNNs are finitely sized and operate in a different regime, called ‘feature learning’, where parameters change substantially during training. This paper provides a theoretical framework to account for finite size effects, and so it allows to investigate the feature learning regime.  To accomplish this, the authors consider the partition function $Z$ associated with the DNN posterior distribution $P(f)$, in the case of training via noisy (full batch) gradient flow with MSE loss. $Z$ is given by an integral over the prior distribution $P_0(f)$, which can be expressed in terms of cumulants. Following their notation, $C$ is the hyper-parameter controlling the over-parametrization. In the infinite-$C$ limit, only the first two cumulants survive, and the standard GP-DNN equivalence is recovered, with the GP kernel given by the second cumulant. For $C$ finite, all cumulants contribute, and the authors have to rely on a saddle point approximation to make progress. By doing so, they arrive at a self-consistent set of equations for the target shift that describes the predictive mean of the DNN. In fact, the predictive mean is given by the same GP-like expression found for the infinite-$C$ limit, though with shifted targets. Next, the authors consider a second order expansion of the action so as to evaluate the posterior covariance.  In order to test their results, the authors consider two examples of DNN architectures that over-perform their associated GPs. Of the two, they mostly focus on the case of a linear two-layer CNN, in a teacher-student set up.  For this simple architecture, the cumulants can be worked out analytically. However, in order to deal with the series appearing in the expression of $P_0(f)$, the authors consider the limit of infinite number of training data, also known as Equivalent Kernel limit. This leads to an analytical expression for $\alpha$, the proportionality factor between the discrepancy in prediction and the target value. Numerical verifications confirm their theoretical predictions (showing mismatches decreasing with the number of training samples). By modelling the empirical weight covariance matrix as a Wishart matrix with a rank-one perturbation, the authors are able to detect a feature learning transition for this model. This is marked by the appearance of outliers in the associated spiked Marchenko-Pastur eigenvalue spectrum.  Finally, the authors consider the case of a two-layer FCN with average pooling and quadratic activations, with a rank-1 teacher. They provide the expression of the cumulant generating function, as well as the equation for the targets shift, and leave the analysis to Appendix I.","In this work, the authors solve for the infinite time mean predictor of a model with weight decay trained via noisy full batch gradient descent. The authors develop a general formalism appropriate for model weights initialized with mean zero and variance related to the weight decay and gradient noise parameters and push this formalism to explicit characterization of the model output for the case of one-hidden-layer linear teacher student CNNs trained with MSE loss and quadratic teacher student fully connected DNNs. In the former case, the authors observe a phase transition between the lazy and feature learning regimes as the model width decreases.",0.24031007751937986,0.3875968992248062,0.21705426356589147,0.2822085889570552,0.13496932515337423,0.0814663951120163,0.1901840490797546,0.10183299389002037,0.26666666666666666,0.09368635437881874,0.20952380952380953,0.38095238095238093,0.2123287671232877,0.16129032258064516,0.2393162393162393,0.14067278287461774,0.16417910447761194,0.1342281879194631
270,SP:adb11a3bd1af2b68720f8f1b48639e31f65295fd,"The authors propose TESSERACT, an aggregation scheme that is robust to the directed deviation attack (proposed in Fang et. al. 2020). ","The paper tackles the problem of adversarial attacks in federated learning settings. The main proposal is a defensive technique to address the “byzantine generals” problem in federated learning: how to ensure that the general ML model is not affected by “poisonous” attempts made by corrupted clients. The proposed technique is experimentally validated on four datasets, outperforms previous defensive methods, and the evaluation also considers adaptive adversaries with increasing degrees of knowledge.   Overall, the presentation of the paper is very good. The quality of the English text is good. Figures are appropriate, Tables require some editing. The topic addressed by the manuscript is trendy and in-line with ICLR’s scope. The references should be improved The contribution is significant  STRENGTHS: + Adaptive adversary  + Trendy subject (federated learning) + Evaluation on multiple datasets + Technically sound  WEAKNESSES - Unclear assumptions and threat model. - Problem or Feature space attacks? - Lack of a concrete use-case - Tradeoff? ","This submission with the title ""Tesseract: Gradient Flip Score to Secure Federated Learning against Model Poisoning Attacks "" discusses defenses against data poisoning in federated learning. The authors propose a novel defense against the recently popularized attack ""Tesseract: Gradient Flip Score to Secure Federated Learning against Model Poisoning Attacks "" by Yang et al. This attack reduces model availability by sending malicious updates from compromised client that maximize sign flips in the global model gradient.  This defense then proposes a measure of change in gradient direction that can be evaluated for each local update and used to dynamically down-weight clients with a large number of flips in direction. ","This paper studied a very important topic in the field of federated learning: how to efficiently resist untargeted model poisoning attacks. In order to defend against such a poisoning attack, the authors developed TESSERACT, an aggregation algorithm that assigns reputation scores to participating clients based on their behavior in the training phase and weights the client's contribution. Extensive case studies have verified the effectiveness of the algorithm. In particular, the experimental results show that TESSERACT provides robustness against even a white-box version of the attack. ",0.2857142857142857,0.3333333333333333,0.42857142857142855,0.12666666666666668,0.12666666666666668,0.18691588785046728,0.04,0.06542056074766354,0.10344827586206896,0.17757009345794392,0.21839080459770116,0.22988505747126436,0.07017543859649124,0.10937499999999999,0.16666666666666663,0.14785992217898833,0.16033755274261605,0.2061855670103093
271,SP:adfe205b335cf87bd4e470efd9f72bb639a4451c,"This paper proposes a new method, ODITS, to enable effective ad hoc cooperation without requiring to predefine a static set of teammate types. This is done by training encoder networks to extract latent variables which capture the ""situation type"", which is trained to be a sufficient statistic for predicting the joint action value. This enables the agents to adapt in online fashion (it doesn't assume static other agents), and additionally the methodology is able to deal with partially observable environments.",The paper focuses on the important problem of building agents that can function as useful teammembers in a ad-hoc team where agents can change behavior over an episode. The paper propose a new framework named ODITS that allows training an agent for ad-hoc teaming applications without strong assumptions on observability or pre-defined roles and categories. It does so my learning a latent variable representation of teammate behaviors in a data-driven fashion during policy training. The framework also adds an information-based regularizer to allow learning these variables from partial local observations of an agent (using all available information during training in a CTDE manner). The paper performs experiments on three different environments to demonstrate their outperformance. ,"In this paper the authors tackle the challenge of ad hoc teamwork with unknown teammate types. Unlike most previous work, the proposed framework, ODITS, does not assume some finite set of teammate types or full observability, and adapts to current teammates via the utilization of a teammate situation encoder-decoder framework, which learns a latent representation of the teammate configuration the agent is currently observing (this observation can be partial). The authors subsequently show this approach outperforms state of the art baselines such as AATEAM on a several domains, namely a modified version of coin game, predator prey, and save the city.","The paper addresses the problem of ad hoc teamwork, where an agent learns how to coordinate with a team of unknown teammates. Previous works typically assume that the teammates belong to one of a finite set of possible (known) types; at test time, the ad hoc agent uses the observed teammate behavior to identify their corresponding type and act accordingly.  The proposed method, instead, builds a predictor of the ad hoc agent's marginal utility (which depends on the teammates) that relies on local information alone. The predictor takes as input the ad hoc agent's current observations, $b_t$, and a latent vector, $c$, representing the current ""teamwork situation"". The teamwork situation representation, $c$, roughly plays the role of ""teammate types"" in previous works. To train the predictor, the proposed method makes use of an encoder-decoder model that, at training time, learns the ""teamwork situation"" representation $c$ using full state-action information. The output of the model is used in an ""integration network"" that estimates the $Q$-function for the underlying MMDP from $c$ and the agent's estimated marginal utility, $u_t$. Finally, since at test time the agent cannot access the full state-action information (and hence cannot estimate $c$ using the aforementioned encoder-decoder model), during training it learns an proxy encoder that builds an estimate $z$ for $c$ using only local information. The overall architecture thus includes two encoder-decoder models jointly trained so that they estimate a coherent representation for the ""teamwork situation""---the first using only local information, and the second using full state-action information. At test time, the agent can use the estimated representation, $z$, as input to its marginal utility predictor and act accordingly.  The paper tests the proposed approach in 3 domains (modified coins environment, predator-prey, save the city) showing positive results over existing architectures and a well-established MARL approach (QMIX), and also presents a brief ablation study establishing the relevance of using the two encoder-decoder models to the overall performance of the method. ",0.19753086419753085,0.2345679012345679,0.30864197530864196,0.16666666666666666,0.2916666666666667,0.37254901960784315,0.13333333333333333,0.18627450980392157,0.07374631268436578,0.19607843137254902,0.10324483775811209,0.11209439528023599,0.1592039800995025,0.20765027322404372,0.11904761904761904,0.18018018018018017,0.15250544662309368,0.17233560090702946
272,SP:ae4bc7f2a00feb13e458ab17804c06709374ceee,"Summary.  The paper is about the parameterized complexity of Bayesian Network Structure Learning. In particular, it is focused to analyze and develop previous contributions on superstructure of the input. The author/s show/s that changing the parametrization yields to achieve fixed-parameter tractability.  The paper also anallyzes how the complexity of structural learning of bayesian networks depends on the input representation, with specific reference to non-zero representation, and additive representation. Finally the paper shows how to extend the main contirbutions presented to the problem of Polytree Learning.","This paper contains a number of significant FPT results for BNSL, as well as providing a concise summary of existing FPT results. A key parameter considered is ""local feedback edge number"" which provides FPT. Results for when local scores are additively decomposable are given. The paper has many results and so many proofs are relegated to the supplementary material.","This is a paper about the parametrized complexity of BN structural learning. To achieve that the authors consider the learning task as a decision problem and distinguish tasks wrt the way the local scores are represented (non-zero, additive, additive with bound on the number of parents). Separately for each class, the authors discuss tractability wrt various parameters (and not only the treewidth) used to describe the superstructure. This in particular suggests a dynamic programming approach for the additive case that is fixed-parameter tractable wrt the treewidth. Those results are also specialized to the case of learning polytree-shaped models.  ","This paper investigates the fixed-parameter tractability of the Bayesian Network Structure Learning (BNSL) problem, according to new parameters related to the network superstructure. Namely, since the BNSL problem is known to be W[1]-hard for various vertex-related parameters, the authors concentrate on edge-related parameters, by showing that fixed-parameter tractability can indeed be achieved in some cases. In doing so, the authors consider two parameters, the feedback edge number (fen) and its “local” version (lfen), which are less restrictive than the tree-cut width (tcw). Orthogonally, the authors examine three families of scoring (function) representations: non-zero representations, additive representations, and bounded additive representations. Finally, the authors explore the fixed-parameter tractability of polytrees.  ",0.10112359550561797,0.24719101123595505,0.29213483146067415,0.22033898305084745,0.22033898305084745,0.2376237623762376,0.15254237288135594,0.21782178217821782,0.22033898305084745,0.12871287128712872,0.11016949152542373,0.2033898305084746,0.12162162162162161,0.23157894736842105,0.25120772946859904,0.16250000000000003,0.14689265536723164,0.2191780821917808
273,SP:ae81e2a23bf6042bf8b04ba41b957bb625268b7e,"The paper proposes a method to craft stronger and more efficient attacks on state observation of the RL agent.  Here, the 'strongest' attacks refer to the attacks that minimize the reward the most. An 'efficient' attacker can craft such attacks using the least computational resources. In the work of (Zhang et al., 2021), the attacking strategy is described by a mapping from the state space of the underlying MDP problem to the same space. The agent observes the falsified states rather than the actual ones and takes actions based on the observed states. This misinformation eventually tricks the agent into taking a different action. In this paper, the authors argue that instead of finding the mapping from the state space to the same space, the attacker can first find the mapping from the state space to the action space that generates the least accumulated rewards to the agent. The 'strongest' attacks on the state observation can then be crafted based on the mapping found. Since in many deep RL applications, the state space is much larger than the action space. Hence, finding the mapping from the state space to the action space is more efficient than finding the one from the state space to the state space.",The paper introduces a novel algorithm to find the optimal evasion attack against RL in scenarios where the agent's policy is known. The key idea is to only learn the aspect of problem depending on the environment and is unknown through a clever decomposition. The algorithm shows promising empirical results.,"The paper studies evasion attacks in deep reinforcement learning (RL). More specifically, the paper considers a novel approach to performing evasion attacks based on two-component design --- director and actor modules, where the latter perturbs a given state based on the policy direction that the former specifies. Effectively, the search for an optimal attack is performed in the policy space, and since the policy space is typically more compact than the state space, the search is more efficient. The paper formally justifies its design choices, and experimentally validates the efficacy of the propose approach, showing that it yields significant improvements compared to the state-of-the-art methods.  ","The paper proposes a method for computing the strongest adversarial perturbation on state observations of an RL agent from a specified set of perturbations. It relates perturbations on states to perturbations on policies. It poses a specific problem to this end, develops a solution, and establishes its optimality. ",0.0821256038647343,0.16908212560386474,0.0821256038647343,0.35294117647058826,0.17647058823529413,0.08333333333333333,0.3333333333333333,0.32407407407407407,0.3541666666666667,0.16666666666666666,0.1875,0.1875,0.13178294573643412,0.22222222222222224,0.13333333333333336,0.22641509433962262,0.1818181818181818,0.11538461538461539
274,SP:af22742091277b726f67e7155b412dd35f29e804,"This paper studies neural contextual bandits and proposes an algorithm that transforms the raw feature vector using the last hidden layer of a deep ReLU neural network, and uses an UCB approach to explore in the last linear layer. Compared with existing neural contextual bandit algorithms, the proposed algorithm attains computation efficiency. Regret guarantees and empirical results are provided to demonstrate the effectiveness of the proposed algorithms","This paper study a novel contextual bandit algorithm: Neural-LinUCB. As in (Riquelme et al 2019), the idea of this algorithm is based on decoupling deep representation learning and exploration. A deep neural network learns the mapping between the context $x_{t,a_t}$, while a linear bandit, OFUL (Abbasi-Yadkori 2011), chooses the arm to play. In contrast to (Riquelme et al 2019), a regret upper bound of the algorithm a regret upper bound of the algorithm is stated in Corollary 4.6. The proposed algorithm is also an improvement over NeuralUCB (Zhou et al 2020) for two reasons: the computational cost of the exploration is lesser, since the exploration is only done in the last layer of weights, and the regret upper bound is tighter. Indeed, in contrast to (Zhou et al 2020) it does not depend on the dimension of the tangent kernel matrix, which can be in O(KT). Experiments, done on four contextual bandit problems, show that Neura-lLinUCB outperforms LinUCB and performs as well as NeuralUCB and NeuralTS. ","The paper presents a new neural-bandit algorithm with shallow exploration and provides a regret bound for the proposed method. The existing approaches have introduced deep neural networks based bandit algorithms to learn reward functions, in which exploration takes place over the entire network parameter space, which can be inefficient for large-size networks which are typical in NTK based approaches. The authors address this by taking an existing approach that decouples the deep neural network feature representation learning from most of the exploration of the network parameters by only exploring over the final layer of the network.   Despite the fact that this idea of shallow exploration has been proposed previously, there has not been a theoretical analysis with a regret bound. The authors analyze a UCB version of this approach, then build from techniques from both deep neural contextual bandits and linear contextual bandits to prove an O(\sqrt(T)) regret bound. Finally, the authors present experimental results to show that their algorithm work well in practice.","Authors tackle the setting of contextual bandits, using deep representation learning combined with an upper confidence bound algorithm. The main contribution of this work is to provide a regret bound for the setup which decouples the representation learning from the UCB search, by searching only over the last layer of the network. The setting had been studied before, but only empirically, and using Thompson sampling rather than UCB. Authors validate their results empirically on several domains from the UCI data repo, as well as on MNIST, comparing against state of the art baselines. ",0.31343283582089554,0.2835820895522388,0.2537313432835821,0.1896551724137931,0.14942528735632185,0.16666666666666666,0.1206896551724138,0.1130952380952381,0.1827956989247312,0.19642857142857142,0.27956989247311825,0.3010752688172043,0.17427385892116182,0.16170212765957448,0.21250000000000002,0.19298245614035084,0.19475655430711608,0.21455938697318008
275,SP:af51c83f2f16cbbd4eb087adb978d7dc1c2d7d76,"This paper presents a simple and novel in-training model pruning algorithm to learn sparse feature representation with reduced computational load w/o compromising accuracy. To do this, authors introduce a single learnable parameter to simplify the optimization involved in enforcing L0 sparsity. Authors further provide theoretic analysis on how reducing the single learnable parameter would enforce L0 sparsity. Finally, authors demonstrate the remarkable performance over various applications comparing with the established strong baselines.    ","This paper proposes a gradual structured pruning method, DAM, that can achieve good performance with various applications. The paper conducts extensive experiments to support the effectiveness of the proposed methods. The method part is reasonable and makes sense.","This paper proposes a novel structured pruning method called DiscriminAtive Masking (DAM) for neuron-level sparsity. A relu-tanh gating function is proposed to apply on the neuron activations, and gradually zeroing out the neurons at lower order by introducing a L0 norm. The proposed method is a single stage method, and does not require further after-prune finetuning. Extensive experiments demonstrate the effectiveness of the proposed method",The authors propose a single stage pruning method (DAM) that jointly prunes and refines weights during training. The method uses a monotonically increasing gate function for the neurons/channels in each layer with one trainable parameter. The gate function only discriminates neurons based on the position of them in the layer. The proposed method achieves reasonable results without any fine-tuning.,0.12162162162162163,0.1891891891891892,0.13513513513513514,0.4473684210526316,0.34210526315789475,0.20588235294117646,0.23684210526315788,0.20588235294117646,0.16393442622950818,0.25,0.21311475409836064,0.22950819672131148,0.1607142857142857,0.19718309859154928,0.14814814814814814,0.32075471698113206,0.26262626262626265,0.2170542635658915
276,SP:af89e1cdd2b39df9982ca5cd9446ec66a4d317f2,"This paper tests a hypothesis of how humans choose the path to explore and maximize reward in unvisited space based on their past experience. The central hypothesis is that human uses program induction to generate prediction of possible spatial map. Based on human behavior in two experiments, the paper further compared different models of map prediction, and demonstrated that humans actually consider the distribution of possible maps instead of only consider the most likely map.  ","This work is motivated by the hypothesis that humans maintain a hierarchical spatial representation when exploring new environments, such that shared patterns (due to the hierarchy) between spatial regions can be used to predict how still unvisited places would look. A simple discrete 2D computational model based on probabilistic program induction is proposed for predicting the map at yet unseen places. The model assumes a discrete set of possible transformations (flips, rotations, concatenations, etc.) of small extracted regions (submaps of previous observations), based on which a distribution over possible completions of empty map regions can be formulated.  A control task is then considered, in which an agent needs to explore a completely new environment and collect rewards (tokens) in the process. Under strong structural assumptions for the test environments (e.g. it is ensured that patterns actually repeat in a very uniform way, rooms look the same, rewards are placed consistently, etc.), it is shown that model-based planning under the map-induction model results in more efficient exploration & reward-collection behavior. Furthermore, a study with actual human subjects is conducted where they need to solve the same task. Results suggest that real-life exploration behavior is consistent with the most expressive version of the proposed computational model, where a full distribution over possible map completions is maintained (as opposed to a MAP estimate or an uninformed model). ","This paper investigates the concept of map induction for exploration in novel environments. This is an important problem for robotics applications, relevant for scaling up navigation of autonomous systems to large environments and for exploring unknown environments.   The article's main contribution is a new task and a set of experiments to evaluate the central hypothesis: ""that humans use program induction to infer possible maps of unseen spaces, as made up of submaps encountered in the observed areas"".  This is evaluated by proposing a novel map induction task that is the main contribution of the paper, as well as a set of probabilistic models that implement the different hypotheses considered in the paper. ","In this paper, the authors try to model how humans explore novel environments. The main hypothesis tested in the paper is that humans think of maps of unseen places as compositions of submaps in observed areas. Towards this, the authors propose a new ""Map Induction Task"" (MIT) to study how humans explore novel maps. Additionally, they try to model the exploration behavior exhibited by humans as a hierarchical bayesian generative framework which generates a distribution over possible maps given past observations, and use this distribution to plan. They present results on their  task (MIT) which validate the aforementioned hypothesis",0.30666666666666664,0.28,0.25333333333333335,0.11403508771929824,0.11403508771929824,0.24778761061946902,0.10087719298245613,0.18584070796460178,0.1919191919191919,0.23008849557522124,0.26262626262626265,0.2828282828282828,0.15181518151815182,0.22340425531914893,0.21839080459770116,0.15249266862170088,0.15902140672782875,0.2641509433962264
277,SP:b147639f58dd3197beb928c609d636e853c6bdd6,"The authors formulate and study dynamic mechanism design in an MDP-like setting. The motivation is principal-agent problems. The principal commits to a policy to choose actions given observations of states; the environment evolves according to some transitions dynamics based on these actions; the agent does not act but is free to misreport states to the principal. Agent and principal receive utility; the principal’s goal is to choose a policy that is incentive compatible and individually rational (standard goals of mechanism design) while maximizing its own expected total utility.  In general, from a full description of the environment and dynamics, the “planning” type problem to find an optimal mechanism is NP-hard (shown via reduction from MAXSAT). If the time horizon of the problem is treated as a constant, then it is possible to formulate a linear program to find the optimal mechanism in polynomial time, and doing so is a major portion of the paper. ",This paper presents and proves a linear program for unstructured dynamic mechanism design. The LP supports payments and different individual-rationality constraints. The key contributions are: 1) Presenting an LP that provably yields an optimal mechanism for an unstructured dynamic environment with a finite time horizon. 2) From this LP formulation and standard LP complexity results follows a polynomial runtime guarantee for finding an optimal mechanism.,"The authors study the problem of computing optimal mechanisms in a dynamic unstructured environment. Unlike a static environment, the principal is allowed to repeatedly interact with a strategic agent and take actions based on the agent's reports of the current state of the world. The authors show that when the time horizon of such a dynamic environment is not finite, the problem is intractable. When the time horizon is small, they show that an efficient mechanism can be computed using an LP","This paper studies automated mechanism design in dynamic unstructured dynamic environments. In the setting of interest, a principal and agent repeatedly interact for a finite number of time steps (finite time horizon); at each time step, the agent reports the current state of the environment, the principal takes action, and the environment transitions to the new state. Importantly, the agent's and the principal's utilities are not necessarily the same, implying that the agent might misreport which further implies the need for a mechanism.  The paper considers a randomized dynamic mechanisms that consists of the policy of the principal and the mechanism's payment function. The main results are: a) an LP for finding optimal mechanisms (constant time horizon), b) a computational complexity result which shows that the principal's utility is hard to approximate within a certain factor for environments with large time horizons.  The paper has additional results, as mentioned in the abstract, but these are not presented in the main text.",0.10126582278481013,0.17721518987341772,0.22784810126582278,0.19696969696969696,0.2727272727272727,0.3132530120481928,0.24242424242424243,0.3373493975903614,0.21818181818181817,0.1566265060240964,0.10909090909090909,0.15757575757575756,0.14285714285714285,0.23236514522821575,0.22291021671826625,0.174496644295302,0.15584415584415584,0.2096774193548387
278,SP:b169c94c8fcc4f13cafdbafbe18eb26cb471ea0b,The paper is based on the idea that most of the computation costs to extended graphs come from triplets. The authors proposed a way to parallelize the computation of triplets in a distributed way. The author also discussed two models that fit into this framework and showed their increase in performance due to the larger parameter count enabled by their parallel framework.,"This paper proposes a method to train large scale graph neural networks containing up to billions of parameters, called Graph Parallelism. The method is used to train large scale versions of the DimeNet++ and GemNet models containing 10-20x more parameters that the vanilla versions. These large GNN models are evaluated on a set of tasks from the Open Catalyst 2020 (OC20) benchmark and show improved performance compared to the smaller baselines. ","This paper proposes a distributed training method for large graph neural networks (GNN) up to billion parameters. The method first distributes the triplet update operations to multiple GPUs and aggregate the updated vectors by global synchronization. It then distributes the edge update operations to GPUs and aggregate the edge vectors (another global synchronization). Finally, it applies node update in parallel and global node aggregation at the end. The method is applied to GemNet and DimeNet up to 1.12 billion parameters and achieved state-of-the-art results on OC20 benchmark.","The authors present an approach to train graph neural networks with many parameters across multiple GPUs. The approach is specifically demonstrated for graphs representing molecular structure and two graph neural networks of similar flavor that have previously been presented to learn from such structures. Higher-order interaction terms make these networks very compute intensive. Leveraging their parallel approach to scale up the existing GNNs, the authors demonstrate compelling results on the OpenCatalyst benchmark. ",0.1935483870967742,0.1935483870967742,0.14516129032258066,0.3194444444444444,0.20833333333333334,0.16483516483516483,0.16666666666666666,0.13186813186813187,0.1232876712328767,0.25274725274725274,0.2054794520547945,0.2054794520547945,0.1791044776119403,0.1568627450980392,0.13333333333333333,0.2822085889570552,0.20689655172413793,0.18292682926829268
279,SP:b1f622cbc827e880f98de9e99eca498584efe011,"The submission is concerned with vaccine design and proposes to model the task as a combinatorial optimization problem that is essentially a generalization of set cover. The authors describe two algorithms: A marginally greedy algorithm using beam search and a mixed integer linear programming approach.  More precisely, the combinatorial problem consists in covering weighted elements (the population) with overlays (peptides) while maximizing the total weight of those elements that are covered at least n times. Each overlay may only be used once, but may cover the same element multiple times. In total, no more than k overlays may be chosen.  Additionally, the submission features a machine learning based tool that predicts the effectiveness of the designed vaccine.","Update: I have read the author feedback and other reviews. Based on the more detailed comparison to existing methods, I have updated several of my scores.  ---  In this work, the authors propose a variant of the set covering problem which aim to formalize peptide-based vaccine design as an optimization problem. The authors first formulate the problem and then draw connections to other set covering problems; they then show that the problem is NP-complete. They then propose a greedy algorithm to perform the optimization. A set of experiments suggests the proposed approach outperforms the formulations considered by the authors.","This paper suggest the n-times coverage problem (a natural generalization of set-cover where items need to be covered n>1 times by the sets we choose) and motivates it with an application for the design of COVID vaccines. This application involves some ML pipeline, and this new combinatorial optimization problem helps with one of the steps.  The problem is shown to be not-submodular, and therefore the most standard methods don't quite work. Nonetheless, the authors present a greedy algorithm to get a reasonable approximation.  The paper evaluates the quality of their algorithm on artificial data and shows that it outperforms the more standard greedy method. Then they show the impact of this improvement for vaccine design.","The paper introduces a variant/generalization of multi set multi cover problem, where the aim is to maximize the weight of the elements covered at least n times by up to k overlays (subsets of a given input familiy of sets over the elements' universe). The authors show that the objective function is not submodular, hence does not admit a classical greedy approach. They show an ILP formulation that provides an optimal solution but not efficiently. They propose a greedy approach based on maiximization of marginal gains endowed with a look-ahead tie breaking that should prevent failure to attain n-coverage because of wrong initial choices guided by the sole marginal gain maximization objective.  The problem is meant to model vaccine design and show empirical evaluation on two toy data sets and presenting comparisons between the greedy approach and the ILP approach for a covid 19 vaccine. ",0.15384615384615385,0.19658119658119658,0.2222222222222222,0.23,0.24,0.26666666666666666,0.18,0.19166666666666668,0.17567567567567569,0.19166666666666668,0.16216216216216217,0.21621621621621623,0.16589861751152074,0.19409282700421943,0.19622641509433963,0.20909090909090908,0.1935483870967742,0.23880597014925373
280,SP:b28a9d1ad4c539d07d53e39376cbd76024d7745c,"This paper proposes an approach to learning optimal striding parameters in convolutional networks. The proposed approach, DiffStride, is a downsampling layer that builds on spectral pooling to allow for integer output dimensions, but arbitrary strides by cropping in the Fourier domain. Unlike spectral pooling, DiffStride relaxes the parameters of the cropping mask to be differentiable, and using the stop-gradient operator in the cropping. Results show that the proposed approach can work well as a drop-in replacement for normally fixed pooling layers. It is also demonstrated that from different random initialisations a variety of different pooling approaches can be learned that acheive similar performance. Use of a regularisation term the attempts to encourage time and space efficiency reduces this variability and allows accuracy to be traded off.","In this paper, the authors propose DiffStride, a technique for learning the stride of downsampling operations in neural networks by gradient descent. Specifically, similarily to SpectralPool, the feature map is transformed to the frequency domain by a Discrete Fourier Transform, where it is then cropped according to learnable parameters. The authors envision a simple soft-relaxation of the cropping in order to allow the gradient to flow towards cropping parameters. Performances against both standard and random stride policies are reported on a number of datasets, both for audio and image recognition. Moreover, the authors introduce a regularization objective that penalizes the use of small strides, in the interest of encouraging downsampling for improving computational and memory cost.","In this work the authors introduce a differentiable stride formulation, which allows for learning the stride value. To this end, they propose learning the size of a cropping mask in the Fourier domain, which allows for learning how to perform resize in a differentiable way. Authors present experiments on several datasets on different domains (image, audio), including large scale datasets.","This paper proposes DiffStride as a drop-in replacement to standard downsampling layers. It extends previous work Spectral Poolnig and learns the size of the cropping box in the frequency domain by backpropagation. Experiments are conducted on audio and image classification, and the results show that the model can learn non-integer stride and adapt different initial stride well.",0.203125,0.109375,0.1484375,0.1623931623931624,0.18803418803418803,0.21666666666666667,0.2222222222222222,0.23333333333333334,0.3220338983050847,0.31666666666666665,0.3728813559322034,0.22033898305084745,0.21224489795918366,0.14893617021276595,0.20320855614973263,0.21468926553672318,0.25,0.21848739495798322
281,SP:b31b1ee7067d4da70916986ba13e80bb14e2fdfe,"The paper studies multinomial logistic regression bandit problem, where the number of outcomes can be greater than two. This is an extension of the well-studied generalized linear bandit problem with two outcomes (e.g., clicks and no-clicks). The authors first propose the problem setup with real-life motivated scenarios and then propose a UCB based algorithm to minimize cumulative regret. The authors analyze its performance and derive an upper bound of \tilde{O}(dK\sqrt{\kappa T}), where d is the dimension of the action space, (K + 1) is the number of outcomes, T is the number of interactions, and \kappa is the degree of non-smoothness, which is inherent to the multi-nomial problem. At the end, the authors provide a simulation based study which supports their theoretical insights.  ","The paper studies multinomial logistic (MNL) regression bandits, a generalization of binary logistic bandits. It differs from previous works on MNL bandits by considering a different decision-making process. That is, the authors do not address the combinatorial action selection of previous MNL bandits, but rather study the problem setting where the agent offers a single item, but with K different outcomes.   The paper introduces a UCB-based algorithm for this problem, for which the authors establish a sub-linear regret bound. By building on recent advances in logistic bandits, they improve the prohibitive impact of the constant \kappa from both the design of their algorithm and its theoretical guarantees.","This paper presents MNL-UCB Algorithm for contextual MNL bandits problem. At every step, the algorithm estimates a K-tuple of parameter vectors in the confidence region. Specifically, they prove that the regret of MNL-UCB scales as \tilde{O}(dK\sqrt{\kappa}\sqrt{T}) where $\kappa$ is a parameter that captures the degree of (non)-smoothness of the MNL model. It improves the constant from linear in \kappa to \sqrt{\kappa}. They further improved algorithm that achieves a regret bound with problem-dependent constant \kappa being pushed into a second order term.  Compared to the most existing work, a major difference is that each choice is associated with a different parameter so a matrix of parameters needs to be learned. ","This paper considers a version of stochastic contextual bandits where the rewards are drawn from a discrete distribution governed by a multionimal logit model. This setting is motivated by applications in online advertising where the user may give various feedbacks in the face of a recommendation beyond a binary ‘clicked’ or ‘did not click’ – e.g. ‘show me again later’, ‘never show me again’. The authors propose and analyse a UCB algorithm for this problem with the confidence sets being based on the work of Faury et al. (2020, ICML) and the regret analysis being based on that of Abbasi-Yadkori et al. (2011, NeurIPS). The particular advantage of utilising the concentration results of Faury et al is that the regret of the resulting algorithm scales favourably (square root order) with respect to a parameter $\kappa$ characterising the smoothness of the logit function around the true parameter. A further algorithm whose dependence on $\kappa$ is relegated to lower order terms w.r.t. the horizon is also developed – however this approach is computationally intractable. In the related (binary) logistic bandit setting, some popular algorithms have been shown to have exponential order dependence on this parameter and the design of algorithms which are robust to its value is useful. It may be useful to note that this version of a multinomial bandit is different from the combinatorial problem in the dynamic assortment selection literature (e.g. Agrawal et al (2017, 2019)) where the different outcomes represent different items.   ",0.2196969696969697,0.19696969696969696,0.2803030303030303,0.18181818181818182,0.2818181818181818,0.2644628099173554,0.2636363636363636,0.21487603305785125,0.14979757085020243,0.1652892561983471,0.12550607287449392,0.12955465587044535,0.2396694214876033,0.20553359683794464,0.1952506596306069,0.17316017316017318,0.1736694677871148,0.17391304347826086
282,SP:b3feb15b01e519e5b2e28b1c4a144056c493e2bc,"Quantum neural networks have two parts: a quantum embedding circuit that takes in classical data and embeds it into a quantum state, and a variational quantum circuit that learns a quantum circuit for evolving the quantum state before measurement.  The authors propose to encode the classical input data into the parameterized angle in the quantum embedding circuit using a tensor-train network (instead of a dense neural network). This results in a 3-4% improvement in prediction accuracy for MNIST (87.12% accuracy).","Summary:  This paper is dedicated to designing an end-to-end learning framework for quantum neural networks. The authors design a novel quantum tensor network for dimension reduction and quantum embeddings generation, since it is quite related to the practical usage in real-world applications where only a small number of qubits could be supported on available NISQ computers at this moment. The key contribution of this paper is to leverage a tensor train network (TTN) to replace the dense layer for dimension reduction, which enables an end-to-end training process fully conducted in a quantum computer.","As it was already studied in the literature, one way to implement a machine learning classifier in quantum computers is to convert classical input data like images into quantum states that are fed to a Variational Quantum Circuit (VQC). However, current quantum computers have a small number of qbits, which requires to perform a dimensionally reduction of the input datasets as a preprocessing step. In this paper, the authors propose to use data compression based on the Tensor Train Network (TTN) model, which is a very well-known compression technique. They compare TTN compression against a straightforward dense layer and the PCA-based dimension reduction techniques. The paper includes experimental results on MNIST dataset by simulating quantum circuits with up to 8qbits for noisy and noiseless scenarios.","This work proposes an end-to-end learning framework TTN-VQC for quantum neural networks. The main problem is that current quantum computer cannot handle large number of qubits.  The idea of this paper is applying QTT to perform dimensional reduction followed by quantum encoding TPE, which can be combined with existing quantum neural network (VQC) as an end-to-end learning model. ",0.21686746987951808,0.30120481927710846,0.12048192771084337,0.23469387755102042,0.2857142857142857,0.14173228346456693,0.1836734693877551,0.1968503937007874,0.15873015873015872,0.18110236220472442,0.4444444444444444,0.2857142857142857,0.1988950276243094,0.2380952380952381,0.136986301369863,0.20444444444444443,0.34782608695652173,0.18947368421052635
283,SP:b491314336c503b276e34e410cf461cb81294890,"This paper introduces a unified view of several speech restoration problems including denoising, decliping, dereverberation and audio super-resolution. In order to solve the problem of the general speech restoration task, the authors propose a U-Net architecture which is trained on all of these tasks simultaneously during training time. The authors conduct extensive experiments for the general speech restoration task as well as the individual tasks where they compare the proposed models with more specialized models bounded to each distortion. The experimental results show that the proposed VoiceFixer combination of the model and the analysis-synthesis procedure are capable of effectively removing the speech distortions and in some cases outperform previous approaches in the literature.","The paper proposes a single system to deal with the speech enhancement tasks of denoising, dereverb, bandwidth extension (BWE) and declipping. The system is a two-stage system composed of an analysis module producing mel-band masks and a synthesis module using a vocoder. Both modules reuse existing architectures. Results comparing the proposed system to author-derived counterparts of it and to some existing systems for specific tasks show improvement for the proposed system in some cases, while achieving similar performance as existing systems in other cases. Data and code for reproducibility are provided.","This paper proposes a general speech restoration (GSR) task that tries to remove multiple distortions in a single model. In addition, it also presents a generative framework called VoiceFixer consisting of analysis and synthesis stages to address the general speech restoration task. In VoiceFixer, the authors employ a ResNet for modeling the analysis stage and a TFGAN-based neural vocoder for synthesis stage. They report that their baseline GSR and VoiceFixer surpass the single speech restoration (SSR) models with more improved results by the latter. Their idea was well described and the experiments are systematical and extensive. The results are consistent and clear.  The contribution of this paper is to incorporate a variety of speech restoration tasks including speech denoising, super-resolution, dereverberation, declipping, etc. in a single unified task called GSR. Another is the proposal of a well-performing generative speech restoration framework called VoiceFixer. ","The paper proposes an approach called VoiceFixer which is aimed at restoring degraded speech signals. The paper considers a variety of speech degradations - additive noise, reverberations, clipping and limited bandwidth. The paper describes a two stage approach in which the first stage aims to produce restored mel-spectrogram and then a vocoder is used to synthesize the  speech from the restored mel-spectrogram. Experiments are done using the VCTK dataset and experiments are done using single distortions as well as combinations of all 4 distortions. ",0.15517241379310345,0.28448275862068967,0.14655172413793102,0.20212765957446807,0.19148936170212766,0.1506849315068493,0.19148936170212766,0.22602739726027396,0.2,0.13013698630136986,0.21176470588235294,0.25882352941176473,0.1714285714285714,0.25190839694656486,0.1691542288557214,0.15833333333333333,0.2011173184357542,0.19047619047619047
284,SP:b4ad4632cd55a85b5403e936c4bd828e484473f7,"This paper studies conditional learning by training an EBM in the latent space of a pre-trained top-down generator such as StyleGANs. The EBM is a joint distribution of data and attributes together, and sampling from it is formulated as solving an ordinary differential equation. Experimental results show that the method outperforms the state-of-the-art in both conditional sampling and sequential editing. The contribution of the paper lies on obtaining state-of-the-art performance of conditional learning by combining existing technologies, such as StyleGAN, latent space energy-based learning, and score-based generative model via stochastic differential equations. ","The paper proposes an effective and efficient approach to conditional image generation using GANs. Two main advances are described 1) The use of EBM in latent space that don’t require the retraining of image generators. This allows for very efficient training since the generator can be held fixed. 2) The use of an ODE solver for sampling that is less sensitive to hyperparameters than traditional LD solvers.  Results are shown on CIFAR-10 and FFHQ datasets. This includes results on sequential generation, compositional generation and zero-shot generation. ","Using labelled real data, an EBM is trained in the latent space of a pretrained GAN, rather than directly in pixel space. An ODE solver is used to sample from the EBM rather than Langevin Dynamics, which is shown to be more robust to hyperparameter settings. Using properties of the EBM, conditional generation compositional editing can be demonstrated, including in zero-shot scenarios.","The paper considers the task of obtaining additional control over a pre-trained generative model without re-training it. Specifically, it aims to generate conditional samples from an unconditional StyleGAN by only training a classifier with respect to the conditioning information. To achieve this, it considers the conditional distribution of StyleGAN latent codes. By Bayes Theorem, this conditional distribution is proportional to the unconditional distribution of the latent codes (which has a known standard normal distribution) times the distribution of conditioning information given the corresponding sample. The latter is modeled as a Gibbs distribution with an energy derived from the classifier. Two approaches to sample from this unnormalized distribution are evaluated: (i) Langevin sampling and (ii) Performing gradient descent on the classifier-derived energy. Experiments demonstrate good performance of this approach with significant improvements over StyleFlow for combinations of conditionings which are not in the training data.",0.1568627450980392,0.14705882352941177,0.20588235294117646,0.19101123595505617,0.19101123595505617,0.20634920634920634,0.1797752808988764,0.23809523809523808,0.14285714285714285,0.2698412698412698,0.11564625850340136,0.08843537414965986,0.1675392670157068,0.18181818181818185,0.1686746987951807,0.22368421052631582,0.1440677966101695,0.1238095238095238
285,SP:b4dcb19fd97a906ed37e6af407260f0dedbbd402,"The paper proposes a decomposition method for large-scale vehicle routing problems (VRPs) that leverage supervised learning. Specifically, the proposed methodology uses regression to identify a smaller, more tractable subproblem from a given initial solution, which is then addressed by another solver and incorporated into a full solution. Numerical results compare different state-of-the-art heuristics (e.g., LKH-3) and exact approaches (e.g., OR Tool) on different variations of the VRP with up to 3,000 nodes. ","The paper proposes new ways to learn good decomposition steps in a heuristic search algorithm for the capacitated vehicle routing problem. The learning algorithm can select good seed points for the decomposition, often permitting significant speed-ups (1.5x) over other baseline decomposition algorithms (random and other simple variations). Additional experiments are conducted to evaluate the impact of the client distribution on the performance of the proposed approach.","The paper introduces a local search technique for vehicle routing problems (VRP) that decomposes the general problem and delegates smaller subproblems to a subsolver, e.g. an exact VRP solver. How to decompose the problem is learned from a generated training set using a transformer model and is the main contribution of the paper. Experiments results with a comparison to recent works in the area show the effectiveness of the proposed method. ","This paper proposes an iterative framework to solve large-scale Vehicle Routing Problems. At each iteration, given the current feasible solution, a learned component selects subroutes that form a VRP subproblem. Then the subproblem is solved using a standard VRP solver (LKH-3) and the subsolution is used to update the current solution. The subproblem selector uses a Transformer encoder and predicts the subsolution cost; it is trained in a supervised way. Experimental results show that the method is able to reach the performance of the VRP solver LKH with significantly shorter computation times. ",0.1875,0.2375,0.25,0.25,0.22058823529411764,0.2916666666666667,0.22058823529411764,0.2638888888888889,0.2127659574468085,0.2361111111111111,0.1595744680851064,0.22340425531914893,0.2027027027027027,0.25,0.2298850574712644,0.24285714285714285,0.18518518518518517,0.2530120481927711
286,SP:b78c78fd0b10a94466c049e97c59a56ea5455df6,"This paper proposes an approach to transferring pretrained models in a deep RL context. Instead of pretraining a neural network and then simply copying the weights and retraining on a new task (as would be the naive approach taken in supervised learning), the paper instead proposes transferring the pretrained policy's behaviour. This is done by modifying the epsilon-greedy exploration strategy to include the ability to use the action suggested by the pretrained policy, which can also be used as a kind of multi-step option for improved exploration.  Pretraining is conducted using the ""Never Give Up"" (NGU) intrinsic reward in a reward-free setting, and experiments are conducted in the full set of Atari games. Results and ablation studies indicate that such a transfer approach can drastically improve the performance of an agent on a new task.   The main contributions are a) the use of NGU as an unsupervised pretraining objective in RL; b) a method for incorporating a pretrained policy when learning a new policy; c) experimental results and ablation studies across the full suite of Atari tasks that demonstrate the improvement resulting from leveraging the pretrained model when exploring in a new task.","The authors present a simple, intuitive method for fine-tuning “behaviors” by pretraining an exploration policy before using it as a temporally extended exploration procedure and a one-step additional action to the downstream policy on a downstream task. They extensively evaluate this on the suite of Atari games and demonstrate strong performance gains. Despite some qualms I have with assumptions and ablations, I think this paper should be accepted to NeurIPS. However, I do hope that the authors address some of my concerns in the rebuttal period. For now, this is a ""weak accept"" but if my concerns are addressed properly the score will be raised appropriately. ","The authors study the problem of transferring pre-trained behavior for exploration in reinforcement learning. They propose an approached called (BT) which relies on the pre-trained policy for collecting experience through temporally-extended exploration, which can be triggered with some probability at any step, or  one-step calls to the pre-trained policy based on value estimates. The experiments presented in this work show that, when combined with large-scale pre-training in the absence of rewards, existing intrinsic motivation objectives can lead to the emergence of complex behaviors.","This paper studies the transfer of unsupervised RL agents in the Atari-57 benchmark suite.   The authors propose behavior transfer (BT), a simple method for transfer that focuses on using the behavior of the pre-trained policy, separately from using the pre-trained policy as an initialization (a.k.a. fine-tuning). BT augments a downstream off-policy learner in two ways: the downstream learner can use the pre-trained policy to explore (for a temporally extended duration), and the pre-trained policy is also added as a pseudo-action to the action space as an option during exploitation (for a single time step).  In the experiments, the authors primarily focus on using the never give up (NGU) objective to obtain the pre-trained policy and assess the impact of BT with this pre-trained policy on downstream learning via recurrent replay distributed DQN (R2D2). The protocol differs from prior works in that each learning phase is much longer: 16B frames per game for pre-training (a 64x increase over [28]) and up to 5B frames per game for transfer (a 12500x increase over [35]). This is shown to be important overall. For standard Atari-57, transfer via BT compares favorably against R2D2 baselines that lack unsupervised pre-training, with more pronounced gains for hard exploration games. Ablations show that both BT methods of using the pre-trained policy are necessary for best performance. The authors also assess on custom tasks in the Ms Pacman and Hero games, with BT causing significant improvement over R2D2 baselines despite the zero-shot pre-trained policy used for BT not achieving any success. Finally, the authors also assess BT with fine-tuning from (some subset of) the pre-trained policy's weights. For standard Atari-57, fine-tuning improves over training from scratch for both R2D2 as well as R2D2+BT. However, finetuning without BT from a partial initialization outperforms any variant involving BT, an interesting negative result. ",0.1116751269035533,0.09644670050761421,0.2182741116751269,0.18518518518518517,0.3055555555555556,0.35555555555555557,0.2037037037037037,0.2111111111111111,0.13230769230769232,0.2222222222222222,0.10153846153846154,0.09846153846153846,0.1442622950819672,0.13240418118466898,0.1647509578544061,0.202020202020202,0.15242494226327946,0.15421686746987953
287,SP:b7ad495901eb2f73a8a26aa5c9325908451cfe09,"This submission addresses the problem of constructing a good initialized pretrained network for the object detection task. Compared to previous work, the authors argue that pretraining should be adapted to object detection by focusing on object-level features learning. Instead of pretraining on ImageNet, the authors pretrain the model by designing an object level contrastive learning framework. Experiments on COCO and PASCAL show that the proposed method achieves higher performance. ","The paper aims to design a self-supervised pre-training method for the downstream object detection task.  The authors propose to perform self-supervised object-level representation learning. Specifically, this method uses selective search to obtain object proposals in unlabeled images and extract the object-level features as in Mask R-CNN. Clear improvements have been achieved on COCO object detection and instance segmentation with Mask R-CNN.","This paper provides a self supervised method tailored for object detection (in the RCNN paradigm), inspired from BYOL.   The general idea is to use a non-parametric method (selective search) to generate some boxes, then apply a BYOL-style loss between ROI-Aligned features of each box coming from different augmentations of the image.   It differs from other SSL methods (BYOL, SWAV,...) in that it instead of pretraining solely the backbone, this methods ensures that all the components of the detector (FPN, RCNN head) are pre-trained.  It shows convincing results on COCO and pascal as well as some subsets, in a few shot setting. The code is made available as part of the supplementary, thus helping with reproducibility. ","This paper proposes SoCo, a method for self-supervised representation learning for downstream object detection and instance segmentation tasks.  They argue for stronger alignment between downstream tasks and self-supervised pre-training. They demonstrate this in the case of object detection by (a) developing a representation learning scheme that encourages object level scale and translation invariance and (b) pre-training several components of the downstream detector such as FPN and R-CNN head unlike prior work which mostly just trains the ResNet backbone.  The former property will help their model detect objects at several locations and scales as is needed for object detection.  The latter property helps them get stronger alignment between the upstream and downstream setups. To this end they propose ""scale aware assignment"". The ideas is that, representations for small objects are taken from finer FPN pyramid levels and for bigger objects are taken from coarser FPN pyramid levels. This matches what happens in the actual FPN where anchors at finer pyramid levels are smaller and at coarser pyramid levels are bigger.  The overall recipe is as follows: Selective search, a hand-crafted region proposal technique, is used to suggest boxes on all images. K of these boxes are randomly selected in each training step per image and three views of that image are constructed using random resized crop and resize operations. FPN features are extracted using RoiAlign for these box locations using ""scale aware assignment"". A BYOL loss is applied between representations of the same box across the three views. As in BYOL an EMA teacher network and projection head are used.  They pretrain R50-C4 and R50-FPN  models upstream and fine-tune corresponding Mask-RCNN models downstream for MS COCO and Pascal VOC object detection and MS COCO instance segmentation. Low data regime is explored using 5% and 10% ""Mini"" COCO setups. ImageNet linear eval results are included in the appendix. Ablation experiments evaluate whether stronger alignment in terms of architecture between pre-training and fine-tuning were helpful and sensitivity w.r.t. hyper-parameters such as view size, momentum for EMA, batch-size etc.",0.2714285714285714,0.2571428571428571,0.3,0.29411764705882354,0.4411764705882353,0.2773109243697479,0.27941176470588236,0.15126050420168066,0.05982905982905983,0.16806722689075632,0.08547008547008547,0.09401709401709402,0.27536231884057966,0.19047619047619044,0.0997624703087886,0.21390374331550802,0.14319809069212408,0.14042553191489363
288,SP:b806dd540708b39c10d3c165ea7d394a02376805,"This paper analyzed the curse-of-dimensionality problem of the vanilla SVGD with Euclidean distance kernel in a qualitative and quantitative way. Specifically, the author first built a connection of SVGD to MMD-descent, where they share identical repulsive forces with different driving forces (if Euclidean distance kernel is adopted). Then, the author argued that the variance collapse problem is rooted in (1) high variance and (2) the deterministic bias of the driving force, which were confirmed by sampling from the isotropic Gaussian. Quantitatively, the author analyzed the stationary variance of MMD-descent and SVGD with isotropic Gaussian under the proportional limit, which confirms the curse-of-dimensionality problem of SVGD. ","This work studies the variance collapse phenomenon of SVGD. By comparing to MMD-descend, the authors argue that the driving force of SVGD suffers from a bias caused by reusing data, and thus tends to underestimate the variance of the target distribution. Theory are developed in the setting of estimating standard Gaussian with a proposal limit (i.e., $d/n \to \gamma$), and explains the understanding in the overparameterized/high-dim setting (i.e., $\gamma > 1$). Experiments are also conducted to verify the understanding. Finally, motivated by the understanding, new algorithm is proposed to fix the issue of SVGD by damping the driving force term in SVGD.","This paper provides an understanding of the variance collapse phenomenon of SVGD. The paper first (1) introduces the reader to the most important concepts and phenomena, then (2) gives an explanation for why this problem occurs, thanks to a comparison with an accurate (yet computationally intensive) algorithm they call MMD-descent. Finally (3) the paper shows how to fix SVGD with damping. The paper provides experiments and theory, nicely combined.","In this paper, the authors analyze the underestimation issue of stein variational gradient descent (SVGD), and propose the maximum mean discrepancy (MMD) descent. From the perspective of the decomposition of the gradient term (driving force and repulsive force), this paper suggested to use another driving force term instead of the original one in SVGD. But the new driving force makes MMD-descent impractical since it depends on an intractable integral of the desired distribution $p$. In addition, the paper identify the log derivative driving force as the problematic term in SVGD, and propose a modified SVGD with particle resampling. They also argue that the proportional asymptotic limit is more relevant to understanding the variance collapse phenomenon. The theoretical dimensional analysis of SVGD on Gaussian also suggested another modified (damped) SVGD.",0.1891891891891892,0.14414414414414414,0.25225225225225223,0.18691588785046728,0.2523364485981308,0.2571428571428571,0.19626168224299065,0.22857142857142856,0.2153846153846154,0.2857142857142857,0.2076923076923077,0.13846153846153847,0.19266055045871563,0.17679558011049723,0.23236514522821577,0.22598870056497172,0.22784810126582278,0.18
289,SP:b89c04e2f8e94c7d0c3686edac835a86fab2d528,"This paper proposes a simple yet effective method, cocktail fine-tuning, for the natural language generation tasks. Their results show that cocktail fine-tuning can handle both In-domain data and Out-of-domain data effectively by combing adapter-finetuning and full-finetuning through knowledge distillation and overall has comparable performance compared to their ensembles. It also provides theoretical analysis on multi-class logistic regression to explain why it works.","The present paper first discusses the trade-off between performance for out-of-domain data and in-domain data with respect to whether the model is fully fine-tuned or lightweight fine-tuned on NLG tasks. Second, it argues that such a trade-off is not necessary if one can make use of both of these two fine-tuning schema in a clever way. To this end, it proposes cocktail fine-tuning, which augments full fine-tuning via distillation from a lightweight model and which achieves equal performance as an ensemble of the two fine-tuning schema. At length, this paper also explains the behavior of the cocktail fine-tuning through a toy model. ",This paper proposes an ensemble model between a full fine-tuning model and a parameter-efficient fine-tuning model to improve the out-of-distribution (OOD) performance of a full fine-tuning model. The proposed method is inspired by the observation that full fine-tuning model achieves good in-distribution (ID) performance while parameter-efficient finetuning model achieves better OOD performance. There are two ensembling methods presented in the paper: linear interpolation between the predictions of the two models; and distill from the predictions of a parameter-efficient model with ID training data. Improved OOD performance is observed with this ensemble method.,"This paper presents interesting an idea of combining lightweight fine-tuning and full fine-tuning to achieve the best of both approaches, i.e. perform best on out-of-domain and in-domain data. The authors proposed two approaches: a simple ensemble method and a so-called cocktail fine-tuning that combines two fine-tuning methods in one single model. They evaluated their tasks in three datasets: WebNLG, XSUM and OpenQA and obtained mixed results. The authors also provided good analyses for more insights. ",0.18571428571428572,0.21428571428571427,0.21428571428571427,0.20175438596491227,0.18421052631578946,0.22549019607843138,0.11403508771929824,0.14705882352941177,0.17857142857142858,0.22549019607843138,0.25,0.27380952380952384,0.14130434782608695,0.1744186046511628,0.19480519480519481,0.21296296296296297,0.2121212121212121,0.24731182795698928
290,SP:b89ec0b50475bfb23399719ca36aa137b389fbf6,"The paper proposed an interesting strategy to reduce the training time for large scale language models consisting of stacking layers with identical structures. Users first trained the models with shared parameters across the layers, then relax the tie constraints so that parameters at different layers are updated differently.  The paper showed some empirical evidence that the proposed strategy converged faster given a limited training time budget and demonstrated the feasibility of training a 10T model.","This work studies the problem of efficient pretraining of large-scale models for language and vision representations, namely the issue of significant memory requirements for models with billions to trillions of parameters. Authors propose two modifications: first, to reduce the memory load and improve convergence at the initial stage of training, they suggest to train a multilayer model with shared parameters and then unshare them. Second, to maximize GPU utilization with offloading, authors develop a method for granular CPU offloading, which keeps larger chunks of the model in GPU memory. When combined, the proposed methods allow the authors to train a 10 trillion parameter model on 512 GPUs.","The authors propose to train very large neural language models via a ""Sharing-Delinking"" paradigm. The proposed method first trains a model with weights shared across layers. In this way, the model appears to be smaller and it can fit into fewer GPUs. At some point, the authors delink the weights, and continue training the model in the conventional way.  The authors also propose a granular CPU offloading mechanism to save CPU memory.","The paper proposes a technique called Pseudo-to-Real (P2R) for reducing the computational and time requirements of training massive (or Giant) models with trillions of parameters. The key idea of P2R is a two phase training approach for Giant models. The first phase involves training a smaller version of the model (a.k.a., Pseudo-Giant) which is obtained by making all layers share parameters. The second phase involves training the Giant model after initializing with Pseudo-Giant weights. The paper further proposes Granular CPU offloading which is to offload some but not all model parameters to CPU memory to reduce GPU memory consumption. Finally, the paper provides some evaluation results to demonstrate P2G.",0.24,0.22666666666666666,0.21333333333333335,0.16666666666666666,0.24074074074074073,0.273972602739726,0.16666666666666666,0.2328767123287671,0.1391304347826087,0.2465753424657534,0.22608695652173913,0.17391304347826086,0.19672131147540983,0.22972972972972974,0.16842105263157894,0.1988950276243094,0.23318385650224213,0.21276595744680848
291,SP:ba80e35d452d894181d51624183b60541c0f3704,"This paper proposes a novel solution to infer the graph structure. It starts with the unrolling algorithm for graph structure inference problem, and then adopt the proximal gradient iterative solutions. The expressiveness is augmented by parameterized with deep neural network, called Graph Deconvolution Network (GDN). ","The authors tries to recover the underlying graph structures from observed symmetric adjacency matrix. The key assumption of the paper is that the observed adjacency matrix can be represented as a polynomial of the adjacency matrix of the true underlying graph, which is reasonable. The experiments show that the proposed model can recover graph structures on provided dataset.","The authors propose the graph deconvolutional network (GDN), which is a novel approach to graph structure recovery from noisy observed graph structures. It is based on an assumption that the observed adjacency matrix is polynomial in the true graph adjacency, and uses a proximal gradient computation to iteratively optimise for this structure. Experiments on both synthetic and brain imaging graph recovery tasks indicate the outperformance of the proposed method against several baselines.","This paper proposes an approach to estimate a latent graph $A_L$ given an observed graph $A_O$ (e.g., the covariance matrix of signals generated through a graph diffusion process). The authors posit a polynomial model and unroll proximal gradient iterations to estimate a variant of the model. The authors conduct experiments on synthetic datasets with supervised $(A_O, A_L)$ pairs as well as a neuroimaging dataset, where $A_O$ denotes the functional connectivity graph and $A_L$ denotes the structural connectivity graph. The proposed approach performs better than several baseline approaches.",0.17777777777777778,0.26666666666666666,0.28888888888888886,0.3275862068965517,0.25862068965517243,0.2777777777777778,0.13793103448275862,0.16666666666666666,0.13829787234042554,0.2638888888888889,0.1595744680851064,0.2127659574468085,0.15533980582524273,0.2051282051282051,0.1870503597122302,0.29230769230769227,0.19736842105263158,0.24096385542168677
292,SP:bacff3685476855a32549d03095375649fd89df2,"This paper proposes a data-driven method, METAOD to unsupervised outlier model selection problem based on meta-learning. METAOD makes use of many detection models on historical outlier detection benchmark datasets and selects an effective model to be employed on a new dataset without. Authors introduce meta-features that quantify the outlying characteristics of a dataset. Experiments have shown the promise of METAOD. The authors have implemented an open-source tool METAOD and our meta-learning database for practical use. ","The paper proposes a method, named MetaOD, which for each input data set selects the most suited outlier detection model together with its relevant hyper-parameter values from a set of given models. The selection is done in a supervised manner. MetaOD works as follows. During the meta-training stage, it considers a set of $m$ models $M = \{M_1, ..., M_m\}$ and a set of $n$ data sets $D = \{D_1, ..., D_n\}$. Each model $M_i$ includes model architecture and model configuration (hyper-parameter values). MetaOD computes performance matrix $P$ where cell $P_{ij}$ is the performance of method $M_j$ on data set $D_i$. Then it learns a data matrix $U \in R^{n \times k}$ and a model matrix $V \in R^{m \times k}$ where $U_i \times V_j^T$ approximates $P_{ij}$. The functions learnt from $U$ and the matrix $V$ are then used during test time to choose the best model (together with its configuration) for each input data set.","This paper presents a data-driven approach to model selection for unsupervised outlier detection. The proposed method, METAOD takes a large number of training datasets with outlier labels and a pool of outlier detection models (methods plus their parameters) and uses a matrix factorization method to learn a model performance predictor, which, for given a test dataset, can predict/select a high performing model from the model pool without requiring test time model evaluation.  The performance of METAOD is evaluated with two implementations with the same pool of 300+ models (8 distinct outlier detection methods with different parameter configurations) but two different groups of training datasets. ","The paper proposes a model selection approach called METAOD for outlier detection. METAOD maintains historical performances of various outlier detection models on different datasets, where the performance of a detection model on a new dataset is estimated based on the historical information. The setting is thus similar to meta-learning. METAOD is motivated by Collaborative Filtering, where the compatibility of a user (i.e., a dataset) and an item (i.e., a model) is estimated. A set of meta-features are crafted for charactering a dataset. Experiments show that METAOD benefits outlier detection model selection, compared with no model-selection baselines.",0.2375,0.2625,0.3,0.14201183431952663,0.15384615384615385,0.2358490566037736,0.11242603550295859,0.19811320754716982,0.2376237623762376,0.22641509433962265,0.25742574257425743,0.24752475247524752,0.15261044176706826,0.22580645161290322,0.2651933701657458,0.17454545454545456,0.1925925925925926,0.24154589371980675
293,SP:bb2a13a4d366140fc0c3e941c354cc674f6a904f,"The paper shows a causal perspective to the adversarial robustness problem. It creates a graph over content and style variable sets. It identifies the spurious correlation between style and label as the main reason for adversarial examples, and then proposes a method to remove it from the trained model. Experiments on three datasets show that the proposed method is better than two baselines. ","This paper presents a causal perspective on addressing adversarially vulnerability. It first constructs a causal graph, which then inspires the design of the distribution alignment method for reducing the gap between adversarial and natural data. Extensive experiments on CIFAR10, CIFAR100, and MNIST demonstrate the robustness of the proposed method against various attack methods.","This paper proposes a causal graph to model the generation process of adversarial attacks. Based on the proposed causal graph, the authors identify the origin of adversarial vulnerability as the spurious correlation between style variable and class label. Under the adversarial distribution, such spurious correlation can be maliciously used to mislead a victim model. In this light, the authors propose a method to align the adversarial distribution and the natural distribution to prevent a model from learning spurious correlation. The proposed method is empirically validated on prevailing datasets under several attacks.","The work presents a causal perspective of adversarial attacks on image-based machine learning models by studying a causal graph of the adversarial data creation process and highlighting how such a process makes the learned models vulnerable. It argues that the main reason for adversarial vulnerability is the reliance of models on spurious correlations between labels and style. Accordingly, it proposes a method to learn models for which the conditional distribution of label given style and image does not vary much when attacked. Empirically, the method is shown to be more robust than two baselines on three datasets.",0.2698412698412698,0.3968253968253968,0.4126984126984127,0.3584905660377358,0.33962264150943394,0.2967032967032967,0.32075471698113206,0.27472527472527475,0.2653061224489796,0.2087912087912088,0.1836734693877551,0.2755102040816326,0.29310344827586204,0.3246753246753247,0.3229813664596274,0.2638888888888889,0.23841059602649006,0.2857142857142857
294,SP:bb74fef9222f227343909f3936f1a8cd2322bbeb,The authors set out to investigate if active learning is an emergent property of pre-training. That is if running active learning with pre-trained models gives better result than using the same models without pre-training. They run several experiments on different text and image datasets first showing that active learning performs better than random sampling on pre-trained models and secondly that pre-trained models perform better than un-pretrained ones for active learning.,"The authors describe interesting empirical observations regarding using uncertainty sampling to select examples to fine-tune models that use pretrained embeddings and provide some hypotheses regarding the reasons for these performance improvements. Specifically, (1) from a methodological perspective, they propose using uncertainty sampling (i.e., least confident selection) to select examples for fine-tuning image/NLP pretrained models and (2) from an empirical perspective, they use Waterbirds/Treeperson/iWildCam2020-WILDS for image classification and Amazon-WILDS for review star prediction based on text and compare with random sampling — noting that these are settings where there is known covariate shift between train/test with semantic meaning to induce interpretable spurious associations (e.g., background in images). The proposed method works overall, especially on the image datasets, and they also dig into the types of examples selected — noting that they align with expected ‘difficult’ examples (depending on the setting).","This paper investigates if using large, pretrained models in an active learning setup helps achieve better performance with lesser data when compared to using randomly sampled data. In order to conduct this investigation the authors study the empirical performance of large pre-trained models on some image datasets and a text dataset. In both cases large pre-trained model is finetuned on a small amount of seed data and then an active learning procedure is used (in this paper the AL procedure is an uncertainty sampling procedure) to collect more data. The datasets are chosen to illustrate several conceptual issues (i) distinguishing causal from spurious correlations (ii) measuring robustness to distribution shifts (iii) role of data imbalance.   Experiments are performed to show that using an active learning procedure indeed helps improve performance using only a small amount of actively labeled training dataset.  The paper is well written and the results are convincing and insightful. ","This paper investigates the active learning performance of pre-trained models vs their non-pre-trained counterparts on both vision and NLP tasks. Specifically, the investigation focuses on datasets with spurious correlation, domain shift, and label imbalance. Empirical results generally show that the pre-trained models with the uncertainty acquisition function performs much better than the random baseline and their un-pre-trained counterparts. ",0.19736842105263158,0.2236842105263158,0.25,0.12244897959183673,0.08163265306122448,0.14935064935064934,0.10204081632653061,0.11038961038961038,0.296875,0.11688311688311688,0.1875,0.359375,0.13452914798206278,0.14782608695652172,0.2714285714285714,0.11960132890365448,0.1137440758293839,0.2110091743119266
295,SP:bc8a9fcf7de41f1a1b0c6d0fc3fdcac5c5f87613,"Information Directed Reward Learning (IDRL) actively selects queries that are most informative at distinguishing policies in terms of their value. By contrast, prior work has typically tried to maximize information gained about the reward function itself -- but some parts of the state space may be unreachable, or never visited by any plausible optimal policy, and so learning about them is irrelevant for task performance. IDRL has a pleasing theoretical motivation, although practical implementations of it require considerable use of approximations and heuristics. Additionally, IDRL is compatible with a broad variety of reward learning queries (e.g. preference comparisons, labeling trajectories with returns) whereas most prior work focuses on a single feedback modality. Experimental results show significant improvements in sample efficiency from use of IDRL relative to a variety of baselines.","The authors propose a Bayesian method for learning the reward function in an RL setting.  They study the setting in which the algorithm can query experts.  The form these queries can take is quite general.  They analyse their algorithm theoretically and empirically.  Their key insight is that queries should be focused on enabling the reward model to be used to find the best policy possible (as efficiently as possible), rather than decreasing the reward model’s uncertainty in general. ","This paper proposes a new active reward learning method called IDRL, designed to select the most informative query for identifying an optimal policy among a set of plausibly optimal policies. First, IDRL selects two candidate policies that maximize the entropy of the difference in expected returns given past queries. Then IDRL chooses queries that reduce the uncertainty of the return difference most among a set of candidate queries. Updating the reward model using the selected queries rather than queries maximizing the information gain about the reward stimulates identifying the optimal policy. Experimental results show that IDRL requires fewer queries than the considered baselines and works with different query types.","The authors introduce a query selection strategy for reward learning in reinforcement learning. This strategy can use different query types and is not restricted to pairwise trajectory queries, as many other approaches. Additionally, the strategy is also considering the environment dynamics. The papers main contribution is a method, that reduces the number of required queries, besides the aforementioned aspects. The authors also show, that the approach is also applicable to Deep RL techniques.",0.08461538461538462,0.16153846153846155,0.1,0.21518987341772153,0.20253164556962025,0.14678899082568808,0.13924050632911392,0.1926605504587156,0.1780821917808219,0.1559633027522936,0.2191780821917808,0.2191780821917808,0.10526315789473684,0.17573221757322174,0.12807881773399013,0.18085106382978727,0.2105263157894737,0.17582417582417584
296,SP:bcb4e7e5c137edf04a9ea2fde014b0984c6ef89b,"The paper tackles a problem of knowledge-grounded open-ended generation and proposes a new model that is an extension of an end-to-end training of the retrieval and the generator. In particular, there is another model called “posterior-guide” that is jointly trained with the other two models using ELBo. Intuitively, this posterior-guide is similar to the retriever that scores the evidence, but conditioned on not only the question but also the response. It is used as a weight of each evidence in the generation in order to encourage the generator to ground more to the evidence that is more relevant to the response. The retriever is also trained to be close to the posterior-guide (minimizing KL-divergence). To my understanding, this is a very clever way to give supervision to the model when it is not easy to obtain distant supervision data (unlike in short answer generation where the gold evidence is easily obtained by whether the short answer is included in the evidence or not).  Experiments are done on Wizard of Wikipedia and MS Marco NLGen. The model achieves significant improvements over the baseline retriever, based on three evaluation metrics: relevance, groundedness and generation quality. ","In this paper authors describe an approach to use the responses/ answers to guide retrieval during training of document grounded response generator. By having the retriever being trained using the posterior (p(document|response,context)), the model could learn a better (supervised) retrieval network (based on ColBERT) which could be used to guide the training of the prior ( p(document|context)).  Documents are retrieved using the dialog context and the top-k documents are used for training the posterior as well as the prior. Since the expectation for ELBOLoss cannot be computed exactly (due to the large document set),  it is computed by sampling documents from the top-r retrieved documents -- from either the posterior or the prior, guided by a parameter called alpha which governs the sampling proportion. The idea of doing variational training using a posterior network isn't particularly novel but the authors have made it work for open-ended response generation using this approximation for computing the ELBOLoss. The networks use BART for language generation and CoLBERT for modeling the retrievers. Experiments have been presented using the Wizard of Wikipedia dataset and the MSMARCO NLGEN. Experiments show an improvement over the baseline model (RAG - referred to as MarginalizedLoss) in both retrieval (success@k, MRR) as well as response generation (text-F1 overlap between response and grounded document and textF1 overlap between response and ground-truth output response). Overall this is a well written easy to read paper  ","The paper tackles the problem of open-ended knowledge-grounded natural language generation, in the context of free-form QA or knowledge-grounded dialogue, where models must ground their generations on passages relevant to the input context. Specifically, the authors explore improving the retrieval component of retrieval-augmented systems by utilizing posterior signal from the label. A “guide retriever” learns the relevant passage to retrieve by including the target output in its input context, and then, using an ELBo loss, provides this signal to the normal, in-system retriever. The authors find that their model improves three-fold over baselines: 1) it retrieves relevant passages more frequently; 2) its generations are more grounded in retrieved passages; and 3) its generations are closer to human generations. ","This work focuses on the knowledge grounded text generation tasks. They argue that multiple passages can be valid and relevant to the context, but not all of them are observed/used in the target response. Therefore they propose that, during training, the target response should be utilized to train a ""guide-retriever"",  which predicts P(passage|context, response) and provides passage weight for the generator. A retriever is jointly trained with the generator and the guide-retriever, and the KL divergence between the retriever and the guide-retriever is included in the loss function.  The contribution of this paper is that they proposed a potential solution to the challenge that the model may be not effectively trained in case that multiple passages are valid but only one of few are used.",0.20398009950248755,0.14427860696517414,0.17412935323383086,0.1078838174273859,0.12033195020746888,0.216,0.17012448132780084,0.232,0.26717557251908397,0.208,0.22137404580152673,0.20610687022900764,0.1855203619909502,0.17791411042944788,0.21084337349397592,0.14207650273224043,0.15591397849462366,0.2109375
297,SP:bd9cb543b5f199ab45e1bf8609c683f12ceb7659,This work is concerned with a very practical scenario of Federated Learning where the participating agents may not have access to labelled data. SimSiam architecture to learn useful feature representations with extensions that incorporate personalisation for local client models. The learnt representation are evaluated against a KNN classifier for analysing their usefulness. ,"The submission is not anonymized, the author's name chaoyanghe appears frequently in the attached SSFL_Sumpplementary/SSFL-Source-Code. Therefore, it should be directly desk rejected.  --- Logging error --- Traceback (most recent call last):   File ""/Users/chaoyanghe/opt/anaconda3/envs/fedml/lib/python3.7/logging/__init__.py"", line 1025, in emit     msg = self.format(record)   File ""/Users/chaoyanghe/opt/anaconda3/envs/fedml/lib/python3.7/logging/__init__.py"", line 869, in format     return fmt.format(record)   File ""/Users/chaoyanghe/opt/anaconda3/envs/fedml/lib/python3.7/logging/__init__.py"", line 608, in format     record.message = record.getMessage()   File ""/Users/chaoyanghe/opt/anaconda3/envs/fedml/lib/python3.7/logging/__init__.py"", line 369, in getMessage     msg = msg % self.args","This paper introduces the self-supervised learning (SSL) framework for FL. Different SSL methods are investigated to study their feasibility under the FL setting. With the popular SimSiam framework, personalized federated SSL is proposed. The performance comparison indicates that representation regularization-based personalization method is able to outperform other variants.","The authors merge self-supervised learning in personalized federated learning to solve the limited label and data heterogeneity problems in the local clients. They test several current algorithms under their framework. Then they propose an algorithm named Per- SSFL considering the balance between the consensus and personalization. Finally, they provide experiment results to support their claims and comprehensive analysis of what they find. The main contribution is to design a self-supervised FL framework with supportive experiment results. They also provide suggestions to choose appropriate algorithms and hyper-parameters under different settings.",0.057692307692307696,0.11538461538461539,0.1346153846153846,0.025423728813559324,0.05084745762711865,0.24,0.025423728813559324,0.12,0.07608695652173914,0.06,0.06521739130434782,0.13043478260869565,0.03529411764705883,0.11764705882352941,0.09722222222222222,0.03571428571428571,0.05714285714285715,0.16901408450704222
298,SP:bec15075409c71f98f3698bc35e34eeb4862d94f,"In this paper, the authors consider using learning-based method for Influence Estimation and Influence Maximization. The author proposed a GNN-based to estimate influence (as an upper bound). Based on the estimated influence, the authors use CELF optimization to find the optimal seed set. To further improve the efficiency, the authors proposed (1) a RL DQN based method and (2) a simplified influence function with only one layer. The author carries out experiments on several synthetic and real-world datasets. ","The paper proposes a neural network approach (GLIE) for estimating the influence of a given seed in a given graph. More importantly, the authors propose three different methods to use the proposed Influence Estimation method for Influence Maximization. The authors show the superior performance of their proposed method in comparison with baselines and ","Authors propose several neural network model-based approaches to influence maximization (IM). First, GLIE estimates influence using a GNN, which can be plugged into optimization algorithms like CELF. Second, GRIM avoids the cost of having to estimate the influence of every candidate by approximating the marginal gain with a two-layer MLP. Finally, PUN avoids the cost of having to estimate the influence for every seed node by approximating the influence with features from GNN hidden states. Experiments demonstrate that the proposed method can generalize to graphs significantly different from training data. Also, PUN provides solution quality close to the strongest baseline with a fraction of compute time.","This paper considers using learning methods to solve the well-known influence maximization problem. The paper proposes to estimate the upper bound of the influence by using graph neural networks, which can be used in subsequent steps for selecting the seed nodes through either Q-learning or a greedy algorithm based on the learned representation. Experiments on various datasets have been provided to evaluate the accuracy of influence estimation as well as the effect of influence maximization.  ",0.1728395061728395,0.2345679012345679,0.25925925925925924,0.32075471698113206,0.2830188679245283,0.1574074074074074,0.2641509433962264,0.17592592592592593,0.2727272727272727,0.1574074074074074,0.19480519480519481,0.22077922077922077,0.208955223880597,0.20105820105820105,0.26582278481012656,0.2111801242236025,0.23076923076923078,0.18378378378378377
299,SP:bf1c45ef27953acab2195d54c8197d360c1e8190,"The paper develops a surrogate function with closed-form solution for objective functions with ""soft constraints,"" here defined as those with the term $\alpha^T \\max\\{0,z(x)\\}$ for affine $z$, component-wise $\\max$, and $\alpha > 0$. The goal is to apply such functions within the context of smart ""predict-then-optimize"" (SPO) by also relaxing the hard constraints with a sufficiently large penalty $\alpha$. To develop their surrogate, the authors evaluate bounds on the utility gained when violating the soft constraints using primarly a geometric perspective, specifically evaluating the angles or corners of infeasible points with respect to the convex hull of feasible solutions (and related properties). Numerical results compare their approach with the standard SPO+ loss function and a DF metric for portfolio optimization.","This paper presents an approach for ""predict, then optimize"" problems that is amenable to derivative-based learning methods. It involves converting hard constraints into soft constraints (i.e. implicitly enforced through objective penalization). The main theoretical contributions are bounds on the magnitude of objective change that can be attributed to infeasibility with respect to the original hard constraints.",The paper proposes a new method to integrate soft constraints in the predict+programming paradigm that focuses on mathematical programming models with a prediction component that needs to be considered during optimization/solving. The new method works by reformulating the problem into piecewise linear constraints and then optimizing a surrogate for the original problem. A theoretical analysis and motivation is provided as well as an experimental evaluation that underlines the effectiveness of the method.,"This paper provides a method for addressing the ""prediction + programming"" problem in the case where the programming problem contains a $\max(z,0)$ term in the objective. To do so, the authors rewrite the optimization problem as an unconstrained optimization problem by converting all hard constraints of the optimization problem to soft constraints in the objective. They then pick the coefficients of those soft constraints in a way that is meant to prevent constraint violations, and then relax these soft constraints via a surrogate function. They then embed the resultant approximate optimization problem within a method where some $\theta$ is output by a neural network, the approximate optimization problem is solved for that value of $\theta$, and gradients are updated by differentiating end-to-end through the approximate optimization problem. The authors demonstrate their method on three synthetic settings, and report improved performance over two-stage methods and SPO+.",0.12598425196850394,0.14960629921259844,0.1968503937007874,0.1896551724137931,0.27586206896551724,0.33783783783783783,0.27586206896551724,0.25675675675675674,0.16778523489932887,0.14864864864864866,0.10738255033557047,0.16778523489932887,0.17297297297297298,0.18905472636815923,0.1811594202898551,0.16666666666666669,0.15458937198067632,0.22421524663677134
300,SP:bf7d2e765c435a943ec9257cfa43d070a64c2b67,"This paper proposes a metric for evaluating the learning stability of data and point out that unstably-learned instances are of low-quality for adversarial training. Through extensive controlled experiments, this paper investigates the impact of low-quality data on three issues in adversarial training, i.e., robust overfitting, robustness overestimation, and robustness-accuracy trade-off. The experimental results show that removing the low-quality instances can mitigate the issues.","**Few sentences summary**: the paper proposes an empirical study on how data quality impacts robust performance in the Lp-norm setting under three angles: robust overfitting (i.e. the difference in accuracy between the best and last checkpoints), robustness evaluation (i.e. consistency of the robust performance when trying various robust evaluations like AutoAttack) and clean/robust accuracy trade-off (i.e. the gap in performance between the clean and robust accuracies). They show that ""high quality"" data (selected with their proposed criterion) performs better than ""low quality"" data for these three performance metrics.  Regarding the **results** and **contributions**: * The authors propose a quantitative definition of ""quality""  based on the average training robust accuracy over epochs per sample. * They show that ""low quality"" samples are better for standard training whereas ""high quality"" samples are better for adversarial training. * Studies on 1) robust overfitting, 2) robustness evaluation and 3) clean/robust accuracy trade-off on the datasets CIFAR-10/100 and TinyImageNet for Adversarial Training and TRADES.","The paper investigates the impact of data quality, measured by the proportion of epochs in which the model classifies a specific input correctly during training, on the robustness, generalization, and robustness-accuracy tradeoff of adversarially trained models.  Unlike with standard training, the authors find that more difficult inputs (lower quality inputs) can hurt adversarially trained models.  They find that compared to randomly removing data during training, removing low quality data can lead to higher robustness, less robust overfitting, less robustness overestimation, and less robustness-accuracy tradeoff.","This paper studies the effect of data quality on adversarial robustness. Specifically, they focus on one measure of data quality (number of times there is a perturbation that is misclassified across training iterations). They study the effect of data quality on robust overfitting, robustness-accuracy tradeoffs and ""robustness overestimation"" (gap between strong and weak attacks). The main conclusions reported are that data quality as measured by their metric plays an important role in all three aspects, and a suggested takeaway is that we need data of higher quality to improve robustness. ",0.32857142857142857,0.2714285714285714,0.2857142857142857,0.13855421686746988,0.13253012048192772,0.23255813953488372,0.13855421686746988,0.22093023255813954,0.21978021978021978,0.26744186046511625,0.24175824175824176,0.21978021978021978,0.19491525423728814,0.24358974358974358,0.2484472049689441,0.18253968253968253,0.17120622568093385,0.22598870056497178
301,SP:c141dc29b487ebfaa20ee50786886b0383d938bc,Paper provided a UMPU test on Mallows model's spread parameter. And based on that paper introduced many variations and discussed their properties. Paper contributes to the testing literature by providing a UMPU test and associated math tools.,"The paper studies the problem of identity testing for the Mallows model, which is a popular parametrized distribution over rankings. It gives identity testers for the non-asymptotic and asymptotic setting which are sample optimal but not necessarily computationally efficient. It also gives a efficient identity tester, with weaker guarantees, and also extends the results to the setting where the central ranking is not unknown.  ","This paper is concerned with the hypothesis testing of Mallows ranking models, which is parametrized by a central ranking and a spread parameter. The authors proposed two different methods: one based on the classical Type I error, and the other based on an optimal learning approach. The authors presented a complete theory regarding testing Mallows rankings, and also provided empirical results to corroborate the theory. ","The authors propose asymptotic and non-asymptotic test for the hypothesis $\phi = \phi_0$ where $\phi$ is the spread parameter of a Mallows model. To do that, they first prove that we can derive a non-asymptotic test from an asymptotic one (Prop 4.1), then show that an optimal unbiased UMP test can be defined is too computationally costly when the number of items to order ($m$) is large (Theorem 5.1), due to the calculation of the probability in Eq (2).  To alleviate this limitation, they propose to estimate the most likely spread parameter $\hat{\phi}$ which allows them to efficiently approximate the quantity in Eq (2) and control the deviation (Theorem 5.4). Finally, the authors address testing for spread with unknown central ranking, by showing that the average rank aggregation is asymptotically close to the central ranking. Finally, they confirm the efficiency of their method in experiments.",0.18421052631578946,0.2631578947368421,0.23684210526315788,0.24615384615384617,0.27692307692307694,0.23076923076923078,0.1076923076923077,0.15384615384615385,0.059602649006622516,0.24615384615384617,0.11920529801324503,0.09933774834437085,0.1359223300970874,0.1941747572815534,0.09523809523809523,0.24615384615384617,0.16666666666666669,0.13888888888888887
302,SP:c1617e79182c6d06c611ced9d892d7b2da5fd9eb,"This paper studies one-shot 2D object detections. The major conclusion is that when keeping the number of training images fixed, increasing the number of training categories can significantly increase the one-shot performance.  This paper demonstrated this conclusion empirically by doing controlled experiments on COCO, Objects365, and LIVS.   Inspired by this observation, this paper improves the state-of-the-art one-shot detection performance on COCO from 22.0 to 27.5 AP50 by training on LIVS.  Other two related conclusions in this paper: - PASCAL VOC is not suitable for evaluating one-shot object detection algorithms. The reason may be that the number of instances per image is low(2.9 ins/img on average). The algorithm only needs to recognize foreground objects rather than objects of target categories.  - Only when the training data is challenging enough, increasing the model size and training time can help improve one-shot performance.   ","This paper shows that the key to reduce the generalization gap between base classes and novel classes is to increase the number of training categories, instead of training samples. The authors did many experiments on four existing datasets (PASCAL, COCO, Objects365, and LVIS) with Siamese Faster R-CNN to verify this point. Experiments show that with more categories in the training set, the generalization gap will be nearly closed. Finally, the author proposes that future data sets should focus on the diversity of categories.","This paper studies the effect of category number in the one-shot object detection task. In the testing of one-shot detection, there exists a performance gap between the base (training) classes and held-out classes. It is claimed that this performance gap can be largely closed by increasing the number of categories used for training. And the number of categories is more crucial than the number of samples per category. Experiments are conducted on VOC, COCO, Object365, and LVIS using a Siamese-style detector to verify the claims.","- The paper considers the problem of one-shot object detection, meaning, the model is asked to detect unseen categories, based on only one provided a template.  - The main discovery made by the authors is that, to generalise better, the model should be trained with data from as many categories as possible, given the same budget on number of samples for training.  - Architecture-wise, a Siamese Faster R-CNN is adapted. ",0.17218543046357615,0.17880794701986755,0.11920529801324503,0.32142857142857145,0.16666666666666666,0.23595505617977527,0.30952380952380953,0.30337078651685395,0.2571428571428571,0.30337078651685395,0.2,0.3,0.22127659574468087,0.225,0.16289592760180993,0.31213872832369943,0.1818181818181818,0.26415094339622647
303,SP:c1b7b550b9f90bd5e9bf5218e22d1977ed1686a5,"This paper proposes a new first-order algorithm using without-replacement strategy to solve a class of composite convex minimization problems. The main idea is to modify the well-known scheme, called Finito/MISO method by applying a without-replacement strategy and a damping step. Under the convexity and L-smoothness of f, the proposed algorithm achieves O(1/k) rate in epoch on the optimality residual. When f is additionally strongly convex, the rate is improved to linear as in standard proximal gradient methods. The analysis relies on a weight norm defined through the order of the underlying shuffling strategy. The authors also compare their method with other schemes such as a coordinate descent method with cyclic rule, and standard GD. Next, the authors also investigate the optimal cyclic rule for sampling and propose an adaptive variant. Numerical examples on standard logistic regression problem are presented to illustrate the performance of the proposed methods.  ","This paper develops a proximal damped version of the Finito algorithm. The algorithm is proved to achieve the same convergence rate as proximal GD, for cyclic sampling, random reshuffling and shuffling-once versions of the algorithm. Further, the authors claim that this is the first* shuffling based variance reduction algorithm to achieve the convergence rate. The paper also gives a new norm that captures the optimality of sampling orders and provides a heuristic based on that for importance based reshuffling.  Besides the theoretical results, the empirical results seem to suggest that the proposed algorithm is indeed faster than other variance reduction algorithms.   *: The authors cite a concurrent work (Malinovsky et al.) that also achieves the same convergence rates for general convex functions, but the algorithms in the two papers are different.","The paper proposes a new method called Prox-DFinito based on the proximal Finito with without-replacement sampling. The authors derive complexity bounds for the proposed method in convex (for making the squared norm of the gradient small) and strongly convex cases (for making the squared distance to the solution small) that match under some additional assumptions the rate of Gradient Descent. Moreover, under additional assumptions on the objective function, the authors show sample-size independent bound in the convex case. The proofs are non-standard but clean and easy to follow. However, the paper has several strong weaknesses.","In this paper, the authors study a stochastic variance reduced method under sampling rules that are without replacement for smooth and non-smooth, convex and strongly convex objectives. More precisely : - they develop a proximal method call Prox-DFinito for which they study convergence rates for random reshuffling, cyclic and shuffling once samplings - in the cyclic sampling, they derive an optimal fixed ordering (and a practical adaptive variant which do not require the knowledge of $z^*$ - Finally, the authors highlight the fact that, when assuming data heterogeneity, their study lead to a convergence rate independent of the number of data samples. This was up until now just known for iid sampling.       ",0.2,0.18064516129032257,0.14838709677419354,0.17557251908396945,0.16030534351145037,0.1919191919191919,0.2366412213740458,0.2828282828282828,0.20909090909090908,0.23232323232323232,0.19090909090909092,0.17272727272727273,0.2167832167832168,0.2204724409448819,0.17358490566037735,0.19999999999999996,0.17427385892116182,0.1818181818181818
304,SP:c347796244fcf9b5de19c68bcc5c811b7448217d,"The paper studies  the gradient tracking (GT) algorithm for stochastic distributed optimization problems over undirected, static graphs; nonconvex, strongly convex and weakly convex local objective functions are considered. The claimed contribution is a new line of analysis unlocking tighter (asymptotic) convergence rates than those already developed in the literature. The impact of some network parameters on the convergence rate is also discussed, and validated via some (albeit limited)  numerical results. ","This paper analyzes Gradient Tracking, a very common decentralized optimization algorithm, in the stochastic setting. The authors first introduce the standard gradient tracking algorithms, along with the assumptions they use to analyze it. Then, they provide convergence results in 3 main settings: non-convex, strongly-convex and weakly-convex. In each case, the leading order term matches the rates of centralized mini-batch SGD, meaning that the methods are fast (at least when this term dominates). It is argued that many interesting extensions naturally fit in this framework and could be adapted without too much efforts.  Then, a proof sketch is given, and highlights the importance of studying the consensus difference X_t - \bar{X_t}, where \bar{X_t} is the average over all nodes, instead of studying just X_t. The other key ingredient is that a simple recursion, can be written with the consensus difference on both GT variables. Although the operator J involved in this recursion is not contractive at each step (spectral radius greater than 1), J^i with i large enough is contractive (spectral radius smaller than 1/2), which allows to prove convergence of the recursion.  Then, the rates obtained for Gradient Tracking are compared with rates previously obtained in the literature, and they compare favorably overall. Finally, some toy experiments are given to analyze the impact of the graph-dependent constants c and p that were introduced and that appear in the higher order terms. These experiments seem to confirm the theoretical rates. ","This paper studies the gradient tracking method for decentralized optimization. While it is widely used, its convergence rate is not optimal compared with some other methods. This paper provides a tighter analysis for nonstrongly convex problems, strongly convex problems, and nonconvex problems. Faster convergence rates are proved in this paper.","This work studies the problem of decentralized nonconvex optimization problems and it is relevant to the conference. The paper provides convergence analysis for an existing and well-studied algorithm -- stochastic gradient tracking algorithms, without providing any improvements. The paper prove its algorithm the same convergence compared to existing works. There are some light and limited experiments associated with the paper, but only limited to quadratic case.",0.34285714285714286,0.2,0.17142857142857143,0.06374501992031872,0.06772908366533864,0.3,0.09561752988047809,0.28,0.18181818181818182,0.32,0.25757575757575757,0.22727272727272727,0.14953271028037382,0.23333333333333334,0.1764705882352941,0.10631229235880399,0.10725552050473186,0.25862068965517243
305,SP:c44d3749d8883fae7eb2a6378417fca28d25a4c9,"This paper introduces a new initialization scheme for the k-medians clustering problem in the general metric space setting. This is based on the construction of metric embeddings via 2-HST’s (Hierarchically well-separated trees). The authors also extend this to the differential privacy (DP) setting. They prove approximation guarantees in both the non-DP and DP settings, improving upon the literature. Finally, they empirically validate algorithms against a number of baselines with both real world and synthetic datasets for multiple metrics.","The paper proposes a new initialization scheme for the k-median problem on graph input (or general metric spaces) using metric embedding tree structure. The paper proposes an algorithm that finds initialization of good centers using HST that gets an approximation factor of O(log min{k,d}) if the data is in Euclidean space  where d is the number of dimensions. Then, the paper studies clustering with differential privacy guarantee and hows that the initialization method could be adapted to give a slightly stronger muliplicative and additive errors. The work complemented these theoretical findings with experiments and show that the proposed initialization imporves the performance of k-median++ initialization.","This paper considers the problem of finding good initial centers for the fundamental problem of $k$-median clustering using a randomized embedding of the original metric into a tree metric.  After setting the initial centers, a standard local search algorithm is applied to produce an improved solution.  This is explored in both the standard context of $k$-median clustering, as well as in the relevant context of differentially private clustering.  In the latter setting, the goal is to minimize the amount of additive error introduced by the algorithm subject to being $\epsilon$-differentially private.  An extension to $k$-means is given in the appendix.  In the standard setting of $k$-median clustering, the main theoretical result is an initialization algorithm which is an $O(\log(\min(\Delta,k)))$-approximation to the optimal k-median clustering.  This is an improvement over k-median++ (which gives $O(\log k)$) when $\Delta$ is small, e.g. for $\Delta = O(d)$ and $d$ is small.  Using this as a seed results for a local search method results in an $O(1)$-approximation overall.  At a high level, their algorithm first constructs an embedding of the original metric into a hierarchically well-separated tree (HST).  From there, the initialization can be seen as finding an $O(1)$-approximate solution on the HST efficiently.  The overall guarantee follows from standard results about HST's  In  the differentially private setting, the main result is a similar guarantee on the quality of the initial solution and also a bound on the quality of the final solution when using a known private local search algorithm.  The quality of the final solution has $O(1)$-multiplicative error and $O(\epsilon^{-1}k^2\Delta\log(n)\log\log(n))$ additive error.  This is an improvement over the additive error of $O(\epsilon^{-1}k^2\Delta\log^2(n)$ due to Gupta et al. 2010.  The number of local search iterations is also improved from $O(k\log n)$ to $O(k\log\log n)$.  The main idea for the initialization is similar to the standard setting, but here they use the structure of the HST to ensure the initial solution is private by injecting a different amount of noise at each level of the tree.  An empirical study is done on a class of synthetic graphs as well as the MNIST dataset.  For the synthetic graphs, the metric space is given by the weighted shortest path distance in each graph, while for the MNIST dataset the metric is given by either $\ell_1$ or $\ell_2$.  The authors compare both the initial costs and the final costs (after running a local search method) for several initialization methods in both the standard and differentially private settings.  The main observation is that the proposed initialization methods tend to have better initial cost and the proposed differentially private method often outperforms the other methods in both initial cost and final cost.    ",The paper suggests an algorithm for the metric k-median problem using ideas from Metric embedding theory. The suggested use of the algorithm is as an initialization routine for the local search based algorithm for k-median. The differentially private version of the algorithm is also given along with bounds on k-median approximation factor. Experiments are conducted over datasets such as MNIST and results compared against the k-means++ algorithm (a popular initialisation algorithm).,0.27710843373493976,0.3855421686746988,0.1927710843373494,0.33636363636363636,0.2,0.065439672801636,0.20909090909090908,0.065439672801636,0.21333333333333335,0.07566462167689161,0.29333333333333333,0.4266666666666667,0.23834196891191708,0.1118881118881119,0.20253164556962028,0.12353923205342236,0.23783783783783785,0.11347517730496455
306,SP:c4b03a1b477ac94438d63beb29ef86d77acf1b1e,"This paper proposes an approach to combine together multiple (pretrained) neural networks into a single model that can simulate each of the considered neural networks, both in terms of output and hidden states. The setup here relies on all the considered networks to operate on the same task, and in particular the same input data type (but perhaps with different sampled training sets). The combined meta-model then considers as inputs not only the input data, but also an encoding (which is learned during its training) of each model. This allows model-dependent latent features to be computed by the meta-model, which can then be decoded to hidden states of each network via specialized decoders that can be thought of as auxiliary computational pathways that operate in parallel to the one producing the main output of the meta-model. The model encoding are then used to visualize and explore the relations between different networks (or instantiations), and furthermore, to further tune the combined model towards an interpolation or extrapolation that goes beyond (and seems to outperform) each of the individual networks used to train the meta-model. ","The manuscript proposes to analyze the similarity of different neural networks trained to solve the same task by developing a meta-model capable of reproducing/emulating both the output and the hidden representation of each individual network.  This meta-model is parametrized by a set of parameters which is common to all the networks, and by other parameters which are network-specific (theta). In this way, each network will correspond to a different theta, and one can analyze the global features of the set of networks by unsupervised manifold learning in the  theta-space. ","This paper introduces a new approach to studying the population of trained neural networks. Specifically, the author's design and train a meta-model which when given a vector, the meta-model emulates the model associated with the given vector. The authors present a few applications of their framework, including clustering and semi-supervised learning. ","The authors propose an algorithm that takes several neural networks and outputs an embedding for such networks and a meta-model. This meta-model can take any embedding of a network, and emulate its hidden states and outputs. The authors suggest that these embeddings can measure models similarity, and show how to interpolate and extrapolate between and from the models in the training set.",0.15425531914893617,0.10638297872340426,0.12234042553191489,0.18085106382978725,0.18085106382978725,0.21818181818181817,0.30851063829787234,0.36363636363636365,0.359375,0.3090909090909091,0.265625,0.1875,0.20567375886524825,0.1646090534979424,0.18253968253968256,0.22818791946308725,0.21518987341772156,0.20168067226890757
307,SP:c4b4914d64e76427435bee0da345fe33b1db7d27,"This paper proposes a method to solve Wasserstein gradient flows based on the JKO scheme using variational formulations of functional objectives, such as the KL divergence or the generalized entropy (non-linear diffusion). Relying on known reformulations of the JKO scheme as optimization over convex functions, the paper departs from recent related methods in expressing certain objectives as f-divergences, and in turn using the dual formulation of these divergences to circumvent the need to do explicit density computation in these. The resulting method involves parametrizing two types of operators as neural networks (one of them as an input-convex neural network), and solving a mini-max objective. The paper presents experiments on simple PDEs (mostly in 1D or 2D) with known solutions. ","The paper proposes a method to compute Wasserstein Gradient Flows (WGFs) via neural networks and the JKO scheme. In contrast to prior works, to compute WGFs of functionals involving f-divergences, the authors use variational approximations rather than direct computations. It is claimed to work faster and perform better.","This paper studies the implementation of some Wasserstein Gradient Flows (WGF) in discrete time but without discretizing the space. The methods proposed are based on the JKO operator to discretize WGF in time. The implementation of the JKO can be challenging. The strategy of the authors is to first reparametrize the JKO as a minimization over a space of functions (instead of measures) via pushforward. Then, when the objective function is a f-divergence, the objective inside the JKO admit a variational representation and can be expressed as a sup. Conclusion: each JKO is written as a min max over a space of functions. To solve it, they parametrize the functions by neural networks and alternatively maximize and minimize the problem using Adam. An important feature is that the objective in the min max can be approximated with samples of the current distribution (its density doesn't appear, only integrals wrt to the current distribution). ","This paper proposes a variational formulation of each JKO step for optimizing functionals on measures. Different from existing recent works on emulating JKO steps by training pushforward neural networks (either directly or as gradients of convex functions), the variational formulation involves another inner maximization of a function, without needing density access that typically requires cubic time complexity due to computing the log determinants of the pushforwards. Experiments are done to demonstrate the practicality of the algorithm. ",0.13821138211382114,0.24390243902439024,0.16260162601626016,0.30612244897959184,0.20408163265306123,0.10967741935483871,0.3469387755102041,0.1935483870967742,0.2631578947368421,0.0967741935483871,0.13157894736842105,0.2236842105263158,0.19767441860465115,0.2158273381294964,0.20100502512562812,0.14705882352941174,0.16,0.1471861471861472
308,SP:c4cee0d44198559c417750ec4729d26b41061929,This theoretical work highlights uncomputability issues arising with energy-based sequence models. It is shown that an EC-complete family (a certain computational model capturing neural networks and transformers that is essentially equivalent to weighted Turing machines) cannot approximate the partition function of energy-based sequence models (EBMs) even given an unlimited amount of time and space. A consequence is the impossibility of model selection for EBMs. This paper also rules out popular estimators such as rejection and important sampling. This work concludes with a discussion of restricted EBMs that avoid such uncomputability issues. ,"The paper works in a computational model where various architectures of energy-based-models are viewed as computational models for accepting languages. EBM's assign a weight to each string. The sum of weights is called the partition function and this needs to be computed (or at best approximated) if the energy is to be thought of as a probability measure. The paper shows that expressive EBM's are turning complete, and uses that to show that the partition function may be uncomputable. And that is even in the case the energy of a sequence could be computed in poly-time. The paper then shows some corollaries and variations of this result, showing that model selection is undecidable as well. The paper concludes by suggesting scenarios where the partition function is computable but this naturally comes at the expense of the expressivity of the model.",The paper studies the trade-off between expressiveness and computability (of the partition function) for energy-based sequence models. The theoretical results show that the high expressively of unrestricted energy based models comes at the cost of un-computability and in-approximability of the partition function. These negative results further show that rejection and importance sampling are not a panacea either.,"This paper studies ""efficiently computable (EC) energy-based sequence models (EBMs)"" which are simply sets of strings equipped with nonnegative weights that can be computed by a poly-time Turing machine (upon normalization, the weights induce a probability distribution over the strings). In practice, it is common for these weights to be computed by neural networks. In fact, certain neural sequence model families like RNNs and Transformers are expressive enough to be Turing-complete. In this paper, the authors find that the expressivity of such sequence model families, so-called ""EC-complete parametric families"", comes at a significant cost in terms of *computability/decidability* of various primitives for inference. For instance, they show that if one could actually take in any vector of parameters specifying a model in such a family and output the corresponding partition function (sum of the weights of all strings) even approximately in expectation, then would be able to decide the halting problem (Theorem 2, 4). They also exhibit a single efficiently computable EBM for which proving (within ZFC) that the partition function is one of two possible values would either disprove Godel's second incompleteness theorem or disprove consistency of ZFC (Theorem 3); this can be extended to show that *asymptotic* estimates like rejection sampling and importance sampling will also fail (Theorems 5, 6). Along similar lines, they can reduce from the halting problem to other tasks like deciding whether two given parameter vectors give rise to the same EBM (""parameter identifiability"", Theorem 7), or deciding which of two given parameter vectors gives rise to an EBM which is distributionally closer to some other given EBM (""model selection"", Theorem 8).  In terms of techniques, the reductions from the halting problem are all based on a simple weight function that is tiny unless if the string corresponds to a valid accepting trace of an input-free Turing machine M. The point is that the weight for any string x can be computed by M simply by simulating M on x, but the partition function is large iff M halts. The ZFC results follow by taking M which enumerates all provable propositions under ZF and halts iff it proves 1 = 0.",0.23404255319148937,0.18085106382978725,0.2978723404255319,0.1310344827586207,0.27586206896551724,0.45901639344262296,0.15172413793103448,0.2786885245901639,0.07692307692307693,0.3114754098360656,0.10989010989010989,0.07692307692307693,0.18410041841004185,0.21935483870967745,0.1222707423580786,0.18446601941747573,0.15717092337917485,0.13176470588235295
309,SP:c4d1c99a2d53e90336c7e110738bc1eb8a38f3b4,"# Overview  The paper proposes a framework incorporating deep reinforcement learning for resolving Ansatz optimization problems, which is with wide impacts on quantum chemistry. The authors have a good introduction and related work section, which covers most standing works in quantum reinforcement learning and variational circuit learning.   Compared with Rotosolve and COBYLA, the proposed DDQN-based method attains state-of-the-art results on estimating the ground-state energy of lithium hydride.  Overall, I like the idea but the baseline comparison and some time complexity could better incorporate to add the depth of this paper. The current version is more application-oriented and less novel as a general ML framework considering existing works in the neural architecture search (NAS) community.   - Justification of DDQN and comparison   When discussing ""why selecting DDQN for the RL"", there is very little discussion.   Since this work is not the first works on applying the heuristic method for quantum circuit architectures search (QCAS), the contribution is more on framing the Ansatz optimization and the benchmark results. It would be more valuable to address the selection and justification of the algorithm. (e.g., DRL-based vs Neuroevolution-based or at least the variants inside DQNs)  For the general audience in the NeurIPS community, instead of only showing the DDQN-based method for QCAS, providing more in-depth discussion between variants of the DRL algorithm (e.g., DQN, DQN with dueling, DDQN with dueling) and its complexity could gain better values of this paper.   ","The variational quantum eigensolver (VGE) is a method that utilizes a Noisy Intermediate Scale Quantum (NISQ) computer to find the ground state of quantum systems. At its core, an Ansatz for the ground-state wave-function is parameterized in terms of a quantum circuit, i.e. a series of single-qubit and two-qubit gates acting on a reference state, and the optimal parameters of the circuit are determined by a classical algorithm that uses the NISQ to evaluate the performance (energy) of the Ansatz.In this manuscript, the authors propose a reinforcement learning (RL) based approach to identify the small circuits (i.e. with the low number of gates, and thus less prone to errors) that still allow to reach chemical accuracy. The RL method is based on double deep-Q learning and curriculum learning. The authors employ their method to find the ground state energy of LiH approximating the electronic Hamiltonian as a 4 or 6 qubits Hamiltonian. For 3 different bond distances, the method proposed by the authors identifies circuits that achieve chemical accuracy, but whose size is lower as compared to other Ansatze","This paper studies the ansatz for Variational Quantum Eigensolver (VQE) quantum circuits. The authors propose a deep reinforcement learning framework to generate the ansatz for VQE circuit, aiming to achieve low estimated energy and shallow circuit depth. The authors leverage curriculum learning to gradually reduce the energy threshold (increase difficulty) to avoid learning failure. The method is evaluated on LiH molecule in different settings and shows shallower circuit than baseline HE and UCCSD circuits. ","This work demonstrates that reinforcement learning with intrinsic motivation can be effectively used to search over the space of ansatzes for the variational quantum eigensolver (VQE) algorithm. This algorithm is used in quantum chemistry to approximate the ground state energy of molecules (e.g. LiH). The author(s) describe how DDQN trained with Adam can learn to optimize a VQE-relevant reward function, in an environment which runs an optimization subroutine (COBYLA or rotosolve) at each step to optimize the rotation angles of the ansatz's gates. Their LiH experiments used a simulated noiseless quantum environment, and they benchmarked the performance against two well-known ansatzes. The results show that in simulation, RL has promising potential for circuit depth improvement.",0.15918367346938775,0.08163265306122448,0.10612244897959183,0.13368983957219252,0.13903743315508021,0.24324324324324326,0.20855614973262032,0.2702702702702703,0.21666666666666667,0.33783783783783783,0.21666666666666667,0.15,0.18055555555555555,0.12539184952978055,0.14246575342465753,0.19157088122605367,0.16938110749185667,0.18556701030927836
310,SP:c511066c38f9793bacb4986c564eafa36e032f39,"In this paper, the authors propose methods for selecting a query set for batch active learning. These methods select a query set by maximizing a criterion based on submodular functions such as facility location function, graph cut function, and log-determinant function. The similarity matrix between data points, used in the definition of submodular functions, is defined by the inner product of the gradients of loss functions.  The authors empirically validate the efficiency of the proposed methods for batch active learning with rare classes, redundancy, and out-of-distribution data points. They adopt standard image datasets and standard deep neural network models. In each experimental setting, some of the proposed methods outperform existing methods for batch active learning.","This paper proposes a ""unified active learning framework"". The key element of this framework is a submodular information measure, denoted by $I_f(A;Q|P)$ where $f$ is a submodular score function, $A$ is a set of unlabeled samples whose ""value"" is measured, and $P$ and $Q$ are set of samples to be specified by the algorithm as parameters. $I_f(A;Q|P)$ is supposed to measure similarity and dissimilarity among $A$, $P$, and $Q$. In each round of the proposed ""unified active learning framework"", it chooses $A$ that maximizes $I_f$. By choosing $P$ and $Q$ properly (for example, for the class imbalance task, setting $Q$ to be a small labeled sample set from rare classes), this paper shows empirically the proposed algorithm outperforms other active learning algorithms on various tasks, including standard AL, AL with class imbalance, AL with redundant data, and AL with out of distribution data.","In this paper, the authors propose a new diversity based active learning algorithm by utilizing a series of information functions in submodular optimization (SO) problems. Specifically, they consider three submodular information measures (SIM) in SO: Submodular Mutual Information (SMI), Submodular Conditional Gain (SCG) and Submodular Conditional Mutual Information (SCMI) and found that SCMI ($I_f(\mathbf{A};\mathbf{Q}|\mathbf{P})$) is the most expressive SIM such that one could tailor $\mathbf{Q}$ and $\mathbf{P}$ to represent SCG and SMI, and thus propose to use it as a “unified” acquisition function for tackling three scenarios in realistic AL: rare class due to class imbalance, redundant data and out-of-distribution(OOD) data. The basic idea is to specify $\mathbf{Q}$ and $\mathbf{P}$ depending on the desired/un-desired classes, such that one could condition on data region that they want to exclude (OOD, redundant data), while up-sample regions that one want to include (rare class) by considering the mutual information w.r.t that region. To instantiate SIM acquisition function, the authors borrow the utility function from three well known graph-based SO problems for diversity coverage: Facility location (FL), Graph cut (GC) and Log Determinant (LOGDET), and use the similarity kernels computed with pairwise cosine similarity of the last layer gradients w.r.t the model-predicted label for every pair of unlabeled data points. They experiment with synthetic datasets rooted from MNIST/CIFAR-10/ImageNet to test for the three special scenarios, and show promising results against several strong baselines including BADGE.","This paper is in the field of Active learning, which has proven to be useful for minimizing labeling costs by selecting the most informative samples from unlabeled datasets, which are then to be labeled by a human in the loop. The goal is to achieve high accuracy of the trained model, while minimising the number of points to be labeled by the human in the loop. However, as the authors say, many existing state of the art active learning methods are trained on datasets which are not very realistic, and they do not perform well in many common real-life situations such as imbalance or rare classes, out-of-distribution data in the unlabeled set, and redundancy. Given the size of these datasets, cleaning them manually is not realistic. The authors propose an active learning framework which they call SIMILAR, which uses submodular information measures as acquisition functions. They argue that SIMILAR performs well on the realistic scenarios mentioned previously, and empirically demonstrate that SIMILAR outperforms previous state of the art methods in these scenarios.  ",0.23728813559322035,0.288135593220339,0.2542372881355932,0.2236842105263158,0.19078947368421054,0.12062256809338522,0.18421052631578946,0.13229571984435798,0.17142857142857143,0.13229571984435798,0.1657142857142857,0.17714285714285713,0.2074074074074074,0.18133333333333335,0.20477815699658702,0.16625916870415647,0.17737003058103978,0.14351851851851852
311,SP:c77b83c667a9b63fe15582336a77a34e96fd667b,"This work studies the problem of learning from label proportions (LLP), that is given bags of vectors and given the average of the positive labels in each bag, we need to find an LTF that satisfies most of the bags. The authors study the case where the bags contain at most 2 vectors. They study two cases, one where the vectors are taken from $\mathcal X\subseteq \mathbb{R}^d$ and they are searching for an LTF; in the other case the vectors are taken from the $d$-dimensional hypercube $\{0,1\}^d$, and they are looking for an OR-LTF. In the first case, they provide an algorithm that satisfies $2/5$ of the bags and if all the bags are non-monochromatic (contain vectors with different labels) then the algorithm satisfies $1/2$ of the bags, for the other case they provide an algorithm that satisfies $(2/5 + \gamma_0/d)$ bags and if all the bags are non-monochromatic it satisfies $(1/2+\gamma_0/d)$ of the bags. Furthermore, for the last case, the authors provide an NP-hardness reduction to the Label Covering problem that shows that their result is qualitatively tight and that in general, we cannot hope for an algorithm that satisfies all the bags.","This paper intends to make a few theoretical contributions for the learning from label proportions framework. In this framework training data arrives in the form of bags that contain several feature vectors and the proportion of positive labels in the bag as ground truth. The authors focus on learning linear threshold functions, while making a restriction to bags of size two.   A few theoretical results are presented. A first theorem gives a lower bound on the number of bags that can be ""satisfied"" with a polynomial time algorithm -- the authors define a bag as satisfied if the predicted proportion of positive labels equals the real proportion in the bag. A second theorem states that finding a linear threshold function that satisfies all bags is NP-hard in the worst case. ","This paper studies proper learning of linear threshold functions (LLFs) in the learning from label proportions model.  There are two main contributions.  The first is a polynomial time algorithm that, given a collection of bags each of size at most two consistent with some (unknown) LLF, identifies an LLF that satisfies at least $2/5$ of the bags.  The second main contribution, which complements the first, shows that it is NP-hard to compute, given a collection of bags all satisfied by some monotone OR, to compute any function of constantly many LLFs that satisfies $(1/2+\epsilon)$ of the bags for any positive $\epsilon$.         ","This work considers the problem of learning a linear threshold function (LTF) from label proportions. In particular they consider the special case where both single (as in the standard PAC learning setting) labeled (as 0 or 1) examples are observed or pairs of points $(x_1, x_2)$ together with their average label (which can be 0, 1, or 1/2) are observed.  They give two main results: a polynomial time algorithm that finds an LTF that satisfies at least 2/5 of the labeled examples (pairs and singletons together) and a lower bound that shows that finding an LTF that satisfies more than 1/2 of the labeled examples is NP-Hard.  For the special case where only mixed pairs, i.e., with label 1/2, are observed, the provided algorithm satisfies 1/2 of the labeled examples matching the lower bound (which also assumes only mixed pairs).",0.16981132075471697,0.1650943396226415,0.20754716981132076,0.19230769230769232,0.23846153846153847,0.3333333333333333,0.27692307692307694,0.3333333333333333,0.2953020134228188,0.23809523809523808,0.2080536912751678,0.2348993288590604,0.2105263157894737,0.22082018927444796,0.24376731301939064,0.21276595744680848,0.22222222222222224,0.2755905511811024
312,SP:c7b724c671def2800694fcc2625fa48d98c7cfe6,"This paper tackles an important problem in federated adversarial training: robustness accuracy significantly drops at the later stage of training. The authors first raise their assumption for the cause of this phenomenon: Adversarial training amplifies the heterogeneity of data distributions across different clients, and overfitted local robustness can not well generalized to other clients. Based on this assumption, the authors proposed \alpha-weighted federated adversarial training, which essentially up-weights the model trained on benign distributions and down-weights those on harsh distributions when averaging them up at the cloud center. Results show the proposed method outperforms previous state-of-the-arts under different adversarial training and federated learning settings. ","This paper introduces the alpha Weighted Federated Adversarial Training algorithm. The key of the idea is that in the aggregate step, the center prefers the local machine that yields smaller lost. Some theoretical results are delivered with numerical experiments. The paper claims that the alpha-weighted mechanism is tailored for the inner-maximization of Federated Adversarial Training, which is the rationale of the whole work.","The authors explore the adversarial robustness of federated learning. They claim that the inner-maximization optimization of AT can exacerbate the data heterogeneity among local clients. They propose an algorithm, $\alpha$--WFAT, which relaxes the inner-maximization of Adversarial Training into a lower bound friendly to Federated Learning.. The authors also experimentally establish that federated learning models are most susceptible to attacks when clients are using non-IID training sets. The experiments are performed over the CIFAR-10 , SVHN and CIFAR-100datasets.","This work studied the limitation of conventional Federated Adversarial Training approach, and proposed an \alpha-weighted relaxation for Adversarial Training in the federated learning setting. Then it proposed a novel \alpha-Weighted Federated Adversarial Training for minimizing a lower bound of the inner-maximization in Federated Adversarial Training. The performance of the proposed \alpha-Weighted Federated Adversarial Training were validated for both IID and Non-IID federated learning settings.",0.19090909090909092,0.15454545454545454,0.21818181818181817,0.24615384615384617,0.3076923076923077,0.2073170731707317,0.3230769230769231,0.2073170731707317,0.34782608695652173,0.1951219512195122,0.2898550724637681,0.2463768115942029,0.24000000000000005,0.17708333333333334,0.2681564245810056,0.217687074829932,0.2985074626865672,0.22516556291390727
313,SP:c7c50c44fdafb15b962e04d713cac309e57bc54b,"This paper proposes a new model ADVAE, which uses a sequence of latent variables which are constructed using cross-attention, which are then used to condition the inference model. The findings show that certain latent variables correlated with different syntactic roles (measured using a dependency parser).  The work claims that the latent variables, in this case, are able to disentangle content effectively, which I am inclined to agree with, however, I feel that the experimental setup is lacking to solidly support this hypothesis (which I detail in the “cons” and “questions” section). ","## Summary  - This paper proposes a probabilistic model called Attention-Driven Variational Autoencoder (ADVAE). This model is another instance of $\beta$-VAE whose encoder and encoders are composed of Transformers rather than previous neural architectures such as RNN. - The authors aim to disentangle the semantics of latent variables according to some syntactic roles (e.g., nouns and verbs) defined by syntax. To achieve this goal, they suggest employing the combination of Transformer and the existing $\beta$-VAE framework which is known to be effective for disentangling the role of each latent variable in VAE. - Moreover, this work presents a new way of quantifying syntactic disentanglement between latent variables, relying on the information obtained from the attention matrices of the Transformer architecture. - The experiments show that the proposed method is quantitatively better than the normal VAE, and that swapping the value of a specific latent variable can impact the generation of the target word (decided by the syntactic role of the latent variable we choose).","This paper propose a framework to obtain the disentanglement of syntactic roles as latent variables for sentence representations. The model is an attention-driven VAE which maps syntactic roles to separate latent variables using an encoder-decoder framework. In the second part of the paper, the authors introduce an evaluation protocol to quantify disentanglement between latent variables and spans both in the encoder and in the decoder, which includes syntactic role extraction, latent variable influence on decoder, encoder influence on latent variables and disentanglement metrics.","The paper proposes a method for unsupervised disentanglement of text components and shows its ability to identify semantic roles. To this end, a neural network is trained to compress the input into a fixed number of independent latent variables which are regularized to be standard Gaussians via the VAE framework. The inference network consists of a Transformer encoder-decoder network, where the decoder inputs correspond to the latent variables, which cross-attend to the outputs of the Transformer encoder, i.e., the encoded sentence. The idea behind this architecture is that attention-based seq2seq architectures align source and target sequences with each other.  The model is evaluated on disentanglement of semantic roles. To this end, they investigate how resampling of individual latent variables impacts the semantic roles in the text generated by the decoder, and how syntactic roles are aggregated into latent vectors via attention. They find that their proposed architecture is more successful at disentangling semantic roles into the latent variables than standard VAEs.",0.2608695652173913,0.22826086956521738,0.30434782608695654,0.15950920245398773,0.2331288343558282,0.3176470588235294,0.147239263803681,0.24705882352941178,0.1696969696969697,0.3058823529411765,0.23030303030303031,0.16363636363636364,0.18823529411764708,0.23728813559322035,0.21789883268482493,0.20967741935483875,0.23170731707317072,0.21599999999999997
314,SP:c80a7392ec6147395a664734601fb389a1eb4470,"For multivariate time series forecasting, this paper proposes to use a tensor network to model the variable space and improve the quality of the variable space by designing the series-variable encoder. Under the variable space, this paper also proposes an N-order residual connection approach and the skip-connection layer for processing the long-term data. The proposed model MVSRTN achieves good results on some datasets. However, this paper has not well explored MVSRTN, and the results are not very competitive. In addition, there are many typos in the paper.","The paper proposes a deep learning architecture for time series forecasting called MVSRTN. The model is composed of 3 blocks with skip-connections in-between them: a 1) ""Series-Variable Encoder"", 2) an ""N-Order Residual Tensor Network"", as the authors call it, and 3) the output layer. Out of these, 1) is essentially a 1D CNN combined with (causal and non-causal) self-attention, and 2) seems to be a tensor network built from taking the tensor product of the sequence entries, and then contracting that by a TT-rank-constrained weight tensor. The computation of the tensor network output is formulated as a recursion across time-steps, and motivated by residual networks, an identity mapping with respect to the hidden state at the previous time-stamp is added to the recursive formulation. Then, by the analogy with higher-order solvers for ODEs, a ""higher-order residual connection"" is introduced. The model is applied to four time series forecasting datasets, where some improvements are achieved, and an ablation study is included to show how the various model blocks contribute to the performance.","This paper proposed the MVSRTN architecture for multivariate time series modeling. The MVSRTN consists of an encoder to extract latent variables and residual tensor network (TN) blocks to capture the interactions in the latent space.  The main contribution of the model is the TN block part. In particular, the authors used tensor-products to fully represent the latent variable space (Eq 2). Then, they proposed an N-order residual TN block to alleviate potential gradient problems of high-order TNs in long-term time series. They conducted experiments on four multivariate time series data for prediction. Moreover, an ablation study shows the effectiveness of the proposed residual TN blocks.  Combining ResNet and TN is an interesting direction. However, I think this paper did not present this problem sufficiently and some notations are confusing. ","The paper considers the problem of forecasting of multivariate time series. The authors propose an architecture for such forecasting that incorporates several layers, including a residual tensor network layer. The idea is to tensorize (via outer products/Kronecker products) the features passed on from the encoder layer and then handle these very large tensorized features with a tensor network. The point of the tensorization is to better incorporate the effect of combinations of variables from different time steps. Experiments are done on four benchmark datasets.",0.27472527472527475,0.3516483516483517,0.18681318681318682,0.18579234972677597,0.14754098360655737,0.18045112781954886,0.1366120218579235,0.24060150375939848,0.2,0.2556390977443609,0.3176470588235294,0.2823529411764706,0.18248175182481752,0.2857142857142857,0.19318181818181818,0.21518987341772153,0.20149253731343283,0.2201834862385321
315,SP:c883fe9c7f4a5f950340ac79b6d7194278b1a1eb,"This paper provides a posterior sampling mechanism for the Dirichlet distribution achieving tCDP and approximate DP privacy, relying on modifying the alpha parameters. It includes an application on histogram release.  The response addressed my comments.","This paper studies the privacy of releasing a sample from the Dirichlet posterior. The paper shows for the Dirichlet posterior sampling with prior set to be $\mathrm{Dirichlet}(\alpha)$, that Dirichlet posterior sampling  satisfies a version of truncated concentrated Differential Privacy(tCDP). Now, using the relationship between tCDP and approximate DP established in Bun et al. (2018), the paper derives the approximate differential privacy of the Dirichlet Posterior sampling.   Given the result, the paper studies the utility of Dirichlet posterior sampling in 2 contexts: 1. One is interested in sampling for the posterior but changing the prior from $Dirichlet (\alpha)$ to $Dirichlet (\alpha’)$, such that $\alpha_i' > \alpha_i$ so as to increase the privacy preserving properties of the the posterior sampling. In this context, the paper derives sample complexity bounds such that $D_{KL}(\mathrm{Dirichlet}(\mathbf{x} + \alpha)|| \mathrm{Dirichlet}(\mathbf{x} + \alpha') )<\varepsilon$. 2. One is interested in releasing the normalized histogram $\mathbf{p} = \mathbf{x}/N$. One can privately release it by sampling from the Dirichlet posterior $Y \sim \mathrm{Dirichlet}(\mathbf{x}+ \alpha)$. The paper derives sample complexity bounds to incur small error $\ell_\infty$ norm. Authors also point out that one can get better sample complexity with Gaussian mechanism where Dirichlet posterior sampling outperforms Gaussian mechanism in small data regime.    ",The paper studies releasing a single sample from a Dirichlet posterior with several variants of differential privacy in the two specific tasks of Dirichlet sampling and private normalized histogram publishing. Both privacy and utility guarantees are analyzed by the authors. The authors compare the results with Gaussian mechanism theoretically and empirically using a simple simulation.,"This paper proposes a method for releasing differentially private samples from Dirichlet posterior distribution. Authors present a privacy analysis for their proposed method and demonstrate that the method out performs Gaussian mechanism under certain conditions. Besides the empirical evaluation, authors also derive analytical utility guarantees for the method.",0.4857142857142857,0.2571428571428571,0.2571428571428571,0.12037037037037036,0.06481481481481481,0.2,0.0787037037037037,0.16363636363636364,0.1875,0.4727272727272727,0.2916666666666667,0.22916666666666666,0.1354581673306773,0.2,0.21686746987951808,0.19188191881918817,0.10606060606060605,0.21359223300970875
316,SP:c8a4254e6fc2d2e7d1d41a76bb64f78f22a8639d,"This paper proposes a new method to learn (stationary) interpretable policies using soft decision trees in partially observed settings. The soft decision tree structure is extended to allow for recursion over time, and account for policy decisions based on history of collected data. An algorithm is presented to optimize the parameters of the soft decision tree as well as the structure/topology of the tree. The algorithm mainly proceeds by splitting nodes and locally optimizing the parameters of the the associated probability representation of the soft node, and recursively split (if local optimization does not improve validation performance) and fixed as leaf otherwise. A global update step is then used after topology is fixed followed by pruning low probability paths in the trees. Experimental validation on surveys with clinicians demonstrate reasonable interpretability and improved prediction performance on imitating clinician policy. ","This paper proposes a novel approach for learning and representing human decision-making policies from observed behavioral data. The proposed approach emphasizes interpretability as a primary aim, while nevertheless seeking to maintain reasonable modeling accuracy. The decision tree model proposed extends canonical decision tree approaches to the probabilistic setting, allow for optimization of leaf-specific parameters via stochastic gradient descent. The proposed approach is evaluated both in terms of its interpretability (subjective measurements from a panel of licensed physicians) as well as its accuracy in recapitulating actions conditioned patient observations. The utility of the approach is demonstrated on both synthetic and real-world datasets.","  The authors argued that many methods failed the merits of interpretability in some important areas, e.g. clinical decision-making. Thus, this paper proposed a (soft) tree-based method for synthetic clinical datasets in the matter of interpretability. The authors model the clinical decision process as a partially observable Markov Decision Process (POMDP), which naturally fits the assumption of medical diagnosis.","POETREE aims to construct an interpretable model for a policy over a time series using decision trees. The healthcare domain is particularly targeted. As opposed to other works, the model directly maps observations of a POMDP to actions. POETREE creates a decision tree from time series data. The decision tree can be conditioned on the history, allowing the tree to be different at different time steps, allowing for example the tree to model that an exam done previously that is no longer informative is no longer likely. Each tree is a soft-probabilistic model first grown incrementally by developing, optimized globally (as it is differentiable), then pruned. Finally, the tree is simplified for interpretability by limiting each condition to a single variable. POETREE is then empirically evaluated and compared to baselines in terms of distribution modeling, interpretability and policy learning.",0.17142857142857143,0.09285714285714286,0.19285714285714287,0.11538461538461539,0.20192307692307693,0.18032786885245902,0.23076923076923078,0.21311475409836064,0.19285714285714287,0.19672131147540983,0.15,0.07857142857142857,0.19672131147540986,0.12935323383084577,0.19285714285714287,0.14545454545454545,0.17213114754098363,0.10945273631840795
317,SP:c8f82ec90f891d7394933483b7f926155ac363ef,"This paper proposes a transformer based visual backbone for vision and language understanding. It uses a vision transformer backbone (Swin Transformer) to extract visual representation, which is then passed to a mask generation module. The visual tokens(including masked tokens) and text tokens embeddings are fed to a multimodal transformer model, where different pretraining objectives are applied. This architecture achieves state-of-the-art results in VQA, SNLI-VE, NLVR^2, Image-Text Retrieval. Authors propose an Inter Modality Flow metric that measure interaction between the visual and textual modalities and correlates well with the downstream results.","This paper introduces Swin Transformer in the visual domain for vision-language pretraining. Previous VL pretraining models feed image tokens and text tokens together into the cross-modal transformer, and the visual relationship learning and inter-modal alignment are encapsulated in the same transformer network. The authors propose that visual relationship between different visual objects or concepts is important for inter-modal alignment learning, and there should be independent processing of visual relationship in the framework. Motivated by this hypothesis, the authors add a visual-only transformer to learn visual relationship before feeding the image and text tokens together to the cross-modal transformer. In addition, the authors propose a metric named inter-modality flow (IMF) to measure the cross-modal interactions. They also propose masked feature regression as a new pretraining task. Pretraining is conducted on in-domain datasets COCO and Visual Genome. Evaluation is conducted on three downstream tasks VQA, visual reasoning, and visual entailment.","This paper provides a novel way of applying masked feature regression (MFR) to transformer-based visual encoders. In addition to this new way of MRM, the paper also provides a quantifiable measurement of how two modalities (image and text) interchange their information with the aids from the attention flow method. This quantity, the inter-modality flow (IMF), is then used to analyze the characteristics of different vision-and-language pretraining models.","This paper proposed to use the vision transformer as the visual backbone for vision-language pre-training. To quantize the information flow transferred between vision and language modalities, the authors proposed a metric called Inter-modality flow that aggregates the attention weight across different layers. Experiments on VQA, visual entailment, and visual reasoning validates that the proposed method achieves state-of-the-art performance.",0.28865979381443296,0.17525773195876287,0.21649484536082475,0.12738853503184713,0.15286624203821655,0.22535211267605634,0.17834394904458598,0.23943661971830985,0.328125,0.28169014084507044,0.375,0.25,0.2204724409448819,0.20238095238095238,0.2608695652173913,0.17543859649122806,0.2171945701357466,0.23703703703703702
318,SP:ca6f11ed297290e487890660d9a9a088aa106801,"The paper provides a simple phenomenological model of neural networks for K-class classification. Each sample is assigned a feature modeling NN features, and these features are evolved via SDEs in which the drift term of a sample is a linear transformation of the average features of each class. Separation, which means the case in which samples from different classes can be separated by hyperplanes, holds w.h.p. when the intra-class drift coefficients larger than the inter-class coefficients in a certain sense, and it does not hold otherwise. Two specific choices of features are considered: isotropic features and logit features. The latter are shown to be better than the former at modeling the feature evolution for NN on CIFAR and a dataset of their own creation. ","This paper proposed a stochastic differential equation model based on the local elasticity assumption of features of neural networks.  The local elasticity phenomenon says samples have greater influence to other samples in the same class than those in different classes. Starting from this assumption, a system of linear SDEs is proposed for features corresponding to each data point, with the coefficients encoding the difference of inter and intra classes impact. Two types of features are studied---the isotropic feature learning model, and the logics-as-features model. For the two models, dynamics of the class means are derived as a system of ODEs and the solutions are found, depending on the coefficients about local elasticity. From the solutions, the separability of classes is theoretically connected with local elasticity. Specifically, the classes are separable if the local elasticity effect is nonzero, while not separable otherwise. The transition between the two cases is sharp.   Numerical experiments are conducted to justify the effectiveness of the proposed model to characterize the actual dynamics of features. Experiments are done on deep convolutional networks on the GeoMNIST and CIFAR10 dataset. Firstly, the coefficients in the SDEs (and the ODEs for the class means) are estimated using real feature dynamics given by the training process (I understand this step as a kind of ""fitting""). Then, the ODEs with estimated coefficients are simulated, and the results are compared with the real feature dynamics. It shows that in many cases (but not all) the trajectories produced by the SDEs recover the real trajectories given by the training. ","The paper proposes a proxy tractable dynamics to study how the latent features of neural networks evolve inter- and intra-class during training. The core of this dynamics is a linear ODE, which is shown to exhibit inter-class separability once a “locally elastic” condition is met. The paper then examines this dynamics for two choices of the H matrix (which is a parameter of the dynamics), and concludes with experiments to demonstrate the relevance of the proposed theory with the actual simulation.   ===================== After rebuttal: Thanks for the detailed reply!  Overall while I am pleased that the authors made a lot of efforts to clarify, I'm not quite convinced. Some notes: - I agree that time-dependence of E(t) is a good point; my concern about linearization (which I believe is also the concern of some other reviewers) is actually about why at each time t, F(X(t),t), in the authors' response, is modeled by a linear relationship with X(t). Why ignoring higher-order terms? Are they much smaller than the linear term? I do not see why the higher-order terms can be absorbed into the noise term. - I do not agree with the argument that $c_1\neq c_2$ assumption can be alleviated by taking the results ""in probability"". Clearly the derivation in the paper requires taking a law-of-large-numbers average.  I agree (and well understood) that the paper does not aim to mimic exactly the dynamics; the aim is to provide a reasonable proxy dynamics that can explain interesting properties. However I feel the paper needs a careful justification to convince readers that this proxy is not oversimplifying.  Given that the paper may attract interests, I'm willing to raise the score, but not fully convinced of the quality to recommend acceptance. If a reviewer is willing to champion the paper, accepting the paper would not be a bad outcome to me.","This paper builds off a recently proposed phenomenon of deep learning models, local elasticity, which, says the that impact in feature space of a gradient update from an input is in general larger for feature of data in the same class as the input rather than a different class. In this work the authors propose a set of SDE's modeling SGD that captures the inter and intra class effect of back propagation. They find that there is a sharp phase transition in the dynamics governed by these SDEs when they do or don't display local elasticity. In particular, when the SDEs are locally elastic then the features of the training data are guaranteed to become linearity separable, while this is not the case otherwise. They demonstrate this theoretical analysis empirically with CIFAR-10 and connect there results to another recently proposed phenomenon, neural collapse. ",0.27906976744186046,0.27906976744186046,0.20155038759689922,0.20930232558139536,0.14728682170542637,0.10869565217391304,0.13953488372093023,0.11180124223602485,0.1780821917808219,0.16770186335403728,0.2602739726027397,0.23972602739726026,0.186046511627907,0.15964523281596452,0.1890909090909091,0.18620689655172412,0.18811881188118812,0.14957264957264954
319,SP:ca846ae9653fa843a6a64ce7361d44a0c31c5990,This paper studies credit assignment problem in neural networks with RL algorithms. They first formulate the problem as a finite-horizon MDP so that they can apply RL. They find nondeterminism in the problem causes suboptimal solutions and propose to use off-policy RL with a critic to resolve it. ,"- This paper revisits an interesting (and truly under-explored) idea from the reinforcement-learning literature: coagent networks [43,44]. The original work by Philip Thomas considered endowing the standard agent, that maps from states to actions, with a richer structure whereby the state can be mapped by multiple coagents both in parallel and in sequence yielding various intermediate outputs (coagent actions) before ultimately rendering a final action to constitute the action output of the overall agent.  - In this paper, the authors consider how the concept of coagent networks may be used to ground neural network training as a finite-horizon reinforcement-learning problem, thereby offering a local, asynchronous alternative to the traditional and widespread method of backpropagation.  - The paper offers a formulation of neural network training as a finite horizon POMDP, where the partial observability arises only to accommodate the non-Markovian nature of the terminal reward signal that must depend on the initial input to the network. While the formulation allows the authors to employ policy-gradient techniques for local, asynchoronous optimization of the network, they highlight shortcomings of using the standard REINFORCE estimator, which align with early findings by [44].   - On the path to resolving these issues, the authors present numerous experiments that tease apart various hypotheses to diagnose the failures of coagent network training relative to standard backprop. While unable to outperform backprop in the standard supervised learning setting, the authors not only provide ample diagnosis of the failure but also conclude their empirical investigation with a continual learning setup that highlights the strength of using local, adaptive policies for neural network training over backprop. ","This paper presents a study that investigate RL methods for the credit assignment problem in neural network. In this study, the credit assignment problem is formalized as a finite horizon RL problem. Subsequently, it is empirically demonstrated that stochasticity of coagents’ policies hinders the learning performance when REINFORCE is used to train coagents. To address this issue, a Q-learning like off-policy algorithm is proposed. The experimental results show that the proposed off-policy algorithm outperformed on-policy algorithm. In addition, the proposed off-policy algorithm outperformed backpropagation in a continual learning task.","The paper investigates using reinforcement learning to train neural networks. Specifically, it considers formulates the problem in which individual layers are treated as steps in a finite horizon partially observable MDP, and explores using either a global agent or individual co-operating agents at each layer to maximize the reward (the negative final error).  The paper then presents an empirical study of using REINFORCE in this cooperative agent framework to train one or two hidden layer networks on MNIST/Boston Housing while varying different parameters like the policy variance, different partitioning schemes of the network into agents, different baseline approaches to improve stability, and activations. The key takeaways of this experiment are that (1) the CoAN+REINFORCE approach generally underperforms backprop, (2) baselines to improve stability don't seem to help, and (3) the primary reason for the poor performance is the non-deterministic behavior of the other agents (as indicated by having a single agent optimizing all layers performs the best by far).   Motivated by this, the paper presents an off-policy Q learning approach to training cooperative agents, which uses bootstrapping and trains by assuming the best action is taken at the next step. While the paper shows this improves over on-policy SARSA, it still underperforms REINFORCE, which underperforms backprop.   Lastly, the paper shows that in a continual learning setting the CoAN+REINFORCE approach can be more robust than backprop. ",0.32,0.38,0.36,0.09701492537313433,0.16044776119402984,0.2872340425531915,0.05970149253731343,0.20212765957446807,0.07725321888412018,0.2765957446808511,0.18454935622317598,0.11587982832618025,0.10062893081761005,0.2638888888888889,0.12720848056537104,0.14364640883977903,0.1716566866267465,0.16513761467889906
320,SP:cac881243abde92a28c110f5bd84d115ed189bda,"The authors present a new benchmark for deep metric learning algorithms, particularly for zero-shot tasks, in order to reflect their generalization under out-of-distribution shifts performance. They propose a novel way of splitting train and test data with increasing difficulty(the gap(distribution shift) between train and test get higher). FID score is chosen to determine the distribution shift between train and test. After an initial split is chosen, the classes are exchanged between the train and test data to obtain a higher FID measure at each time iteratively. The authors suggest to use ROC curve to obtain a one single criteria, while keeping the results for each split in order to observe the performance based on task difficulty. They include experimental analysis on their proposed criteria and show results for some existing methods and architectures. The authors also analyze query-support framework in few-shot learning and its effect on the generalization performance.","This paper proposed a new benchmark for deep metric learning under varying degree of distribution shift. First, it introduces a method for creating train/test split with increasing FID --- thereby increasing distribution shift between train and test. Second, various metric learning methods are compared under the benchmarks of varying difficulties.","The authors of this paper investigate how generalization is affected by varying the training and testing data splits in deep metric learning (DML). Specifically, they create train/test splits (splitting on classes) which increase in difficulty (meaning that the distributions are more different) and evaluate DML generalization. To measure the gap between train and test distributions, they use FID with ResNet-50. Experimentally, they use popular benchmarks and the Recall@k metric to show that performance decreases as the train/test FID distances become larger. This, and other experiments, show that using a fixed train/test split can lead to misleading conclusions about generalization.","This work proposes a new way to measure OOD performance of metric-learning algorithms. It introduces a proxy-metric for measuring ranking dataset generalization difficulty based on FID score. They construct several train/test splits of increasing difficulty based on this score. They show that splits constructed in such a way are well correlated with performance, thus validating the proposed proxy-metric of OOD generalization difficulty.  Several experiments are performed to assess the performance of modern metric learning methods. An AUC type score over all splits (difficulties) is proposed as an overall score for various methods. Finally, additional experiments that include increasing network capacity, using self-supervised pre-trained models, and fine-tuning with few-shot learning are presented. ",0.15384615384615385,0.16666666666666666,0.16025641025641027,0.26,0.34,0.18269230769230768,0.48,0.25,0.21008403361344538,0.125,0.14285714285714285,0.15966386554621848,0.2330097087378641,0.2,0.18181818181818185,0.16883116883116883,0.20118343195266272,0.17040358744394618
321,SP:cb38b58054581db865d8c2a4065f062724ca0a5e,The authors proved that the generalization error for a CNN can be made independent of the data dimension under certain assumptions. They study the problem in a teacher-student framework and the test error proportional to $P^{-\beta}$ with P to be the training set and $\beta$ to be only dependent on the local kernel size of student network. The authors empirically validate this finding. Furthermore they showed that similar learning curve exponent achieved in the ridgeless case can be achieved in kernel ridge regression with decreasing ridge coefficient.,"This paper studies the two elements of CNNS, 1. local patch; 2. translation-invariant. Under a teacher-student framework for kernel regression, the authors conclude that (under reasonable assumptions), it is the locality of CNNs that defeats the curse of dimensionality. The exponent $\beta$ in the learning curve (with respect to the sample size) is studied theoretically and empirically verified.","# Post Rebuttal:  I partly accept the claims of the authors in the rebuttal and have decided to raise my score a little bit to reflect that. I still believe the paper would benefit greatly from clarifications. In particular, I advise the authors to incorporate the changes mentioned in their response to Remarks 1-2 in the rebuttal, as these strengthen the paper in my opinion. As for Remarks 3-4, I disagree with the authors' claim; my point that convolutional neural networks do not provide any benefit over other simpler hypothesis classes in the regime of lazy training still stands and I strongly encourage the authors to omit any reference to neural networks as motivation for studying the problem. I find this reference to be too artificial and somewhat forced.  Lastly, I would like to wish the authors the best of luck in the future.    # Original Review:  The paper studies the expected generalization error when learning in a teacher-student setting where the teacher is a Gaussian random field and the student is a kernel exhibiting a certain convolutional structure. It is shown that under strong assumptions the expected generalization error decays at a rate which does not depend on the input dimension but rather on the filter size of the student. Additionally, experiments are performed to corroborate the theoretical findings in the paper.","In this paper, the authors study the generalization error of kernel regression in a teacher-student scenario where the kernels are obtained as NTK of either one-hidden layer locally-connected NNs or convolutional NNs. They consider the teacher as a Gaussian random field of covariance the same kernel as the student kernel but with different parameters, and proceed to characterize the learning curve exponents. Based on a decomposition of the two kernels in orthonormal basis functions, they use recent (non-rigorous) statistical physics predictions to deduce an exponent that can be shown rigorously to be an upper bound in some cases. This exponent does not depend on the ambient dimension d, only on the patch size, and is the same for the two kernels. The authors deduce that locality is the important property that breaks the curse of dimensionality and notice that adding the invariance by translation only lower the prefactor.  Finally, using a Gaussian universality assumption and a recent framework, they prove a lower bound on the learning curve exponent.  They further check their predictions in numerical experiments on synthetic data.",0.1797752808988764,0.2808988764044944,0.3146067415730337,0.3,0.36666666666666664,0.16071428571428573,0.26666666666666666,0.11160714285714286,0.15300546448087432,0.08035714285714286,0.12021857923497267,0.19672131147540983,0.21476510067114093,0.15974440894568692,0.20588235294117646,0.12676056338028172,0.18106995884773663,0.17690417690417692
322,SP:cb9530f5517f1092513c200b3f32e55420fdd768,"This paper proposes a new initialization scheme for ResNet. The key feature is the initialization values are set to all zeros and ones, rather than random. The development of this initialization scheme is motivated by avoiding the ""dead neuron"" problem and ""training degeneracy"" problem for networks with zero-initialized weights. Some toy examples are discussed to illustrate the motivation. Last, experiments on standard benchmarks are conducted to verify the initialization scheme.","To initialize a ResNet with only zeros and ones, this paper analyses the problem of all-zero initialization and then proposes a  method named ZerO to achieve the goal by augmenting the standard ResNet architectures with a few extra skip connections and Hadamard transforms. The proposed idea is interesting and somewhat novel. However, it seems to introduce a new structure rather than only initialization, and empirical results are not really good.","The paper proposes a deterministic weight initialization scheme called ZerO for residual networks.  ZerO works by first augmenting existing ResNet architectures with extra skip connections and Hadamard transforms and then initializes network weights with only zeros and ones (instead of random weights). The authors show that the proposed scheme has various benefits such as improving reproducibility, training networks without batch normalization, and improved performance.","In this paper, the authors propose some modifications to the classic ResNet architecture and initialization scheme, so that the network can be trained with fully deterministic initialization, even without any normalization layers. The authors motivate their design choices by identifying and analyzing the problems with the forward and backward dynamics of some naive ideas. Then the authors present their approach and explain why each choice they make can solve the specific problems they have presented. Then they evaluate their proposed method along with some baselines on two popular image classification datasets, showing that their deterministic initialization method is on par with the ones with random weight initialization. Finally, they present related work and conclusion.",0.2112676056338028,0.2112676056338028,0.22535211267605634,0.2535211267605634,0.2112676056338028,0.203125,0.2112676056338028,0.234375,0.14035087719298245,0.28125,0.13157894736842105,0.11403508771929824,0.2112676056338028,0.2222222222222222,0.17297297297297295,0.26666666666666666,0.16216216216216214,0.14606741573033707
323,SP:cccdcc95c4177b5531bad23b662060fdd0d88849,"  The paper proposes an extension of particle belief   propagation which allows the factor parameters (e.g.,   neural network weights for factors modeled as NNs) to be   learned using standard stochastic gradient descent.    This algorithm is then applied to several   continuous-domain state estimation tasks involving   articulated objects (e.g., hand pose estimation from   images).    The main contribution of the paper is adapting an   existing line of nonparametric belief propagation methods   to this setting, which allows (partial) end-to-end   learning.    The learning is only partly end-to-end as the particle   resampling stage is non-differentiable. Instead, the   proposed approach supervises the belief at every step of   the algorithm, but does not backpropagate through the   entire inference procedure opting to instead maximize the   belief of the GT values for the unobserved labels at each   step.    Experimental results on synthetic and real datasets for   articulated object state estimation show that the   proposed algorithm is able to learn meaningful NNs for   the unary and pairwise potentials while also producing   reasonable uncertainty estimates. The proposed approach   sometimes outperforms the baselines (e.g., an LSTM), but   in the task of parametric human hand tracking, it lags   behind the state of the art by a large margin.  ","Supervised learning of Markov Random Fields (MRF) by MLE requires to compute pairwise and unary marginals of the current model estimate at each iteration of the likelihood maximisation. This task is not tractable except for MRFs on trees with finite hidden state spaces. The authors consider MRFs with infinite state spaces, propose to model the pairwise and unary potentials by neural nets and aim at developing an approximated belief propagation (BP) approach for learning these networks. BP is known to be exact on trees but is not tractable for infinite state spaces. The authors apply their method on a challenging task of hand pose estimation in  RGB-D image sequences taken from the first-person perspective.","This paper enables end-to-end learning of the factors of a graphical model for nonparametric belief propagation (NBP) methods by using neural networks. It calls this method ""Differentiable nonparametric belief propagation"" (DNBP).   The aim is to replace domain-specific hand crafted factors with learned factors, by replacing each factor with a neural network. Compared to vanilla neural net based solutions, DNBP also reports uncertainty.  The method is evaluated on a couple of toy examples of articulated pose tracking, as well as using hand pose estimation on the FPHAB dataset.  The method is compared against learned, neural network based baselines. ","The method (DNBP) proposed in the paper considers a non-parametric belief propagation method where the unary potential functions, the pairwise potential functions and the particle diffusion function are modeled as feed-forward neural networks. It allows them to learn the parameters of these networks using labeled data which is the main contribution of the paper.  DNBP is evaluated on three tasks (simulated double pendulum, simulated articulated spider, first-person hand action). In each application, DNBP is not able to outperform the considered baseline but it is able to provide measures of uncertainty associated with its predictions. ",0.11386138613861387,0.11386138613861387,0.12871287128712872,0.1724137931034483,0.12931034482758622,0.17,0.19827586206896552,0.23,0.26804123711340205,0.2,0.15463917525773196,0.17525773195876287,0.1446540880503145,0.152317880794702,0.17391304347826086,0.1851851851851852,0.14084507042253525,0.17258883248730966
324,SP:cd01ab8f03cfb1bb067478ca82944d3a42826ca4,"Meta-learning (few-shot learning) algorithms, in general, equally treat all tasks for the training process. Tasks are sampled uniformly at random. This paper proposes a novel meta-learning framework that controls the task sampling probabilities according to difficulties. This paper measures the difficulties of tasks using the negative log-likelihood and leverages the difficulties to introduce different weights to tasks. The main idea of the learning framework is that the prior of the difficulties follows a normal distribution while the target sampling distribution could be uniform or some other interesting forms. Then, important sampling provides different weights to different tasks. From experiments, the uniform target distribution shows good results. ","This submission studies episodic sampling, which is used to sample datasets (consisting of support and query sets) for training meta-learning models. The authors define a notion of episodic difficulty (the negative log-likelihood of the query set given the support set and model parameters) and propose using importance sampling to train using several different types of episode-sampling strategies for meta-training. By using importance sampling, there is no sampling required for the target distribution but just weighting the episode loss based on the proposal and target distribution. They conduct experiments both to confirm some of their assumptions that are critical for their proposal (that episodic difficulty is normally distributed) and to show how their episodic sampling strategies compare against random episodic sampling, which is the dominant strategy used by work in this area. ","This paper investigates the effect of episode difficulty and sampling schemes on the performance of few-shot classification algorithms. It defines difficulty based on the negative log predictive likelihood of the few-shot learner and proposes to use importance sampling to mimic various sampling strategies. Experiments demonstrate that episodic difficulty is (a) approximately normally distributed, (b) tends to transfer across architectures and learning algorithms, (c) tends to be consistent over the course of training, and (d) that a uniform sampling scheme over difficulty tends to produce the best results. An online scheme for computing importance weights is also proposed.   === Post-rebuttal ===  I have read the authors' response and other reviews and will maintain my original rating.","This paper explores episode/task difficulty in few-shot learning. The task difficulty is defined as the negative log-likelihood over the query set given the support set and model parameters. The authors find evidence that episode difficulty likely follows a normal distribution and proposes a new method, called UNIFORM, based on importance sampling. The new method achieves similar or better performance than other sampling procedures (EASY, HARD, and CURRICULUM). The paper also presents two variants of UNIFORM: offline and online. ",0.20909090909090908,0.2,0.20909090909090908,0.2222222222222222,0.1925925925925926,0.19827586206896552,0.17037037037037037,0.1896551724137931,0.2839506172839506,0.25862068965517243,0.32098765432098764,0.2839506172839506,0.1877551020408163,0.19469026548672566,0.2408376963350785,0.2390438247011952,0.24074074074074078,0.233502538071066
325,SP:cd1e11b270f74d5dca9efd9fe1903c0a24bcba12,"This paper derives a Fenchel duality formulation of the maximum likelihood loss of F1-EBMs, which turns the optimization into a min-max problem on probability measures over the sample space. A dual algorithm with mean-field dynamics is proposed to solve this problem. Using the dual aogrithm they also draw a connection between maximum-likelihood training and score matching. Simple numerical experiments on two-layer ReLU network show that the algorithm converges much faster than the original MLE. ",The authors formulated the Fenchel dual problems to maximum likelihood and score matching training applied to energy-based models defined by shallow neural networks. The authors then proposed practical algorithms based on the Hahn decomposition and compared this to various predecessor algorithms. An interesting part is that the learning dynamics can interpolate between maximum likelihood and score matching. There are also rich results in the Appendix. ,"Two approaches exist for learning a Gibbs measure: either by MLE (minimizing the KL divergence) or by score matching (maximizing the Hyvarinen score). With the same goal to minimizing the KL divergence, the author proposes an alternative training approach by augmenting an auxiliary $\gamma$ measure and thereby optimizing a more tractable dual problem. The author claims that this new training method has a quicker convergence rate than the primary problem.  ","This paper studies the training of Energy Based Models (EBM). The “standard” technique is maximum likelihood maximization of the observed dataset. While theoretically sound, there are practical difficulties in simulating the MCMC dynamics due to large basins in the energy landscape. For this reason a class of alternative methods, like Score matching, have been explored in the literature. Score matching, however, is deemed to suffer in statistical power.  This work explores how, leveraging known results about Fenchel dualities, it is possible to connect the two training modalities (Maximum Likelihood and Score based) on a continuum, and proposes a practical algorithm based on such considerations. The authors consider for their theoretical analysis of the expressiveness of the models,  shallow neural networks in the lazy and kernel regime (spaces F_1 and F_2 respectively).  Section 2.1 introduces the two considered spaces, referring to the works of Chizat and Bach.   An energy based model is defined at the beginning of section 2.2 by means of the Radon Nikodym derivative. Training an EBM model via maximum likelihood corresponds to maximizing eq. (2), that, when considering networks in F_1, can be rewritten as the expression (3). At the end of Section 2.2. score matching is rigorously defined and the relationship between theoretical and empirical implementations (Hyvarinen) is clarified.  Section 3 is the core of the paper. If Assumption 1 holds (growth conditions), then Thm 1 states that eq. (3) is the Fenchel dual of (5). As (5) is a static functional, the authors propose to use results from Chizat and Santambrogio to rewrite it as the dynamical system (eq. (6))  together with the definitions in (7). Importantly, this formulation contains \alpha, a free parameter >0, that determines relative time-scales over which functions \gamma and \nu are updated. System (6) is still “algorithmically” unsolvable. The authors then propose to use the two following approximations: first, the mean field technique known as propagation of chaos, is used to switch from (6)  to the system (8), where gamma is substituted by neural network  parameters and \nu by and SDE. Then the continuous time system (8) is discretized using Euler-Maruyama. Algorithm 1 is the result of the process.  Section 4 explores how, in the infinite alpha regime, the dynamical system (6) is equivalent to score matching. By means of eq. (11) and proposition (3) the technical analysis is performed. Intuitively, parameters are updated infinitely faster than the samples.  Section 5 explores the implementation of Algorithm 1 on a synthetic dataset. The authors consider two cases (the two teacher neurons aligned to 164 and 78 degrees respectively). Comparison of the primal and dual (alpha<<1, alpha>>1) shows that the dual performs much better than the primal.  The conclusions stress the theoretical contribution based on Fenchel duality and propose as future work the exploration of convergence rates of (6) and (10) and numerical investigation of the restarting probability p_R. In addition, the authors suggest that “porting” the presented work to deeper network architectures (e.g. to work on realistic dataset, such as images) is straightforward from the practical point of view, and unpractical from the theoretical point of view. ",0.189873417721519,0.13924050632911392,0.35443037974683544,0.13636363636363635,0.42424242424242425,0.2857142857142857,0.22727272727272727,0.15714285714285714,0.05313092979127135,0.12857142857142856,0.05313092979127135,0.03795066413662239,0.2068965517241379,0.1476510067114094,0.09240924092409242,0.13235294117647056,0.09443507588532883,0.06700167504187604
326,SP:ce3cde67564679a8d9a0539f1e12551390b91475,"##########################################################################  Summary: This works proposes dialogue policy learning by incorporating a hierarchical policy. The proposed hierarchical reinforcement learning-based approach has two models, i.e., one master model that instantiates low models. The low models have several symptoms checkers and disease classifier. Experiments are conducted on both real and synthetic datasets to demonstrate the efficacy of the proposed work.    ##########################################################################","This paper introduces hierarchical reinforcement learning (HRL) into automatic disease diagnosis, which reduces the action space and improves training efficiency. Besides, the authors also expand an existing public dataset and build a synthetic dataset for evaluation. The Experimental results show that their proposed hierarchical framework achieves higher accuracy and symptom recall in disease diagnosis than existing several baselines. ","The paper tackles the problem of automatic disease diagnosis through reinforcement learning under a setting of task-oriented dialogues. The authors proposed to integrate hierarchical policies with two levels (one high-level master model, and one low-level policy) into the dialogue policy learning. Experiments on both real-life and synthetic data suggest the proposed approach is effective.","This paper applies Hierarchical Reinforcement Learning (HRL) to automatic disease diagnosis in task-oriented dialogues setting. The authors argue that applying RL to automatic disease diagnosis is challenging because the action space (i.e., symptoms) is very large. Therefore, they propose to learn a hierarchical dialogue policy where the high level policy is for categorizing patents into different groups and the low level policy is for checking symptoms and classifying diseases within a group.  Experimental results on multiple datasets show that the proposed HRL strategy achieve higher disease diagnosis accuracy compared to existing RL systems.     ",0.1724137931034483,0.3275862068965517,0.2413793103448276,0.1896551724137931,0.43103448275862066,0.3448275862068966,0.1724137931034483,0.3275862068965517,0.14736842105263157,0.1896551724137931,0.2631578947368421,0.21052631578947367,0.1724137931034483,0.3275862068965517,0.1830065359477124,0.1896551724137931,0.326797385620915,0.26143790849673204
327,SP:ce6a93847209a0926ed0be5190378a3f61db1935,"This paper studies the nonlinear low-rank completion of matrices and tensors. Specifically, it first presents the theoretical results showing why nonlinear deep matrix factorization is better than the ordinary matrix factorization model. Then, it proposes a model named two-mode nonlinear deep matrix factorization to make full use of the nonlinearity of the nearly square matrices. The authors also extend this method to tensor factorization by further factorizing the factor matrices in the Tucker decomposition using the deep factorization method. Impressive results are obtained using both synthetic and real-world datasets.","To bridge the gap between deep learning and tensor decomposition, this paper presents two novel approaches named as two mode non-linear deep matrix factorization and multi-mode nonlinear deep tensor factorization (extension of two mode model to multi-mode scenario). The main contribution of the methods lie in full exploration of non-linearity of data in matrix and tensor factorization. To better motivate the proposed models, the authors provide theoretical analysis for why and when nonlinear deep matrix factorization outperforms linear deep matrix factorization in matrix completion. The experimental evaluation demonstrates that in some datasets, the proposed models outperform the existing models on matrix and tensor completion tasks.","This paper summarizes the theoretical guarantee for the existing LRMC and LRTC algorithms, and provides the theoretical analysis for a new proposed Multi-Mode Nonlinear deep tensor factorization. The analytical results show that when n2 is larger than n1, the nonlinear DMF provides a tighter generalization bound than MF. Similar analysis has been extended to two-mode matrix factorization and multi-mode  tensor factorization. Experimental results in synthetic data and real data show better results of the proposed algorithm as compared to other algorithms in completion tasks. ","The paper provides a multi-mode framework for the deep learning based tensor decomposition, which could be useful for dealing with nonlinear high-dimensional data sets. In particular, it extends the deep matrix factorization (DMF) method and proposes a multi-mode deep matrix factorization method for matrix completion with convergence guarantee. Based on this, it also develops a multi-mode nonlinear deep tensor factorization method with convergence guarantee. The proposed models are solved by various optimization algorithms. Numerical experiments on synthetic and real data sets of the matrix/tensor form have shown that the proposed methods outperform other state of the arts. ",0.25,0.25,0.2391304347826087,0.2018348623853211,0.24770642201834864,0.28735632183908044,0.21100917431192662,0.26436781609195403,0.21568627450980393,0.25287356321839083,0.2647058823529412,0.24509803921568626,0.22885572139303484,0.2569832402234637,0.2268041237113402,0.22448979591836735,0.2559241706161137,0.26455026455026454
328,SP:cf781d756cf0bed5f7cdeb94be49e6d4409eeda4,"This paper introduces a novel model, HeTVAE, for probabilistic interpolation of time series that are irregularly sampled. HeTVAE builds on prior work by complementing it with a learned time-dependent output variance in the VAE and architectural improvements. The latter include a new branch accounting for the distribution of the sampled timestamps in the series and the addition of a deterministic branch bypassing the stochastic variational latent variable. The performance of HeTVAE is evaluated on multiple datasets against various baselines and via ablation studies.","This paper introduces several improvements over the previous work mTAN to better support probabilistic interpolation. Specifically, intensity encoding is introduced to make the model be aware of information about input sparsity. Also, the homoscedastic output distribution used by previous work is replaced by a heteroscedastic distribution. Experiments results show that this improved model (HeTVAE) achieves both better likelihood estimation and mean prediction compared to previous works.","This paper introduces a VAE-based model for interpolation of irregularly sampled time series. The temporal input data is mapped to a latent representation over fixed reference points with an attention mechanism, using an intensity network that allows to encode data sparsity information. This latent representation can then be used to interpolate points at new time steps. Thanks to the intensity network and the heteroscedastic output layer, the proposed HeTVAE model can capture uncertainty estimates over the interpolated points.  The model is tested on a number of datasets containing irregularly sampled points, and outperforms competing methods in the interpolation task.","The paper introduces a novel model of Variational Autoencoder that deals with irregularly sampled time series with a probabilistic approach to do time series interpolation.  The main contribution is the architecture by itself, its components, and the training process.  The model was evaluated on both real-world data sets from the medical and climate domain and synthetic data. ",0.14285714285714285,0.27380952380952384,0.23809523809523808,0.19696969696969696,0.15151515151515152,0.19,0.18181818181818182,0.23,0.3448275862068966,0.13,0.1724137931034483,0.3275862068965517,0.16,0.25000000000000006,0.2816901408450704,0.1566265060240964,0.16129032258064518,0.24050632911392403
329,SP:cf857736e3dc01325948488c791cbafc24b1c0fe,"This work proposes a Hybrid Neural Pareto Front (HNPF) framework to solve multi-objective optimization problems. The proposed method needs to first generate a huge number of feasible solutions to cover the whole decision space (e.g., random sampling in this work), and then use a two-stage approach to select the Pareto optimal solutions. In the first stage, it builds a Fritz-John Condition (FJC) based neural network to verify the weak Pareto optimality for given solutions. In the second stage, it further filters the truly strong Pareto optimal subset from the obtained solutions. Experiments have been conducted on several low-dimensional problems to validate the proposed method's performance. ","This paper has two contributions. First it argues for the need to benchmark multi-objective problems  using analytic functions for convex and non-convex conditions. Second, it presents a two-stage method for finding the Pareto front. ","This paper intends to find points on the Pareto frontier of a multiobjective optimization problem. Despite other baseline methods for this problem, they claim that their proposed algorithm is suitable for non-convex optimization, and their solutions are evenly spread across the frontier. Also, their algorithm can handle constraints for the MOO problems similar to solutions proposed in existing operation research methods. Empirical evaluations show that the proposed algorithm can find Pareto points and omit non-dominated points in the final stage.",The authors present key shortcomings of MTL solvers in addition to Hybrid Neural Pareto Front (HNPF) that aims to handle non-convex functions and constraints.  The authors claim the following contributions: - New strategy for weak Pareto front identification based on Fritz-John conditions. - New Pareto filter to remove dominated points from the weak Pareto front. ,0.10810810810810811,0.12612612612612611,0.13513513513513514,0.24324324324324326,0.21621621621621623,0.15853658536585366,0.32432432432432434,0.17073170731707318,0.2727272727272727,0.10975609756097561,0.14545454545454545,0.23636363636363636,0.16216216216216217,0.1450777202072539,0.18072289156626506,0.15126050420168066,0.17391304347826086,0.18978102189781024
330,SP:cf9b6963c32d8689f7203dd41b17461676d08739,"The paper presents an alternate approach for distributional DRL via proposing an objective inspired from Cumulative Prospect Theory (CPT, Tversky and Kahneman, 1992). They use this distributional objective in conjunction with policy gradient methodology to propose a distribution policy gradient method for risk-sensitive RL. Under their approach, the distribution of the returns is optimized to maximize some chosen function of its CDF. They experiment with different such possible distributional objectives (risk profiles) on the OpenAI Safety Gym environments and show that their approach performs better than PPO. ","This paper considers a generalization of the policy gradient method to optimize for arbitrary utility functions with weightings that depend on the entire CDF (rather than the expected reward). This generalization has two aspects: (1) a utility function on top of the trajectory reward and (2) a weighting function for the CDF of the trajectory reward with respect to which the expectation is performed.  The paper derives an expression for the policy gradient and also generalizes the standard variance reduction baselines. Inspired by the PPO loss, the authors then propose a clipped version of the policy gradient calling it C3PO and evaluate this on some benchmarks from the OpenAI Safety Gym, where it is found that the conservative weightings can offer improvements over the standard formulation.","The article propose a policy gradient method for optimizing a CDF based criterion, inspired by CPT.  By varying the weighting function inside the objective, it is possible to change the risk-aversion of the agent. The authors derives the policy gradient for the aforementioned objective and propose an estimation technique for it. Then, they propose an algorithm which extends PPO for optimizing their objective. Empirical analysis is carried on to evaluate the approach on some modified Safety Gym environments, in which a fixed negative rewards corresponded to adverse events. The authors evaluate different objectives obtained by employing a Wang weighting function, with different values of the parameter $\eta$. They show that optimizing a cautious (or risk-averse) objective allow to obtain better results in terms of average reward w.r.t. optimizing an aggressive (or risk-seeking) one. By further exploring the parameter space, the author demonstrate that some risk-averse values of the parameters allow to outperform also the risk-neutral version of PPO w.r.t. the average reward objective.","Risk objectives have long been investigated in reinforcement learning (RL). Most of the focus has been on classic risk measures, like exponential utility, value-at-risk (VaR), conditional value-at-risk (CVaR), leaving out, however, the cumulative prospect theory (CPT) developed by Tversky and Nobel Prize Kahneman in 1992, which has not yet been considered.  The advantage of CPT is to better model human decision-making, still allowing a wide class of risk measures, based on the utility $u$ and weighting $\omega$.  Hence, the authors consider a new risk-aware objective. Following some derivation, they compute a sample-based estimation of the gradient of this new objective w.r.t. the policy parameters.  The authors propose a PPO-like algorithm (called C3PO), which incorporates the new, risk-aware, gradient estimator.  They perform an empirical analysis on some tasks of ""Safety Gym"", showing that proper risk-awareness helps increase the performance of classic PPO.",0.20454545454545456,0.25,0.25,0.2222222222222222,0.21428571428571427,0.1569767441860465,0.14285714285714285,0.12790697674418605,0.1437908496732026,0.16279069767441862,0.17647058823529413,0.17647058823529413,0.16822429906542058,0.16923076923076924,0.1825726141078838,0.1879194630872483,0.1935483870967742,0.16615384615384618
331,SP:cfd501bca783590a78305f0592f537e8f20bce27,"Proposes cycle self-training (CST) an unsupervised domain adaptation (UDA) algorithm that cycles between standard self-training of a target classifier on (source model generated) target pseudolabels, and a new cycle self-training step that updates backbone representations so as to generalize the target classifier to the source domain. An additional self-training objective based on the Tsallis entropy is introduced. Theoretical and empirical results are presented on several standard UDA benchmarks.","This paper proposes an unsupervised domain adaptation method based on cyclical self-training. There are two main components to the method: 1. a cyclical self-training algorithm that solves a bilevel optimization problem based on an in the inner loop which trains a target classifier with target pseudo-labels  and an outer loop that makes the target classifier perform well on the source domain by updating the shared representations in the outer loop. 2. an uncertainty measure based on the Tsallis entropy, which adaptively minimizes the uncertainty in target pseudo-labels, which replaces the standard Gibbs entropy used by self-training methods. Experiments on computer vision and NLP datasets show that proposed method outperforms baseline methods by a decent margin. Ablation studies show that both components of the algorithm---cyclical self-training and Tsallis entropy---meaningfully contribute to performance improvement. Qualitative and quantitative studies show that target pseudo-label quality is improved by the proposed approach. ","This paper presents a novel approach for unsupervised domain adaptation (UDA) based on cycle self-training. Instead of conventional self-training, there are a generated and two attached classifiers with different purposes. The source classifier, mainly trained on the source loss, is applied to infer pseudo target labels. The target classifier is trained by the self-training loss based on the pseudo labels provided by the source classifier.  The authors also propose the Tsallis entropy as the regularization term, which can be seen as a side contribution.","The paper tackles the problem of domain adaptation with a newly proposed cycle self-training algorithm. Given observed that the pseudo labels are noisy and existing de-noising methods require ad-hoc hyper-parameters for specific tasks, the authors attempt to progressively refine the pseudo labels by the capability of the network itself. With the intuition of transferring the source-domain knowledge to the target domain, the authors propose to regularize the pseudo labels with a cyclic training pipeline. Tsallis Entropy is further introduced to improve the label quality. Experiments on both visual classification and linguistic sentiment classification indicate the effectiveness of the introduced method.",0.375,0.3194444444444444,0.25,0.20512820512820512,0.19230769230769232,0.27586206896551724,0.17307692307692307,0.26436781609195403,0.17142857142857143,0.367816091954023,0.2857142857142857,0.22857142857142856,0.23684210526315785,0.2893081761006289,0.2033898305084746,0.26337448559670784,0.2298850574712644,0.25
332,SP:cfd6cf88a823729c281059e179788248238a6ed7,"In this work, the authors introduce a Motion-Aware Unit (MAU) for video prediction. Basically, they discover the previous motion and aggregate it via attention module and fusion module. In the attention module, they compute correlation between spatial states and use it as attention for aggregation the previous temporal states. In the fusion module, they design spatial and temporal update gates for fusing attentive temporal state and current spatial state.","This paper proposes a Motion-Aware Unit (MAU) for video prediction tasks. The MAU includes two modules, the attention module and the fusion module. The attention module aims to learn the correlation between the current spatial state and previous spatial states. The learnt attention map can be used for augmenting the motion information (AMI). The fusion module is used for aggregating the augmented motion information (AMI) and current spatial state. The authors have demonstrated superior performance compared to state-of-the-arts.","This paper proposes a method for video prediction that relies on a newly proposed Motion-Aware Unit (MAU). MAU acts as a temporal receptive field based on an attention mechanism that is used to aggregate features from previous frames in order to predict future frames. The aggregate features are combined with “content” features extracted from the last observed frame and the decoder is connected to the encoder via an “Information recalling scheme” that directly passes the last observed frame features from each layer of the encoder to each layer of the decoder. In experiments, the authors show superior performance against the baselines pixel based metrics and a perceptual metric.",The paper proposes the Motion-Aware Unit (MAU) for video prediction. This module attempts to model the temporal information in videos by enlarging the temporal receptive field and aggregating it through an attention mechanism. The proposed module is employed in the bottleneck part of an autoencoder where the spatial and temporal information are combined at different MAU layers. The proposed method is compared to previous works on several benchmarks for next frames prediction and early action recognition showing state-of-the-art results.,0.4714285714285714,0.2,0.32857142857142857,0.2926829268292683,0.3170731707317073,0.2018348623853211,0.4024390243902439,0.12844036697247707,0.27710843373493976,0.22018348623853212,0.3132530120481928,0.26506024096385544,0.4342105263157895,0.1564245810055866,0.3006535947712418,0.2513089005235602,0.3151515151515152,0.22916666666666669
333,SP:d07fd26d0cb245e1fd1343472dd3c8300c39752a,"This paper proposes a novel learning scheme for generative replay methods in continual learning. Different from the original generative replay method, the generated examples are treated as negative examples for new classes. Experimental results on CORe50 and ImageNet-1000 demonstrate the effectiveness of the proposed method.","In Continual Learning (CL) scenarios, storing data from previous experience is helpful to mitigate catastrophic forgetting. However, privacy issues or storage overhead makes replay methods impractical. Aware of this problem, generative models have been proposed in previous work to generate data that represent previous experiences. Still, these methods suffer from low performance because of the continual training of the generating model and/or the generation of high dimensionality data. In this paper, the authors propose Generative Negative Replay. Instead of using the generated data as a positive example of previous classes, they used it as a negative example for the classes present in the current experience. While training with the negative examples, the authors propose to freeze the weights of the classification head corresponding to the previous classes, similar to what happens in CWR. This solution is particularly relevant in situations where there are not as many classes from experience.",The paper proposes a method to make generative replay for continual more effective even if generated data quality is not perfect. The method uses replayed data only as negative samples for current tasks and not as training samples to remember. They apply their method on one model on Core50 and ImageNet data. The replay process is realized in the latent space.,"The key idea proposed in this paper is to use generative replay as negative samples to improve performance in class-incremental scenarios of continual learning. The authors argue that samples from a generative model have a lot of artefacts due to challenges in training/adapting a low resource generative model. Hence, its use as a positive sample for future experiences fails. But, these imperfect samples can still be relied on as negative samples for classes being trained in the current experience to prevent ""learning in isolation"" problems. The proposed approach is compared to positive replay, negative replay and no-replay baselines for two complex datasets in both NC and NIC scenarios.",0.3695652173913043,0.3695652173913043,0.34782608695652173,0.1,0.18666666666666668,0.26229508196721313,0.11333333333333333,0.2786885245901639,0.14414414414414414,0.2459016393442623,0.25225225225225223,0.14414414414414414,0.173469387755102,0.3177570093457944,0.20382165605095542,0.14218009478672985,0.2145593869731801,0.18604651162790695
334,SP:d09c2fad308249261a9742505e4ccaed2b3578b3,"This paper proposed a training-free solution by using only good representations to detect noisy labels. The author designed two detection methods of voting and ranking to filter instances that are likely to be corrupted. The major contribution is proposing a training-free solution to efficiently detect noisy labels, which is different from most current methods. Also, theoretical analysis is conducted on the worst-case error bound and the choice of K.","The authors propose a training-free approach to detect samples with noisy labels by leveraging representations learned from pre-trained models. The models can be obtained by either supervised or self-supervised pre-training. The authors then argue that samples in the pre-trained manifold should be closer if they share the same clean label, therefore one can use (a) local voting or (b) ranking methods to detect samples with corrupted labels. Experiments on CIFAR10/100 show improvement over other learning-based approaches (CORES, CL, TracIn).","The paper proposes a training-free instancewise noise label detection method. The main motivation of this paper is the observation that deep models generalize poorly because memorizing noisy labels in supervised training while using only representations may avoid this issue.  The authors suppose a good representation extractor is given and generate the initial soft labels based on the clusterability of representations using kNN. Followed by that,  they perform a local voting and global ranking-based scoring system to detect the corrupted labels. The main contribution of this paper is: It introduces a representation-based method instead of directly training a deep model on the corrupted data, which is more efficient and may avoid overfitting noisy labels.  ","This paper proposes a method to detect noisy labels given good representations (e.g., ones pre-trained by contrastive learning approaches).  The proposed noisy-label detection method uses  the neighborhood information defined by a good set of representations in two ways:   1) checks the noisy label consensuses of nearby representations  2) scores each instance by its likelihood of being clean and filters out a guaranteed percentage of instances with low scores as corrupted ones   The work provide definitions for good representation and further proves a worst-case error bound for the ranking-based method given a 'good enough' representation. It also provides empirical results for its proposed method. ",0.2222222222222222,0.3194444444444444,0.2777777777777778,0.2441860465116279,0.19767441860465115,0.1810344827586207,0.18604651162790697,0.19827586206896552,0.18518518518518517,0.1810344827586207,0.1574074074074074,0.19444444444444445,0.20253164556962022,0.24468085106382978,0.22222222222222224,0.20792079207920794,0.1752577319587629,0.1875
335,SP:d254b38331b6b6f30de398bae09380cd5c951698,"The paper tackles the problem of robustness against multiple perturbations and proposes extreme norms adversarial training (E-AT) that adaptively alternates between $\ell_1$ and $\ell_\infty$-norm. Furthermore, the paper fine-tunes Gowal et al. (2020) to improve its multi-norm robustness. Finally, the experiments are conducted on CIFAR-10 and ImageNet with APGD for training, showing the proposed method's effectiveness.","This paper addresses the problem of multiple perturbation adversarial robustness for attacks subsumed within $\ell_p$ regions for $p\in{1,2,\infty}$. The main contribution of this work is to show how a model robust to a particular attack type (typically $\ell_\infty$) can be fine-tuned (at low cost) to be robust against multiple (or alternate) perturbation types. The authors build on prior formalization about the geometry of $\ell_p$ balls (by Croce et. al.) to empirically demonstrate its effect. The results are convincing and evaluated against AutoAttack which is","This paper mainly studies the problem of defending multiple norm adversarial perturbations. The authors propose extreme norms adversarial training (E-AT), which leverages different geometry of the $\ell_{p}$-balls to conduct adversarial training by adaptively alternating between the $\ell_{1}$ norm and $\ell_{\infty}$ norm. They also show that using E-AT fine-tune could turn $\ell_{p}$ robust model into a model that is robust against the union of $\ell_{p}$ adversarial perturbations. The authors also provide some theoretical proof for their method. ","This paper proposes a method to produce image classifiers which are adversarially robust against multiple $\ell_p$ threat models—in particular, against $\ell_1$, $\ell_2$, and $\ell_\infty$ attacks. The method involves training against $\ell_1$ and $\ell_\infty$ attacks with the hypothesis that this will additionally give robustness for $\ell_p$ threat models with $1 \leq p \leq \infty$. This hypothesis is supported by prior results that proved that affine classifiers robust to $\ell_1$ and $\ell_\infty$ threat models are also be robust to other $\ell_p$ threat models. The authors test their method on CIFAR-10 and ImageNet for both training classifiers from scratch and for fine-tuning robust models trained on one $\ell_p$ threat model to the other $\ell_p$ threat models.",0.2857142857142857,0.38095238095238093,0.3333333333333333,0.22826086956521738,0.2717391304347826,0.25882352941176473,0.1956521739130435,0.2823529411764706,0.1640625,0.24705882352941178,0.1953125,0.171875,0.23225806451612904,0.3243243243243243,0.2198952879581152,0.23728813559322035,0.22727272727272727,0.20657276995305163
336,SP:d2656ae0259accc5207234fc4206f6f7be9598d9,"This paper proposes to use Cluster Learnability (CL) and Intrinsic Dimension (ID) to evaluate the representations learned by self-supervised learning methods. The authors collected 30 checkpoints and show that their method is more predictable when compared to other methods, e.g., alignment and uniformity. Moreover, the authors modified the labels generated by K-means in DeepCluster to improve the learnability of the representations and the results demonstrate slight improvements. ","This paper tries to give a measurement method to evaluate the learned model, which is highly correlated with the final test accuracy. The measurement method depends on two key factors: Intrinsic Dimension (ID) and CLuster Learnability (CL). This paper claims that the model with higher ID and CL performs better. Using ID and CL to predict top-1 accuracy can achieve a Pearson correlation coefficient of 0.93, which is better than existing predictors (e.g., alignment and uniformity). Inspired by the above observation, this paper proposes a modified DeepCluster algorithm to increase the final performance.","This paper proposes Intrinsic Dimension (ID) and Cluster Learnability (CL) as an alternative solution to efficiently evaluate learned representation quality of pre-trained networks, without using down-stream tasks/labels. These two frameworks are inspired by finding “expressiveness” – finding the smallest number of variables to approximate the representation; and “learnability” – number of data samples needed for KNN clustering.  Extensive experiments have been evaluated on models trained with both supervised and unsupervised methods, and results showed that ID + CL would produce a high correlation with prediction accuracy from these methods, comparing to baselines. Finally, the authors argue that CL can also be incorporated into DeepCluster (an unsupervised method), as an auxiliary loss to further improve the prediction performance. ","This paper proposes two metrics for assessing the quality of self-supervised learning representations in terms of expressiveness and learnability. Expressiveness expects to maximize the mutual information between the representation and the original data, while learnability emphasizes the representation suppose to be simpler and more learnable. Based on these two intuitions, the author applies Intrinsic Dimension (ID) and Cluster Learnability (CL) as the measurements of expressiveness and learnability respectively to predict downstream classification performance. The authors collect 30 checkpoints of recent self-supervised methods to validate the proposed metrics on two datasets, ImageNet and STL_10, and the results show a high correlation between the proposed metrics and the downstream classification performance.",0.3,0.3,0.3,0.21875,0.2604166666666667,0.23076923076923078,0.21875,0.1794871794871795,0.1875,0.1794871794871795,0.22321428571428573,0.24107142857142858,0.25301204819277107,0.22459893048128343,0.23076923076923075,0.19718309859154928,0.2403846153846154,0.23580786026200876
337,SP:d369e2144544908fbcaaa53aab9555d71080ced8,"This paper introduces a systematical framework to discover the relationship between the brain representations of programs and their corresponding code models. This framework helps us to understand the code properties encoded in the human brain so that we could evaluate whether ML models faithfully represent human brain representations of computer code comperhensions.  This paper focuses on answering two questions by showing the results of related experiments on a dataset of 72 programs and 24 persons' brain recordings: First of all, the authors show that how well each of the four brain systems considered in this paper including Multiple Demand, Language, Vision, and Auditory systems encode specific code properties using a ridge regressor. Then they demonstrate another ridge regressor that can map brain representations to the corresponding learned representations by computational language models of code with different model complexity. ","This work examines the relationship between fMRI recordings of people who read short programs and different properties and representations of the programming code. The aim of the work is to understand what properties of code are encoded by different brain systems, and to understand how similar the representations of code in the brain are to those encoded by self-supervised language models that are pretrained to encode programming code. The authors find that several program properties can be significantly decoded from 3 brain systems (the multiple demand system, the language system, and the visual system). They further find that representations of the programs extracted from several machine learning models of varying complexity can also be significantly decoded from these brain systems.   ","This paper investigates encoding of computer code in the human brain. The author build decoding models for fMRI responses to predict 1) various properties of python code and, 2) representations of python code derived from different machine learning models. The main conclusion is that the responses from the Multiple Demand system in the brain is capable of provide significance decoding performance of properties and model representations of computer code, such as runtime information.","In this work, the authors explore the relationship between 4 different brain regions (multi-demand network, language network, visual system, auditory system) and different features of program code. Specifically, they look at hidden state representations of code language models (seq2seq, CodeBERTa, CodeTransformer, XLNet), tf-idf and BoW representations of the input. For non-LM based features, they look at a code vs. sentence contrast, variable language (English vs. Japanese variable names), data types (strings vs. numerals) and control flow (for vs. if vs. no branching) To analyze relationships between the BOLD signal and stimulus features, they build linear classifiers/regressors from the BOLD activity of each system to each of the stimulus features. Models are evaluated using classification accuracy and linear correlation for regression. In the case of hidden-state features, model are evaluated using rank accuracy.  Overall, the authors find that the visual system is capable of significantly predicting several of the hand-crafted code features suggesting that these features are correlated with low-level stimulus properties like program length. While differences between MD & LS are not significant, these models successfully predict 5/6 hand-crafted features. In the code representation prediction task, the authors find that the MD, LS and Visual systems are able to rank significantly above change. However, the LS and visual systems do not beat a random token embedding baseline. Aside from CodeBERTa, the MD system is also not significantly above the random baseline.",0.2463768115942029,0.15942028985507245,0.1956521739130435,0.2066115702479339,0.2892561983471074,0.3013698630136986,0.2809917355371901,0.3013698630136986,0.11297071129707113,0.3424657534246575,0.14644351464435146,0.09205020920502092,0.2625482625482625,0.20853080568720378,0.14323607427055704,0.2577319587628866,0.19444444444444445,0.14102564102564102
338,SP:d39765dcc8950d4fc1d43e4c167208736578882e,"Neural processes (NPs) aim to stochastically complete unseen data points based on a given context dataset. This paper incorporates the stochastic attention in NPs to capture the context information. We empirically show that our approach on 1D regression, predator-prey model, image completion, and MovieLens-10k dataset.  ","This paper proposes a neural process enhanced with stochastic attention to focus more on the context dataset. The method replace the classical attention used in ANP with the Bayesian attention module, showing that this design choice improves the performance also in noisy scenarios or when target datasets mismatched (by changing the kernel used to generate the target dataset). Moreover, the paper offers an interpretation of the method from an information theory perspective, proving that the NP with stochastic attention can be seen as a regularization of the latent space such that it pays more attention to the context dataset. The method is tested on both synthetic and real-world datasets and improves the scores especially in noisy or more complicated scenarios.","This paper proposes to improve attentive neural processes (ANP; Kim et al.) by replacing deterministic attention with a Bayesian attention module. Since the inference requires variational approximation, I think the proposed method is a variational counterpart of ANP, similar to the relation between VAEs and autoencoders.","The paper proposes a novel method for the Attentive Neural Processes paradigm by adding stochasticity to the weights of the cross-attention module between the context and target representations. These weights are drawn from a proposal distribution which is a Weibull distribution with parameters determined by the context and target input points. A new KL regularization term is added to the total loss to enforce the proposal distribution be close to the gamma distribution determined only by the context points, in a similar fashion to the regularization term for the latent representation between the whole set of target and context points and that of the context points only. The authors include, from Fan’s paper, the closed-form KL divergence between the Weibull and Gamma distributions, making the whole loss be differentiable w.r.t. the network parameters thanks to the reparametrization trick, as in the original NP paper. The new loss is linked to the gain of the target distribution by means of information theory, by making the assumption that the new regularization implies not only that the mutual information between the target distribution and the target points is maximized, but also the mutual information between the representation for the context and target points and the latent representation of the context variables only is minimized, enforcing the latent variable to also consider the context points, and not otherwise as suggested by the original NP formulation. ",0.3191489361702128,0.14893617021276595,0.2978723404255319,0.14049586776859505,0.2727272727272727,0.3695652173913043,0.12396694214876033,0.15217391304347827,0.059322033898305086,0.3695652173913043,0.13983050847457626,0.07203389830508475,0.17857142857142858,0.15053763440860216,0.0989399293286219,0.2035928143712575,0.18487394957983194,0.12056737588652482
339,SP:d3ff3012c614638c8d86322cfe461a9383f082ab,"This paper proposed a new model-based offline RL algorithm, COMBO. The algorithm applies model-based learning to be able to generate random rollouts, and the generated data is combined with the given fixed dataset for value function training. The paper shows by using the generated rollouts as data for Conservative Q-learning provides better constraint and better empirical performance compared to the orignal CQL algorithm. ","The paper presents a new approach for model-based offline RL. Here, the model is not used to estimate the return directly using roll-outs, but to generate synthetic data that is then used to train the Q-function. In a way, the method is a combination of CQL and MOPO. What is new is the idea of using this synthetic data to, penalize extrapolation instead of using uncertainty estimation. This modification performs much better on the selected benchmarks. As benchmarks a selection of D4RL benchmarks is used (3 benchmarks) and two vision based benchmarks. ","This work investigates model-based reinforcement learning with offline dataset. In particular, most of the work on mbrl with offline datasets requires uncertainty estimation to determine the out-of-distribution states and actions. This work eliminates the need to have uncertainty estimation by using the existing Conservative Q-Learning(CQL) algorithm for states and actions from the dataset as well as the rollouts generated from the dynamics using the policy being learned. They show that inclusion of new dataset from the generated rollouts and by being pessimistic about their values, they tend to be better than existing Model-based offline Reinforcement Learning which requires uncertainty estimation","This paper introduces a Conservative Offline Model-Based policy Optimization RL algorithm, called COMBO, that learns a pessimistic model by enabling a lower bound optimization on the policy performance without requiring uncertainty quantification. COMBO employs an actor-critic method to learn the value function on both offline and synthetic datasets. The idea is to penalize Q-values learned from state-action pairs that are out-of-support of the offline dataset.  ",0.3181818181818182,0.25757575757575757,0.24242424242424243,0.17894736842105263,0.17894736842105263,0.1509433962264151,0.22105263157894736,0.16037735849056603,0.22535211267605634,0.16037735849056603,0.23943661971830985,0.22535211267605634,0.2608695652173913,0.19767441860465118,0.23357664233576642,0.1691542288557214,0.20481927710843373,0.1807909604519774
340,SP:d44f0ebc2847695ecb4ed0bb3df61d6cd8cc6a40,"This paper studies the expressivity, complexity and unpredictability of emergent languages in referential games. The authors demonstrate that the expressivity of emergent languages is a trade-off between the complexity and unpredictability of the context that the languages are used in. They also introduce a contrastive loss based training method for referential games that alleviates the collapse of message types often seen when using other standard training methods.","The authors investigate what aspects of scenario design affect the resultant communication protocol in emergent communication. Specifically, the authors investigate whether scenario complexity (in terms of having many similar distractors) and scenario unpredictability (in terms of future batches not containing items to previous batches) affect the expressivity (measured in terms of learnability) of the resulting protocol.  Further, the authors show that defining a softmax loss over the entire batch of possible references (dubbed the 'contrastive' loss) outperforms the 'referential' loss used in previous works. ","This paper studies the ""expressivity"" of emergent language in language games. To my understanding, ""expressivity"" is empirically the transferring ability of the language to unseen data. In this paper, the authors propose two factors of the underlying language game that can affect the expressivity of emergent language: context complexity and unpredicability. In referential games, the context is the distractor candidates for a sample. Complexity denotes how likely a ""close"" distractor will be included in the candidates. Unpredictability is how likely the context for a single sample will be different among epochs. This paper proposed hypotheses that both context complexity and unpredicability can improve language expressivity, but these two factors are ""contradictory"" and we need to have a trade-off between them. The hypotheses are supported by empirical experiment results.","The paper studies the properties of emergent languages in DL-based language games. In particular, they look at referential games where speakers emit messages and listeners need to identify the target object observed by the speaker from some set of candidate objects.    They define the expressivity of an emergent language as the amount of discriminatory information required to encode the inputs so that a listener can correctly decode them. In this setting, complexity refers to the similarity between different objects in a given context. In a context with higher complexity, more similar objects will be present, and more discriminatory information will have to be encoded so that a listener is capable of identifying the correct target. In their definition, the notion of unpredictability can be thought of as how stable the information necessary for encoding is across different trials.    Their primary contribution is to support the hypothesis that the expressivity of emergent language is determined by (and a trade-off between) the complexity and unpredictability of context in language games. The authors introduce a new measure to evaluate expressivity based on partial ordering between languages in terms of their generalization across tasks. They argue that mutual information is not the most appropriate measure to evaluate the expressivity of languages. They propose a contrastive loss which they show helps mitigate the issue of the collapse of message types. ",0.23529411764705882,0.38235294117647056,0.5588235294117647,0.20238095238095238,0.2857142857142857,0.31007751937984496,0.19047619047619047,0.20155038759689922,0.16740088105726872,0.13178294573643412,0.10572687224669604,0.1762114537444934,0.21052631578947367,0.2639593908629442,0.25762711864406784,0.15962441314553993,0.15434083601286175,0.2247191011235955
341,SP:d4ce49411198fe65b8f4c2d80af222e0732a4728,"This paper conducted an experimental study over a range of tricks that are often exploited to facilitate ensemble deep reinforcement learning. The experiment results show several interesting findings. For example, it was found that commonly used additive action noise may not be necessary for effective exploration. Meanwhile, experiments show that the initialization of critics perhaps has a higher impact on learning performance than the initialization methods adopted for actors. These findings can be quite important to guide future design of more effective ensemble reinforce learning algorithms.","The paper presents an empirical study evaluating the commonly accepted design choice in off-policy Deep RL algorithms in continuous control settings. The use of additive exploration noise, initialization choices, update frequency, and precision for retraining are tested empirically highlighting some interesting results. The paper also introduces ED2 - an ensemble method utilizing the design choices from the study which is demonstrated to achieve SOTA results on Mujoco benchmarks.","This paper has two main contributions: it introduces an ensemble-based actor-critic method, and it answers some pertinent questions in policy optimization by focusing on its different components. The ensemble is different from multi-actor learners that interact with multiple environments simultaneously, violating the standard RL setup. Instead, the learner of this paper maintains multiple actors and critics but uses only a single actor at a time to interact with the environment. All actors and critics are trained on a common replay buffer. The base method is the streamlined off-policy (SOP) method, which unlike soft actor-critic (SAC) doesn’t use an entropy bonus. Additionally, no exploration noise is added, resulting in their Ensemble Deep Deterministic (ED2) method.  The proposed algorithm ED2 is shown to be superior and more stable in performance according to different measures compared to existing methods. It is also revealed that actor initialization affects performance less than critic initialization. ED2 uses deterministic actors, and its exploration comes from sampling among the actors. Such a form of exploration is also shown to be superior to UCB-style exploration. ","This paper presents a deep reinforcement algorithm, Ensemble Deep Deterministic Policy Gradients (ED2), for continuous control tasks. The algorithm is empirically derived and is claimed to represent SotA performance on several tasks and while providing more stable results. These claims are justified based primarily on the (reward and stability) results on 4 MuJoCo environments.  ",0.12790697674418605,0.19767441860465115,0.10465116279069768,0.2647058823529412,0.19117647058823528,0.08196721311475409,0.16176470588235295,0.09289617486338798,0.16666666666666666,0.09836065573770492,0.24074074074074073,0.2777777777777778,0.14285714285714288,0.12639405204460966,0.1285714285714286,0.14342629482071714,0.21311475409836064,0.12658227848101267
342,SP:d5608d3317c2b246375eb14006b9e6a6026e0ab6,This paper proposes a simple and effective method(VICReg) that prevents the collapse in self-supervised learning with two regularizations terms. The variance term maintains the variance of the embedded vector along each dimension independently. The covariance term prevents the network encode similar information to the same dimension in the embedded space.,"This paper proposes a Variance-Invariance-Covariance regularization (VICReg) for self-supervised learning. The total loss function is the weighted sum of three terms: 1. distance between two views of the same input batch; 2. hinge loss on the variance of the embedding variables, encouraging them to stay away from zero; 3. loss on the covariance between different embedding variables, encouraging them to be close to zero, and hence preventing a representation collapse. The proposed model does not require the embedding branches to be identical, nor does it require mining of contrastive pairs. Experiments are conducted to demonstrate the effectiveness of the proposed regularization.","The authors propose a self-supervision technique with a more flexible structure requirements such as  not requiring batch norm, normalization, weight sharing or quantization.  Their model uses 2 encoders and a loss function with 3 variance and covariance regularization terms.  The authors mention that compared with the previous works, their main novelty lies in one of their variance terms which adjusts the variance of each input dimensions and, in return expected to mitigate collapsing problem. The authors claim that not sharing weights and having regularization terms separate on each encoder branch make their model better suited for multimodal inputs and tasks. They show results mainly on ImageNet classification, object detection tasks, with an ablation study on their and other baseline methods by varying regularization terms.","Summary: This paper proposes a new self-supervised learning method. It makes self-supervised learning very elegant: it does not require techniques such as weight sharing between the branches, batch normalization, feature-wise normalization, output quantization, stop gradient, memory banks, etc., and achieves results on par with the state-of-the-art results on several downstream tasks. It does not require that the inputs be of the same nature. This opens the door to the use of non-contrastive self-supervised joint-embedding for multi-modal signals, such as video and audio. I really like this paper. Still, I have some concerns about this paper. ",0.38461538461538464,0.23076923076923078,0.3076923076923077,0.15384615384615385,0.23076923076923078,0.144,0.19230769230769232,0.096,0.1523809523809524,0.128,0.22857142857142856,0.17142857142857143,0.2564102564102564,0.13559322033898305,0.20382165605095542,0.13973799126637554,0.2296650717703349,0.1565217391304348
343,SP:d681e4e28c03f610acf6817a9e57db0c41c196b4,"This paper tackles the question of whether neural networks can be provable better than their induce neural tangent kernel for some problem setting. This question is answered in the context of a binary classification problem, in which the signal is shift invariant and embedded in a background of noise. The authors show that for this problem, a two layer neural network with relu activations and logistic loss efficiently learns (in sample and time complexity) this problem class with constant number of neurons, whereas its neural tangent kernel counterpart requires at least order d - the dimension of the (local) inputs. The authors then illustrate this basic problem in image classification settings where the problem setting is conceptually present, with results that align with their results in the toy-like distribution. ","The authors study a toy binary classification problem where a certain CNN architecture is trained to detect one of two templates embedded in gaussian noise. The motivation is twofold: the problem is argued to capture relevant structures present in natural image classification tasks where CNNs succeed in practice (i.e., the detection of an object/motif in the image in the presence of a nuisance background), and the theoretical analysis establishes an architectural separation via the rates one gets by analyzing the problem with the neural tangent kernel approach and via the authors' analysis. Concretely, the data model consists of a fixed unknown template embedded in one of $k+1$ consecutive disjoint length-$d$ blocks of a length $d(k+1)$ 1D signal, with the other $k$ blocks containing i.i.d. gaussian noise of a certain variance (the blocks are imagined to be disjoint ""image patches"", and the template a motif of interest) -- samples from this data model have the patch where the template is contained uniformly randomized and contain independent noise, and the template is given either the positive or negative sign (representing the two classes in the problem). The noise variance is set to be large enough that it is not possible to perform naive matched filtering to recover the template. Accordingly, the CNN that is trained to solve this task contains a custom denoising nonlinearity, the standard soft thresholding function -- this corresponds to a one-layer, two-neuron ReLU network that computes two $d$-strided convolutions with specific weight sharing and output averaging (the shared biases end up corresponding to the soft thresholding function's parameter) -- which is trained using finite samples from the data generating distribution by mini-batch SGD on the logistic loss with gaussian random initialization. The authors show that this procedure yields a classifier for the data whp given polynomial samples and suitable step sizes; in contrast, they show that the network's convolutional NTK is unable to solve the task whp as long as the number of filters satisfies $o(d)$ (the neural network studied corresponds to a two filter network). Experiments involving embedding CIFAR-10 images into random backgrounds (either from ImageNet or having gaussian noise) with varying intensities and comparing neural network performance to their NTK's performance computationally on the classification tasks thus generated verifies the predicted performance gap.  ","The paper provides a theoretical analysis comparing the training dynamics and performance of a specific 1 hidden-layer convolutional neural network with those of its corresponding finite width CNTK on a simple data distribution. This analysis is used to support their claim that the gap between neural networks and kernel methods may lie in the ````''Local Signal Adaptivity'', in the sense that neural networks outperform kernel methods in distinguishing localized influential features from noisy background. Furthermore, they empirically identify a performance gap between the CNN and finite width CNTK on noise extended variants of the CIFAR-10 dataset, which emphasizes the influence of the ``''Local Signal Adaptivity''. The authors conclude that this setting degrades the performance of kernels in a more pronounced way than it affects neural networks. ","This paper introduces the notion of local signal adaptivity (LSA), which the authors claim is a new perspective on the benefits of feature learning in NNs over kernel methods like the NTK. LSA denotes the ability of non-linear NNs to find a sparse signal in the presence of large noise, and the authors theoretically show that in a particular data distribution setting with a sparse signal, a single filter CNN is able to train with GD to learn the signal. On the other hand, they show that for an underparameterised linear NN (linearised at initialisation in parameter space) the model is unable to learn the signal. Experimental results are provided comparing the degradation of non-linear and linear NNs under increasing noise, demonstrating that non-linear NNs are indeed more robust to noise in line with the theory.  **update after rebuttal** raising my score to 7",0.27906976744186046,0.20155038759689922,0.2248062015503876,0.08695652173913043,0.10997442455242967,0.1875,0.09207161125319693,0.203125,0.19727891156462585,0.265625,0.2925170068027211,0.16326530612244897,0.13846153846153847,0.20233463035019453,0.21014492753623187,0.13102119460500963,0.15985130111524162,0.17454545454545456
344,SP:d6d144be11230070ae9395db70b7c7743540bad4," * This paper proposes an interesting approach to model and predict human behavior.  * The approach is called Boltzmann policy distribution (BPD). It improves the Boltzmann rational model in that BPD considers the systematic suboptimality in human behavior.     * Systematic suboptimality means that the human could be consistent in producing suboptimal behavior. Hence, to capture systematic suboptimality, it is necessary to combine the human's reward function (optimal behavior) and trajectory data (deviation from optimality) in human modeling and prediction.  * This approach predict human policies, rather than trajectories, so that it can capture the (systematically suboptimal) human behavior that is reflected in the human action choices over time.  * Approach detail: the approach follows GAN (generative adversarial networks):     * The goal is to compute the human BPD policy `= \integral \pi(a|s) * p_PBD(\pi | s1,a1,...,s_{t-1},a_{t-1})` (Eq5).         * `\pi(a|s)` is approximated as a **generator**, `f_\theta(s,z)` (Eq.6).         * `p_PBD(\pi | s1,a1,...,s_{t-1},a_{t-1})` is approximated as sampled human trajectories, `q_\theta(z | s1,a1,...,s_{t-1},a_{t-1})` (Eq.6).     * The base measure, `p_base(\pi)`,  is defined as the optimal human behavior based on the known human reward function with suboptimality parameter \beta (Eq4).     * The **discriminator**, `d`, used to distinguish between the policy generated by the base measure, `p_base(\pi)`, and the policy generated by the sampled human trajectories, `q_\theta(\pi)`.     * The networks can be seen as: `\pi(s) -> d(\pi) -> z -> f_\theta(s,z) -> \pi(s) reconstructed`.     * The training for the generator, `f_\theta(s,z)`, is in Eq8,9 and the training for the discriminator, `d`, is in Eq10.   * Results     * Apple picking gridworld with simulated human data         * Cross entropy: BPD < Boltzmann rational models     * Overcooked (prediction only) with real human-with-human play data         * Cross entropy: BPD = behavior cloning < random < PPO+self-play, Boltzmann rational models     * Human-AI collaboration (prediction and control in Overcooked) with ""human proxy"" policy from previous work         * Mean return: human-aware RL policy (prev work) > policy based on Boltzmann rationality, self-play policy, behavior cloning policy         * Mean return: policy based on BPD > policy based on Boltzmann rationality, self-play policy, behavior cloning policy         * Mean return: policy based on BPD > Human-aware RL policy (prev work) in 2 out of 3 cases. ","The paper proposes a new model BPD to account for the suboptimality of human behavior, and provide an approximation inference method for BPD. The authors illustrate BPD through a simple simulation and compare BPD with existing methods on the overcooked game. They show that their model can match the performance of the data-extensive BC method and outperform BR. They further evaluate their model on human-AI collaboration and show that their model can match or even outperform the human-aware RL model.","The goal of this paper is to train models that imitate human behavior using limited samples. A human reward function is given but human behavior may be suboptimal according to this reward function and therefore we cannot use the standard assumption, Boltzmann rationality. The authors propose an imitation learning algorithm that uses the known reward function to form a prior over human policies, the Boltzmann policy distribution (BPD). A posterior over policies is then inferred from human behavior. This approach is empirically more data efficient than behaviorally cloning human behavior. ","The paper addresses the problem of modeling human behavior in a case where their deviation from the optimal behavior is consistent over time, rather than independent (systematic suboptimality). They claim that systematic suboptimality can be modeled by predicting policies rather than trajectories. To this end, they introduce Boltzmann policy distribution (BPD), as an alternative to Boltzman rationality, as a prior over human policies. BPD enables adaption to human policies over time. While in Boltzmann rationality past behavior cannot be used to predict future behavior (since current action is independent of all the previous actions), the paper claims that since humans are consistent one can use past actions to predict future ones which leads to introducing BPD. They consider that human is sampling over policies rather than trajectories which results in the previous actions and states inducing a posterior over policies. By taking the expectation over the posterior they predict the next action. They further use deep generative models in order to sample from the BPD and minimize the KL divergence between the two distributions (P_BPD and the distribution induced by the generative network) to ensure the predicted distribution is close to the BPD.  Through experiments, they demonstrate cases where Boltzmann rationality is not able to predict the behavior of a suboptimal human while BPD can adapt to the behavior over time and predict the correct next action. They also show that their method is able to predict human behavior as well as the baseline while using fewer data.",0.06527415143603134,0.0783289817232376,0.1279373368146214,0.1566265060240964,0.2891566265060241,0.32222222222222224,0.30120481927710846,0.3333333333333333,0.19678714859437751,0.14444444444444443,0.0963855421686747,0.11646586345381527,0.1072961373390558,0.12684989429175475,0.15506329113924053,0.15028901734104044,0.14457831325301204,0.1710914454277286
345,SP:d6f11fb32851f97af287f962f83220d27a8bc76a,"This paper presents a novel approach for task-based games. The authors used an object-oriented POMDP formulation for the task. The key contribution is to learn the dynamics of the environment with a graph neural network (with either object supervision or self-supervision), and apply an online planning algorithm (e.g., MCTS) to solve the problem. The authors show strong improvements over other approaches in terms of sample efficiency and task scores.","This paper tackles the difficult problem of learning to play text-based games. This domain is particularly challenging, since the text observations received at each time-step are variable length, and will only provide a partial description of the world, from which the full state must be reconstructed. Additionally, the player must track the history of past states in order to make the correct decision.  This paper introduces a new framework for describing the time-evolution of a game: Object-Oriented Partially Observable Markov Decision Processes (OO POMDP). This work's main contribution is a learned Object-Oriented Text Dynamics (OOTD) model that learns the transition and reward function of a OO POMDP game.  Briefly, the presented method for predicting the transition function is: given a representation $z_t$ of the objects and $a_t$ of the action, create a graph, where the nodes are objects and the edges denote relationships. Perform message passing, and obtain new node representations $e_t$.  Then, (1) given $e_t$, predict a new representation $v_{t}$ using dual stream attention with the action $a_t$. Then, (2) use $v_t$ to predict the updated state $z_{t+1}$ of each object, using an independent set of parameters per object.  The dynamics model is trained using a Evidence lower bound objective (ELBo) in two varieties: one that relies on a deterministic extraction of the graph and one that learns directly from rewards and observations.  To show its effectiveness, the learned dynamics model is used to model-based training (Dyna-Q, MCTS) is used to learn a planner. The learned planner is shown to out-perform the baselines (DQN, DRQN, GATA), both on the test cases, and in terms of sample efficiency. Ablations of (1) and (2) find qualitative advantages of the proposed method, since the learned object representation are both more separable and better semantically clustered. Additionally, the ablations are shown to reduce the accuracy of the graphs and state recreated from the object representations.","This work proposes Object-Oriented Text Dynamics for model-based RL in the TextWorld environment. OOTD ensembles existing methods such as ComplEx, R-GCN to build a memory graph, which contains object-level information about each object's attribute and its relations to others, from history observations and predicts states at the next time step. Therefore, the authors can combine the learned model with Dyn-Q or MCTS methods to select actions. If I am correct, under an object-supervised setting, it assumes the existence of a dataset that contains a ground truth memory graph. In contrast, the network has to discover graphs from predicting observation and rewards in the self-supervised setting. Experiments show that OOTD achieves superior performance than various baselines, including model-free RL and previously learned graph models.",This paper studies object-oriented text dynamics (OOTD) model and planning in text-based games. OOTD learns internal representation of object dynamics and uses transition layers to predict the belief of object states. Empirical results shows a performance boost compared to model-free baselines. Ablation studies are performed to explain the importance of OOTD components.,0.3972602739726027,0.2328767123287671,0.1643835616438356,0.08484848484848485,0.07575757575757576,0.12878787878787878,0.08787878787878788,0.12878787878787878,0.21818181818181817,0.21212121212121213,0.45454545454545453,0.3090909090909091,0.14392059553349876,0.16585365853658535,0.18750000000000003,0.12121212121212122,0.12987012987012989,0.18181818181818182
346,SP:d789e92c1e4f6a44de373210cd732198a6f809be,"This paper proposes a transformer-based model, called MaskFormer for semantic segmentation task. MaskFormer treats semantic segmentation as a mask classification problem, and predicts a set of binary masks with the corresponding class labels.  The proposed approach can be easily extended for panoptic segmentaion task. Experimental evaluations for semantic segmentation and panoptic segmentation tasks demonstrate the effectiveness of proposed method.","In this paper the authors present a technique to directly predict the mask and class label for semantic and panoptic segmentation. This is different from current state of the aret where per pixel labels are produced which are then process to obtain region masks and labels. The method consists of a backbone encoder which produces an image embedding, whic is fed into the backbon decoder to get pixel level embedding and to the transform decoder, the out of the transformer decode processed by an MLP provides the mask embedding. The mask embeddings and the pixel embeddings are used to get mask prdication. Whereas the class predictions are obtained throught the MLP output.  The method is tested on semantic segmentation datsets and produce comparable or better results on four datasets.","This paper focuses on the problem of semantic segmentation. Traditional semantic segmentation methods mainly adopt the FCN kind of structure and treat the task as per-pixel classification. While in this paper, the author reformulates the task into binary mask prediction and extra-label classification. The proposed method named MaskFormer could be utilized for both semantic segmentation and panoptic segmentation tasks, and the experimental evaluations on different datasets look promising.",This paper discusses per-pixel classification and mask classification for semantic segmentation. It shows that mask classification is sufficiently general to solve semantic and instance segmentation in a unified manner. Thorough experiments are conducted on several benchmarks for both semantic and panoptic segmentation. ,0.25,0.3333333333333333,0.23333333333333334,0.15503875968992248,0.10852713178294573,0.18571428571428572,0.11627906976744186,0.2857142857142857,0.32558139534883723,0.2857142857142857,0.32558139534883723,0.3023255813953488,0.15873015873015872,0.30769230769230765,0.2718446601941748,0.20100502512562812,0.16279069767441862,0.23008849557522124
347,SP:d88f2bb3ed48deb04fae1b8f008ca69d8566819f,"In this paper, the authors propose the task of parameter prediction with a new DeepNets-1M dataset. By using a improved Graph HyperNetwork, they can achieve reasonable performance for unseen and diverse networks without iterative optimization. The learned neural architecture representation also performs better than previous works.","This paper studied the problem of parameter prediction for deep neural networks. Especially, the authors focused on the task of predicting parameters for various network structures with a single hypernetwork. The method is based GHN and introduces several important modifications. To evaluate the performance of the proposed method, a standardized benchmark is introduced.  The improved method showed impressive performance on CIFAR-10 and ImageNet even with large neural networks. Besides, the method is also proved to be useful in architecture representation learning.   ","This paper proposes and analyzes a method for generating trained weights already optimized for a given single task for a large class of diverse architectures using Graph hypernetworks. They introduce a new DEEPNETS-1M benchmark with in-distribution and out of distribution architectures. The work also proposed three main improvements to improve performance of their GHN: normalization of predicted parameters, enhanced long-range interactions in the GHN, and meta-batching of architectures.  They demonstrate that their method is able to produce weights that do surprisingly well on unseen and diverse networks in less than 1 second on CPU or GPU for CIFAR-10 and Imagenet. They demonstrate that even for large Resnet-50s which are out of the training distribution, they could achieve 60% accuracy on CIFAR10.",This paper propose a way to predict/generate the parameters of an image based deep network instead of training it via SGD. It's a thought-provoking paper and the authors appears to be very detailed in their implementation. The results are surprisingly good but not better than standard optimization algorithms.,0.3617021276595745,0.2978723404255319,0.2127659574468085,0.25609756097560976,0.15853658536585366,0.09448818897637795,0.2073170731707317,0.11023622047244094,0.19607843137254902,0.16535433070866143,0.2549019607843137,0.23529411764705882,0.2635658914728682,0.16091954022988503,0.2040816326530612,0.20095693779904306,0.19548872180451124,0.1348314606741573
348,SP:db07c2c0afdf27692dc504c9c54387c20211d469,"The authors present EDO-CS an approach to multi-objective optimisation that ensures the high quality and diversity of generated RL policies. Instead of uniformly sampling solutions from the Pareto front, sampling policies from learned clusters is introduced, as a selection mechanism within the ES optimisation. The quality and diversity are further achieved by modifying the objective function of the ES algorithm so it includes both the fitness and behaviour-diversity terms, balanced with a hyperparameter $\lambda$. This hyperparameter is set using a multi-arm bandit approach, which is adapted during the training.  The performance of the proposed approach is evaluated on several MuJoCo continuous control tasks, as well as compared to state-of-the-art QD benchmarks. ","The authors present a new method for finding robust policies in reinforcement learning, those policies which give high rewards but also come from a diversity of behaviours. The authors accomplish this by using a clustering algorithm to first divide policies into relevant clusters and then use evolutionary algorithms to optimise policy choice. The result is an algorithm that performs well over several representative benchmarks, achieving SOTA performance with high convergence rates. ","In the paper ""Evolutionary Diversity Optimization With Clustering-Based Selection For Reinforcement Learning"", the authors introduce a new selection mechanism for Quality-Diversity based algorithms. This new selection mechanism is based on the K-Means algorithm, to cluster the behaviour space into cells and then select only the best policies within each cell. The paper also uses a linear combination of the reward and novelty score when performing the policy updates. The weights the of the linear combination are automatically adjusted using a bandit algorithm so that the best reward is obtained. ","This paper performs research in the area of diversity/novelty-seeking agents under evolutionary optimization approaches. The core idea in this area is to optimize a weighted combination between raw environment reward and diversity during training (even though evaluation is still on the raw environment reward only). The diversity acts as a ""regularizer"" in order to achieve better exploration, which can lead to higher rewards.  The specific approach (EDO-CS) by the paper suggests using a pool/""archive"" $A$ of previous policies, which are then clustered via K-means according to the distance metric defined by the behavior function $b(\cdot)$. Each cluster then produces a new policy (and thus there are a total of $K$ new policies), which are then updated via ES over a regularized reward (via diversity metric, along with an adaptive weighting $\lambda$), and put back into the archive.   Experiments are performed over various continuous control benchmarks for testing exploration, and results show that EDO-CS achieves SOTA.",0.1440677966101695,0.16101694915254236,0.1864406779661017,0.22535211267605634,0.23943661971830985,0.25,0.23943661971830985,0.20652173913043478,0.13580246913580246,0.17391304347826086,0.10493827160493827,0.1419753086419753,0.1798941798941799,0.18095238095238095,0.15714285714285714,0.19631901840490795,0.1459227467811159,0.18110236220472442
349,SP:dbf896dd31627b27f0a902c716aff940e5ab7ac2,"This paper presents a deep neural network for processing small sets of points in three-dimensional space with a rotation-invariance and a permutation equivariance.  The proposed method is twofold. One utilizes a geometric product of geometric algebra on input vectors to achieve the rotation-invariant attributes. The other is a permutation-equivariant reduction over the geometric products using an attention mechanism. The authors demonstrate applications in physical science such as crystal structure identification, molecular force regression, and backmapping of coarse-graining operators. ","The paper proposes a geometric algebra attention network for small point clouds. The attention based on geometric algebra’s multivector is rotation equivariant and permutation equivariant. Specifically, attention is composed of four functions that operate on tuples and respect the desired equivariance. Moreover, the paper validates its proposed geometric algebra attention on three domain-specific applications including crystal structure identification, molecular force regression, and back mapping of coarse-graining operators. ","The paper proposes a method for learning functions that take small point clouds as input, such that they are both permutation and rotation equivariant. To aggregate information from a tuple of points in a rotation equivariant way, it utilizes the geometric product of the points' 3D coordinates. Permutation equivariance is guaranteed by the standard attention framework. The model is evaluated on three scientific tasks: Crystal structure identification, Molecular force regression, and backmapping of coarse-grained operators in molecule simulations.","This paper proposes a rotation and permutation equivariant geometric deep learning model for problems where the data is represented as small points clouds. The equivariance properties are achieved by leveraging geometric algebra formulations. More specifically, rotation-equivariance is accomplished by geometric products of multivectors and permutation-equivariance by using an attention mechanism over invariant terms of these products. This model is evaluated in three different applications showing better or comparable results than existing approaches. These models also offer additional features like the analysis of the attention maps produced.",0.3253012048192771,0.3493975903614458,0.24096385542168675,0.4,0.2714285714285714,0.24050632911392406,0.38571428571428573,0.3670886075949367,0.22727272727272727,0.35443037974683544,0.2159090909090909,0.2159090909090909,0.35294117647058826,0.35802469135802467,0.23391812865497075,0.37583892617449666,0.24050632911392406,0.22754491017964074
350,SP:de6c4c1a418d1ebadc294d77dda18612c163d9c0,"The authors of this paper consider the individual fair k-clustering problem, as introduced by Jung et al. [FORC 19]. In this model, for every point $j$ there is a given radius $r_j$, and the goal is to choose a set of centers such that each point has an open center within distance $r_j$ from it, and also one of the standard clustering objective functions is minimized.  Specifically, under the k-median and k-means objectives, the authors provide improved guarantees compared to the state of the art results (Mahabadi and Vakilian [ICML 2020]). For the k-median objective, they give an $(8,8)$-bicriteria algorithm, while the best known result was an $(84,7)$ one. For k-means their result is an $(4,8)$-bicriteria algorithm, while the result of Mahabadi and Vakilian was a $(\gamma,7)$ one, with $\gamma$ being a very large constant. (an $(a,b)$-bicriteria solution has objective cost at most $a$ times the optimal, and for every $j$ it has an open center within distance at most $b \cdot r_j$ from it).  The authors achieve their results via standard LP-rounding techniques.","This submission proposes an approximation algorithm for individual fair k-clustering in a metric space. Here, individual fairness means that each point v has an individual radius $r(v)$ and we require an open facility within radius $r(v)$ of $v$. We may use the radius $r(v)$ to provide each point $v$ with a ""fair share"" of its assigned facility by, for instance, choosing $r(v)$ as the smallest radius such that $n/k$ points lie within radius $r(v)$ of $v$; as originally proposed by Jung, Kannan, and Lutz.   The approximation algorithm is the main contribution of the submission. It works for any p-norm objective and is bi-criterial, guaranteeing an $2^{1+2/p}$-approximation of the  objective while ensuring an open facility within radius $8r(v)$ of each point $v$. However, it requires a metric space which means, in particular, that all facilities/centers have to be chosen from the point set.  The algorithm roughly works in the following way: First, the authors compute an optimum solution to a linear programming relaxation of the problem. The solution is turned into an input for a filter function by Plesnik/Rita, Moro and Cortez; this function replaces the original point set by a smaller set of representatives. In the original version of the filter, the function may be run until only $k$ representatives remain (a point may represent another if it can serve as the point's facility; hence, in the original filter, the representatives are then chosen as the facilities to open). In this case, however, it may be that the filter stops early as the fairness restriction on the connection radius may make it impossible to cover all points with $k$ representatives. Hence, a final step is required to reduce the set of representatives down to $k$ points. This is done via an LP-rounding algorithm and turns out to be a substantial amount of work. Finally, in order to improve the running time of the algorithm, the authors propose a sparsification technique for the LP-relaxation (whose size is otherwise quadratic in the number of points and could be prohibitive for larger point sets).  An empirical part shows improvements over the previous approach.",The paper studies the problem of \ell_p norm and center-based clustering with a fairness consistent. The fairness constraint roughly says that no point should be further from its assigned center than it is from its (n/k)-th furthest neighbor. The authors consider a Linear Programming based (approximation) algorithm for this problem. ,This paper introduces a new algorithm for fair k-clustering problem(specifically when cost is measure with l_p norm for different p values) based on LP rounding. It provides a bi-criteria approximation guarantee for the clustering objective and experiments are provided to validate the theoretical properties. It is also shown how the runtime of the algorithm can be improved using sparsification with minimal effects on the objective.,0.24083769633507854,0.08376963350785341,0.09947643979057591,0.04878048780487805,0.06775067750677506,0.2037037037037037,0.12466124661246612,0.2962962962962963,0.2753623188405797,0.3333333333333333,0.36231884057971014,0.15942028985507245,0.16428571428571428,0.1306122448979592,0.14615384615384613,0.0851063829787234,0.1141552511415525,0.1788617886178862
351,SP:dff08f0b290f3d138fd0299933052f3dc363b2d3,"The central part of the paper is the design of a learned positional encoding (LPE) that can be used in graph transformers in a similar way the positional encoding is used in text transformers. The authors propose to use eigenvalues and eigenfunctions to encode node positions in a graph, and demonstrate how to incorporate them into a transformer model that achieves high expressivity. The architectural choices are based on the theoretical introduction in which some concepts are borrowed from physics, such as the analogy to electric potential between nodes. The LPE is designed to address five principles that were overlooked by different methods. The authors also implement a new model, Spectral Attention Network (SAN), that utilizes the LPE resulting from the theoretical findings. The experimental results confirm the efficacy of the method.","This paper presents an interesting idea on designing a novel graph transformer with learning positional encoding by a spectral attention network (SAN) using the full Laplacian spectrum. Three theoretical limitations from previous efforts are overcome by this spectral attention, namely WL test and universality, over-squashing and physical interactions. Experimental results on four standard datasets demonstrate the effectiveness of the proposed SAN.","This paper proposes a learned position encoding mechanism which can fully make use of the eigenvalues and eigenvectors information from the graph Laplacian. The learned position encoding is then concatenated with the embedded node feature. In addition, the edge feature is also served as an input feature in order to completely leverage the graph structural information. Also, by fully connecting the graph, it helps tackle the common over-squashing problem in many GNNs. The paper also proposes an alternative version of attention so that real edges and added edges are taken into account separately when computing the attention. Eventually, the paper empirically tests the performance of the proposed model on real graph datasets.","This work proposes a transformer designed for graph structured data. As opposed to GNNs, the transformer allows non-neighboring nodes to communicate. To avoid loosing information on the structure of the graph, the transformer needs some form of structure encoding of the nodes, a non trivial problem for graphs. A common PE scheme consists in using the spectrum of the graphs Laplacian but, as detailed by the authors, this raises some issues. The authors propose an improved positional encoding circumventing these problems and demonstrate that a fully connected transformer is competitive with GNNs on four datasets.",0.13636363636363635,0.19696969696969696,0.12121212121212122,0.22580645161290322,0.24193548387096775,0.17699115044247787,0.2903225806451613,0.23008849557522124,0.16666666666666666,0.12389380530973451,0.15625,0.20833333333333334,0.18556701030927836,0.2122448979591837,0.14035087719298245,0.16,0.189873417721519,0.19138755980861244
352,SP:e0432ff922708c6c6e59124d27c1386605930346,"This paper presents a method on generalization of segmentation from synthetic data to real street scene data. To adapt the model pre-trained with synthetic source domain data such as GTA and SYNTHIA to target domain such Cityscapes, BDD and IDD, the authors proposes Instance-adaptive Batch Normalization (IaBN). In addition, as a method to learn from a single sample, TT-SEG was proposed in which a pseudo GT mask is estimated from augmented images with the initial model and the last parts of the network are finetuned with the pseudo GT mask. In the experiments, the proposed method which employs both IaBN and TT-SEG successfully outperformed all the baselines, which indicated the effectiveness of the proposed method.  ","This paper contributes two techniques to improve generalization of semantic segmentation networks. The first technique is an adaptation of the test-time behaviour of batch normalization, where the statistics of the sample under consideration are blended into the training-time statistics of the batch-norm layer. The second technique is to use test time augmentations to first derive a pseudo-labeling from the sample and then to update parts of the weights to maximize the probability of the pseudo-labels. The paper additionally contributes a multi-dataset evaluation procedure and shows favorable performance of the proposed techniques on this evaluation protocol.","The article proposes two techniques for test-time adaptation. The first is instance-adaptive bn, to combine statistics from source data with each single test sample.  The second is test-time training, based on pseudo labeling of the test sample. The two parts lead to promising results.","This paper studies an existing problem - domain generalization for semantic segmentation of urban scenes. The technique keys of this work has two main points: instance-adaptive batch normalization and testing-time training using pseudo labels. Instance-adaptive batch normalization aims to bias to the data distribution of individual testing examples, which is a tradeoff of previous t-BN and p-BN. This paper generate pseudo labels for test-time training, which is not quite novel. To evaluate the effectiveness of the proposed method, this paper conducts experiments on GTA5/SYNTHIA -> Cityscapes, BDD100k and IDD, and shows better results. ",0.21008403361344538,0.10084033613445378,0.18487394957983194,0.1782178217821782,0.2376237623762376,0.2978723404255319,0.24752475247524752,0.2553191489361702,0.22448979591836735,0.3829787234042553,0.24489795918367346,0.14285714285714285,0.22727272727272727,0.14457831325301204,0.20276497695852533,0.2432432432432432,0.24120603015075376,0.19310344827586207
353,SP:e253d49bbfadb76b2f7c4e7cdd1cc33d0cebc3e7,"The authors explore several explanation methods for image classifiers via a user study. They study a toy environment containing images of two animals: stretchy (who has stretched legs) and peeky (who has a head that extends beyond its front legs). While the true label of peeky vs. stretchy is defined deterministically and specifically, the toy environment enables the authors to generate spurrious correlations between attributes like background color, animal position, shape, etc. and the label (which the model picks up on). The goal of the explanation methods is to help users identify which features (spurrious and not) the model is picking up on. Through a series of pre-registered user studies comparing a simple baseline to counterfactual explanations to concept highlighting, they explore whether or not users can accurately reconstruct which features the model is using in its predicitons. They find that concept highlighting performs far worse than the baseline of simply showing some model predictions in a grid, and that the baseline and the counterfactual method performed similarly. They release their procedures and generation code as a challenge to the community: can a new interpretability method outperform their baseline?","This paper proposes a dataset called TWO4TWO to conduct a user-study on two interpretability methods: counterfactual and concept-based explanations. Since the dataset is generated and can be fully controlled by users, the ground-truth important attributes to the decision are known. In this case, we know the ground-truth features and can thus assess whether the model explanation also reveal the correct features. As a baseline explanation, the authors group the input according to the model’s output logits. The result from the user study shows that the two sophisticated explanation models don’t surpass the simple baseline method, which indicates that explanation techniques shall be evaluate in user studies. ","The paper introduces an experiment design and an approach to synthesizing the dataset for the experiment. The experiment asked the systems to classify whether the shape of the ""animal"" is Peaky or Stretchy. The advantages are that authors can controllably generate trainable examples under arbitrary biases of the predefined features (shape, color, etc). For experiments, human subjects are asked to predict the systems' output. The authors compared the visual explanation (concept explanation) and counter-factual explanation with the baseline explanation that uses the output logits and found that the the current good explanation approaches do not make humans understand the system significantly better","The paper proposed a synthetic dataset to explore the bias contained in the dataset. Because the dataset is synthetic. We can manually change its attributes, add or eliminate the bias. Then, two main user study is conducted. One is to see if users can find the bias and another is to investigate if explanations are helpful.",0.1368421052631579,0.12631578947368421,0.08421052631578947,0.20535714285714285,0.11607142857142858,0.11650485436893204,0.23214285714285715,0.23300970873786409,0.2857142857142857,0.22330097087378642,0.23214285714285715,0.21428571428571427,0.17218543046357618,0.16382252559726962,0.13008130081300814,0.21395348837209302,0.15476190476190477,0.15094339622641512
354,SP:e278079529d6da9e2ea26b47730dbc1256ffe2db,"The authors find that past trustworthiness predicting methods are prone to overfitting to the training set where correct predictions are dominant. The label imbalance makes the trustworthiness predictors over-confident to even incorrect predictions. To solve the problem, this work proposes a new training loss named steep slope loss, which aims at improving the generalizability of trustworthiness predictors. The intuition behind this is to emphasize the negative class during training. The experiments are conducted in ImageNet. The results show the shortcomings of past methods with cross-entropy loss, focal loss, and TCP loss and then demonstrate the effectiveness of the proposed method. ","The paper proposes the steep slope loss for learning trustworthiness under a supervised training setup. A pre-trained neural network is further trained on the steep slope loss to learn trustworthiness as a binary classification problem. The approach is evaluated on ImageNet for a ResNet and a ViT model. In addition, evaluations on MNIST and CIFAR-10 are provided for comparisons with the related work. In all experiments, the proposed approach shows good performance. ","This paper tests methods for training a classifier for predicting trustworthiness-- formulated here as predicting whether a given image will be classified correctly or not by an target classifier -- on large scale datasets and finds that using existing loss functions like uncertainty estimates/focal loss on this binary classification task cannot detect untrustworthy samples, all samples are classified as positive resulting in 100% FPR (0% TNR). They attempt to fix this shortcoming by introducing a steep slope loss which essentially separates features w.r.t. correct and incorrect predictions from each other, and the loss is bounded in its magnitude. The paper extensively tests whether its possible to predict trustworthiness with two oracle/target classifier combinations on Imagenet and its stylized/adversarial test sets.","The paper proposes Steep Slope loss, a loss function that is designed to improve performance in the binary classification task of prediction trustworthiness. The paper reports that common loss functions used in this domain such as cross-entropy loss, focal loss, and true classification probability loss (TCP), all suffer from an issue where correct predictions tend to dominate over incorrect predictions, due to issues of class imbalance and evaluations on simple datasets where classifiers have achieved very high accuracy.   Steep Slope loss solves this issue by controlling the slope parameters on both the negative and positive classification thresholds with two hyperparameters that control the derivative and gradient updates for datapoints that are correctly classified and incorrectly classified separately. These two controlled slopes are a function of the signed distance of the output of the classifier from the hyperplane of classification.  The paper evaluates this loss on the MNIST, CIFAR, and Imagenet datasets and shows empirical improvements on multiple binary classification metrics such as AUPRC, AUROC, and FPR95, and importantly, SS loss shows marked improvements in the TNR, therefore being more robust to detecting incorrectly classified points.",0.16666666666666666,0.1568627450980392,0.2549019607843137,0.20270270270270271,0.2972972972972973,0.1774193548387097,0.22972972972972974,0.12903225806451613,0.13978494623655913,0.12096774193548387,0.11827956989247312,0.11827956989247312,0.19318181818181818,0.1415929203539823,0.18055555555555552,0.1515151515151515,0.16923076923076927,0.14193548387096774
355,SP:e2d33c7331db7f52b84ad1018152564d91a9f126,"This paper introduces a gradient-based approach to continual learning in neural networks called Recursive Gradient Optimization (RGO). RGO modifies the gradient direction at each update by multiplying it with a projection matrix P that is designed to minimize the increase in loss on previously encountered tasks. RGO is theoretically designed to prioritise performance on the current task and, among the optimal solutions for the current task, find the one that causes least interference with previous tasks - thus it assumes that the network is overparameterised. The derivation of the method starts by approximating the continual learning loss with the “recursive least loss”, which involves a sum of the Hessians of the previous tasks and assumes that tasks are fully trained and their solutions are close by to each other. It is then shown that the recursive least loss can be upper-bounded by an expression involving the projection matrix P, via which the solution for P that minimizes this upper bound is derived. It is shown that the expectation of the step size can be preserved by guaranteeing that trace(P)=dim(P) and by introducing random task-specific permutations at each layer - this preserves the “current-task-first” principle and reduces the need for hyperparameter tuning. Experiments are run on a number of standard continual learning image classification benchmarks, demonstrating an extremely strong performance of RGO in comparison to competing methods, sometimes surpassing the performance of a baseline that trains a separate model for each task.","The paper proposes a continual learning approach based on recursive gradient optimization. To this end, a projection matrix is derived for the gradient modification. This matrix, P, is computed incrementally and updated by integrating the Hessian on each task locally. ","The paper introduces a new method, Recursive Gradient Optimization (RGO), for continual learning in the task-incremental scenario. This method modifies the direction of gradients on a new task in order to minimise forgetting on previous tasks, and unlike many previous works, does not require storing past raw data to do so. The authors introduce a Feature Encoding Layer to achieve this. The authors provide experiments on 4 benchmarks of varying size (MNIST to miniImageNet), showing good performance of their algorithm, with different architectures.","The paper addresses a long-standing problem of deep neural networks where new training data for new tasks arrive continuously and data does not stay forever (continual learning). The network has to learn to cater for all the tasks without forgetting what it learnt for an older task as data from older tasks are no longer available. The authors came up with a formulation where learning will mean minimizing the (expectation of) forgetting and forgetting is formalized as increase of losses corresponding to the old tasks. The authors then move on to find a Taylor series expression of forgetting and an equivalent Recursive Least Loss (RLL) formulation of the loss (aka forgetting). Next the authors introduce a modification in the direction of gradient update of normal SGD that makes sure to minimize forgetting. The modification implied the introduction of a positive definite matrix and the optimization problem transformed into an optimization problem on finding the optimal positive definite matrix minimizing the RLL. As the relation between RLL and the positive definite matrix is not straightforward, an upper bound of RLL is proposed and the optimal solution is found under this upper bound. One more interesting proposal is a virtual feature encoding layer (FEL) that applies task-specific random rearrangement to the input feature maps. FEL eliminates possible interference between tasks by decorrelating features among old tasks without expanding the memory footprint. The experiments show great improvement over sota approaches on benchmark datasets and sometimes (rotated MNIST, Split ImageNet) even goes beyond supposed upperbound performance given by single task learning showing evidence of positive transfer.  Overall, I liked the approach, the presentation and the experimental results with only a few minor concerns detailed below.",0.08906882591093117,0.11740890688259109,0.1862348178137652,0.3,0.475,0.38095238095238093,0.55,0.34523809523809523,0.1625441696113074,0.14285714285714285,0.06713780918727916,0.11307420494699646,0.15331010452961671,0.17522658610271905,0.17358490566037735,0.19354838709677416,0.11764705882352941,0.1743869209809264
356,SP:e38efcfcf63f0488b6e20a74a86b78aad1ead363,"The paper proposes to use the max-affine spline function to understand network pruning in the deep learning architecture. Specifically, it uses two examples (one in fully-connected NN and one in CNN) to show the splines visualization with different pruning rates. Inspired by the theory, it then proposes a policy that is based on calculating the cosine similarity between slope and biases to determine the importance. The experiment results show the proposed metric could achieve a similar or better accuracy with good energy efficiency in multiple datasets with both structured pruning and unstructured pruning.","This paper conducts a series of empirical studies, which aim at relating node/filter/channel pruning with the partition of input space by neural nets using spline operators as activation functions. In particular, the final decision boundary is only determined by a few splines defined by a few filters of the network. This is verified in the paper through case studies of both fully-connected networks and convolutional networks, which implies that pruning does not severely degrade the accuracy as long as the important splines are not removed by pruning. Another observation is the early-bird tickets: the paper argues that the important splines do not change too much after a few early epochs since the binary activation patterns of data converge rapidly, so prunning can be applied after only a few epochs. The paper then proposes a pruning strategy that sequentially finds the most similar filter pairs and removes them. In experiments, they show that the proposed prunning method is more efficient than some recent pruning methods to achieve similar accuracy with pruning ratio <=70% for CIFAR10/100 and <=50% for ImageNet. ","This paper presents a novel methodology for deep network pruning from the perspective of the max-affine spline. The key idea of the paper is to remove the redundant subdivision splines and find winning tickets. This is an interesting and very nicely written paper that bridges the max-affine spline formulation and empirical pruning techniques. The authors provide a sufficient and straightforward introduction of the behind motivation. Beyond these, the authors discuss robustness considerations and perform thorough experiments on various benchmarked models and datasets, showing the favorable performance of their method.","This paper mainly proposes a new angle to understand the deep neural network pruning based on Spline theory and propose a new pruning algorithm approach. First, the author introduces the back ground of the Spline theory and current pruning methods. Second, the author analysis the different pruning methods from space partition perspective and introduce the proposed algorithm. Third, the author run experiments to evaluate the proposed algorithm in different aspects. ",0.29473684210526313,0.16842105263157894,0.17894736842105263,0.1366120218579235,0.1092896174863388,0.2087912087912088,0.15300546448087432,0.17582417582417584,0.24285714285714285,0.27472527472527475,0.2857142857142857,0.2714285714285714,0.2014388489208633,0.17204301075268816,0.20606060606060606,0.18248175182481752,0.15810276679841895,0.2360248447204969
357,SP:e51123a76713f1a1031d252e092985bd9b298fdf,"This paper studies the problem of distributed linear stochastic approximation for a group of agents over time-varying directed communication networks (to be more specific, the setup is decentralized). The authors propose two decentralized algorithms, (1) consensus-based linear stochastic approximation using row-stochastic mixing matrices (Eq. (1)), and (2) a push-sum type algorithm using column-stochastic mixing matrices (Eq. (9)). They further provide results on the asymptotic and finite-time mean-square errors to the equilibrium point of the (proper) ODE for each proposed algorithm.","This paper addresses a consensus problem in stochastic approximation, which has application to policy iteration in reinforcement learning. Specifically, the authors address the case where the interaction graph between agents is stochastic, but not doubly stochastic. Under certain assumptions, the authors prove convergence of their proposed algorithm in a certain sense. ","This paper studies multi-agent distributed optimization under general consensus-type interactions between agents. The main contribution of the paper is to study linear stochastic approximation in a multi-agent setup without using bi-directional communication among agents. Non-asymptotic upper bounds on the associated error function, in a mean squared sense, are derived. ","An interesting submission dealing with distributed stochastic approximation driven by Markovian noise, in which the communication topologies considered among agents are captured by a stochastic matrix (in contrast to the doubly stochastic matrix studied in the related literature). Both asymptotic and finite-time error bounds are established and it was shown that the algorithm converges to some unspecified convex combination of the equilibrium points of local tasks. This paper finally proposes a push-type distributed stochastic approximation algorithm and provides its finite-time bounds for the performance by leveraging the analysis for the consensus-type algorithm with stochastic matrices. All analysis and error bounds are directly applicable to distributed TD algorithms.  ",0.16091954022988506,0.1724137931034483,0.21839080459770116,0.21568627450980393,0.21568627450980393,0.2222222222222222,0.27450980392156865,0.2777777777777778,0.17117117117117117,0.2037037037037037,0.0990990990990991,0.10810810810810811,0.20289855072463767,0.21276595744680854,0.19191919191919193,0.2095238095238095,0.1358024691358025,0.14545454545454545
358,SP:e536acfe82bb5e41fa61929d44dad0b8f7c5ab19,"This paper proposes a new method to identify ""regions of disagreement"" in a setting with human decision-makers assigned to individuals (eg. judges assigned to court cases). The ""regions"" are framed as subgroups of the data (eg. males over 30 years old) which can be interpretable (based on the chosen model). The paper formalizes this task as that of identifying the subgroup where the identity of the specific decision-maker has a causal effect on the decision, proposes an optimization algorithm to find this subgroup, develops generalization bounds (under some simplifying assumptions), and demonstrates compelling performance on semi-synthetic and real-world datasets.","The authors are concerned with identifying regions in a covariate space. A region is characterised by having high variation in (potential) treatment outcome, as compared to the expected (potential) outcome. A method such as this could be used to identify variation of practice in the legal or medical domain (as indicated by the authors).   This is a relevant topic which the authors rightfully relate to the causal literature. Specifically, the authors chose the potential outcomes model, where the treatment is now a clinician or judge; the outcome is binary (amounting to a decision, e.g. bail or no bail); and the condition is an individual. Contrasting potential outcomes, the authors are interested in learning subsets of X, rather than Y(a).","In this paper, the authors develop an algorithm for identifying cases (e.g., defendants or patients) on which decision makers (e.g., judges and doctors) ""disagree"" on the decision.   ## Identification The authors consider a setting in which cases are summarized by features $X$. Each case is associated with a decision maker $A$ and a binary decision $Y$. In the pretrial release setting, each case is a defendant, the features $X$ are observable characteristics of the defendant (such as demographics or charge information), the decision maker $A$ is the assigned judge and the binary decision $Y$ is to either release or detain the defendant. The authors define the potential decision Y(a), which describes the decision that would be made by decision maker $a$.   The authors propose a summary measure of agent disagreement using average contrasts of potential decisions across agents given sets of the observable features. In particular, the authors focus on the ""conditional relative agent bias"" as  $$ \mathbb{E}[Y(a) - Y(\pi(x) | A = a, X \in S], $$ where $Y(\pi(x))$ denotes the expected decision under the assignment mechanism of decision makers to cases. The authors key identification assumption is that decision makers are as-if randomly assigned to cases conditional on the features (i.e., $A \perp Y(a) \mid X$). This measures the extent to which decision maker a's average decisions over $X \in S$ differs from the average decision that is made under the assignment mechanism. In other words, this measure of disagreement depends on the assignment mechanism of agents.  The conditional relative agent bias measures the extent to which a single agent a's decision disagrees relative to the average decision over $X \in S$. The authors then propose aggregating this measure across agents. For some grouping of agents G, the authors define the estimand $$ Q(S, G) = \sum_{a : G(a) = 1} P(A = a \mid X \in S) \mathbb{E}[Y(a) - Y(\pi(x) | A = a, X \in S]. $$ The authors main identification result (Theorem 1) is to show that if agents are as-if randomly assigned to cases, then this estimand can be identified as the conditional covariance between the observed decision and the agent assignment $$ Q(S, G) = \mathbb{E}[Cov(Y, G \mid X ) \mid X \in S]. $$  ## Estimating Regions of Heterogeneity. Using this identification result, the authors goal is to search over regions of the feature space $S$ and groupings over agents $G$ to find the combination that maximizes this proposed measure of disagreement. To do so, the authors propose to solve  $$ \max_{S, G} Q(S,G) \mbox{ s.t. } P(X \in S) \geq \beta, $$ which requires that the region have probability at least $\beta$ (a tuning parameter to be specified). The authors develop an iterative algorithm to optimize this objective function, and provide some conditions under which they are able to derive a generalization bound on the solution return after the first iteration (Assumptions 2-4 and Theorem 2).  ## Empirical Applications The authors apply their proposed algorithm in two empirical applications.   First, the authors analyze the behavior of their proposed algorithm in a semi-synthetic experiment that is calibrated to data from Lin et al. (2020) on recidivism risk predictions made by MTurk workers. The authors simulate data from two types of agents that follow different decision rules for detention vs. release. The authors compare their algorithm against direct models that fit logistic regressions for the decision on agent identifiers and observable features (direct models) and TARNet. The authors report that their algorithm does better at recovering the ""region AUC.""  Second, the authors analyze an observational dataset on medical treatment decisions for diabetes. The data contain information on 3,956 patients and 458 group practices, and the authors search for the region of disagreement across these group practices. ","Summary:  This paper considers settings where decisions for individuals are each made by different decision makers (agents). The paper defines the problem of finding a region in feature space in which the agents administer highly varying decisions. For example, perhaps judges mete out different sentences for misdemeanors by young men. The paper defines new causal criteria to characterize such regions in the feature space, where the choice of agent has an effect on the decision. The paper introduces an iterative algorithm to maximize this criteria to identify regions of heterogeneity, and provides theoretical results about the optimality of the proposed algorithm. The paper empirically demonstrates the algorithm on a semi-synthetic dataset and real clinical data about doctor recommendations for diabetic patients.  Contributions:  + A new criteria for defining regions where there is heterogeneity in decisions administered across agents. + Results for estimating the criteria from observational data and an iterative algorithm for optimizing the criteria. + Theoretical results about the algorithm and empirical validation of the algorithm.",0.18446601941747573,0.3883495145631068,0.2912621359223301,0.36363636363636365,0.17355371900826447,0.0880503144654088,0.15702479338842976,0.06289308176100629,0.18181818181818182,0.06918238993710692,0.12727272727272726,0.3393939393939394,0.16964285714285715,0.10825439783491204,0.22388059701492535,0.11624834874504625,0.14685314685314685,0.13982521847690388
359,SP:e6622975c9889cf6d3357ab439c2e268c4f4200e,"This paper proposes that given a set of skills or options, the vector of the value functions for the individual skills becomes the abstract state representation in the approach proposed by this paper. The authors then go on to develop a model-free RL algorithm akin to DQN and a model-based planning algorithm for planning in the state-space defined by the value functions of the component skills, and the action-space being the set of executable options.   The authors demonstrate that the value function space serves as a better representation than some baseline algorithms for representation learning for RL in a maze solving and robotic manipulation task.  The key assumptions made are as follows:  - The options are pre-defined and each option has an associated value function  - The new task is solvable with the defined set of options.",The paper presents a novel state representation technique that is based on Value Functions (VFs). The core idea of the paper is to use VFs to construct a high-level space representation in an hierarchical reinforcement learning (RL) scenario. The contributions of the paper are:  - Value Function Spaces (VFS): learned state representation - Practical algorithms with VFS both for model-free and model-based RL settings - Evaluation of VFS in 2 scenarios (MiniGrid and manipulation task),"The paper proposes a state abstraction in hierarchical reinforcement learning (HRL), called Value Function Spaces (VFS), that is constructed from the value functions of task-conditioned lower-level policies (or skills). The idea is that the value functions capture affordances of the lower-level skills while ignoring task-irrelevant information. The lower-level policies and the corresponding value functions are given a prior, and the state is constructed by concatenating the value estimate of each lower-level policy. The state abstraction is evaluated in a model-free Q-learning and model-based MPC type scenario.","This paper presents Value Function Spaces (VFS), an abstract ""skill-centric"" representation for reinforcement learning and long-horizon planning. The core idea of VFSs is to leverage the learned value functions that are often trained alongside skills during skill learning to abstract knowledge about the world. Each element of this low-dimensional representation corresponds to the value estimated from one of the learned value function, so that the representation (by construction) tends to ignore distractor information unnecessary for accomplishing the skills. Using this representation on a set of pre-trained skills, the authors demonstrate improved performance on long-horizon planning tasks in grid-based maze and ""locked door"" environments, showing improved success probability on a number of such tasks compared to competitive baselines. They also show the ability of their abstraction to facilitate model-based goal-directed planning by learning a state-transition model (in VFS-space) that allows one to predict the outcome of executing a skill.",0.17142857142857143,0.17857142857142858,0.20714285714285716,0.30666666666666664,0.30666666666666664,0.2736842105263158,0.32,0.2631578947368421,0.18354430379746836,0.24210526315789474,0.14556962025316456,0.16455696202531644,0.2232558139534884,0.2127659574468085,0.1946308724832215,0.27058823529411763,0.19742489270386265,0.20553359683794464
360,SP:e789c71cef2094ff2bac51b523ca912f1f04c2c9,"The paper proposes a new federated learning scheme that is suitable for devices with heterogeneous resources. The proposal, namely Split-Mix, trains multiple models of different sizes and adversarial robustness levels, tailored to the budget of each device. Empirical results demonstrate the efficacy of the method against the main competitor.","This paper presents a new federated learning approach named Split-Mix FL that allows clients to train customized models efficiently while considering heterogeneity in data and computation resources. The key idea is threefold: 1) the global model is first split into several sub-networks called base models that each have different sizes, thus requiring a different amount of computational resources; 2) Each selected client trains a random subset of base models, under its computational resource constraints; 3) updated base models are aggregated at the server-side and distributed again. Furthermore, the proposed approach can train both accurate and robust models in a joint fashion, where all but batch-norm layers are shared for efficiency. Experimental results on multiple datasets (CIFAR10, Digits, DomainNet) demonstrate the effectiveness of the proposed approach compared to FedAvg and HeteroFL.","Split-Mix is a Federated Learning strategy aimed at easing the problems that arise when a heterogeneous pool of devices/clients (i.e. some with more compute/memory capabilities than others, different data distributions) collaboratively train a global model. Split-Mix trains a model that can later on be customized in terms of model size and robustness. As the name suggests, Split-Mix has two stages: During `split`, a large model is split into smaller base models. Base models are constructed by discarding channels but maintaining the the number and type of layers in the network. During `mix`, the server samples a fraction of the base models (depending on a given client's compute capabilities) and fuses them into a single one that is send to a client to train. All base models are trained on all clients (should these meet the compute requirements of a sub-model) in a federated manner. This means that the more capable devices, train all base models. This is envisioned to happen in a parallel fashion in a given device. Once FL training is completed, a customized model can be deployed to a device/client in hardware-aware and robustness-ware fashion. The Authors refer to this as _in-situ customization_ ","This paper proposes a customisation strategy named ""Split-Max"" for federated learning. The authors identify the heterogeneity of devices and data in FL scenarios. They present the importance of considering devices' budgets and dynamics when dispatching training models. Split-max can adjust the model size according to the devices' budget while maintaining good accuracy and robustness.  Split-max works in steps. First, multiple base models from different initialisations are trained to improve diversity. These base models are randomly given to clients to extract generalisable features. Then, base models are aggregated to the server. Secondly, to provide devices with models with different robustness, it trains two similar models together to capture both the standard-training accuracy and adversarial-training accuracy. Then layer-wise mixing is conducted to achieve both standard accuracy and adversarial accuracy.   Experiments show that Split-Mix achieves better accuracy than naive approaches. Moreover, with customisation, the models are smaller and more robust under budget constraints.",0.38,0.36,0.32,0.2462686567164179,0.208955223880597,0.1642512077294686,0.1417910447761194,0.08695652173913043,0.10191082802547771,0.15942028985507245,0.17834394904458598,0.21656050955414013,0.20652173913043478,0.14007782101167313,0.15458937198067632,0.1935483870967742,0.1924398625429553,0.18681318681318682
361,SP:e9d9ad4fb9dc3cb25f7282c0979a8ccb252f692a,The manuscript entitled “Can fMRI reveal the representation of syntactic structure in the brain” proposed a word embedding scheme with the consideration for incomplete sentence (used to test how future sentence structure is represented in the brain) and partial parses (used to represent the multiple top-down paths of the brain in interpreting language syntaxes). Words embedded by the proposed schemes are then mapped to the corresponding fMRI signals (ensured by the experiment design where each word is strictly 0.5 second) through ridge regression. Voxels in the brain that are significantly correlated to certain combination of the syntax structures are studied in the results.,"In this paper, the authors propose new representational spaces that capture syntactical information in natural language above and beyond complexity-based metrics that have been typically used in language neuroscience. First, they build representations based on the constituency tree of a sentence as every word is processed by the parser. Then, they use a subgraph embedding algorithm to convert the subtree into a 15-D representation. To test several hypothesis about the type of syntactic information being encoded, the authors look at 3 different types of subtrees- the largest subtree completed when the current word is processes, the incomplete subtrees that can explain the sequence seen thus far under PSG production rules and the set of complete parses produced by a probabilistic model that are weighted by their probabilities. The 3 subspaces are compared against 4 complexity/load-based metrics and additionally, a semantic feature space derived from BERT. They go on to build encoding models for the different feature spaces and compare them through variance partitioning. Overall, the paper finds that the 3 graph-based spaces explain additional variance across the cortex as compared to traditional complexity-based features. Further, this effect is not localized to any region. Instead, it seems to be distributed across a network that is largely explained by semantic features, suggesting that syntax and semantic share neural substrates and there exists no isolated region for syntactic processing.  The study uses publicly released fMRI data from 9 subjects reading a chapter from Harry Potter .","The paper proposes a new set of syntactic tree embeddings, and uses them capture additional variance in fMRI data beyond simpler complexity metrics. I think the paper setup is compelling overall, it is well-situated relative to prior work, and the results are intriguing. My primary concern is that the paper doesn't conclusively establish that these new embeddings are purely syntactic and disentangled from either complexity metrics or (more ambitiously) anything that BERT can capture, and this damages the downstream claims about neural correlates of syntactic representations.   I hope this can be addressed and solved in rebuttal/revision, since I find the paper good otherwise and hope to see it at the conference.   EDITED: I think the authors have reasonably addressed my concerns, and am raising my rating accordingly. ","* The authors conduct a brain encoding study testing whether fine-grained information about the content of incremental syntactic representations contribute to predictions of neural activity, over and above measures of syntactic complexity. * They design custom subgraph embedding representations to operationalize the idea of ""syntactic representation."" They find that these subgraph embeddings capture additional variance in brain activity above what is predicted by complexity metrics, and that this improvement in prediction is distributed throughout the language system.",0.23809523809523808,0.1523809523809524,0.13333333333333333,0.10483870967741936,0.08870967741935484,0.13076923076923078,0.10080645161290322,0.12307692307692308,0.18421052631578946,0.2,0.2894736842105263,0.2236842105263158,0.141643059490085,0.13617021276595745,0.15469613259668508,0.13756613756613756,0.13580246913580246,0.1650485436893204
362,SP:ea9a6880083555a89f5ed22dca21ba2dc109c1a2,"In this work, the authors propose the finite volume neural network to solve advection-diffusion partial differential equations. The authors define a flux kernel as the sum of sub-kernel f_i on each element. And the authors define specific modules \phi_D, \phi_A, \phi_N according to the form of the advection-diffusion equation. The model requires very few parameters to achieve the state of art results.",The paper introduces Finite Volume neural network for modelling fluid dynamics inspired by the finite volume method. The paper models the velocity and the spacial derivatives as the neural networks. The work demonstrates more precise fluid simulation than the related physics-inspired models.,"The authors propose to model advection-diffusion partial differential equations as a composition of multiple neural networks. According to the authors, this leads to better generalization over different initial and boundary conditions for the physics system, and also the ability to learn different factors of the process as modeled by the terms in the PDE. Extensive experimental results are presented to support the author’s claims that their framework performs better than state-of-the-art.","This paper presents a compositional physics-aware neural network (FINN) for learning spatiotemporal advection-diffusion processes. It claims that the FINN outperforms pure machine learning and other state-of-the-art physics-aware models in all cases—often even by multiple orders of magnitude. However, the design of the network depends too much on the form of the equation, which leads to a very narrow application of the method.",0.17391304347826086,0.3333333333333333,0.2318840579710145,0.2558139534883721,0.27906976744186046,0.18421052631578946,0.27906976744186046,0.3026315789473684,0.2318840579710145,0.14473684210526316,0.17391304347826086,0.2028985507246377,0.21428571428571427,0.3172413793103448,0.2318840579710145,0.18487394957983194,0.21428571428571427,0.19310344827586204
363,SP:eb54e84275266d8909fcbfe1589da1c4396c3164,"The paper proposes a new method to select positive pairs for contrastive learning: using adjacent frames in a sequence, referred to as Contrastive Learning Through Time (CLTT). This is relevant since CL approaches have been shown to depend a lot on what kind of augmentation is used. The method is integrated with a number of well-known CL approaches, and evaluated on three different datasets, two of which are novel --the method is shown to approach fully supervised results. The paper also provides a valuable discussion on how the notion of time can help representation learning. The method is also tested for latent representation similarity when one object or pattern is systematically shown after another, something which has been studied in the field of biological learning, and a similar trend is found here as well. ","In this paper, author introduce an image-based contrastive learning framework which substitutes traditional data argumentation by using successive images along with the temporal dimension. A new dataset is also generated in this paper for better studying the topic. Extensive experiments demonstrate that the proposed method is able to learn as good feature representation as supervised learning  in some datasets.","The authors propose to perform self-supervised learning (SSL) by enforcing temporal consistency (as opposed to augmentation consistency in most recent efforts). They do this by adapting existing SSL approaches (SimCLR, RELIC and BYOL) to leverage video frames that are close in time as opposed to different augmentations of a given image (akin to a temporal augmentation). In addition, the authors introduce two new datasets which they use to train and study the learnt representations in detail, showing promising results.","This paper has developed a framework for contrastive learning through time (CLTT). Instead of using augmentation operations to create positive pairs for contrastive learning, this paper creates several datasets with a near-photorealistic training environment by changing different view directions. The temporally close video frames of the new data are thus similar and should align in representation, which can naturally be considered as positive data.   ",0.1037037037037037,0.1111111111111111,0.14074074074074075,0.15,0.18333333333333332,0.1375,0.23333333333333334,0.1875,0.2923076923076923,0.1125,0.16923076923076924,0.16923076923076924,0.1435897435897436,0.13953488372093023,0.18999999999999997,0.1285714285714286,0.17600000000000002,0.15172413793103448
364,SP:eb68e98d9baf9118381d25d4b2da030a6f78577f,"The paper argues for the existence of the so-called “principal component bias” in the learning of deep wide nets, with the main focus on linear networks. This refers to the phenomenon that the learning that is associated with larger PCs of the data is typically faster. The paper does so via a theoretical analysis of the early stage of learning in linear nets and a series of experiments, involving both linear and nonlinear models.","The paper studies the evolution of a deep over-parametrized linear network under gradient descent.  The main claim of the paper is that the convergence rate of the weights is faster along directions corresponding to the larger principal components of the data, at a rate governed by the singular values. The paper also supports its argument in an extensive experimental study.","This work studies the training dynamics of over-parameterized deep linear networks. The authors propose the Principal Components bias (PC-bias) convergence pattern that characterizes the convergence behavior of deep linear networks training, which is supported by theoretical analysis and empirical results. This works also investigates the Learning Order Constancy effect (LOC-effect) and identified the connection between PC-bias and LOC-effect. Empirically, the authors also study several implications of PC-bias, including early stopping, convergence behavior under label noise, and provide interesting observations.","The authors study a principal component bias of deep linear neural networks when trained with gradient descent using a small learning rate. They show that when the network is sufficiently wide, deep linear networks behave like single layer linear networks during the first phase of training, where the rate of convergence is governed by the largest principled components of the data. It is further shown that at later stages of training, under some assumptions, the PC bias remains to some extent.",0.24,0.24,0.25333333333333335,0.2786885245901639,0.3114754098360656,0.21176470588235294,0.29508196721311475,0.21176470588235294,0.2345679012345679,0.2,0.2345679012345679,0.2222222222222222,0.2647058823529412,0.22499999999999998,0.2435897435897436,0.23287671232876714,0.2676056338028169,0.21686746987951808
365,SP:eb760d20f3820827c41358ff191d22f4fb78847e,"The main problem the paper tries to tackle is reducing the peak memory footprint of the neural networks by deviating from the usual layer-by-layer execution. The main intuition seems to be very similar to tiling combined with layer fusion-like ideas. However, the main spark of the paper seems to be leveraging this dimension in the context of Neural Architecture Search, which seems to be novel.","This paper addresses the problem of large activations that quickly arises in devices with little memory, such as MCUs, when deploying larger networks. The main contribution proposes a relatively simple but effective approach to reducing the memory footprint of activations (by a large 4x-8x factor) while introducing some unavoidable extra compute (resulting in up to 15% MACs, which translate into  up to ~1.27x higher latency). The impact in terms of compute overhead of the proposed per-patch processing is alleviated by redistributing the receptive filed. Which means: reducing the filter WxH in early layers and use striding>1; then stack more blocks in deeper parts of the network where the memory peak of activations is not so high anymore. A NAS-based mechanism is introduced to jointly discover architectures and receptive filed redistribution strategies. ","Working memory utilization for most CNN based models typically is more than what can be afforded on Tiny devices. Thus, it is often quite difficult to extract reasonable performance for visual tasks on tiny devices.  Authors attack this problem by a) introducing a patch based inference method that improves peak memory utilization at the cost of compute and b) introducing a redistribution process that reduces the overall compute while maintaining/improving performance. ","Instead of the conventional layer-by-layer execution, this paper proposes a patch-by-patch scheduler to reduce the peak memory required by a CNN for inference on MCU. The author also proposes a network redistribution method, which finetunes the network architecture (CONV kernel size), to mitigate the computation overhead caused by their patch-by-patch execution. The results show that, with a certain computation overhead, the proposed methods can effectively reduce the inference peak memory while not losing accuracy.",0.27941176470588236,0.08823529411764706,0.20588235294117646,0.08088235294117647,0.14705882352941177,0.1527777777777778,0.13970588235294118,0.08333333333333333,0.175,0.1527777777777778,0.25,0.1375,0.18627450980392157,0.08571428571428572,0.18918918918918917,0.10576923076923078,0.18518518518518517,0.14473684210526316
366,SP:ebed8b8a25cead3629832c2ba52caf0059971d3d,"This paper extends the definition of the hidden subnetworks in randomly initiated neural networks. The new notion of subnetworks, namely the disguised subnetworks, apply a transformation on the hidden subnetwork weights to obtain final weights (a posteriori finding the hidden subnetwork).   Mathematically speaking, the main decision variable of the underlying optimisation problem of finding “hidden subnetworks” is the so-called masking variable that is a binary vector, the variable that decides which components of the randomly initiated weights will be zero, constrained to a desired level of sparsity. On the other hand, the underlying optimisation problem of “disguised subnetworks” has an additional variable U that applies a transformation on the set of weights after being ‘sparsified’. The selection of U = I, for I being the identity transformation, recovers the problem of hidden subnetworks, showing that the latter is a generalisation.  The author(s) present a heuristic algorithm that solves the forenamed optimisation problem. The idea is to first find a solution for the masking variable where (i) U = I is taken, (ii) the objective function is independent of a training set. Afterward, the algorithm proceeds by solving the problem for U given the solution of the previous step, where the space of U is restricted to the class of transformations where only sign-flips are allowed. If we think of the above solution process as a two-phase problem, the author(s) use the literature on sparse neural networks for the first phase and use the literature on binary neural networks for the sign flipping phase.","This paper proposes the idea of a ""disguised subnetwork"", which are hidden random subnetworks that can be transformed into a well-performing subnetwork. The paper introduces PaB as a way to uncover these subnetworks, by first searching for a mask over the random weights using pruning-at-initialization techniques, and then learning a transformation on the subnetwork. The paper further shows that this PaB process can be efficiently implemented, offering significant advantage over prior work.","Optimizing sparse neural networks is an important topic due to their computational and space savings.  Building on the work of the lottery ticket hypothesis, others have shown there exist hidden subnetworks within randomly initialized NN that have good performance. The authors extend this definition to disguised subnetworks, which contain hidden subnetworks as a subclass. Moreover, the authors present a novel combination of existing methods  into a single algorithm they call Peek-a-Boo (PaB) which can efficiently find such networks. ","This paper presents an algorithm named peek-a-boo (PaB) to optimize network pruning (at initialization) and optimization (limited within flipping the sign of weights). This setting has not been studied by prior works. A two-step algorithm was designed -- pruning first, optimization second. Experiments show competitive performance compared to prior methods with similar optimization complexity.",0.0859375,0.09375,0.046875,0.18666666666666668,0.16,0.1,0.29333333333333333,0.3,0.21428571428571427,0.175,0.21428571428571427,0.14285714285714285,0.13293051359516617,0.14285714285714285,0.07692307692307691,0.18064516129032254,0.183206106870229,0.11764705882352941
367,SP:ec18e1450dd918b1ca95e301bc9262e072d77b52,"The authors consider the problem of inductive graph partitioning, which they formulate as clustering or partitioning multiple snapshots of a time-evolving graph for which we have no node correspondence. In other words, we cannot link the nodes in snapshot $t$ to those in snapshot $t-1$, which prevents incremental or evolutionary clustering algorithms from being applied. The propose a complicated dual graph neural network (GNN) architecture for this problem setting and demonstrate potentially good clustering accuracy with low computation time on simulated and real networks.","This paper considers the problem of solving the graph partitioning problem repeatedly over many different graphs. Graphs are sampled i.i.d. from an unknown but fixed distribution and the goal of the algorithm is to solve the GP problem on each of the graphs. There are two quantities that are opposing; efficiency (on one hand we can solve an NP-hard problem each time but would be prohibitively time consuming) and quality (on the other hand we can generalize from the learnings on the other graph since they are related via the i.i.d. distribution). This paper proposes a NN architecture that uses a subset of the graphs in the family to learn an embedding, and then derives the solution to the other instances by using this embedding via a matrix multiplication. Using a number of simulated and real-world datasets they show that this method works well empirically.","In this work the authors focus on an important problem, building an inductive framework for graph partitioning. This is a major problem with the potential of improving the performance of various classic transductive  algorithms for graph partitioning, both in terms of output quality, but also in terms of speed when a new snapshot  of a system needs to be partitioned into communities. This is a recent line of research, see, e.g., Nazi et al. 2019 ""GAP: Generalizable Approximate Graph Partitioning Framework"".  The proposed framework can address snapshots with differing number of nodes, by projecting them down to coarsened versions of the network. There are two versions of the framework, based on normalized cut and modularity objectives respectively. The framework can be potentially used with other measures as well. A nice idea is to leverage the unsupervised communities extracted from modularity optimization, or spectral algorithms through a dual GNN structure, that can then be used to partition fast unseen instances.  ","The authors propose an inductive graph partitioning framework across multiple evolving graph snapshots to alleviate the NP-hard challenge. It first conducts the offline training of a dual graph neural network on historical snapshots to capture the structural properties of a system. The trained model is then generalized to newly generated snapshots for fast high-quality online GP without additional optimization, where a better trade-off between quality and efficiency is achieved. ",0.19767441860465115,0.20930232558139536,0.19767441860465115,0.17880794701986755,0.10596026490066225,0.125,0.11258278145695365,0.1125,0.2361111111111111,0.16875,0.2222222222222222,0.2777777777777778,0.14345991561181434,0.14634146341463414,0.21518987341772153,0.17363344051446947,0.14349775784753363,0.1724137931034483
368,SP:ecb9c7c11dfb450d8e76504d42309b1888023d26,"The paper proposes a RL-based memory management policy for CIL problem. They have two-step formulation of policy, which first determines how to allocate the memory to old and new tasks, then determines how many samples to store per each class within the task. A standard RL framework of policy gradient method is used to optimize their policy. Experimental results how that their RMM can further improve the recent state-of-the-art methods, such as LUCIR or PODNet. ","This work addresses the problem of memory management in Class Incremental Learning (CLI). In this setup, models store exemplars from old classes and must decide what exemplars to drop to make space to new memories. The authors propose a hierarchical reinforcement learning approach where an agent first chooses how much of the old memory to release to make space for new samples and then it chooses how much of the new space allocate to high-entropy samples and low-entropy samples (they divide new classes in two groups). Experiments show that the the proposed method attains better performance than POD-AANets and LUCIR on CIFAR and ImageNet. In the supplementary material they show that the proposed memory management system also improves the performance of already-existing systems. Additional studies show how policies transfer between datasets, the impact of hierarchical RL, and how old and new memories balance over time. ",A common approach in class-incremental learning is to retain a fixed set of examples of the old classes to help avoid forgetting. The paper presents an approach to dynamically retain exemplars for the class-incremental learning problem. Reinforcement learning is used for dynamic memory management.,"The paper proposes a new memory sampling strategy which replaces random/herding. The strategy consists of two parts: first level determining the distribution of #samples of each class in memory and second level determining samples to be selected for every class. This is learnt via RL policy. The sampler is trained by emulating a pseudo-CL problem with the data available in the pretraining phase-- where there are an equal number of tasks made as expected in the CL setup to enable transfer. Given a sampling configuration, a CL method trains the model and the performance on the pseudo-CL test set is given as reward to the RL algorithm. This trains the policy components, which optimize sample size and samples per class and train CL. Then, the best policy is used to create the memory which is used to train the actual CL problem with state-of-the-art continual learning approaches. This memory selection achieves better performance than previously used memories across CIFAR100, Imagenet100 and Imagenet1000 datasets.",0.225,0.1,0.325,0.10067114093959731,0.18120805369127516,0.32608695652173914,0.12080536912751678,0.17391304347826086,0.15384615384615385,0.32608695652173914,0.15976331360946747,0.08875739644970414,0.15720524017467247,0.126984126984127,0.20883534136546186,0.15384615384615385,0.16981132075471697,0.1395348837209302
369,SP:ecc173185ec28d0ef75c60df260ac4faba059f61,"The paper studies the dynamics of privacy loss for noisy gradient descent algorithm, whose continuous-time analogue is the Langevin diffusion process. They obtain a tight bound on the Renyi divergence between the pair of probability distributions over parameters of models with neighboring datasets. They prove for smooth and strongly convex loss functions, the privacy loss converges exponentially in the number of iterations, instead of square root in the number of iterations as in composition-based analysis of the same algorithms in the literature. For Lipschitz, smooth and strongly convex loss functions, they prove optimal utility for differential policy algorithms with a small gradient complexity.  ","The paper analyze Differentially Private Gradient Descent (DPGD) for optimizing a function f, which perform gradient descent while adding Gaussian noise to the gradients. The authors consider when f is the sum of loss functions of users in a database, and we want to output a minimizer of f with approximate differential privacy. They analyze the privacy loss and error of DPGD by viewing it as an instance of Langevin Dynamics, which samples from the distribution p with pdf proportional to e^{-f(x)}. The main result of the paper is a bound on the Renyi divergences between two DPGD runs on adjacent databases. The main qualitative improvement of this bound is that the dependence on the number of iterations of DPGD, K, is something like (1-e^{-K}) when the loss functions f are strongly convex and smooth. That is, the divergence bound increases with K, but the asymptotic dependence on K is O(1). In comparison, many past bounds have a small polynomial dependence on K, since they effectively bound the divergence between the full chain of iterates produced by DPGD, rather than the final iterate. In particular, if we took K to be sufficiently large, this would enable sampling from a distribution very close to p with approximate DP. The authors show that indeed, this gets loss that has optimal dependence on dimension, epsilon, and the number of users n.",This paper investigates the information leakage of noisy gradient descent (GD) algorithms for smooth and strongly convex loss functions. The authors analyze the dynamics of (Renyi) privacy loss during training (over K iterations) when the internal state of the training algorithm is not observable. This leads to an upper bound on the rate of privacy loss of the released model by way of the Renyi divergence between a pair of distributions over model parameters learned on neighboring datasets. The bound turns out to be much better than that obtained using composition-based approaches. ,"This paper presents a theoretical framework for analyzing the privacy loss of Noisy Gradient Descent during its execution. Using the Renyi divergence as the differential privacy metric, the authors aims to understand how much information leaks for an optimization function $\frac{1}{n} \sum_{x\in \mathcal{D}} \ell (\theta; \mathbf{x})  $ and for  the dataset $\mathcal{D}=(\mathbf{x}_1,\cdots,\mathbf{x}_n).$  In order to do that, they interpolate the discrete algorithmic method of ""Noisy Gradient Descent"" to its stochastic continuous analogue, the famous stochastic differential equation of Focker-Plank Equation. The authors carefully construct this tracing diffusion process among two consecutive steps such that $\theta_k = \Theta_{t=\eta k}$ but $\lim_{t\to \eta (k+1)} = \Theta_{t=\eta(k+1)}$ , where $\theta_k, \Theta_{t=\eta k}$ are the  discrete/continuous parameters for the optimization. This semi-continuous interpolation permits the authors to analyze tightly the information disclosure of the initial method of Noisy Gradient Descent using Gaussian perturbation for the case of  of smooth and strongly convex loss functions.",0.2857142857142857,0.22857142857142856,0.2761904761904762,0.12446351931330472,0.14163090128755365,0.24731182795698925,0.12875536480686695,0.25806451612903225,0.16477272727272727,0.3118279569892473,0.1875,0.13068181818181818,0.1775147928994083,0.24242424242424246,0.20640569395017794,0.17791411042944785,0.16136919315403425,0.17100371747211895
370,SP:ed4e2896dc882bd089f420f719da232d706097c5,"This paper studies the problem of how to fine-tune a pre-trained model and obtain better results for both ID and OOD. Two methods, fine-tuning and linear probing, are investigated and compared, then a new two-step variant called LP-FT is derived. Results further verify that LP-FT obtains the best performance for ID and OOD tests compared with FT and LP.","This paper contrasts fine-tuning (i.e., modifying all network weights) and linear probing based on their relative ID/OOD performance. It is known that fine-tuning (FT) outperforms linear probing (LP) ID. This paper presents that the reverse is true OOD (FT outperforms LP). This paper suggests that this occurs because fine-tuning distorts features in conjunction with the final linear layer. Instead, if a final linear layer is trained first, the features do not have to move that much during full fine-tuning. The authors refer to this method as LP-FT and show it often outperforms LP and FT both OOD and ID.","This paper discovers an interesting behavior of model fine-tuning: the performance is worse compared to linear probing on OOD data (i.e., data from other domains), especially when the distribution shift between inner distribution and out of distribution are big. The explanation provided in the paper is that fine-tuning distorts the feature representations, overfits on inner distributions, and thus has a higher error on OOD data. The authors also provide a simple solution to this issue by fine-tuning with a classification head initialized from linear probing and had better results in all the benchmarks they have in the paper. ","This paper explores how different strategies for fine-tuning affect in- and out-of-distribution performance. The authors contrast linear probing (updating only the parameters of the final linear layer), end-to-end fine-tuning (updating all parameters of the model) and a two-stage approach, where linear probing is followed by end-to-end fine-tuning. While end-to-end fine-tuning typically improves in-distribution performance, the authors show that it can also underperform linear probing out-of-distribution. The paper theoretically analyzes the tradeoffs in a simplified scenario with two-layer networks, finding that end-to-end fine-tuning can ""distort"" pre-trained features. Their experiments on a number of datasets including CIFAR, WILDS-FMoW and others, confirm the intuitions from their theory. The proposed mitigation strategy, a two-stage fine-tuning approach where end-to-end fine-tuning follows linear probing is found to be beneficial, especially out-of-distribution.   **Update:** The authors addressed most of the concerns raised by this and other reviews, and I am raising my score accordingly.",0.2923076923076923,0.24615384615384617,0.3076923076923077,0.2169811320754717,0.27358490566037735,0.28431372549019607,0.1792452830188679,0.1568627450980392,0.11428571428571428,0.22549019607843138,0.1657142857142857,0.1657142857142857,0.2222222222222222,0.19161676646706588,0.16666666666666666,0.22115384615384615,0.20640569395017794,0.20938628158844763
371,SP:ee0b94238c3fde59cb8b67a687b77984fe7d3454,"This paper proposed a personalized FL framework with partial model personalization. It separates the model parameters into two parts, shared model and personalized model, and optimize them in an interleaving manner. The authors proposed two optimization algorithm, named as FedSim and FedAlt, with partial clients participation. They also analyzed the algorithms' convergence rate on general smooth nonconvex function. In experiments, they consider 3 different model split methods: 1. input layer as personalized model, the rest as shared model; 2. output layer as personalized model, the rest as shared model; 3. adding adapter as personalized model.  The experiments conduct on NLP or vision tasks also demonstrate that the proposed algorithm outperforms other personalization method.","The paper discusses using partial personalization objective function defined in equation (3) to achieve personalized models in federated learning. While the idea of partial personalization and the objective of (3) have been widely studied in previous literatures, the main contributions of the paper are the convergence analysis of two proposed algorithms FedSim and FedAlt in nonconvex case. The authors also did extensive experiments to compare partial personalized models with fully personalized models by using various model structures and on different datasets.","This paper studies personalization in federated setting, i.e., instead of collaboratively training a global model, personalizing a model for each client.  This paper proposed to personalize only part of the model parameters instead of the full model, and studied two algorithms FedSim and FedAlt. In local client updates, FedSim will simultaneously train the shared and personalized parameters, while FedAlt will train the personalized model first, then train the shared parameter. ","To overcome statistical heterogeneity among data shards in collaborative federated learning, this paper proposes a novel personalization schema which only requires smaller memory footprint in clients and possibly is less susceptible to catastrophic forgetting. The main idea is to split the trainable parameters into  shared and personalized parameters where, unlike existing personalization schema, only the shared model is exchanged with the server in communication rounds to be aggregated- thus partial personalization. In regression, this corresponds to learning the residual error of shared model via personalized model and in the classification setting corresponds to output averaging (unlike interpolation based personalization methods that do parameter mixing). The authors propose two algorithms FedSim (simultaneous updating of shared and personalized models locally) and FedAlt (alternative updating of shared and personalized models locally), to learn in this setting, theoretically analyze the convergence rates in non-convex settings and conduct empirical studies on image classification and next-word prediction to evaluate the proposed methods. ",0.1504424778761062,0.18584070796460178,0.23893805309734514,0.2345679012345679,0.2839506172839506,0.29577464788732394,0.20987654320987653,0.29577464788732394,0.17088607594936708,0.2676056338028169,0.14556962025316456,0.13291139240506328,0.1752577319587629,0.2282608695652174,0.1992619926199262,0.24999999999999994,0.19246861924686193,0.1834061135371179
372,SP:ee24606a968ab17b7827e7f3982af11636f6a2ee,"The authors deal with personalized model learning under the user-level joint differential privacy. In this problem, each user aims to build a user-specified model to make a prediction for the user's data. In the training process, the users utilize knowledge from the other users' data to improve their model's performance. In this setup, we want to prevent the leakage of the sensitive information in the user's data to the other user. The authors develop the differentially private algorithms for the model personalization task and reveal the upper bounds on the personalized model's accuracy of these algorithms. ","This paper focuses on a setting, where there are multiple users each holds a training dataset. In this setting, users have to share information about their model or data to learn a meaningful representation and to solve their tasks better than they could without using the shared, low-dimensional representation. The paper proposes an approach for this problem that provides strong user-level differential privacy guarantees at the same time. For the privacy guaranteees, the method uses exponential mechanism and utility analysis of the algorithm is made by using that.","This paper studies the problem of model personalization with user-level differential privacy in the setting where users don't have enough data to find a good solution on their own but can leverage information in similar learning problems shared by other users. The problem is viewed as learning a 2-layer neural network, where the first embedding is the shared structure and the second layer is trained individually. This paper provides two types of algorithms: inefficient algorithms based on the exponential mechanism that establish information-theoretic upper bounds on an achievable error and efficient algorithms based on alternating minimization, which starts from an initial embedding and then repeatedly uses a DP minimizer to minimize the error. For the specific case of linear regression with squared error loss, the paper shows the alternating minimization framework can converge to a near-optimal embedding. "," The paper presents an algorithm for differentially private (DP) learning of personalized models. A set of users with their own private data collaborate to privately learn a good embedding of the data followed by a linear regression based on this embedding.   The paper contribution is a theoretical analysis of the proposed algorithm with DP guarantees and utility bounds.   As far as I know there is no such bounds in the literature within the same setting. The author compare their result with the bounds in the non private case and with an upper bound obtained using the exponential mechanism which provides an information theoretic bound but cannot be efficiently implemented. In the later case, the bounds on the excess risk differs by O(1/\epsilon) and O(1/\sqrt(m)), m being the number of samples and \epsilon the privacy budget.  ",0.16666666666666666,0.24509803921568626,0.22549019607843138,0.23333333333333334,0.23333333333333334,0.18309859154929578,0.18888888888888888,0.176056338028169,0.16428571428571428,0.14788732394366197,0.15,0.18571428571428572,0.17708333333333334,0.20491803278688522,0.19008264462809918,0.18103448275862072,0.1826086956521739,0.1843971631205674
373,SP:ef18f4188426bc01be309633b486884b0e7a81a4,"This paper presents a theoretical analysis of the advantages of learning pruned neural networks . This analysis considers a teacher-student setup with a finite number of training samples. The theoretical results presented in the paper show that pruned networks can have multiple advantages, such as faster training convergence, a lower number of samples required for successful convergence and an enlarged convex region.","The paper analyzes the possibility of recovering the weight of a teacher network using a student network which is as dense as the original one. The authors prove that with enough samples drawn from the teacher network, the recovery can be done on one-hidden-layer neural networks with linear convergence speed. The authors also discuss the correlation between the needed samples and the sparsity of the teacher network.  To sum up, the well-designed structure makes the workﬂow clear and easy to follow, but the further analysis and discussion are expected to clarify some contributions in the techniques as well as in the evaluation section.","This work takes a theoretical view of LTH, leveraging the geometric structure of the objective function to analyze the generalization error of a pruned network trained in a teacher-student fashion. In particular, they prove that, as a model is pruned, the desirable (convex) region with high-guaranteed generalization performance enlarges, providing explanation for improved performance of winning tickets. Then, the sample complexity of training the pruned network to achieve zero generalization error is analyzed, finding that the number of samples required is proportional to the number of un-pruned weights. Interestingly, the work finds that pruned models enjoy faster convergence to high performance, providing another possible explanation of why winning tickets outperform dense networks. ","The paper gives theoretical explanation for improved generalization error of winning tickets for which only empirical results are known. Their results are based on teacher-student setting in which training samples are assumed to be generated from an unknown teacher network and student network is supposed to learn only from those samples. They give an accelerated gradient descent method for learning the pruned  network and convergence and sample complexity analysis for this algorithm. The empirical risk function is shown to have an enlarged convex region for a pruned network, which justifies the importance of the winning ticket. Learning on pruned network with the AGD algorithm gives a model closer to the teacher network with the same number of iterations, which implies better generalization of the trained pruned network. These findings are validated with experiments on synthetic and real datasets (MNIST, CIFAR-10).  ",0.24193548387096775,0.3387096774193548,0.24193548387096775,0.19626168224299065,0.205607476635514,0.21739130434782608,0.14018691588785046,0.1826086956521739,0.1056338028169014,0.1826086956521739,0.15492957746478872,0.176056338028169,0.17751479289940827,0.23728813559322032,0.14705882352941177,0.1891891891891892,0.17670682730923695,0.19455252918287938
374,SP:f0ad7cbc190113bb4612b7beca98d07aeff661fd,"The paper presents a method that uses latent label representations to model label correlations implicitly, for the multi-label text classification (MLTC) task. The method concatenates a set of randomly generated latent labels to input text tokens. Then the method uses this as the input to the BERT model. At last, the contextual encodings of these latent labels are used to generate predictions for the actual labels. The model has been tested and compared against the LACO algorithm [1] that sets the SOTA on the AAPD and RCV1-V2 datasets, and outperforms LACO using Hamming Loss and Micro-F1. Especially, the proposed method has even better performance than the baseline LACO algorithm, on the low-frequency labels and intensive-label samples.  1. Enhancing Label Correlation Feedback in Multi-Label Text Classification via Multi-Task Learning","This paper addresses the task of multi-label text classification by modeling the label correlations implicitly. Different from the previous works that explicitly model the label correlations, such as the label embedding methods, this paper proposes modeling the label correlations via latent labels. The proposed method outperforms the baselines on two multi-label text classification benchmarks in the reported experimental results.","This paper proposes to implicitly model label correlations in multi-label text classification. Different from previous studies (e.g., tree-based models) that describe label correlations explicitly, this paper appends ""latent labels"" to the beginning of each document and feeds it into a BERT classifier. These ""latent labels"" are randomly initialized, and the concatenation of their output is used for classification.  The authors conduct experiments on two benchmark datasets, AAPD and RCV1. Experimental results show that their proposed method outperforms several baselines for multi-label text classification. Ablation studies further show that using ""latent labels"" is better than using ""actual labels"" in their framework.",This paper consider multi-label text classification problem and propose a cross attention Transformer encoder to model the correlation between latent labels and input text sequence. The hidden states of those latent labels are concatnate as input to layers of MLP for the classification head. The experiment results show marginal gain over the baselines.,0.16296296296296298,0.22962962962962963,0.14814814814814814,0.4426229508196721,0.2459016393442623,0.19230769230769232,0.36065573770491804,0.2980769230769231,0.37037037037037035,0.25961538461538464,0.2777777777777778,0.37037037037037035,0.2244897959183674,0.25941422594142255,0.21164021164021163,0.32727272727272727,0.26086956521739135,0.25316455696202533
375,SP:f1eb66f24a14808d404f9ad9773ef4288efa060e,"The paper proposes a new slice-based approach to efficiently compute the Wasserstein distance between two distributions $\nu$ and $\mu$. The method termed ASWD (augmented sliced Wasserstein Distance) first  projects the samples from $\nu$ and $\mu$ onto a higher dimensional space using a non linear injective mapping function and then uses the classical random linear projections onto 1D to compute the sliced Wasserstein Distance. Overall, the procedure amounts to applied a spatial Radon Transform to perform the slicing. Theoretical results establish conditions  under which ASWD is a metric. A numerical algorithm is given along with the design of the injective mapping using NN. Empirical evaluations on simulation datasets and on generative modeling highlight the potential of the proposed method over existing approaches.","This manuscript introduces the concept of augmented sliced Wasserstein distances. The main idea is to extend the sliced Wasserstein distance based on mapping samples to higher -dimensional hypersurfaces. The proposed distance is shown to be a metric. Moreover, given that the optimal choice of the nonlinear maps is rather computationally intensive to obtain, an approximation based on neural networks is proposed. Several experiments are shown where a better performance is obtained with respect to existing methods.","This paper introduces the augmented sliced Wasserstein distance (ASWD), a new variety of sliced Wasserstein distance (SWD), that allows comparing two probability distributions by combining a nonlinear embedding of the sample data points to a higher-dimensional space with a slicing scheme to calculate 1D Wasserstein distances between uniformly projection directions. The authors introduce the spatial Radon transform, which includes the standard Radon transform and the special case of polynomial generalized Random transform (introduced in Kolouri et al. 2019). They further prove that ASWD is a valid metric if and only if the mapping is injective. Several experiments are conducted on generative modelling (CIFAR10, CelebA,  MNIST, color transferring).","This paper proposes a variant of sliced Wasserstein distance, named augmented sliced Wasserstein distance. ASWD maps input data points to hypersurfaces using neural networks, then calculates SWD on the hypersurfaces. ASWD alleviates the low efficiency problem of SWD for high-dimensional data. Various tasks including flow, generative modeling, and barycenters show the advantage of AWSD against some existing methods.",0.18032786885245902,0.23770491803278687,0.16393442622950818,0.27631578947368424,0.19736842105263158,0.14814814814814814,0.2894736842105263,0.26851851851851855,0.3389830508474576,0.19444444444444445,0.2542372881355932,0.2711864406779661,0.22222222222222224,0.25217391304347825,0.22099447513812154,0.2282608695652174,0.2222222222222222,0.19161676646706585
376,SP:f1f1df92e3e7c6b3b9e326a78a708c0d5d990c83,"This paper studies the the statistical error of the Deep Ritz Method and Physics-Informed Neural Networks using neural networks and truncated Fourier basis in solving PDEs. The static Schrodinger equation is used as a prototype PDE. With appropriate assumptions, the authors established upper and lower bounds of the error for both methods. The upper bound derived in this paper improves existing results with a faster rate. The authors also proved that the upper bound of PINN is nearly optimal. Some numerical experiments are conducted to verify the results.","This paper establishes statistical lower bounds and upper bounds for (a modified) Deep Ritz method and PINNs based learning of solutions of PDEs when the estimators belong either to a class of sparse neural networks or lie in truncated fourier basis. They utilize the fact that the objective in DRM and PINNS is strongly convex, and use it to get a faster generalization bound O(1/n) instead of O(1/\sqrt{n}). Given that the upper bound for the initial non-modified version of DRM does not match the lower bound, they introduced a modified deep ritz method, where the number of samples to estimate the gradient squared is greater than (the ration is provided in the statement) rest of the objective. This enables them to achieve minimax optimality for DRM as well.   Through their experiments they verify that the number of training samples n and the test error follows a power low with \alpha = 1/d as indicated by the derived rates.","This paper carries out a variety of studies, both theoretical and numerical, on numerical solution of a Schrodinger equation using deep learning inspired methods.  The main results are upper and lower bounds on a power law scaling for sample complexity, function of dimension and regularity, which are tight for one of the methods.  Another method (Deep Ritz) has a proposed improvement.","Applying deep learning (DL) to solving PDEs numerically has been a very exciting research directions. The current paper studies certain statistics properties of approximating linear Elliptic PDEs using neural networks (and truncated Fourier series). Under certain (quite strong) assumptions on the function class, the authors proved sharper (in some cases, tight)  bounds for the approximation errors. ",0.30337078651685395,0.16853932584269662,0.1797752808988764,0.0975609756097561,0.08536585365853659,0.16393442622950818,0.16463414634146342,0.2459016393442623,0.2857142857142857,0.26229508196721313,0.25,0.17857142857142858,0.21343873517786563,0.19999999999999998,0.22068965517241376,0.14222222222222222,0.1272727272727273,0.17094017094017094
377,SP:f2bee0c4a6c558970538b422e5e36750447cd9bc,"This paper considers the two-alternative voting problem with imperfectly informed voters. In this setting, each voter receives a signal which is positively correlated to voter's true preferences (over two alternatives). The goal is to aggregate these signals to find out the majority voting winner w.h.p. Specifically, the paper gives a mechanism such that  1) the mechanism will output the majority voting winner under the true preferences w.h.p.  2) the mechanism incentives every voter to truthfully report the signal they received.  The main technical contribution of this work is that their mechanism uses the idea of ""surprisingly popular"" approach (Prelec et al., 2017)  from the information aggregation literature. They show that ""surprisingly popular"" approach can be well adapted to two-alternative social choice setting and can incentivize voters to report truthfully (for two-alternative). In addition, they show their mechanism is user-friendly, and does not require voters to understand deep mathematics.","The authors consider elections that have two alternatives and three distinct classes of voters: candidate-friendly, candidate-unfriendly, and contingent agents. Candidate-friendly and candidate-unfriendly agents have made up their minds before the election, but each contingent voter's opinion depends on an unobservable state variable; they each receive a private signal about this state variable. The authors present a mechanism that elicits private signals from voters and which outputs the correct answer based on the state of the world. This mechanism is also strategyproof against even coalitions in the sense that truthful reporting is a strong Bayes Nash equilibrium. The mechanism draws on the ""surprisingly popular"" method of Prelec et al. ",The paper considers two alternative elections in which the voters are imperfectly informed. There is an unobservable state that may affect some voters’ preferences. Each voter receives a signal correlated with the state. The voter’s signals are conditionally independent and follow the same publicly known distribution. The designer’s goal is to elicit truthful information and aggregate the information to find the alternative that is preferred by the majority. The paper proposes a mechanism that guarantees truthful reporting is an approximate strong BNE and outputs the right alternative with high probability. ,"This paper studies a setting that combines social choice and information aggregation.  As in a classic social choice problem, individuals with preferences want to choose between two options, but as in an information aggregation setting they do not have the knowledge to know exactly what their preferences are, instead having only a signal correlated with the true state of the world.  The paper introduces a “Wisdom-of-the-Crowd-Voting” Mechanism, which implements the majority wish in an (approximate) strong Bayes Nash equilibrium.",0.15286624203821655,0.20382165605095542,0.12101910828025478,0.21238938053097345,0.1592920353982301,0.20652173913043478,0.21238938053097345,0.34782608695652173,0.2289156626506024,0.2608695652173913,0.21686746987951808,0.2289156626506024,0.17777777777777778,0.25702811244979923,0.15833333333333333,0.23414634146341465,0.1836734693877551,0.21714285714285714
378,SP:f5bf6d43bcc90a3bc5f2157fcc041f18224f95e0,"I think the paper sufficiently characterizes the federated linear contextual bandits problem (i) it proposes an algorithm for both the shared parameter and disjoint parameter setting (ii) proves regret guarantees and (iii) characterizes a reasonable class of policies for which a lower bound holds; this lower bound shows that the achieved regret guarantees are near optimal. The communication cost of the algorithm is also reasonable w.r.t to prior work. I think the writing of the paper can be improved in some areas and some technical points need to be explained more, but over all I think this is a good paper. ",The paper studies a federated setup where each client faces a stochastic contextual bandit.  The parameters of the bandits are coupled across the clientsm. The clients share their local estimates with the server which aggregate them and share back with the clients. The client are hetogenous and the server leverages the geomereic structure of the linear rewars to facilitates the clients learn the optimal arms.  Authors propose a collaborative algorithm called Fed-PE that do not require the nodes to exchange the local feature vectors thus preserving the privacy of the nodes. The authors consider the cases where the paramers are disjoint across the arms or coupled. The performance of the algorithms are valued on both synthetic and real datasets. ,"This paper discusses the federated multi-armed bandit problem with linear reward functions. The formulation utilizes a linear structure with agent-dependent contexts to model differences between agent reward functions. The authors propose an algorithm based on a multi-agent G-optimal design with competitive theoretical performance under heterogeneity. The authors additionally establish minimax rates under a reasonable collinearity assumption. The algorithms FedPE and Enhanced FedPE additionally provide competitive experimental performance on both simulated and real-world benchmarks. In summary, the paper is a good contribution, however, there are some issues that I would like to see addressed as well.","This paper considers a linear contextual bandits problem in a federated setting where different users have different contexts but the rewards are generated through common global parameters. The goal is to minimise the regret for each user while sharing the exploration across users. The paper proposes an algorithm termed Fed-PE with regret bounded as O(\sqrt{dMT}), where d is the dimension of contexts, M is the number of users and T is the time-horizon. A matching lower bound is also presented showing that this regret is tight. The main challenge in this setting is to \emph{balance} the exploration across users as contexts might be very diverse. In order to address this challenge the paper proposes a new G-optimal design which optimises for balanced exploration across users. ",0.18446601941747573,0.1553398058252427,0.23300970873786409,0.15833333333333333,0.18333333333333332,0.18,0.15833333333333333,0.16,0.183206106870229,0.19,0.16793893129770993,0.13740458015267176,0.17040358744394618,0.15763546798029557,0.20512820512820515,0.17272727272727273,0.1752988047808765,0.15584415584415584
379,SP:f5c80f76cb1e651fd808e7da4bfe6fdfd75b7155,"This paper proposes a new approach for continual learning that actively forgets the old knowledge when learning the new tasks. The approach uses Bayesian continual learning with synaptic expansion-convergence for active forgetting. The paper evaluates the new method on several continual learning benchmarks, including CIFAR10 regression, visual classification, and Atari reinforcement tasks.",This paper investigates how to avoid negative transfer in Bayesian continual learning setting. The main contribution is the proposition of an active forgetting mechanisms inspired by biological neural networks for mitigating negative transfer in CL. The experiments on CL benchmarks have been conducted to validate the performances of the proposed method. ,"The paper proposed a continual learning method that attempts to promote better forward transfer among similar tasks. Towards this, the authors attempt to suppress the negative knowledge, the knowledge that impedes learning a new task, from the previous task while learning a new task. This is done by forgetting such knowledge from the previous task without increasing the forgetting catastrophically. The authors claim that such “active forgetting” is biologically inspired. Similar to the Elastic Weight Consolidation (EWC), the authors derive the learning objective, by applying the laplace approximation to the posterior of a new task. The only difference is that the posterior of the previous task is replaced by a weighted product distribution, where a weight of ‘\beta’ is introduced to capture active forgetting. This change allowed them to derive a new regularization penalty whereby a separate model is trained for a new task and then subsequently merged with that of the previous one. It is in this merging that the negative knowledge of the previous task is eliminated. Experimentally, the authors show strong performance on regression, classification and RL tasks. The authors further show that their method can be added to other regularization- and memory-based methods to improve their performance.  ","This paper proposes a method for continual learning that aims to mitigate the problem of catastrophic forgetting by actively forgetting knowledge from previous tasks. The authors use a Bayesian continual learning framework to design their method. AFEC relies on identifying a set of expanded network parameters which are specific to the new task. The method incorporates a term in the loss function that considers the merge between these parameters and the network parameters being updated for the current task, therefore encouraging active forgetting. Catastrophic forgetting is also controlled by including a term that merges between parameters for old tasks and network parameters for the current task (i.e. the EWC term). Experimental results are reported for datasets on regression, classification and reinforcement learning tasks. These results show some advantages of the method in particular in the forward transfer setting. ",0.20754716981132076,0.39622641509433965,0.49056603773584906,0.3333333333333333,0.3333333333333333,0.19306930693069307,0.21568627450980393,0.10396039603960396,0.18705035971223022,0.08415841584158416,0.1223021582733813,0.2805755395683453,0.21153846153846154,0.16470588235294117,0.2708333333333333,0.13438735177865613,0.1789473684210526,0.2287390029325513
380,SP:f5e9fc0b1b6a41e43ba4dd0cfd99d5ec7008eedf,"The paper proposes a new approach to abstract syntax tree-based automatic program repair. The novel technique called deleted-subtree reconstruction is based on dropping parts of the syntax tree and training the model to grow them back. The method is evaluated against edit-based and sequence-based approaches, where it outperforms only the edit-based ones.","This paper presents a model over sequential structural tree edits, used for program repair based on ASTs. The model itself uses a graph encode and decoder to predict the sequential tree edits. The authors also introduce a method for pretraining the model on existing (non program repair) code data: they delete subtrees of arbitrary size from the code, and predict their reconstruction. The resulting model performs comparably to several other state of the art code repair models on the Patches in the Wild Java repair dataset, but with fewer parameters than several of the best-performing pre-trained models (CodeBERT, CodeT5). ","Automated program repair benefits from knowledge of its many properties, which includes its inherent (parse-)tree structure and graphical properties such as data-flow. This work proposes a graph-based encoder coupled with a tree-edit decoder, and optionally pretrained on a tree-based objective comparable to masked language modeling. The resulting model efficiently leverages relatively few parameters to achieve near-SOTA performance on a benchmark compiled from real-world bug fixes.","The paper presents GRAPHIX, a graph edit model for program repair. The work is directly related to Hoppity (Dinella et. al. 2020) which proposed using a sequence of graph edit for program repair. GRAPHIX employs multi-head graph encoder which improves upon Hoppity in terms of accuracy and complexity. Notably GRAPHIX is able to learn longer edit sequence and thus work on more program repair samples. The work has also proposed a pre-training task to improve model performance. Empirically the authors evaluated GRAPHIX on the *Patches in the Wild* Java bug-fix benchmark. It outperforms various baselines without pre-training. With pre-training, GRAPHIX-P stays roughly on par despite having much smaller model.",0.2631578947368421,0.15789473684210525,0.2631578947368421,0.10891089108910891,0.26732673267326734,0.20833333333333334,0.1485148514851485,0.125,0.13043478260869565,0.1527777777777778,0.23478260869565218,0.13043478260869565,0.189873417721519,0.13953488372093023,0.17441860465116282,0.1271676300578035,0.25,0.16042780748663102
381,SP:f63c10ba7d6f5ef1c167faa8a221b3ab5cc06006,"Inspired by asymptotic results from singular learning theory, the authors of the paper propose using a generalised gamma mean-field distribution with a normalising flow (that targets the ""desingularization map"") to perform variational inference. The authors additional build on prior work to derive a tighter bound on the log normalised evidence for variational inference using this approximate posterior combined with the ""correct"" desingularization map.   Note: while I am familiar with the Bayesian Neural Network literature, I am not familiar with singular learning theory. This is reflected in my confidence score. As such, I was unable to verify the theory in the paper, and assess the significance of the theoretic results included in the paper.","The paper analyzes the approximation of the log-ratio (called normalized evidence in the paper) (the log-probability under the model minus the log-probability under the true model). Since the model is singular, the Laplacian approximation of the normalized evidence needs a new method. The mean-field distribution and a resolution map allow us to estimate two important numbers (lambda and m) in the approximation.   The paper has made several contributions to the study of this problem in several aspects. First, the paper improves previous results by relaxing the assumption. Second, the paper actually uses a normalizing flow to actually learn the transformation.    ","Working with Bayesian neural networks, this paper proposed a variational algorithm to approximate posterior distribution of the network weights. To overcome model singularities, the authors used the idea of normalizing flow by transforming the weights through an affine coupling network, and subsequently worked on the desingularized parameter space. In addition, they derived an asymptotic expression for the ELBO, and compared the Gaussian and generalized gamma approximating families in the experiments.","The paper builds on the recent theoretic results of [Bhattacharya et al. (2020)], which shows that a mean-field variational approximation with carefully chosen approximation family leads to an ELBO $\Psi$ which is sharp up to a constant C(d) which only depends on the dimensionality d of the parameter space. For the proof, [Bhattacharya et al. (2020)] assumes that the approximation family is a Gamma distribution truncated to [0,1].  Therefore the authors of this paper conjecture that the Gamma distribution truncated to [0,1] is a good choice for the source distribution of a normalizing flow. Their experimental results suggest that this choice leads a higher ELBO than using as source distribution a normal distribution.",0.18421052631578946,0.14912280701754385,0.18421052631578946,0.1346153846153846,0.20192307692307693,0.21428571428571427,0.20192307692307693,0.24285714285714285,0.1794871794871795,0.2,0.1794871794871795,0.1282051282051282,0.1926605504587156,0.18478260869565216,0.18181818181818182,0.16091954022988506,0.19004524886877827,0.160427807486631
382,SP:f663a1e64155f1d4c890a6fefae596f67ef3cb11,This paper proposes a novel ERM-based method for classification task with group annotated training data. The goal is to be group distributionally robust while enhancing the minority performance. The authors make an improvement to an existing method named Group-DRO by modifying the focus on the group with the highest regularized loss to focus on the group that leads to the largest decrease in average training loss. They analyze the convergence and present detailed comparisons with Group-DRO.,"The paper proposes a new method for robust ML under distribution shifts. Past work has looked at formulations that minimize the worst group error. This paper adds a new twist on it and instead argues for focusing on the group that leads to the greatest decrease in average training error for all the groups. This intuition is combined into an algorithm and the paper proves that though their proposed algorithm doesn't minimize a specific loss function, it still finds first-order-stationary points. The results are shown on several synthetic datasets as well as on the WILDS Robust ML benchmark that show the superior performance of the proposed algorithm over several baselines.    Main Contributions:   1). The paper proposes a new approach for robust ML under distribution shifts that performs gradient descent not on the group with worst error but on the group which decreases the average error of all other groups.  2). Results are shown on synthetic and real-world datasets which show the superior performance of the proposed method in achieving group robustness. ",The paper gives a new algorithm for the setup where the test distribution is different from the train distribution. The setup includes multiple groups whose information is present during the training time but not during test time and the relative proportion of these groups change during test. The most commonly used method group-DRO does distributionally robust optimization or finds a classifier which performs well on the group with worst loss. This paper proposes to focus instead on the group which leads to maximum decrease in the loss while training instead of the group which has the maximum loss. The paper present several synthetic toy cases where their approach could be useful and concludes with experiments on a variety of benchmarks for this setup and shows improved results.,"The paper provides an efficient method to generalize to all groups in the presence of sub-population shifts and domain adaptation. The paper conducts extensive simulations to derive insights and also numerical experiments on the benchmark dataset to demonstrate the performance. The proposed method is intuitive, easily implemented, and has good performance.",0.31645569620253167,0.3670886075949367,0.16455696202531644,0.17714285714285713,0.10285714285714286,0.109375,0.14285714285714285,0.2265625,0.25,0.2421875,0.34615384615384615,0.2692307692307692,0.19685039370078738,0.28019323671497587,0.1984732824427481,0.2046204620462046,0.15859030837004404,0.15555555555555556
383,SP:f6b88e1fa1a84d82302d960c6a596fc2ba320bf5,"This paper extends works performing RL-based object localization by: 1. Conditioning the localization on a exemplary set of images, instead of more classical hardcoded finite set of classes 2. Introducing a new reward signal which doesn’t explicitly use IoU but instead enforces a rank-preserving metric space (where distances reflect how well 2 bounding boxes match up).  This is shown on multi-MNIST data, CUB and COCO. They are targeting rather challenging generalisation setups, where a policy is only trained to localize 1 digit class (in the MNIST case say) and extrapolates to all other ones. ","This paper presents a new metric learning method on the RL formulation of query object localization. The metric learning method is based on a contrastive learning formulation of ordinal embeddings, which are pretrained with data augmentations and a loss formulation that respects IoU orderings. The experiment results have demonstrated that the proposed embedding metric increased the task performance compared with the baseline metric (IoU), and shown better generalization behavior on novel image categories. ","The paper proposes a reinforcement learning approach for localization in images. The idea is to consider and environment where the states are crops of an image, the actions are changes in the current coordinates of the current crops starting at the full image. The method consists of learning both a reward function and a policy that maximizes that reward function. The policy is learnt with REINFORCE with entropy regularisation. The manner in which the reward function is formulated and learnt is quite appealing. To allow for more flexibility at test time, the reward function is made conditional on some exemplars and learnt in a few shot manner based on state features. Using a triplet loss to ensure the reward function is increasing as the overlap is increasing is quite good.  The experiments are quite extensive and I find compelling for the most part. Ablation of the elements of the method is well executed and benefits of few short learning are compelling enough in my opinion. The results on COCO are a bit limited but I would be willing to overlook that.   ","This paper proposes a new method for the task of query object localization. It learns an embedding network such that the image crop that is closer to the query object will have a closer embedding distance as well, and the improvement of the embedding distance will be used as the reward function. The RL agent is then employed to maximize the reward. The paper found that the proposed approach compares favorably to DDT on object location on CUB and FTA on few-shot object detection on COCO.",0.15306122448979592,0.17346938775510204,0.1326530612244898,0.2465753424657534,0.2876712328767123,0.15469613259668508,0.2054794520547945,0.09392265193370165,0.14942528735632185,0.09944751381215469,0.2413793103448276,0.3218390804597701,0.1754385964912281,0.1218637992831541,0.14054054054054058,0.14173228346456693,0.2625,0.208955223880597
384,SP:f6e7229b653a5a56a2993864cdb70809f5b6f9b4,This paper proposes Proto-Trex model to increase the interpretability of the text classification systems. The proposed model mainly adds a bunch of prototype layers to learn the similarity between the query and prototypes. They also propose an extension caleld iProto-Trex to interactively learn from users' feedbacks. Experimental results and example cases show that the Proto-Trex could give comparable classification accuracy compared to plain classification model (w/o explanation) and could provide reasonable explanations. ,"This paper introduces a method for improving interpretability of black box transformer based text classifiers. The approach is based on “case based reasoning” where the network classifies an input by comparing it against a library of learnt prototypes (each with a corresponding label) and classifying the input based on a weighted similarity with the prototypes. Thus, in a sense it is more interpretable than an end-to-end classifier since a user can directly look at the similarity scores with the prototypes to understand the labeling decision. The approach also promises to be more “faithful” than post-hoc interpretability methods since the predictions are based directly on similarity scores. The paper also shows that end-users of the system can interact with it by either editing prototypes (if they are experts) or providing weak feedback about which prototypes are good, and this feedback can be integrated to re-learn better prototypes. From experiments we see that their approach is competitive with standard end-to-end finetuning while being more interpretable. Based on sufficiency and comprehensiveness scores, we see some evidence that the explanations from prototypes are faithful, though it is unclear if these scores are competitive. ",This paper aims to model explanation and task prediction such that task performances are not (or less) traded off for interpretability. It proposes a novel framework for transformer models where classification and explanation generation are based on shared prototype embeddings which are learnt from training data by a combination of losses. The framework is also compatible with settings that requires human in the loop for extra supervision on prototype learning. Experiment results show that adding the proposed ProtoTrex benefit task performances on 3 sentiment classification tasks.,"The premise of the paper is that example train instances with the corresponding label can be an effective explanation of the model's prediction rather than post-hoc explanation which is not deterministic. The framework jointly trains label prediction and prototype clustering. Then, the framework computes the similarity between input and prototypes to retrieve the most similar prototype as an explanation while predicting the label as well. Moreover, the framework can reflect human-in-the-loop feedback on the prototype. The primary result of the paper is that their framework can show the proper explanation by a similar example while preserving the performance.",0.27631578947368424,0.18421052631578946,0.19736842105263158,0.09183673469387756,0.12244897959183673,0.19767441860465115,0.10714285714285714,0.16279069767441862,0.14563106796116504,0.20930232558139536,0.23300970873786409,0.1650485436893204,0.15441176470588236,0.17283950617283952,0.16759776536312848,0.1276595744680851,0.16053511705685616,0.1798941798941799
385,SP:f7a7c81ed2b6e9eb958b8b751deed8166622540c,"The authors proposed a novel transductive novelty detection method using the disagreements of the ensemble models. More specifically, the authors directly utilize the unlabeled test set samples, providing different labels for those unlabeled samples, and training multiple models with fine-tuning. Then, this framework tries to identify the OOD samples using the disagreement of those samples. The authors provide some promising experimental results to discover the OOD samples in transductive settings.",The authors develop an ensemble-based procedure for semi-supervised novelty detection (SSND). It utilizes a mixture of unlabeled ID and OOD samples to perform on near OOD data. A regularization technique is further used  to promote diversity on the OOD data while preserving agreement on ID data.,"The paper presents an ensemble-based semi-supervised learning method for novelty detection.  The goal of their training scheme is to create an ensemble of models that has a high disagreement on the out-of-distribution (OOD) samples in the unlabeled set. The training resembles a self-training algorithm that labels the unlabeled pool and uses implicit regularization via early-stopping to find a ""sweet spot"" in terms of disagreement on OOD samples between the models in the ensemble. The final decision of whether an input is considered as out- or in-distribution is based on a hypothesis test using the average disagreement between the softmax outputs in the ensemble. In-distribution samples should be accurately classified with high confidence and show little disagreement in the ensemble while out-distribution samples should be detectable as the different models will produce different outputs. ","The authors introduce a semi-supervised ensemble approach to novelty detection. The main idea consists in generating base classifiers that disagree on the out-of-distribution data (ODD). An early-stop criterion is used to achieve the wanted level of disagreement among the component of the ensemble.  The main assumption is the availability, for training, of clean labeled in-distribution (ID) data, and of unlabeled data which include both ID data and out-of-distribution (OOD) data. The unlabeled data follows the same distribution of the data the ensemble is tested on.  Each base classifier is first trained on the labeled in-distribution (ID) data. For a given base classifier, the unlabeled data is randomly assigned to one of the labels (a different label is chosen for each base classifier), and the classifier is tuned on the resulting labeled data. The tuning is stopped when the optimal tradeoff between high validation accuracy (on inliers) and low training error are achieved. This regularization process is aimed at avoiding fitting the incorrect assigned labels in the unsupervised portion of the training data and to achieve disagreement among the base classifiers on the OOD data.",0.16901408450704225,0.3380281690140845,0.3380281690140845,0.3958333333333333,0.4375,0.2605633802816901,0.25,0.16901408450704225,0.125,0.13380281690140844,0.109375,0.19270833333333334,0.20168067226890757,0.22535211267605632,0.18250950570342206,0.19999999999999998,0.175,0.2215568862275449
386,SP:f7b7dfafb03090a2c940ba738234a6c80bd5ad0e,"The paper explores a gradient-free constrained optimization framework for generating diverse policies. Its main contributions are: (i) a computationally light metric to measure the diversity of a policy with respect to an existing set of policies, based on the Wasserstein metric W2; (ii) a practical algorithm with instant feedback at every timestep, inspired by constrained optimization to generate diverse policies. Experiments on continuous benchmarks show that the proposed algorithm, IPD, can generate diverse as well as well-performing policies, and outperform competitors relying on multi-objective optimization.","The paper aims at enabling the learning algorithms with the capacity of solving the task with multiple solutions. The paper introduces a new metric to evaluate the difference between policies. It also proposes a practical novel policy-seeking algorithm, derived from the interior point method in the constrained optimization literature. The algorithm is evaluated on multiple mujoco tasks.","The paper proposes Interior Policy Differentiation, a variant of Interior Point Method, for learning diverse policies in RL. The paper uses the empirical Wasserstein distance  to measure the difference between policies, then formulates the novel policy generation problem as a constrained optimization problem. However, the novelty of the paper is limited and the empirical evaluation is not convincing to make it pass the bar of ICLR 2022. ",The authors describe an approach to regularise policy iteration methods towards regions of novel policies. They propose that constraint optimisation is superior to regularised objective functions. They attempt to to derive the algorithm form theoretical principles and demonstrate its superiority on a couple of artificial RL experiments. ,0.17045454545454544,0.18181818181818182,0.11363636363636363,0.27586206896551724,0.10344827586206896,0.11940298507462686,0.25862068965517243,0.23880597014925373,0.2127659574468085,0.23880597014925373,0.1276595744680851,0.1702127659574468,0.2054794520547945,0.20645161290322583,0.14814814814814814,0.25599999999999995,0.11428571428571427,0.14035087719298248
387,SP:f7e8602b40b37f26277e3f44f60a11f879978986,"This paper handles the distributional shift observed in data in clients in a federated learning production setting. In particular, the distributional shift is modeled as a mixture of distributions.Furthermore, a multi-branch network is used to encapsulate the shifting distribution and a Federated Expectation-Maximization algorithm enhanced by Temporal priors of the shifting distribution (FedTEM) is proposed. Experiments for image classification on EMNIST and CIFAR datasets, and next word prediction on the Stack Overflow dataset show the efficacy of the proposed algorithm.","his paper proposes a federated learning method to address a specific non-IID challenge over clients, i.e., when some clients are only available during the daytime and others are only available at the night, and these two types of clients' data are drawn from a mixture of two corresponding distributions. They study a multi-branch model, similar to multi-task learning, such that all clients share the same backbone network to extract data representations but a different branch is applied on top of the backbone network for different distributions. Only the clients with the same data distribution use the same branch to produce the final prediction. Under the assumption that the data representation produced by the backbone network follows a mixture of isotropic Gaussian distributions, this paper proposes an EM algorithm to assign clients to different clusters: the E-step determines the (soft) membership of each client belonging to each cluster and assigns it to the branches, while the M-step estimates the mean and variance of the representation for each Gaussian component in the mixture. Some additional heuristics are shown to be important for this method to achieve promising performance, e.g., label smoothing, a temporal prior of the ratio of clients from day and night, linear/cosine/soft schedule of this ratio, one-hot (hard) assignment of clients to clusters, etc. In experiments, they simulate the day-night distribution shift on EMNIST, CIFAR, and Stackflow datasets in FL by splitting each dataset into two parts with different data types. They evaluate the proposed method under different shifting settings (different p) with a few baselines proposed in this paper. They also empirically studied the effects of label smoothing and different period lengths.","The paper is to solve the distribution shift between daytime modes and nighttime modes in federated learning with a mixture of distributions. The proposed method could be viewed as a two-group clustered federated learning method. In particular, the client clustering is based on the model parameters of prediction layers.","This paper investigates the problem of periodic client distribution shift in cross-device FL, as previously investiged by Eichner et al (semi-cyclic SGD). The authors propose to split the NN model in a shared part, with different heads for each component. A GMM model is added on top of the output of the shared part to allocate each client to a given head during training, with a temporal prior for the shift between classes. This GMM is fit via a federated EM scheme, in addition to the federation of the NN weights themselves. Experiments are conducted on EMNIST, CIFAR10/100 and StackOverflow datasets, for 2 components. The results demonstrate the superiority of the proposed method with respect to baseline approaches, including the addition of the temporal prior.",0.3493975903614458,0.21686746987951808,0.30120481927710846,0.06690140845070422,0.11971830985915492,0.32,0.10211267605633803,0.36,0.1953125,0.38,0.265625,0.125,0.15803814713896458,0.2706766917293233,0.23696682464454977,0.11377245508982034,0.16504854368932037,0.1797752808988764
388,SP:f7f96d545a907887396393aba310974f4d3f75ff,The paper proposes a new graph neural network model for predicting the dynamics of n-body systems. The two main innovations of this model are: 1. Ability to handle systems with rigid-body constraints.  2. A new 3D transformation equivariant layer with universality properties.  ,This paper develops a model that augmented interaction networks with mechanical constraints and develops equivariant message passing schemes to ensure physical realism in the model outputs. The work focuses on a set of toy systems that contains particles connected by sticks and hinges and the developments are made for these systems in particular. There are some results on generalised equivariant message passing and the universality of the proposed method. ,"The authors propose Graph Mechanics Networks (GMN), a E(n) equivariant model that explicitly models systems with rigid constraints, with exact constraint preservation by means of using generalized coordinates. In my opinion the main contributions are as follows:  C1. Establish math for writing models that operate on cartesian coordinates, make predictions in very generalized coordinates and then converted back to cartesian. This is an approach that has been used in previous work, but is usually hidden as an implementation detail.  C2. Extend the E(n) (Satorras et al.) equivariant message function to take and output a variable number of vectors with a proof demonstrating that the approach is flexible enough to actually approximate any E(n) equivariant function. I think this is a novel low level modeling contribution.  C3. Extent the E(n) (Satorras et al.) model to build an E(n) equivariant model for exact constraint preservation. Although a bit niche, this is the first model of that nature.","The paper focuses on the formulation of a graph neural network approach towards the learning of update rules of constrained systems, extending prior work (Interaction entwork, EGNN) by learning the updates of constrained components in the generalized coordinates expressing those constraints and incorporating forward and inverse kinematics in the algorithm. Comparisons are made with Linear, Basic GNN, TensorForce Networks, SE3-Transformers, Radial Field and EGNNs by evaluating them on a constrained version of the N-body problem introduced by Kipf et al. 2018 and it is shown that the proposed method better adheres to constraints and generalizes better on the studied problem.",0.25,0.25,0.3409090909090909,0.2318840579710145,0.2463768115942029,0.13125,0.15942028985507245,0.06875,0.14705882352941177,0.1,0.16666666666666666,0.20588235294117646,0.19469026548672566,0.10784313725490198,0.20547945205479454,0.13973799126637554,0.19883040935672516,0.16030534351145037
389,SP:f831d25830efa88434b43e900241a5ad81119360,"This paper proposes a method for modularizing neural networks inspired by programming functions and compilation. Building on the success of attention and transformer architectures, the neural interpreter (NI) proposes a new architecture built on layered functions, each of which is composed of stacked MLP and attention layers. Functions and input variables both have embeddings, called “signature and type” respectively, that allow NIs to learn specialized functions that operate over a subset of inputs. The NI is applied to a suite of image datasets as well as to Raven matrices, where it is shown to outperform vision transformers.","This paper focuses on improving the generalization ability of deep models to the unseen but related distributions to training data, and thus enhancing interpretability. In specific, a Neural Interpreter is proposed, which factorizes the inference of self-attention networks as the dynamic routing within functions. The Neural Interpreter can flexibly compose computation on a per-sample basis, and it is end-to-end trainable. Some interesting experiments including image classiﬁcation and visual abstract reasoning are conducted to validate the effectiveness of the proposed method.","This paper proposes Neural Interpreter (NI), a drop-in replacement for Transformer layers that can be applied on set-valued inputs. The motivation behind the architecture is that common computation units in deep learning such as convolution layer is rigid in how they process the input, so to overcome this, NI leverages self-attention to dynamically determine computation path using its modular functions depending on the given input. The paper shows how NI performs well in transfer-learning and systematic generalization tasks.","The paper proposes a model based on self-attention, factorised in different modules. It processes a sequence of tokens and could be viewed as a generalisation of a Transformer. The key idea is to have different modules (associated with a function) that behave differently but are general and actually share parameters. The core module of ModAttn can be viewed as a self-attention module with dynamic weights that depend on a context / code c. Using n_f learnable codes, each assigned to a ‘function’ leads to n_f modules, each with different functionality. An important aspect is the fact that each function only affects a subset of tokens, just the ones that are close to a type (learnable parameters) of the function. ",0.17525773195876287,0.16494845360824742,0.21649484536082475,0.16470588235294117,0.21176470588235294,0.17073170731707318,0.2,0.1951219512195122,0.1721311475409836,0.17073170731707318,0.14754098360655737,0.11475409836065574,0.18681318681318682,0.17877094972067037,0.19178082191780824,0.16766467065868262,0.1739130434782609,0.13725490196078433
390,SP:f885c992df9c685f806a653398736432ba38bd80,This paper proposes a novel defense to prevent model stealing by requiring users to solve proof-of-work puzzles. The authors evaluate their method against different types of model extraction attacks. The results show that the defense will result in the attacker costing higher computational time (100x) than legitimate users (2x). ,"This paper proposes a new defense to the model stealing problem that an attacker queries a supervised machine learning model service to have data labeled to use as training data and copy the functionality of the model. The proposed method is proactive defense that slows down the attacker in obtaining the labeled data, based on the information leakage estimator. The evaluation shows that existing attacks using out-of-distribution data can be slow down more than a regular user querying in-distribution data.","The paper discusses ML model extraction attacks while using APIs for accessing them on the public networks. Although some methods exist for preventing or making the attacks hard, all of them have substantial impacts on legitimate users’ experience while using the system, including the slower models or lower accuracy in results. This paper proposed a method for dissuading the attacker by increasing the cost of the attack. Solving a puzzle for all users before getting the final response (POW) would be the solution noting that the difficulty will be increased if the system identifies any adverse behaviors. This method requires no modification of the victim model and can be applied by machine learning practitioners to guard their publicly exposed models against being easily stolen.","This paper proposes a novel defense against model extraction attacks. The proposed approach slows information leakage by asking all users to answer a puzzle before receiving the response to their query (proof-of-work). The difficulty of the puzzle allows to keep the computation overhead low for legitimate users, while rendering information leakage prohibitively expensive for attackers. The puzzle difficulty is calibrated based on an estimate of how much information each user has already acquired. This is based on PATE, a differential privacy metric. Experiments are performed on multiple datasets, opposing the proposed defense to a wide range of attacks, including adaptive adversaries.",0.35294117647058826,0.21568627450980393,0.37254901960784315,0.20481927710843373,0.21686746987951808,0.16129032258064516,0.21686746987951808,0.08870967741935484,0.18446601941747573,0.13709677419354838,0.17475728155339806,0.1941747572815534,0.2686567164179105,0.1257142857142857,0.24675324675324675,0.16425120772946858,0.19354838709677422,0.17621145374449337
391,SP:fa11c4da16c01c6a3449f15b25a6e4e228ebbf4a,"This paper mainly proposes a new hierarchical topic discovery framework that is guided by prior (tree-structured) knowledge. The construction of the framework is broken down into two steps: Modeling semantic hierarchy with a Gaussian-embedding-based topic model (Gaussian SawETM), and injecting the prior semantic knowledge by regularizing via entailment relationship. The model is evaluated on a set of benchmarks with perplexity, topic quality, and clustering/classification protocols. Qualitative studies are also shown to demonstrate the interpretability of the model.","The authors propose a topic model that aim at embedding words and topics into a latent space via Gaussian embeddings. Additional constraints in the training procedure allows to incorporate prior knowledge on the topics, so that they are able to include semantic topic relations.  The model is evaluated on a large set of public datasets and on a number of tasks. It achieves better results with respect to baselines on clustering tasks, and comparable on classification tasks. However, the main advantage is the increase interpretability of topics and topic hierarchy.","The paper builds on the recently proposed SawETM hierarchical topic model and combines it with a knowledge graph so that each node in the knowledge graph corresponds to a topic. They change it so that every topic is embedded with a Gaussian distribution. This enables to enforce a hierarchical ordering constraint on the topics. Experiments show that the model discovers deeper, more interpretable topics and finds better document representations.","(Summary) This paper proposes a generative model that can learn hierarchical topics that coherently incorporate a structured prior knowledge graph. By introducing Gaussian embeddings to an existing SawETM, a neural topic model where topics in different levels can relate each other, the authors first propose the Gaussian SawETM. Then their TopicNet based on Gaussian SawETM can learn hierarchical topics coherently to an input knowledge-graph. TopicNet is not only capable of learning hypernym-induced topics as hierarchies, but also equipped with rich interpretability. ",0.2839506172839506,0.16049382716049382,0.2345679012345679,0.2111111111111111,0.17777777777777778,0.17391304347826086,0.25555555555555554,0.18840579710144928,0.2289156626506024,0.2753623188405797,0.1927710843373494,0.14457831325301204,0.2690058479532164,0.1733333333333333,0.23170731707317072,0.23899371069182387,0.18497109826589597,0.15789473684210525
392,SP:fa34d40d07c0f154a69841b241a2743fe721f95c,"This paper studies how regularization affects the robust population risk in the overparameterization regime. The authors study the behavior of linear regression and logistic regression under sufficient statistical settings. In particular, even in the noiseless regime, the population robust risk benefits from the regularization, but not the population test risk. The argument is empirically verified. ","The authors analyze the generalization performance of regularized adversarial linear regression and classification models for standard Gaussian data in the overparameterized setting.  The analysis allows for l^2 weight decay as well as l^\infty perturbation of the inputs.  Using similar methods to Hastie, Montanari, Rosset, and Tibshirani [18] and Javanmard and Soltanolkotabi [19], they are able to develop precise formula for the generalization error in the limit as n/d -> constant as n,d -> infinity.  They demonstrate that non-zero regularization has strictly better generalization in the robust setting.  ","The paper studies the benefits of non-vanishing regularization for improving generalization in overparameterized classification and regression problems. In this setting, previous works studied the implicit bias effect. Particularly, it was shown that training models *without* explicit regularization can still yield good generalization performance. In contrast, in this paper, the authors show that explicit regularization can further improve generalization, especially w.r.t. the adversarial robust risk.",This paper compares the effect of interpolation and regularization on robust generalization. It shows that in the case of regression under squared loss and classification under the log loss when the ground truth is a linear model robust generalization error is smaller for regularized estimators. The robust generalization error is defined as the error under consistent adversarial perturbations. ,0.23636363636363636,0.21818181818181817,0.2727272727272727,0.17777777777777778,0.15555555555555556,0.19402985074626866,0.14444444444444443,0.1791044776119403,0.25862068965517243,0.23880597014925373,0.2413793103448276,0.22413793103448276,0.1793103448275862,0.1967213114754098,0.26548672566371684,0.20382165605095542,0.18918918918918917,0.20800000000000002
393,SP:fa405481f36da10f8ca8d9d5c066458236806a12,"The authors proposed a novel active learning framework integrated with the neural process. The neural process is used to mimic the simulator dynamics which is later used for Bayesian active learning. They also proposed a new acquisition function utilizing the latent from the neural process. The experimental results show that INP works better to accelerate stochastic simulation than GP, and the proposed LIG leads to faster convergence compared with alternatives.  ","The manuscript entitled, ""Accelerating Stochastic Simulation with Interactive Neural Processes"", presents a novel approach to the problem of statistical emulation for mechanistic models of epidemic disease transmission.  To this end, structured neural processes are developed to exploit and respect the temporal and spatio-temporal character of these models.  An active learning strategy is developed to train these neural processes to minimise the computational costs of generating training instances of disease simulator outputs.  The methodology developed is applied to two SEIR compartmental model examples: a minimal one with a single homogenous population and a maximal one with many age and space delimited cohorts.","This paper tackles learning a flexible distribution approximator to approximate the output of a high-dimensional and computationally intensive stochastic simulator.  The contributions of this paper then reduce into three main components:   1 - Definition of a spatiotemporal neural process that can succinctly model a more richly structured latent process.  2 - Definition of a new acquisition function and using a neural process in an active learning setting.  3 - Application to epidemiological simulators to “compile” estimation.  The method appears to work, validated on a toy-ish SEIR model, and a more sophisticated pre-existing epidemiological model.   ","This study proposes a new method for learning surrogate models of stochastic simulators. The new method, Interactive Neural Process (INP), builds on Neural processes and leverages the spatiotemporal structure of the problems at hand to reduce the complexity of the inference task. On a few problems in epidemiology -- a low dimensional SEIR problem (2 parameters, 100-dimensional output), and a complex spatiotemporal LEAM-US problem --, the approach is shown to appropriately learn the surrogate in few iterations.",0.2,0.21428571428571427,0.18571428571428572,0.18627450980392157,0.16666666666666666,0.1595744680851064,0.13725490196078433,0.1595744680851064,0.16883116883116883,0.20212765957446807,0.22077922077922077,0.19480519480519481,0.16279069767441862,0.18292682926829268,0.17687074829931973,0.19387755102040813,0.18994413407821228,0.17543859649122806
394,SP:fa4bc3f6ad3f2a0113a930fb49d68660d63910e8,"This paper focuses on time-series (TS) segmentation. They claim that in the typical approach for this problem -- where you is to apply a model (e.g., a temporal conv net) over fixed windows in time in sliding window fashion -- it is challenging to predict precise breakpoints, especially when the labels change frequently relative to the sampling rate of the input data/sensors. They also claim that these approaches ignore long-term dependences. They introduce a network which they ""obviates"" the need for sliding windows and can precisely find breakpoints.  Their claimed contributions are:  * A conceptual framework for TS segmentation * An architecture for solving TS segmentation problems * An adaptation of DeepLabv3+ for TS segmentation problems * State of the art results","The paper presents a supervised method (called SegTime) for time series segmentation that is based on stepwise time series classification. The method avoids sliding windows (which is the typical approach), thus avoids the specification of window size and stride. It also seems to be insensitive to the label changing frequency and this constitutes a major advantage over other approaches. The network architecture is based on two core modules: a novel multiscale skip LSTM (called MSS-LSTM) that employs LSTMs with skip connections and a very deep CNN (called 1D-DS-ResNet). Several other modules are also included such as 1D Depthwise Separable and Atrous Convolutional layers and the Atrous Multiscale Pooling module (AMSP). The method is evaluated on two datasets, one with fast changing labels and one with slow changing labels. ","The paper presents a new deep architecture for segmentation of time series, i.e. to find the subsequences inside of a time series corresponding to different classes and to determine the bounds of those subsequences. The proposed architecture includes two core components: a) a kind of LSTM with skip connections to deal with the multi-scale problem of time series, b) an encoder-decoder module based on CNN and ResNet. The outputs of the two components are then used in a convolutional layer to provide a stepwise classification at each time step. The approach is evaluated on two classical datasets wrt 3 baselines.","The paper presents a stepwise segmentation for time series data, namely SegTime. Contrary to the sliding window approach, SegTime takes the whole sequence as input, and process it in two separate modules: the MSS-LSTM network and the 1D encoder-decoder network. Outputs from the two separate networks are concatenated and taken as the input to the final convolutional layer to produce the final output, which has class labels for each time-step segment. This paper’s contributions can be summarized as follows:  (1)	By predicting in a step level rather than a sliding window, it works well on both fast- and slow-changing labels.  (2)	High parameter efficiency is achieved with depthwise separable convolution, atrous convolution, and skip-LSTMs. (3)	Long-term dependency is captured, using MSS-LSTM and 1D convolutional layers.   ",0.15,0.14166666666666666,0.13333333333333333,0.24427480916030533,0.22137404580152673,0.2621359223300971,0.13740458015267176,0.1650485436893204,0.12030075187969924,0.3106796116504854,0.21804511278195488,0.20300751879699247,0.14342629482071712,0.15246636771300448,0.1264822134387352,0.27350427350427353,0.2196969696969697,0.22881355932203387
395,SP:fa9e46f1dc70dbe87ff53a6b8dd5419c14b40ef3,"This paper focuses on developing a robust GNN model to defend against adversarial attacks on both graph structures and node features. More specifically, a general unified graph neural network is proposed to learn a graph structure and node features to correct the adversarial attacks. This a two-step approach. First, one operation is applied to reconstruct the graph with the node features. Then, the node features are updated by another operation with the learned graph structure. Experiments are conducted on four small datasets for evaluation.","The authors propose a General Unified Graph Neural Network (GUGNN) framework to address graph and feature denoising. They use this framework to design a graph neural network (R-GUGNN) which is Robust against poison adversarial attacks in a semi-supervised node classification setting. The authors demonstrate on four common datasets (Cora, Citeseer, Cora-ML & Polblogs) that their method is generally more robust against perturbation compared to commonly used graph neural networks (GCN, GAT) and models which are designed to be robust in the same setting (RGCN, GCN-SVD, Pro-GNN).","Considering the problem of adversarial manipulations of graphs, the authors propose a framework for ""cleaning"" the graph structure and its features to obtain more robust predictions. The core contribution, compared to existing works, is the additional considering of the feature cleaning. Technically, the authors first perform the graph cleaning (phrased as some kind of sparse, trace minimization problem) followed by some feature cleaning (phrased as some kind of feature diffusion/propagation). Experiments on standard (small) benchmark datasets shows some improvements.","The paper proposes a robust model for graph neural networks (GNNs) to defend against adversarial attacks. While most of the defense mechanisms in previous work focus on identifying and fixing the perturbed structure of the underlying graph, this paper proposes to jointly fix the perturbed structure as well as the node features. The proposed objective function captures these components along with weighting by some hyper-parameters. The experiments include several baselines and show results on different attack models: targeted attack, untargeted or global attack, and random attack. The experiments also have results on ablation study as well as hyper-parameters. ",0.21176470588235294,0.21176470588235294,0.2823529411764706,0.14444444444444443,0.15555555555555556,0.1875,0.2,0.225,0.24,0.1625,0.14,0.15,0.20571428571428574,0.21818181818181817,0.2594594594594594,0.15294117647058822,0.1473684210526316,0.16666666666666663
396,SP:faad5fe1eefbcc2e24638383d0bde7ad7975ff4e,"This paper attempts to alleviate the shortcomings of existing local-feature-interaction explainers that assume symmetrical feature interactions by proposing a bivariate feature-explanation map that can capture asymmetrical (directional) feature interactions; this analysis provides evidence of mutual and directional redundancy, offering a comprehensive understanding of the features most influential for a given prediction. This proposed approach can also be instantiated using any univariate feature-based explanation method. Empirical results on image, text, and tabular data show the ability of the proposed method to accurately identify mutual and directional redundancies.","In previous studies, Shapley value has been widely used to explain model predictions, in which previous studies generally characterized importance for each feature instance separately. While some works have explored the effect of combinatorial features on prediction, no works have explored how features interact to influence each other, such as whether feature A is redundant if feature B is present.   In order to achieve this goal, this paper offers an intuitive solution. The degree of feature interaction is estimated using a matrix called a bivariate Shapley explanation map. The j-th column and i-th row of the matrix represents how feature i influences the prediction if feature j has been included. Item ij can be seen as a revised Shapley value: rather than summing up the marginal contributions of all coalitions, it only considers them when feature j appears. They then propose four different types of interactions based on this definition: least/most influential features, directional/mutual redundancy. The experimental results indicate that the proposed method can efficiently discover these four types of feature interactions. ","In this paper, the authors generalize the univariate Shapley method to bivariate Shapley method. The authors first build a directly graph based on the asymmetric bivariate Shapley value (adding feature j to all sets contained feature i). Then several graph algorithms are applied to analyze the directly graph to derive (1) univariate feature importance available in univariate approach and (2) relations like mutually redundancy only available in bivariate approaches. Experiments on several datasets with comparison to existing methods demonstrated the superiority of the proposed method.",This work proposed to study the directional feature interactions to explain deep models. The proposed method is a graph-based explainer and the data can be considered as graphs. Then it studies the Bivariate Shapley values to consider the directional feature interactions. Experiments on several datasets show very promising results. ,0.25555555555555554,0.18888888888888888,0.15555555555555556,0.10795454545454546,0.10227272727272728,0.2,0.13068181818181818,0.2,0.28,0.2235294117647059,0.36,0.34,0.17293233082706763,0.19428571428571428,0.2,0.14559386973180077,0.1592920353982301,0.2518518518518518
397,SP:fb0efa670729796471a7a562b231172103bb8749,"In this paper, the authors present a general and scalable approach to obtain shallow embeddings of many entities. The authors achieve this by first mapping each entity into a code vector using LSH, and then learning a neural decoder on top of the code vectors to reconstruct the original embeddings/adjacency matrix. The authors then apply their approach to node classification problems, where node features are unavailable (the authors ended up artificially removing node features from existing OGB datasets). The superior performance is demonstrated over the random code vector baseline.","This paper studies the embedding compression problem related to GNNs and graph representation. A two-stage method is proposed to generate the compressed embeddings: firstly, it encodes each node into its composite code with hashing; secondly, it uses a MLP module to decode the embedding for the node. Experiments are performed to evaluate the compression effect with both pretrained graph embeddings and node classification task with GraphSage. ","In this work, the authors propose a hashing-based node embedding compression approach, which utilizes the random projection hashing method to generate a code vector for each node using auxiliary information such as the input graph adjacency matrix. The proposed method is memory-efficient in the training procedures of Graph Neural Networks (GNN) models. Experiments also demonstrate that the proposed method outperforms other coding schemes in both the embedding reconstruction task and node classification task.  ","The paper presents a vertex embedding method with good scalability to large graphs.  The proposed method is based on locally-sensitive hashing and compositional coding: a memory-efficient binary representation of each vertex is constructed from a hashing technique and then, when needed, a decoder creates real vector embeddings of the vertices via compositional coding from a pool of codebooks. The encoding is constructed from prior knowledge and requires no training. Conversely, the decoder is trainable and can be learned end-to-end. The experiments show improved performance. A negative note is that some parts are unclear to me.",0.18888888888888888,0.2,0.2,0.26865671641791045,0.19402985074626866,0.21333333333333335,0.2537313432835821,0.24,0.18181818181818182,0.24,0.13131313131313133,0.16161616161616163,0.2165605095541401,0.2181818181818182,0.1904761904761905,0.2535211267605634,0.1566265060240964,0.1839080459770115
398,SP:fb935a5c44d7df6958d39ab1ef877956df08994e,"The paper proposes a new neural architecture named MRMD-AE that can be applied on noisy fMRI data in different tasks. Subject-specific decoders are used to more directly recover individual signals, while the encoder is shared across every subject under the assumption that every person will share common low-dimensional features for the same stimuli. A key component of this architecture is the usage of a regularisation term named PHATE (previously introduced in the literature) which allows the latent space to not be split into individual embeddings and be extendable for unseen data. The paper empirically shows how this architecture improves metrics on classification tasks when compared to previously used techniques. ","The paper proposes a model to learn a low-dimensional representation of fMRI data over multiple subjects of the same experiment. The model is built as an auto-encoder, with an encoder shared across subjects, and a separate decoder per subject. The model is regularized so that the first layer of each decoder gives a representation close to a pre-computed manifold embedding. An optional regularization also constrains the shared representation to be similar across subjects.  The paper then proposes a series of experiments to demonstrate the benefits of the learn representation. The experiments consider the tasks of (a) projecting new test samples to the learned manifold, (b) classify some stimulus features from the embedding (decoding task), and (c) predict brain recordings of a new subject.","The submission proposes a new model for functional alignment of fMRI datasets from multiple subjects. It combines a one-in-many-out autoencoder with two regularization loss terms (one inspired by GRAE) to develop a model that can encode every subject's data to a shared latent space, from which each subject's data is then decoded by a separate decoder. Experiments are provided that demonstrate benefits to some downstream tasks.  ","This paper proposes a neural network-based modeling strategy to learn a common latent space from multi-subject fMRI data. In addition to capturing a useful common latent space, the proposed technique is further able to disentangle common representational patterns from subject-specific variations through the use of subject-specific decoders. The authors impose meaningful and desirable priors on the latent embedding, like geometric regularization and cross-subject embedding alignment. Unlike other manifold learning techniques, the proposed deep neural network modeling framework lends itself well to extendibility to new data (stimuli) since the PHATE embeddings are only required at training time in the geometric regularization loss. The proposed framework is tested on two large fMRI datasets and an improved stimulus decoding (from the shared space) and cross-subject translation accuracy is achieved over competitive baselines. ",0.20535714285714285,0.125,0.1875,0.1746031746031746,0.20634920634920634,0.19718309859154928,0.18253968253968253,0.19718309859154928,0.15555555555555556,0.30985915492957744,0.1925925925925926,0.1037037037037037,0.19327731092436976,0.15300546448087432,0.1700404858299595,0.22335025380710657,0.19923371647509577,0.13592233009708735
399,SP:fbf023a772013e6eca62f92982aecf857c16a428,"[UPDATE: Thanks to the authors for engaging with the comments in the review.  I hope to see them fully addressed in the revision and look forward to seeing the next version of the paper!]  The paper investigates how pretrained representations might be expected to help prediction on a downstream task.  In some sense, they should ""obviously"" help: the downstream predictor can learn to exploit any mutual information between its target output and the pretrained representations.  That's the standard motivation for multitask learning.  However, the paper exhibits specific settings in which the target output can be *exactly* predicted from the pretrained representations, using only a *simple* probe such as a linear classifier.    The paper is primarily theoretical, although a few experiments are included by way of illustration.  It is not clear what the experiments are intended to verify.  To make the analysis possible, the pretrained representations in the paper are not underlying representations such as word embeddings.  Rather, they are identifiable given the pretraining data, which makes it clear what ""successful pretraining"" means.  For example, the representation of word x_i in a sentence is the conditional distribution of x_i given all other words in the sentence, as predicted by a pretrained cloze language model.  The analysis assumes (though the experiments do not check) that the predicted conditional distribution actually matches the true conditional distribution.  This should be the case in the limit of infinite training data, provided that the cloze language model has adequate capacity and can be trained to a global optimum of log-loss.  It is shown that if the vocabulary is large enough, or under other assumptions, then this representation of x_1 is enough to recover the posterior distributions over h_1 and h_0, which are assumed to be sufficient for the downstream task.  Caveat: In the analysis, the observed sentences are assumed to be generated by a latent-variable model.  One would then expect the downstream task target output to be predictable from the actual latent variables.  However, the paper assumes more strongly that the target is a function of the *posterior distribution* of the latent variables given the observed sentences.  In other words, the goal of the downstream task is not to recover what the speaker *actually* meant, but only to recover what a hearer who knows the true generating distribution *thinks* it meant (given only the sentence and no other context).  Two structures are considered for the latent-variable model and its relationship to the downstream task.  For the more complex version, the predictor is also more complex, requiring argmax attention over all positions i.  In both cases, the paper considers allowing the predictor to prepend an arbitrary word embedding x_1 to the input.  This soft prompt can serve to shift the posterior predictions of x_i so as to be more informative for the downstream task.   ","This work puts forward a theoretical analysis of pre-trained language models and head and prompt tuning on downstream tasks. In particular it studies the assumptions on the distribution of language and downstream tasks under which the downstream label can be recovered from a pretrained LM by using head or prompt tuning, by considering a generative model of language that can be represented as a HMM. The key components of the analysis include: 1. The data distribution is generated from a HMM (or a memory augmented HMM). 2. The initial state of the HMM, $H_0$, contains all the information necessary to recover the downstream label for the sequence.  Under this setup, for head tuning, the downstream labels are recoverable under the condition that the token emission probability matrix $W$ has linearly independent columns (implying that the dimensionality of the HMM hidden states is smaller than the vocabulary size of the data generating distribution). On the other hand, prompt tuning requires only a relaxed version of this condition, only the columns corresponding to the support of the downstream task need to be independent, eliminating the condition on the hidden state dimensionality.  Under memory augmented HMMs, for downstream tasks that can be represented as a linear function of the memory (with a fixed number of cells, $N$), the non-degeneracy condition can be relaxed even for head tuning. For prompt tuning, the independence assumption can be further relaxed to hold only for the set of memory cells that the downstream task depends on.  The authors further perform simulation experiments to validate the findings on different settings of the data generating distribution. Simulation results agree with the findings: 1. Prompt tuning outperforms head tuning in the setting where hidden state size is significantly larger than the token vocabulary size. 2. When the data is generated using memory-augmented HMMs, and the downstream task information can be recovered entirely from the memory, even head tuning gets close to perfect performance for all hidden state sizes.","The paper studies the question why head and prompt tuning work for pre-trained LM. Analyzing complicated neural networks is a daunting task. The paper offers an analysis framework based on the Hidden Markov Model (HMM) and memory augmented HMM. Under this framework, the paper makes a connection with masked LM (BERT) that at time step i, the output of BERT(`[mask]_i`) represents the marginal P(X_i) of the HMM. The framework further allows for analysis of a binary classification task using head tuning or prompt tuning via the posterior distribution of a HMM. To this end, the paper runs a simulation study with data generated from HMM and memory augmented HMM. ","This paper proposes an analysis framework to link the pre-training and downstream tasks. In this framework, the downstream task needs to predict the properties of the posterior distribution over latent variables in an underlying generative model. And When the generative model is a standard HMM, the downstream recovery is possible with a simple classification head under strong non-degeneracy assumptions. It further shows that changing the generative model to a memory-augmented HMM or using prompt tuning can relax the non-degeneracy conditions.",0.16736401673640167,0.07112970711297072,0.06485355648535565,0.12048192771084337,0.0963855421686747,0.21052631578947367,0.24096385542168675,0.2982456140350877,0.36904761904761907,0.3508771929824561,0.38095238095238093,0.2857142857142857,0.19753086419753085,0.11486486486486486,0.11032028469750892,0.17937219730941706,0.15384615384615385,0.24242424242424243
400,SP:fcb14510ef8541f320ef1c3cab4c0c017e2e15b9,"The paper presents a multi-view method for undistorting documents. The method makes use of an existing method (IDR) for reconstructing distorted documents in 3-D, with an additional bijective neural network to parameterize distorted surfaces by rectangles. The method significantly outperforms SOTAs in the experiments.  ","This paper presents document unwrapping based on optimization of neural networks. Four MLPs are defined to model the implicit surface of the target, the colors of the target, the mapping function from 3D points to 2D texture coordinates, and the inverse mapping from 2D to 3D. Similarly to NERF, the target example is approximated by training the MLPs using differentiable rendering.   ","**High-Level Overview**  This paper proposes a neural rendering technique to learn how to estimate an implicit surface and its UV-parameterization from a set of 2D images. The proposed approach is applied to the problem of document unwarping, wherein a wrinkled, folded, or bent document (e.g. a piece of paper with text on it) can be digitally flattened with its contents preserved. For good measure, the paper also shows that learning the texture mapping enables the contents of the unwarped document to be edited in the texture space. The proposed approach is evaluated against a prior work, DewarpNet (Das et al., 2019) on the basis of Multi-Scale Structural Similarity (MS-SSIM) and Local Distortion (LD) as well as the auxiliary task of OCR.  **Key Contributions**  * Builds on IDR (Yariv et al., 2020) to learn an explicit bijective texture mapping for a learned implicit surface * A loss function formulation for the framework * Application of learned implicit surfaces for document unwarping with reduction in texture distortion and improvements in subsequent OCR metrics  **How It Works**  * Starts with the same problem formulation as IDR: an MLP to model the surface (Eq. 1) and an appearance MLP to model the color (Eq. 2) * Modifies the appearance MLP to also be a function of texture coordinates (Eq. 3) * Adds an additional forward and backward MLP to learn the bijective UV mapping * Jointly trains all MLPs using a pixelwise rendering loss","The authors present a method to jointly learn a shape and UV parameterization of a creased document observed from multiple views. They use an SDF based implicit shape representation in combination with a recent neural volumetric renderer IDR, and optimize the energy composed of the main photometric term (a discrepancy between the observed and rendered image) and regularization terms enforcing well behaved UV space and a fixed scale conformal mapping which corresponds to a behaviour of a real paper material. Since the implicit field representation does not explicitly model a surface, the authors cannot directly optimize the 3D->2D mapping using standard graphics techniques and rather adapt the IDR renderer to be conditioned on the locations sampled from the UV space and train it jointly with the MLPs defined for the bijective mapping 2D->3D and 3D->2D. The authors show that they can get better results both quantitatively and qualitatively when compared to one of the SotA works DewarpNet.",0.15217391304347827,0.30434782608695654,0.2391304347826087,0.4098360655737705,0.3114754098360656,0.14285714285714285,0.11475409836065574,0.058823529411764705,0.06875,0.10504201680672269,0.11875,0.2125,0.13084112149532712,0.09859154929577464,0.10679611650485436,0.16722408026755856,0.17194570135746604,0.1708542713567839
401,SP:fd0d72d0689f170f8157dc7f79deb01348e414b3,"This paper studies the task of multilingual Open-Domain Question Answering.  The task setting requires building a system that can answer natural language questions in any language on an open set of domains. Models are required to answer in the same language the question was asked in.  The authors tackle the task using a fairly conventional retrieve-and-read ODQA model, with the necessary modifications to make both the retriever and reader component language-agnostic, by exploiting multi-lingual pretrained models, training on available multilingual training data, and using and a clever data augmentation setup to increase multilingual training data.   The proposed solution “CORA”, operates over wikipedias in many different languages.   The retrieval component “MDPR” is a multi-lingual extension of DPR, using MBERT. This model can retrieve evidence documents across different languages, rather than having to match the language of the question. This means that a question expressed in a low-resource language can benefit from retrieving evidence from high resource languages. It also allows for a question written in one language on a cultural topic associated with a different language to retrieve from the most appropriate language’s wikipedia.   The reader “mGEN” is a generative reader, implemented using mT5, that takes as input the concatenation of the top-K retrieved passages from the retriever (which may be several different languages) and the question, and generates the answer in the question language, token by token.   The main difficulty of the proposed solution is how to train the MDPR and MGEN model components, given the limited multilingual raining data. The initial model is trained with NQ (english) and TyDi QA and Xor-TiDi QA (multilingual data). To increase the amount of multilingual training data and language coverage,  the authors introduce a data augmentation strategy where the initial model is used to harvest potential additional data. This exploits the wikipedia “language links” resource. This iterative approach works by retrieving passages, looking up passages on the same topic in other languages using “language links”, and using the reader to see whether these can be used to generate the correct answer - if they can, they are added as additional training data for the next round. Another expansion mechanism exploits the fact that most answers are wikipedia entities, and can be translated using annotations available in the wikipedia “language links” resource.  CORA is thoroughly evaluated on TyDI QA and an evaluation-only dataset called MKQA, and performs very strongly, improving the state-of-the-art by a wide margin in almost every setting.. Thorough ablations give insights and quantify the the effects of the system, and demonstrate zero-shot question answering abilities   ","This paper presents a multilingual dense passage retrieval model called mDPR to deal with the multilingual/cross-lingual IR and QA tasks. The mDPR part is firstly trained using existing labeled multilingual Q-P pairs. Then, possible passage pairs are selected from a given multilingual corpus based on the trained mDPR and used to train an answer generation model called mGEN. Next, given each question, the trained mDPR is further used to retrieve more possible passage candidates in other languages from Wikipedia corpus, where the retrieved passages that can lead to correct answers are considered as positive instances, the others as negative instances. With these expanded new question-passage pairs, the training of mDPR is performed again. The above procedure is conducted in an iterative manner. Evaluations are conducted on TyDi QA and MKQA in the open-domain QA setting. Several baselines are included and compared. The key contribution is to do DPR in the multilingual/cross-lingual setting.",The paper attempts to provide a solution to an interesting problem of cross language retrieval based question answering in a zero shot (without requirement of any language-specific annotated training data). Authors propose a new DPR algorithm that to be able to retrieve documents relevant to the question across languages (for multiple languages). The paper extends retrieve-the-generate based open domain QA system to a cross-lingual setting. ,This paper addresses the multilingual open QA task by combining a cross-lingual passage retrieval algorithm with a multilingual autoregressive generation model. The approach is thus capable of answering questions where the answer and the question are in different languages.  The authors extended existing annotated data using an iterative training approach that improves retrieval for languages with limited resources. The experimental results showed that the proposed method (CORA) outperformed the previous state of the art on multilingual open question answering benchmarks across 26 languages.,0.11187214611872145,0.06164383561643835,0.0821917808219178,0.12578616352201258,0.1509433962264151,0.2028985507246377,0.3081761006289308,0.391304347826087,0.42857142857142855,0.2898550724637681,0.2857142857142857,0.16666666666666666,0.1641541038525963,0.10650887573964497,0.13793103448275865,0.1754385964912281,0.19753086419753085,0.1830065359477124
402,SP:fd1a9b4c5ee36159286f4a35fa93ed0c23120906,"This paper mainly focuses on the reweighting algorithms (e.g. Importance Weighting, Group DRO) for the worst-group performance. It tries to theoretically explain why overfitting problems always appears in reweighting algorithms. Specifically, the authors prove that under several conditions (e.g. assumptions for algorithms, wide fully-connected neural networks, squared loss), the worst-group test performance of the reweighting algorithm will converge to the same level as ERM.","This work studies the problem of training for worst-case subgroup performance. This has been a very prominent research avenue in the past few years and provides a natural approach towards learning fair and ""causal"" models (that do not perform too badly for any pre-specified subgroup). In practice however there have been some difficulties with existing approaches, and e.g., in the overparameterization regime, these methods do not perform well in terms of  worst-case performance on test sets. This paper provides a theoretical explanation for this fact (using linear models and linearized neural networks) and also illustrates it using numerical experiments on two datasets. Furthermore, the authors assess the role of $L_2$ regularization in training. The practical takeaway is that existing approaches based on reweighting require substantial regularization or early stopping to perform well in terms of test accuracy (in the overparamerized regime). ","This paper theoretically proves the empirical observation that has been made in a lot of recent works regarding the role of importance reweighing in overparameterized deep networks. For squared loss and independent data points, it theoretically proves that reweighing the data points has no effect on the final model learned for linear models and linearized neural networks. This work also analyzes the role of regularization in preventing this behavior and proves that regularization has to be large enough to prevent small training error to have any effect. This work also has supporting empirical results on two datasets- celeba and waterbirds.","The authors study the problem of improving the worst-group accuracy in the over-parametrized setting. They offer theoretical and empirical evidence that reweighting does not affect the final solution in this setting; in other words, the algorithm would converge to the same interpolators as that of basic ERM. The theoretical analysis is done for linear models, linearized networks, and wide fully connected networks.   The authors also argue that the solution does not change unless a considerable amount of regularization is incorporated, so much so that it affects the training error. Again, some theoretical and empirical evidence is provided.  ",0.2753623188405797,0.2028985507246377,0.2318840579710145,0.1780821917808219,0.1917808219178082,0.21,0.13013698630136986,0.14,0.16161616161616163,0.26,0.2828282828282828,0.21212121212121213,0.17674418604651163,0.16568047337278108,0.1904761904761905,0.2113821138211382,0.22857142857142856,0.21105527638190957
403,SP:fd3c33c9237d0f1e33d896858a46c48da2216fe3,"This paper proposes a block coordinate descent algorithm for rotation learning. The algorithm is based on Lemma 1 and Theorem 1. The rotation matrix on SO(n) is decomposed into diverse simple Givens rotation matrices. Then the optimized variable is converted into these Givens rotation matrices so that the rotation matrix is always on SO(3) and the projection is not required anymore. The authors also discuss how to select the coordinate, including random strategy, greedy strategy, and steepest strategy. Different from the existing work, it considers multiple Given rotations matrices in one step. ","Current rotation learning methods are trying to minimize quantization distortion for fixed embeddings, which are not applicable to an end-to-end training scenario where embeddings are getting updated constantly. Therefore, this paper tries to address this issue to fully enable end-to-end training of Product Quantization (PQ) based embedding index with retrieval models, by using mathematical studies of the decomposition of orthogonal group. They proposed a family of block Givens coordinate descent algorithms to learn rotation matrices that are provably convergent on any convex objectives by leveraging geometric intuitions from Lie group theory. Authors claimed that their algorithms are much more parallelizable, reducing runtime by orders of magnitude on modern GPUs, and converge more stably according to experimental studies in comparison to the state-of-the-art SVD method.   Their main contributions can be summarized as follows:  - Changing the landscape of learning rotation matrix in approximate nearest neighbor (ANN) search from SVD based to iterative Givens rotation-based, to be applicable to end-to-end neural network training.  - Proposing a family of Givens coordinate block descent algorithm with complexity analysis and convergence proof.  - Proves that for the fixed embedding, their algorithm shows similar convergence result as the existing rotation matrix learning algorithms. Therefore, their proposed algorithm is able to learn the rotation matrix more effectively for the end-to-end training.",This paper proposes to learn rotation matrix by Givens coordinate descent algorithms in the context of minimizing the quantization distortion for efficient storage. The proposed family of Givens coordinate descent algorithms are based on geometric intuitions of the special orthogonal group and are provably convergent on any convex objectives. The experiments show that the proposed algorithms are much time efficient and lead to performance improvement in an end-to-end training of embedding indexes.,"The paper address rotation matrix learning during product quantization in modern ANN embedding search systems. The main contribution is addressing rotation matrix learning via gradient descent of small rotation updates. The approach relies on the decomposition of any small rotation matrix into a product of Given rotations, so that partial derivatives can be obtained in parallel, but the product causes the computation of the rotation matrix itself to be slow, in O(n^2) matrix multiplications, hence the need to select a subset of coordinates and do coordinate descent. Experiments on product quantization show a marginal improvement in results over existing approaches OPQ and Cayley.",0.24468085106382978,0.18085106382978725,0.2127659574468085,0.14798206278026907,0.11659192825112108,0.22972972972972974,0.1031390134529148,0.22972972972972974,0.19047619047619047,0.44594594594594594,0.24761904761904763,0.1619047619047619,0.14511041009463724,0.20238095238095238,0.20100502512562815,0.22222222222222227,0.15853658536585366,0.1899441340782123
404,SP:fd4ab1cb777b541c22a923c1c86d82ac1d8384fd,"The paper proposes a series of techniques and a new model to learn to solve the TSP using RL. First, a number of preprocessing steps are presented in order to transform a TSP instance into a standardized form. Then, a new encoder-decoder architecture is proposed, based on GNN, MLP and attention modules. The authors propose a modification of the policy gradient algorithms for training by replacing the value of the policy-generated solution by its value after applying a local search heuristic, and use the original value as a baseline. Finally they use a curriculum learning approach to increase the difficulty of the instances seen during training, the difficulty here being represented by increasing the size of the instances, within a given interval. Experimental results are presented on standard synthetic and realistic TSP instances of size up to 1000 nodes.  ","The paper employs equivariance properties and local search heuristics to derive a deep learning model capable of generalising to TSP instances of different sizes. The model constructs a tour, one city at a time, by learning a distribution (policy) over unvisited cities which is optimised via REINFORCE. The solution is further refined with a combination of different local search heuristics, and policy gradients are computed with respect to the final improved solution to smooth the loss landscape. The equivariance in TSP is mostly exploited during preprocessing steps—where rotation, translation and reflection operations are applied to the coordinates of each city—but is preserved by the model architecture which consists of a graph neural network and attention mechanisms. The model is trained with stochastic curriculum learning only on small TSP instances ($\leq 50$ cities) but is shown to have competitive performance on problems with up to 1000 cities.","This paper proposes a novel combination of policy gradient and local search algorithms for the traveling salesman problems (TSP). The main idea is to apply local search to tours generated by policy rollout and compute policy gradients using the (potentially) improved tour from local search. In addition to the algorithmic contribution, this paper also presents a collection of preprocessing steps to ensure problem instance features are equivalent. Empirical studies are provided on random TSP instances as well as those in TSPLIB. Finally, ablation studies show the importance for each component of the proposed algorithm.","This paper introduces a deep RL approach combined with an equivariant model (to handle Euclidean symmetry) and local search heuristics (to improve a tour) to solve traveling salesman problems (TSP), in particular focusing on the generalizability of large-scale instances. The model consists of a graph neural network (GNN), a multi-layer perceptron (MLP), and an attention mechanism. In the training part, this model involves smoothed policy gradient, and stochastic curriculum learning to speed up the training and make the policy more generalizable. The experiments results show that the proposed approach significantly outperforms most learning-based solvers on large-scale randomly generated TSP and realistic TSP.",0.24113475177304963,0.15602836879432624,0.20567375886524822,0.1554054054054054,0.20270270270270271,0.20212765957446807,0.22972972972972974,0.23404255319148937,0.27358490566037735,0.24468085106382978,0.2830188679245283,0.1792452830188679,0.23529411764705882,0.18723404255319148,0.23481781376518215,0.19008264462809918,0.2362204724409449,0.19
405,SP:fdabafe7d5ca2239a241eba04e1f16cb1ac2316b,"The paper propose a faster algorithm to learn approximate differentially private NLP models. Pretrained NLP models are often very large. Practical procedure involves fine-tuning NLP model on private data, which may leak private information. To avoid leakage, DP-SGD (and DP-AGAGRAD, DP-ADAM) uses norm clipping on each sample’s gradient, and then add isotropic noise to aggregated gradients for samples in a batch. Then the normal update steps of SGD, ADAGRAD, or ADAM are performed. This will ensure privacy under differential privacy definition. However, this procedure requires computing per-sample gradient and keep them in the memory so that the norm of the gradient can be calculated, and the per-sample gradient clipping can be performed. This will introduce memory overhead proportional to the batch_size$\times$#params, which is impossible for very large NLP models.   This paper propose GhostClipping method to save the memory, without the need of per-sample gradient instantiated. The idea is compute the partial sum of gradient element-wise square using two small matrices of the size of $T\times T$, where $T$ is sequence length (<=1024 in practice), and aggregate them to obtain the per-sample norm (just a scale for each). And then it performs a second back-propagation to compute aggregation on the clipped gradient. It will uses almost the same memory as standard SGD (or ADAM), with one forward pass and two backward passes.   This paper also introduce two additional techniques to improve, one is choose a larger batch size, and the other is to introduce multi-task finetuning (fine-tuning includes masked prediction task on the target dataset). These two are important in boost the performance of the final models.   The paper evaluates on text classification, data-to-text generation, and dialog generation tasks. The results shows it gains prior DP methods.     ","This paper investigated the problem of privately fine-tuning large language models for downstream NLP tasks, including sentence classification and language generation. The authors showed that by appropriately selecting hyper-parameters (including batch size, learning rate, training epochs, and clipping norm) and making the fine-tuning task aligned with pretraining tasks, directly fine-tuning large language models with DP-SGD yields strong performance, and provided an empirical guideline for setting a good training configuration. The authors also proposed ghost clipping trick for further memory saving when fine-tuning large language models. Finally the authors showed through experiments that low dimensional updates do not necessarily lead to better performance.","In this paper, the authors propose a method that applies DP-SGD to NLP tasks. DP-SGD protects the privacy of the model training against that the individual information about the training samples is detected or inferred. The method is applied to the fine-tuning phase of the pre-trained language models (e.g. bert, gpt), thus it achieves good performances for many applications. To adapt DP-SGD to NLP models, this paper proposes ghost clipping that allows clipping in DP-SGD to run without instantiating per-example gradients for any layer in the model.","This paper adapts the widely used DP learning algorithm, DP-SGD, to language models. It achieves to fine-tune the dataset while protecting the private information in the dataset. In this paper, the authors conduct some empirical studies on language models and find some useful conclusions (e.g. fine-tuning on a part of parameters with DP is enough). The authors verify the model on sentence classification, table-to-text generation, and dialog generation tasks, using various pre-trained language models (e.g. GPT, Bert).",0.07213114754098361,0.08852459016393442,0.09836065573770492,0.17592592592592593,0.14814814814814814,0.23157894736842105,0.2037037037037037,0.28421052631578947,0.35294117647058826,0.2,0.18823529411764706,0.25882352941176473,0.10653753026634383,0.13499999999999998,0.15384615384615383,0.18719211822660098,0.16580310880829016,0.24444444444444446
406,SP:feb4664dfd5066cff582f6b4f9b17c6169049ceb,"The authors study two graph partitioning problems MAX-K-CUT and correlation clustering (MAX-AGREE) under a limited memory setting. The authors propose polytime sampling-based algorithms for these two problems that require $O(n + |E|)$ memory while preserving the best approximation guarantees ($n$ is the number of vertices and $E$ is the edge set). In addition, for the case of dense graphs the authors eliminate the quadratic memory requirements by utilizing spectral sparsification results in the streaming model of computation.  ","This paper studies several methods for speeding up / reducing memory of computing max k-way cuts, via methods such as sampling and sparsification. They rigorously prove that the semi-definite program can be approximated to an error of \epsilon in about m \epsilon^{-2} space, and furthermore, the quality of approximations are preserved with the use of spectral graph sparsifiers.  A Matlab implemention of this method is then tested on a multitude of clustering instances, showing that the memory usage is quite low, but at a cost of a fairly large (between 15~40%) relative error.","The paper presents an algorithm for approximating MAX-$k$-CUT to within the SDP bound that avoids the $\Omega(n^2)$ memory requirement of the SDP formulation. The improvement is to $O(n+m)$ space. Here, $n$ is the number of nodes of the input graph and $m$ is the number of edges. Coupled with known sparsification techniques, the algorithm runs using $O(n\log n)$ space (hiding a factor that depends on how close one gets to the SDP guarantee. The algorithm uses Frank-Wolfe with Gaussian sampling on a relaxation that replaces the constraints by some regularization. Similar results apply to correlation clustering, though such bounds were known previously.","The methods with best approximation guarantees for max-K-cut and the max-agree variant  of correlation clustering involve solving semidefinte programs (SDPs) with $n^2$ variables and $n^2$ constraints, which use  a lot of memory. 	This paper proposes a modified polynomial-time Gaussian sampling-based algorithms from [27] that reduce the memory from $\mathcal{O}(n^2)$ to $\mathcal{O}(n + |E|)$ and nearly achieve the best approximation guarantees. The paper further extend their approach to the dense graphs cases by combining it with graph sparsification and the memory is reduced to $\mathcal{O}(nlog n / \tau^2)$.",0.16049382716049382,0.2716049382716049,0.3333333333333333,0.19791666666666666,0.1875,0.1891891891891892,0.13541666666666666,0.1981981981981982,0.27,0.17117117117117117,0.18,0.21,0.14689265536723162,0.22916666666666669,0.2983425414364641,0.18357487922705312,0.1836734693877551,0.1990521327014218
407,SP:ff2433f2de48d4ed8017e27bd6cf606845cdea9e,"This paper introduces a learned centralized exploration reward for multi-agent settings. The exploration reward is factorized into an on/off gate (dubbed ‘switching control’) and a scale function. Some mathematical derivations are included (sketched in the main text, with details in the appendices) to provide theoretical guarantees on how the exploration reward changes the solutions the training procedure might find. Evaluations on gridworld environments that target specific difficulties of multi-agent exploration show promising results. Some maps from the SMAC benchmark are also included, and again show good results.","This paper proposes a method, Learnable Intrinsic-reward Generation Selection (LIGS) to improve coordinated exploration. LIGS incorporates an extra agent, called Generator to learn what state to give what intrinsic reward for each agent. The intrinsic reward is potential-based, so it preserves the optimality. Experimental results on several domains show its advantages over several MARL methods. ","The paper describes a novel reinforcement learning algorithm for multi-agent system (MARL) that employs a generator of intrinsic reward and a switching control system that helps to regulate intrinsic control. Crucially, the intrinsic reward is learned to better fit the particular task being learned. The paper claims that the proposed algorithm helps with exploration as well as preservation of known policies. The paper has a strong theoretical background with a section that illustrates the properties of convergence and optimality. The experimental results appear to justify the approach with superior performance with respect to the baselines. The paper deals with an emerging and interesting area of RL and proposes a new mechanism for co-ordinated RL agents.  ","The paper focuses on learning intrinsic rewards for multi-agent reinforcement learning, which is an important problem. Different from previous works on this topic, the authors propose to train an agent with a learnable gating function that incentives other agents. Theoretical analysis and empirical evaluation are provided to prove the effectiveness of the proposed method.",0.14444444444444443,0.2,0.15555555555555556,0.24561403508771928,0.14035087719298245,0.1282051282051282,0.22807017543859648,0.15384615384615385,0.2545454545454545,0.11965811965811966,0.14545454545454545,0.2727272727272727,0.17687074829931973,0.17391304347826086,0.19310344827586207,0.16091954022988506,0.14285714285714285,0.1744186046511628
408,SP:ff3c787512035e2af20778d53586752852196be9,"This paper propose a wrapper tool that mounts on top of supervised Lifelong Machine Learning (LML) frameworks, leveraging a well-known method data programming. The contributions of this paper can be summarized in three aspects. 1)  Adapting automatic label generation by semi-supervised learning/data programming to LML in some special scenario. 2) Implementing a LML wrapper that can accomplish some tasks under some restrictions. 3) Through detailed experimental results prove the superiority of its method.","This paper proposes  data programming method, named Mako,  for semi-supervised continual learning. Mako automatically generates labels for unlabeled data with a set of weak labeling functions, each of the functions is trained on subset of training set with bootstrapping. Experimental on several datasets demonstrate the effectiveness of the proposed methods. ","This paper presents an interesting idea of using data programming techniques to enable continual semi-supervised learning with limited labeled data. It proposes a stage-wise pipeline where probabilistic pseudo labels are first produced by a Snuba based Data Programming framework, then calibrates them by the temperature scaling, and finally inputs into the mounted Lifelong Machine Learning (LML) tools. Experiments show that the proposed framework achieves similar performance to fully supervised methods.","In contrast to previous works on lifelong machine learning (LML) that put their focus on the supervised learning settings, this paper concentrates on the scenario that only a limited amount of data is available. The proposed method MAKO is mounted on the top of supervised LML model, without introducing additional knowledge based overhead, for better leveraging the unlabeled data. Labeling new data can be realized by using the data programming method which is supervised by the labeled data. The target of this paper is to design a SSL LML framework that minimizes the performance between using partially labeled data, and the upper-bound performance using fully labeled data. Several experiments on standard image classification data sets including MNIST, CIFAR-10 and CIFAR-100 are used to evaluate the the effectiveness of MAKO. ",0.15789473684210525,0.17105263157894737,0.27631578947368424,0.27450980392156865,0.3333333333333333,0.2361111111111111,0.23529411764705882,0.18055555555555555,0.1590909090909091,0.19444444444444445,0.12878787878787878,0.12878787878787878,0.1889763779527559,0.17567567567567569,0.2019230769230769,0.22764227642276424,0.18579234972677597,0.16666666666666666
409,SP:ff7b9e6ff5303f8a4f0321d06d9d9573e4853c5f,"This paper presents a model for detecting irregularities in images, such as would occur in manufacturing acceptance testing.  As a form of anomaly detection, the claims made are for a principled modeling approach and an adaptive algorithm quick to learn from just a few samples.   Based on ""energy based models"" for modeling data densities, the paper claims improvements to avoid the need for retraining for new tasks, such as, instead of synthesizing negative samples rom noise, using a more targeted method of “learning from inpainting” operation to learn anomalies.   ","The paper proposes a framework for anomaly detection and localization that allows fast adaptation to new tasks.  Specifically, the authors propose an energy-based model (EBM) with an adaptive sparse coding layer directly trained with normal features of a target task.  A meta-learning process is followed to extract common knowledge across tasks enabling few shots adaptation.  Shrinkage functions, sparse coding with large receptive fields, and learning by inpainting are introduced to improve and accelerate the EBM training. ","This paper proposes an image classification system, where a set of normal patterns are stored as a ""dictionary,"" and the degree of deviation is used as the anomaly score.    The overall architecture seems to feature a conventional deep encoder and a pattern matching module that compares the encoded latent vector against the pattern dictionary. The pattern matching is done by solving the lasso regression problem.   The authors propose a certain online learning approach, following existing few-shot learning methods, and also a synthetic sample generation approach using random perturbation combined with a gradient method. ",The paper presents an anomaly detection algorithm that uses an EBM (Energy Based Model) to distinguish between 'normal' and 'anomaly'. This model generates pseudo-anomaly instances on-the-fly for each 'normal' instance and then learns to assign low energy to normal instances and high energy to pseudo-anomaly instances. The model is further designed to be able to adapt quickly (few 'normal' labeled examples) to new tasks.,0.15730337078651685,0.1797752808988764,0.14606741573033707,0.1794871794871795,0.1794871794871795,0.11702127659574468,0.1794871794871795,0.1702127659574468,0.19117647058823528,0.14893617021276595,0.20588235294117646,0.16176470588235295,0.16766467065868265,0.17486338797814208,0.16560509554140124,0.1627906976744186,0.19178082191780824,0.1358024691358025
