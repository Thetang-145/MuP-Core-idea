{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lib import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "from datasets import load_metric"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dts):\n",
    "    path = f'MuP_dataset/{dts}_complete.jsonl'\n",
    "    try:\n",
    "        with open(path, 'r') as json_file:\n",
    "            json_list = list(json_file)\n",
    "        col_name = [\"paper_id\",\"summary\"]\n",
    "    except:\n",
    "        print(f\"Warning: Did not load dataset from {path}\")\n",
    "        return\n",
    "    summary_df = pd.DataFrame(columns=col_name)\n",
    "    for json_str in json_list[:]:\n",
    "        result = json.loads(json_str)\n",
    "        df = pd.DataFrame([[result[\"paper_id\"], result[\"summary\"]]], columns=col_name)\n",
    "        summary_df = pd.concat([summary_df,df])\n",
    "    return summary_df\n",
    "\n",
    "def split_sum_num(df):\n",
    "    num_paper = df.groupby(['paper_id']).count()\n",
    "    num_paper['num_paper'] = 1\n",
    "    num_paper.groupby(['summary']).count()\n",
    "    num_paper = df.groupby(['paper_id']).count()\n",
    "    num_paper['num_paper'] = 1\n",
    "    num_paper.groupby(['summary']).count()\n",
    "    num_paper.drop('num_paper', inplace=True,axis=1)\n",
    "    num_paper.sort_values([\"summary\"])\n",
    "\n",
    "    df_list = []\n",
    "    for i in range(0, max(num_paper['summary'])):\n",
    "        paper_id = (list((num_paper[num_paper[\"summary\"]==(i+1)]).index))\n",
    "        df_i = df[df.paper_id.isin(paper_id)].sort_values(\"paper_id\")\n",
    "        df_list.append(df_i.groupby('paper_id').apply(lambda df_: df_[['summary']].values.flatten()).apply(pd.Series).reset_index())\n",
    "        \n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dts = \"training\"\n",
    "summary_df = load_data(dts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = split_sum_num(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list[3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_scores(df, subscore_col):\n",
    "    n = len(df.columns)-1\n",
    "    pairs = [f'{i}-{j}' for i in range(n) for j in range(i+1, n)]\n",
    "    col = pd.MultiIndex.from_product([subscore_col, pairs])\n",
    "    scores = pd.DataFrame(columns=col)\n",
    "    scores.insert(0, \"paper_id\", df[\"paper_id\"])\n",
    "    # scores.set_index(\"paper_id\", inplace=True)\n",
    "    return scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUGE Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rougescore = load_metric(\"rouge\")\n",
    "\n",
    "def rouge_cal(df):\n",
    "    n = len(df.columns)-1\n",
    "    print(f\"Calculating ROUGE on {n} summaries\")\n",
    "\n",
    "    rouge_list = ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\n",
    "    score_list = ['precision', 'recall', 'fmeasure']\n",
    "\n",
    "    df_score_dict = {}\n",
    "    mux = pd.MultiIndex.from_product([[\"summary\"],(list(df.columns))[1:]])\n",
    "    df_score = pd.DataFrame(columns=mux)\n",
    "    df_score.insert(0, \"paper_id\", df[\"paper_id\"])\n",
    "    for col in df:\n",
    "        if col != 'paper_id':\n",
    "            df_score[('summary', col)] = df[col]\n",
    "    df_score = df_score.merge(n_scores(df, score_list), left_on='paper_id', right_on='paper_id')\n",
    "    for r in rouge_list:\n",
    "        df_score_dict[r] = df_score.copy(deep=True)\n",
    "\n",
    "    df_len = len(df)\n",
    "    for idx, row in df.iterrows():\n",
    "        sys.stdout.write(f\"\\r{idx+1}/{df_len}\")\n",
    "        sys.stdout.flush()\n",
    "        for i in range(n):\n",
    "            for j in range(i+1,n):\n",
    "                pair = f'{i}-{j}'\n",
    "                score = rougescore.compute(predictions=[row[i]], references=[row[j]], use_stemmer=False)\n",
    "                for r in rouge_list:\n",
    "                    df_score_dict[r].loc[idx, ('precision', pair)] = ((score[r]).low).precision\n",
    "                    df_score_dict[r].loc[idx, ('recall', pair)] = ((score[r]).low).recall\n",
    "                    df_score_dict[r].loc[idx, ('fmeasure', pair)] = ((score[r]).low).fmeasure\n",
    "\n",
    "    return df_score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, df in enumerate(df_list):\n",
    "    if n+1 > 1:\n",
    "        dict_result = rouge_cal(df)\n",
    "        for key, val in dict_result.items():\n",
    "            # val.to_csv(f\"visualization_data/rouge-between-sum/{key}/{dts}_{key}_{n+1}sum.csv\")\n",
    "            pass\n",
    "dict_result['rouge1']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertscore = load(\"bertscore\")\n",
    "def bertscore_cal(df):\n",
    "    n = len(df.columns)-1\n",
    "    print(f\"Calculating BERTScore on {n} summaries\")\n",
    "    \n",
    "    score_list = ['precision', 'recall', 'f1']\n",
    "\n",
    "    mux = pd.MultiIndex.from_product([[\"summary\"],(list(df.columns))[1:]])\n",
    "    df_score = pd.DataFrame(columns=mux)\n",
    "    df_score.insert(0, \"paper_id\", df[\"paper_id\"])\n",
    "    for col in df:\n",
    "        if col != 'paper_id':\n",
    "            df_score[('summary', col)] = df[col]\n",
    "    df_score = df_score.merge(n_scores(df, score_list), left_on='paper_id', right_on='paper_id')\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i+1,n-1):\n",
    "            pair = f'{i}-{j}'\n",
    "            summary1 = list(df_score.loc[:, ('summary', i)])\n",
    "            summary2 = list(df_score.loc[:, ('summary', j)])\n",
    "            print(pair)\n",
    "            result = bertscore.compute(predictions=summary1, references=summary2, lang=\"en\", rescale_with_baseline=True)\n",
    "            for score in score_list:\n",
    "                df_score.loc[:, (score, pair)] = result[score]\n",
    "\n",
    "    return df_score\n",
    "    \n",
    "bertscore_cal(df_list[2][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, df in enumerate(df_list):\n",
    "    if n+1 > 1:\n",
    "        result = bertscore_cal(df)\n",
    "        val.to_csv(f\"visualization_data/bertscore-between-sum/{dts}_bertscore_{n+1}sum.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c41a15a33a59f0eeae9566b7a824606ae2e90e5c5eb8240ec6145ee367642a6c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
